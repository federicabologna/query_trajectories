{
    "query": "What are the current technical methods (such as parameter-space merging, mixture-of-experts architectures, dynamic gating) for merging or integrating multiple specialized language models into a single unified system, and how do these approaches address practical challenges like task interference and knowledge conflicts, as discussed in recent literature?",
    "user_id": "lib_user",
    "task_id": "002550d6-a4f9-4679-9ac4-5e21a0c85089",
    "timestamp": "2025-06-23T22:44:59.423920",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 42,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.640215,
    "decomposed_query": {
        "rewritten_query": "Technical methods for merging or integrating multiple specialized language models into a single unified system, and how these approaches address practical challenges like task interference and knowledge conflicts.",
        "keyword_query": "parameter-space merging mixture-of-experts architectures dynamic gating language models integration task interference knowledge conflicts",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010233,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "8458211",
                    "name": "Giannis Karamanolakis"
                },
                {
                    "authorId": "2302332301",
                    "name": "Victor Soto"
                },
                {
                    "authorId": "1681193",
                    "name": "Anna Rumshisky"
                },
                {
                    "authorId": "2302332615",
                    "name": "Mayank Kulkarni"
                },
                {
                    "authorId": "2257407889",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2302633316",
                    "name": "Jianhua Lu"
                }
            ],
            "abstract": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
            "corpus_id": 276095183,
            "sentences": [
                {
                    "corpus_id": "276095183",
                    "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                    "text": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
                    "score": 0.609232120836815,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.859375
                },
                {
                    "corpus_id": "276095183",
                    "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                    "text": "Large language models (LLMs) pretrained on a wide-variety of corpora have achieved notable success in multiple tasks (Touvron et al., 2023;Ope-nAI, 2023;Brown et al., 2020;Liu et al., 2024a). With significant progress, there is increasing interest in how to continuously improve the performance of LLMs in new domains, including math (Yu et al., 2023), code (Roziere et al., 2023), Wikipedia knowledge (Shao et al., 2024), or legal domains (Cui et al., 2023). One straightforward approach is through continual pretraining (CPT) on domain-specific data, which, however, is challenging for multiple target domains, as it can cause catastrophic forgetting on previously learned tasks (Luo et al., 2023). \n\nAn alternative approach is Mixture-of-Experts (MoE) merging, where dense experts are first CPTed in parallel for each domain and then merged into a unified MoE model, usually by keeping feedforward neural network (FFN) layers separate and averaging non-FFN layers (Sukhbaatar et al., 2024;Kang et al., 2024). Compared with dense models of similar size, the MoE model uses just a subset of parameters during inference by learning to route tokens to the top few experts, thus reducing inference costs. Unlike training an MoE model from scratch, MoE merging offers modularity, as individual experts are domain-specialized, and is substantially less expensive, as CPT-ing experts in parallel requires less compute than training the entire MoE on large datasets from the beginning (Sukhbaatar et al., 2024). \n\nIn this paper, we investigate how to effectively merge different domain expert models into a unified MoE model. The current state-of-the-art (SoTA) MoE merging approach, such as Branch-Train-Mix (BTX) (Sukhbaatar et al., 2024) assumes experts are branched from the same ancestor model and merges experts by simply unweighted averaging the non-FFN layers.",
                    "score": 0.47749294501851636,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 700
                        },
                        {
                            "start": 703,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1505
                        },
                        {
                            "start": 1508,
                            "end": 1619
                        },
                        {
                            "start": 1620,
                            "end": 1862
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 153,
                            "end": 172,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.859375,
            "relevance_judgment_input_expanded": "# Title: MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, Jianhua Lu\n## Abstract\nThe recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.\n## Introduction\nLarge language models (LLMs) pretrained on a wide-variety of corpora have achieved notable success in multiple tasks (Touvron et al., 2023;Ope-nAI, 2023;Brown et al., 2020;Liu et al., 2024a). With significant progress, there is increasing interest in how to continuously improve the performance of LLMs in new domains, including math (Yu et al., 2023), code (Roziere et al., 2023), Wikipedia knowledge (Shao et al., 2024), or legal domains (Cui et al., 2023). One straightforward approach is through continual pretraining (CPT) on domain-specific data, which, however, is challenging for multiple target domains, as it can cause catastrophic forgetting on previously learned tasks (Luo et al., 2023). \n\nAn alternative approach is Mixture-of-Experts (MoE) merging, where dense experts are first CPTed in parallel for each domain and then merged into a unified MoE model, usually by keeping feedforward neural network (FFN) layers separate and averaging non-FFN layers (Sukhbaatar et al., 2024;Kang et al., 2024). Compared with dense models of similar size, the MoE model uses just a subset of parameters during inference by learning to route tokens to the top few experts, thus reducing inference costs. Unlike training an MoE model from scratch, MoE merging offers modularity, as individual experts are domain-specialized, and is substantially less expensive, as CPT-ing experts in parallel requires less compute than training the entire MoE on large datasets from the beginning (Sukhbaatar et al., 2024). \n\nIn this paper, we investigate how to effectively merge different domain expert models into a unified MoE model. The current state-of-the-art (SoTA) MoE merging approach, such as Branch-Train-Mix (BTX) (Sukhbaatar et al., 2024) assumes experts are branched from the same ancestor model and merges experts by simply unweighted averaging the non-FFN layers.",
            "reference_string": "[276095183 | Zhou et al. | 2025 | Citations: 3]"
        },
        {
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321405806",
                    "name": "Wei Ruan"
                },
                {
                    "authorId": "2263682353",
                    "name": "Tianze Yang"
                },
                {
                    "authorId": "2325891087",
                    "name": "Yifan Zhou"
                },
                {
                    "authorId": "2349736445",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2331910055",
                    "name": "Jin Lu"
                }
            ],
            "abstract": "Model merging has achieved significant success, with numerous innovative methods proposed to enhance capabilities by combining multiple models. However, challenges persist due to the lack of a unified framework for classification and systematic comparative analysis, leading to inconsistencies in terminologies and categorizations. Meanwhile, as an increasing number of fine-tuned models are publicly available, their original training data often remain inaccessible due to privacy concerns or intellectual property restrictions. This makes traditional multi-task learning based on shared training data impractical. In scenarios where direct access to training data is infeasible, merging model parameters to create a unified model with broad generalization across multiple domains becomes crucial, further underscoring the importance of model merging techniques. Despite the rapid progress in this field, a comprehensive taxonomy and survey summarizing recent advances and predicting future directions are still lacking. This paper addresses these gaps by establishing a new taxonomy of model merging methods, systematically comparing different approaches, and providing an overview of key developments. By offering a structured perspective on this evolving area, we aim to help newcomers quickly grasp the field's landscape and inspire further innovations.",
            "corpus_id": 276937513,
            "sentences": [
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "Model merging, while gaining traction and demonstrating significant potential, still encounters several key challenges that must be addressed to achieve broader adoption and improved efficiency. Firstly, as the number of tasks increases, merged models often underperform compared to independent expert models. Maintaining consistent performance across diverse tasks without extensive task-specific tuning remains a significant hurdle. Moreover, merging models trained on different tasks or domains can result in conflicts where knowledge from one model interferes with another. This issue is particularly pronounced in scenarios like multi-task learning and continual learning, where tasks often have distinct requirements. \n\nAdditionally, the lack of comprehensive theoretical frameworks for model merging limits the ability to predict and guarantee performance. Many current methods rely heavily on heuristic or empirical strategies, leaving room for improvement through deeper theoretical exploration. \n\nLastly, identifying optimal merging strategies, such as determining appropriate weight coefficients for models or parameters, is difficult. Fine-grained optimization approaches often come with high computational and resource demands, making them less feasible in practice. \n\nAddressing these challenges will require innovative methodologies, advanced computational tools, and a stronger theoretical foundation to support the efficient and reliable implementation of model merging across diverse applications. \n\nCombining model compression with model merging represents one of the promising future directions for advancing model merging techniques [Wang et al., 2024;Lu et al., 2024b]. Model compression, which involves reducing the size and complexity of a model while preserving its performance, helps mitigate interference between models during the merging process. By streamlining individual models before merging, compression can enhance compatibility and lead to more effective and seamless integration of model parameters, ultimately improving the overall merging outcomes. \n\nApart from leveraging model merging techniques, task merging or classification can also be explored as potential approaches. For instance, the Disperse-Then-Merge (DTM) method provides an innovative framework for addressing alignment tax in supervised fine-tuning of large language models [Fu et al., 2024]. This method tackles the issue of data biases by dividing the instruction-following dataset into several clusters, training separate sub-models on these clusters, and subsequently merging the sub-models into a single model. By doing so, DTM effectively distributes and mitigates dataset-specific biases, maintaining the model's instruction-following capacity while reducing the detrimental effects of such biases on knowledge and reasoning benchmarks.",
                    "score": 0.5962320521704888,
                    "section_title": "Challenges and Future Perspectives of Model Merging",
                    "char_start_offset": 29181,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 434
                        },
                        {
                            "start": 435,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 723
                        },
                        {
                            "start": 726,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1004
                        },
                        {
                            "start": 1007,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1279
                        },
                        {
                            "start": 1282,
                            "end": 1515
                        },
                        {
                            "start": 1518,
                            "end": 1691
                        },
                        {
                            "start": 1692,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2086
                        },
                        {
                            "start": 2089,
                            "end": 2213
                        },
                        {
                            "start": 2214,
                            "end": 2396
                        },
                        {
                            "start": 2397,
                            "end": 2619
                        },
                        {
                            "start": 2620,
                            "end": 2847
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1654,
                            "end": 1673,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1673,
                            "end": 1690,
                            "matchedPaperCorpusId": "270702345"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83935546875
                },
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "The increasing prevalence of open-source models trained on diverse tasks provides unprecedented opportunities to utilize pre-trained weights for various applications. However, access to the original training data is often restricted due to privacy concerns, proprietary limitations, or other constraints, posing significant challenges for tasks requiring cross-domain capabilities [Jin et al., 2022]. Model merging techniques address this issue by enabling the combination of model weights without relying on original data, thereby equipping models with the ability to handle multiple tasks effectively. Currently, model merging has emerged as a promising solution, enabling the combination of multiple models with similar architectures to harness complementary strengths. This approach not only enhances task-specific performance but also fosters greater adaptability across tasks [Yang et al., 2024a;Tam et al., 2024]. \n\nModel merging provides several key advantages [Yang et al., 2024a;Yu et al., 2024b;Zhao et al., 2024]. Firstly, it allows for the aggregation of knowledge across multiple models without requiring extensive retraining, thereby offering a more resource-efficient alternative to traditional fine-tuning and transfer learning. Additionally, model merging can mitigate issues like catastrophic forgetting and offers a pathway to create models that encapsulate the strengths of multiple training regimes. For instance, weight average [Wortsman et al., 2022;Choshen et al., 2022] and task arithmetic merging [Ilharco et al., 2022] are widely adopted methods for retaining model capabilities while maintaining robustness across varied domains. \n\nIn recent years, model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements. These advancements underscore the potential of model merging as a versatile solution, capable of adapting to a range of tasks and minimizing resource demands.",
                    "score": 0.44994258670938425,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 166
                        },
                        {
                            "start": 167,
                            "end": 400
                        },
                        {
                            "start": 401,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 772
                        },
                        {
                            "start": 773,
                            "end": 920
                        },
                        {
                            "start": 923,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1658
                        },
                        {
                            "start": 1661,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 2059
                        },
                        {
                            "start": 2060,
                            "end": 2247
                        },
                        {
                            "start": 2248,
                            "end": 2406
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 969,
                            "end": 989,
                            "matchedPaperCorpusId": "267499590"
                        },
                        {
                            "start": 1006,
                            "end": 1024,
                            "matchedPaperCorpusId": "259937385"
                        },
                        {
                            "start": 1524,
                            "end": 1546,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7919921875
                },
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "This type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.",
                    "score": 0.4095363344231575,
                    "section_title": "Decomposing weight type",
                    "char_start_offset": 19865,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 282
                        },
                        {
                            "start": 285,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1276
                        },
                        {
                            "start": 1279,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1992
                        },
                        {
                            "start": 1995,
                            "end": 2149
                        },
                        {
                            "start": 2150,
                            "end": 2337
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 753,
                            "end": 772,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1255,
                            "end": 1275,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1762,
                            "end": 1780,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 2126,
                            "end": 2148,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.720703125
                },
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "In the pretraining-finetuning paradigm, different model branches that share a common pretrained initialization exhibit specific linear properties. These properties, such as Cross-Task Linearity (CTL), allow for the linear interpolation of weights and corresponding feature spaces across tasks. This phenomenon enables effective model merging by ensuring consistency between parameter spaces and feature representations, even when the models are fine-tuned on different tasks. It provides a foundation for combining diverse taskspecific models into a unified framework [Zhou et al., 2024b]. \n\nDirect Merging type is a relatively simple model merging method that avoids the complexities of resolving conflicts and interferences during the merging of model weights. This approach does not require retraining, significantly reducing computational costs. Moreover, many advanced model merging methods are built upon these straightforward foundational approaches, highlighting their importance in the field. Despite their simplicity, these foundational methods serve as the cornerstone for further innovations and advancements in model merging techniques. Several model merging methods fall into this category. The following sections introduce these approaches. \n\nModel Soup ( Weight Averaging ): It is a method for merging fine-tuned models by averaging their weights, improving accuracy and robustness without additional training or inference costs. It works well when models share the same pretrained initialization but differ in hyperparameters [Wortsman et al., 2022]. Variants like uniform averaging and greedy soup selectively optimize performance. Model soup is efficient, flexible, and outperforms individual models, making it a robust alternative to ensembles for tasks like image and text classification. However, this method does not account for weight conflicts during model merging, which often results in performance that falls short of more sophisticated model merging approaches. \n\nTask arithmetic: It is a method for editing pre-trained models by leveraging task vectors, which are computed as the difference between a model's fine-tuned weights and its pre-trained weights (\u03c4 t = \u03b8 t \u2212 \u03b8 0 ). These vectors encapsulate the changes needed for a model to perform a specific task and can be applied to modify model weights through simple operations like addition, subtraction, and analogy-based reasoning [Ilharco et al., 2022].",
                    "score": 0.4147317064157072,
                    "section_title": "Direct Merging",
                    "char_start_offset": 9842,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 146
                        },
                        {
                            "start": 147,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 589
                        },
                        {
                            "start": 592,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1255
                        },
                        {
                            "start": 1258,
                            "end": 1445
                        },
                        {
                            "start": 1446,
                            "end": 1567
                        },
                        {
                            "start": 1568,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1990
                        },
                        {
                            "start": 1993,
                            "end": 2205
                        },
                        {
                            "start": 2206,
                            "end": 2438
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 568,
                            "end": 588,
                            "matchedPaperCorpusId": "267499590"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65185546875
                }
            ],
            "relevance_judgement": 0.83935546875,
            "relevance_judgment_input_expanded": "# Title: From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches\n# Venue: arXiv.org\n# Authors: Wei Ruan, Tianze Yang, Yifan Zhou, Tianming Liu, Jin Lu\n## Abstract\nModel merging has achieved significant success, with numerous innovative methods proposed to enhance capabilities by combining multiple models. However, challenges persist due to the lack of a unified framework for classification and systematic comparative analysis, leading to inconsistencies in terminologies and categorizations. Meanwhile, as an increasing number of fine-tuned models are publicly available, their original training data often remain inaccessible due to privacy concerns or intellectual property restrictions. This makes traditional multi-task learning based on shared training data impractical. In scenarios where direct access to training data is infeasible, merging model parameters to create a unified model with broad generalization across multiple domains becomes crucial, further underscoring the importance of model merging techniques. Despite the rapid progress in this field, a comprehensive taxonomy and survey summarizing recent advances and predicting future directions are still lacking. This paper addresses these gaps by establishing a new taxonomy of model merging methods, systematically comparing different approaches, and providing an overview of key developments. By offering a structured perspective on this evolving area, we aim to help newcomers quickly grasp the field's landscape and inspire further innovations.\n## Introduction\nThe increasing prevalence of open-source models trained on diverse tasks provides unprecedented opportunities to utilize pre-trained weights for various applications. However, access to the original training data is often restricted due to privacy concerns, proprietary limitations, or other constraints, posing significant challenges for tasks requiring cross-domain capabilities [Jin et al., 2022]. Model merging techniques address this issue by enabling the combination of model weights without relying on original data, thereby equipping models with the ability to handle multiple tasks effectively. Currently, model merging has emerged as a promising solution, enabling the combination of multiple models with similar architectures to harness complementary strengths. This approach not only enhances task-specific performance but also fosters greater adaptability across tasks [Yang et al., 2024a;Tam et al., 2024]. \n\nModel merging provides several key advantages [Yang et al., 2024a;Yu et al., 2024b;Zhao et al., 2024]. Firstly, it allows for the aggregation of knowledge across multiple models without requiring extensive retraining, thereby offering a more resource-efficient alternative to traditional fine-tuning and transfer learning. Additionally, model merging can mitigate issues like catastrophic forgetting and offers a pathway to create models that encapsulate the strengths of multiple training regimes. For instance, weight average [Wortsman et al., 2022;Choshen et al., 2022] and task arithmetic merging [Ilharco et al., 2022] are widely adopted methods for retaining model capabilities while maintaining robustness across varied domains. \n\nIn recent years, model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements. These advancements underscore the potential of model merging as a versatile solution, capable of adapting to a range of tasks and minimizing resource demands.\n\n## Direct Merging\nIn the pretraining-finetuning paradigm, different model branches that share a common pretrained initialization exhibit specific linear properties. These properties, such as Cross-Task Linearity (CTL), allow for the linear interpolation of weights and corresponding feature spaces across tasks. This phenomenon enables effective model merging by ensuring consistency between parameter spaces and feature representations, even when the models are fine-tuned on different tasks. It provides a foundation for combining diverse taskspecific models into a unified framework [Zhou et al., 2024b]. \n\nDirect Merging type is a relatively simple model merging method that avoids the complexities of resolving conflicts and interferences during the merging of model weights. This approach does not require retraining, significantly reducing computational costs. Moreover, many advanced model merging methods are built upon these straightforward foundational approaches, highlighting their importance in the field. Despite their simplicity, these foundational methods serve as the cornerstone for further innovations and advancements in model merging techniques. Several model merging methods fall into this category. The following sections introduce these approaches. \n\nModel Soup ( Weight Averaging ): It is a method for merging fine-tuned models by averaging their weights, improving accuracy and robustness without additional training or inference costs. It works well when models share the same pretrained initialization but differ in hyperparameters [Wortsman et al., 2022]. Variants like uniform averaging and greedy soup selectively optimize performance. Model soup is efficient, flexible, and outperforms individual models, making it a robust alternative to ensembles for tasks like image and text classification. However, this method does not account for weight conflicts during model merging, which often results in performance that falls short of more sophisticated model merging approaches. \n\nTask arithmetic: It is a method for editing pre-trained models by leveraging task vectors, which are computed as the difference between a model's fine-tuned weights and its pre-trained weights (\u03c4 t = \u03b8 t \u2212 \u03b8 0 ). These vectors encapsulate the changes needed for a model to perform a specific task and can be applied to modify model weights through simple operations like addition, subtraction, and analogy-based reasoning [Ilharco et al., 2022].\n\n## Decomposing weight type\nThis type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.\n\n## Challenges and Future Perspectives of Model Merging\nModel merging, while gaining traction and demonstrating significant potential, still encounters several key challenges that must be addressed to achieve broader adoption and improved efficiency. Firstly, as the number of tasks increases, merged models often underperform compared to independent expert models. Maintaining consistent performance across diverse tasks without extensive task-specific tuning remains a significant hurdle. Moreover, merging models trained on different tasks or domains can result in conflicts where knowledge from one model interferes with another. This issue is particularly pronounced in scenarios like multi-task learning and continual learning, where tasks often have distinct requirements. \n\nAdditionally, the lack of comprehensive theoretical frameworks for model merging limits the ability to predict and guarantee performance. Many current methods rely heavily on heuristic or empirical strategies, leaving room for improvement through deeper theoretical exploration. \n\nLastly, identifying optimal merging strategies, such as determining appropriate weight coefficients for models or parameters, is difficult. Fine-grained optimization approaches often come with high computational and resource demands, making them less feasible in practice. \n\nAddressing these challenges will require innovative methodologies, advanced computational tools, and a stronger theoretical foundation to support the efficient and reliable implementation of model merging across diverse applications. \n\nCombining model compression with model merging represents one of the promising future directions for advancing model merging techniques [Wang et al., 2024;Lu et al., 2024b]. Model compression, which involves reducing the size and complexity of a model while preserving its performance, helps mitigate interference between models during the merging process. By streamlining individual models before merging, compression can enhance compatibility and lead to more effective and seamless integration of model parameters, ultimately improving the overall merging outcomes. \n\nApart from leveraging model merging techniques, task merging or classification can also be explored as potential approaches. For instance, the Disperse-Then-Merge (DTM) method provides an innovative framework for addressing alignment tax in supervised fine-tuning of large language models [Fu et al., 2024]. This method tackles the issue of data biases by dividing the instruction-following dataset into several clusters, training separate sub-models on these clusters, and subsequently merging the sub-models into a single model. By doing so, DTM effectively distributes and mitigates dataset-specific biases, maintaining the model's instruction-following capacity while reducing the detrimental effects of such biases on knowledge and reasoning benchmarks.",
            "reference_string": "[276937513 | Ruan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327007623",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2237427680",
                    "name": "Guibing Guo"
                },
                {
                    "authorId": "2279402395",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "2282189838",
                    "name": "Lefei Zhang"
                },
                {
                    "authorId": "2316150631",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "2212029373",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Multi-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.",
            "corpus_id": 273662099,
            "sentences": [
                {
                    "corpus_id": "273662099",
                    "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
                    "text": "Multi-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.",
                    "score": 0.41602397374366834,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83935546875
                },
                {
                    "corpus_id": "273662099",
                    "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
                    "text": "The goal of multi-task learning (MTL) is to utilize a single model to perform multiple related tasks concurrently, thereby facilitating information sharing and knowledge transfer among the tasks. In recent years, the rapid development of deep learning has prompted a learning paradigm shift, where the mainstream paradigm now focuses on fine-tuning downstream tasks using powerful pre-trained models, rather than training an expert model from scratch [2,3,4,5,6,7]. This shift typically results in significant reductions in both data requirements and computational resources. Additionally, the open-source ethos of the deep learning community has encouraged developers to release a vast array of expert models fine-tuned on various downstream tasks. To date, over one million diverse models have been made available on Hugging Face 1 . These above diverse factors have given rise to a new MTL paradigm, enabling the direct merging of multiple independently trained expert models to create a multi-task model without requiring access to their original training data [8,9,10,11]. \n\nHowever, due to potential task conflicts and interferences among multiple tasks, simply merging parameters from independently fine-tuned models may lead to a sharp decline in MTL performance [12,13]. Recently, an increasing number of studies have aimed to address the MTL performance degradation resulting from model merging [11]. A notable example is task arithmetic [14], which introduces the concept of 'task vectors' to extract task-specific knowledge from the fine-tuned models. By linearly weighting the task-private knowledge of multiple tasks into the pre-trained model, task arithmetic enhances the model's ability to process multiple downstream tasks. Inspired by task arithmetic, recent advancements have proposed techniques to alleviate sign conflicts among task vectors [15,16], merge them in a fine-grained manner [17,18,19], or combine task vectors within subspaces [20,21,22,23]. While these methods have considerably improved task arithmetic's performance, a noticeable performance gap still exists between the merged MTL model and the independently fine-tuned expert model (or the joint-trained MTL model). \n\nIn this paper, we claim that the observed performance gap primarily arises from the fact that existing model merging methods focus on finding a static multi-task optimal solution within the original parameter space.",
                    "score": 0.392864247673286,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 575
                        },
                        {
                            "start": 576,
                            "end": 749
                        },
                        {
                            "start": 750,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 1077
                        },
                        {
                            "start": 1080,
                            "end": 1279
                        },
                        {
                            "start": 1280,
                            "end": 1410
                        },
                        {
                            "start": 1411,
                            "end": 1563
                        },
                        {
                            "start": 1564,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2204
                        },
                        {
                            "start": 2207,
                            "end": 2422
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 456,
                            "end": 458,
                            "matchedPaperCorpusId": "257038341"
                        },
                        {
                            "start": 462,
                            "end": 464,
                            "matchedPaperCorpusId": "257505035"
                        },
                        {
                            "start": 1068,
                            "end": 1070,
                            "matchedPaperCorpusId": "259022411"
                        },
                        {
                            "start": 1968,
                            "end": 1971,
                            "matchedPaperCorpusId": "269757600"
                        },
                        {
                            "start": 1971,
                            "end": 1974,
                            "matchedPaperCorpusId": "270067773"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6484375
                }
            ],
            "relevance_judgement": 0.83935546875,
            "relevance_judgment_input_expanded": "# Title: Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging\n# Venue: arXiv.org\n# Authors: Li Shen, A. Tang, Enneng Yang, Guibing Guo, Yong Luo, Lefei Zhang, Xiaochun Cao, Bo Du, D. Tao\n## Abstract\nMulti-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.\n## I. INTRODUCTION\nThe goal of multi-task learning (MTL) is to utilize a single model to perform multiple related tasks concurrently, thereby facilitating information sharing and knowledge transfer among the tasks. In recent years, the rapid development of deep learning has prompted a learning paradigm shift, where the mainstream paradigm now focuses on fine-tuning downstream tasks using powerful pre-trained models, rather than training an expert model from scratch [2,3,4,5,6,7]. This shift typically results in significant reductions in both data requirements and computational resources. Additionally, the open-source ethos of the deep learning community has encouraged developers to release a vast array of expert models fine-tuned on various downstream tasks. To date, over one million diverse models have been made available on Hugging Face 1 . These above diverse factors have given rise to a new MTL paradigm, enabling the direct merging of multiple independently trained expert models to create a multi-task model without requiring access to their original training data [8,9,10,11]. \n\nHowever, due to potential task conflicts and interferences among multiple tasks, simply merging parameters from independently fine-tuned models may lead to a sharp decline in MTL performance [12,13]. Recently, an increasing number of studies have aimed to address the MTL performance degradation resulting from model merging [11]. A notable example is task arithmetic [14], which introduces the concept of 'task vectors' to extract task-specific knowledge from the fine-tuned models. By linearly weighting the task-private knowledge of multiple tasks into the pre-trained model, task arithmetic enhances the model's ability to process multiple downstream tasks. Inspired by task arithmetic, recent advancements have proposed techniques to alleviate sign conflicts among task vectors [15,16], merge them in a fine-grained manner [17,18,19], or combine task vectors within subspaces [20,21,22,23]. While these methods have considerably improved task arithmetic's performance, a noticeable performance gap still exists between the merged MTL model and the independently fine-tuned expert model (or the joint-trained MTL model). \n\nIn this paper, we claim that the observed performance gap primarily arises from the fact that existing model merging methods focus on finding a static multi-task optimal solution within the original parameter space.",
            "reference_string": "[273662099 | Shen et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 181,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1930733",
                    "name": "D. Hagos"
                },
                {
                    "authorId": "2312327093",
                    "name": "Rick Battle"
                },
                {
                    "authorId": "2260694752",
                    "name": "Danda B. Rawat"
                }
            ],
            "abstract": "The emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.",
            "corpus_id": 271329267,
            "sentences": [
                {
                    "corpus_id": "271329267",
                    "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
                    "text": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
                    "score": 0.5742730316668667,
                    "section_title": "E. Model Merging",
                    "char_start_offset": 24254,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1326
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "244345933"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83642578125
                }
            ],
            "relevance_judgement": 0.83642578125,
            "relevance_judgment_input_expanded": "# Title: Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: D. Hagos, Rick Battle, Danda B. Rawat\n## Abstract\nThe emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.\n## E. Model Merging\nModel merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
            "reference_string": "[271329267 | Hagos et al. | 2024 | Citations: 26]"
        },
        {
            "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
            "venue": "",
            "year": 2025,
            "reference_count": 94,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150449851",
                    "name": "Zihuan Qiu"
                },
                {
                    "authorId": "2321686264",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2190031555",
                    "name": "Chiyuan He"
                },
                {
                    "authorId": "1706784",
                    "name": "Fanman Meng"
                },
                {
                    "authorId": "47775696",
                    "name": "Linfeng Xu"
                },
                {
                    "authorId": "144816629",
                    "name": "Qingbo Wu"
                },
                {
                    "authorId": "2300984960",
                    "name": "Hongliang Li"
                }
            ],
            "abstract": "Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\\% on average across diverse task orders.",
            "corpus_id": 278739786,
            "sentences": [],
            "relevance_judgement": 0.82666015625,
            "relevance_judgment_input_expanded": "# Title: MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging\n# Venue: \n# Authors: Zihuan Qiu, Yi Xu, Chiyuan He, Fanman Meng, Linfeng Xu, Qingbo Wu, Hongliang Li\n## Abstract\nContinual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\\% on average across diverse task orders.\n",
            "reference_string": "[278739786 | Qiu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313634509",
                    "name": "Mohammed Al-Maamari"
                },
                {
                    "authorId": "2074055134",
                    "name": "Mehdi Ben Amor"
                },
                {
                    "authorId": "2259357506",
                    "name": "Michael Granitzer"
                }
            ],
            "abstract": "This research combines Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient multilingual language models. Key objectives include evaluating adaptive versus fixed alpha methods in KD and comparing modular MoE architectures for handling multi-domain inputs and preventing catastrophic forgetting. KD compresses large language models (LLMs) into smaller, efficient models, while MoE enhances modularity with specialized tasks. Experiments showed similar performance for both KD methods, with marginal improvements from adaptive alpha. A combined loss approach provided more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved 99.95% precision, recall, and F1 score, with Logistic Regression being the most effective classifier. Evaluations of modular MoE architectures revealed that Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed similarly, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. Including a common expert in MoE-CE improved its performance. Studies on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches and the MoE approach mitigated this issue. The MoE architecture preserved knowledge across multiple languages effectively. The research contributes open-sourced resources including the dataset (https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation tool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the research codebase (https://github.com/ModMaamari/mixture-modular-experts).",
            "corpus_id": 271533631,
            "sentences": [
                {
                    "corpus_id": "271533631",
                    "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
                    "text": "\"Mixtral of Experts\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. The model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational resource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens to experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and effectiveness. \n\n\"Branch-Train-MiX\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized domains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific experts, and integrating them into a unified MoE model. This approach improves training efficiency and model performance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and computational efficiency. \n\n\"Branch-Train-Merge\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the efficiency of training large language models. BTM facilitates independent training of subparts of the model on different data subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging. BTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it a scalable and efficient training paradigm. \n\nOur research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike Mixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work emphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain adaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language models.",
                    "score": 0.40919233161605917,
                    "section_title": "Mixture of Experts",
                    "char_start_offset": 4092,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 115
                        },
                        {
                            "start": 116,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 471
                        },
                        {
                            "start": 474,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 939
                        },
                        {
                            "start": 942,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1218
                        },
                        {
                            "start": 1219,
                            "end": 1287
                        },
                        {
                            "start": 1288,
                            "end": 1445
                        },
                        {
                            "start": 1448,
                            "end": 1552
                        },
                        {
                            "start": 1553,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1906
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82275390625
                },
                {
                    "corpus_id": "271533631",
                    "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
                    "text": "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.",
                    "score": 0.43059950311922235,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 541
                        },
                        {
                            "start": 544,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2304
                        },
                        {
                            "start": 2307,
                            "end": 2613
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 132,
                            "end": 135,
                            "matchedPaperCorpusId": "11280500"
                        },
                        {
                            "start": 181,
                            "end": 184,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 637,
                            "end": 640,
                            "matchedPaperCorpusId": "211532645"
                        },
                        {
                            "start": 1384,
                            "end": 1387,
                            "matchedPaperCorpusId": "256808267"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6875
                }
            ],
            "relevance_judgement": 0.82275390625,
            "relevance_judgment_input_expanded": "# Title: Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models\n# Venue: arXiv.org\n# Authors: Mohammed Al-Maamari, Mehdi Ben Amor, Michael Granitzer\n## Abstract\nThis research combines Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient multilingual language models. Key objectives include evaluating adaptive versus fixed alpha methods in KD and comparing modular MoE architectures for handling multi-domain inputs and preventing catastrophic forgetting. KD compresses large language models (LLMs) into smaller, efficient models, while MoE enhances modularity with specialized tasks. Experiments showed similar performance for both KD methods, with marginal improvements from adaptive alpha. A combined loss approach provided more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved 99.95% precision, recall, and F1 score, with Logistic Regression being the most effective classifier. Evaluations of modular MoE architectures revealed that Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed similarly, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. Including a common expert in MoE-CE improved its performance. Studies on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches and the MoE approach mitigated this issue. The MoE architecture preserved knowledge across multiple languages effectively. The research contributes open-sourced resources including the dataset (https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation tool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the research codebase (https://github.com/ModMaamari/mixture-modular-experts).\n## Introduction\nLanguage models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.\n\n## Mixture of Experts\n\"Mixtral of Experts\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. The model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational resource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens to experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and effectiveness. \n\n\"Branch-Train-MiX\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized domains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific experts, and integrating them into a unified MoE model. This approach improves training efficiency and model performance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and computational efficiency. \n\n\"Branch-Train-Merge\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the efficiency of training large language models. BTM facilitates independent training of subparts of the model on different data subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging. BTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it a scalable and efficient training paradigm. \n\nOur research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike Mixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work emphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain adaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language models.",
            "reference_string": "[271533631 | Al-Maamari et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345924598",
                    "name": "Thibault Rousset"
                },
                {
                    "authorId": "2326795128",
                    "name": "Taisei Kakibuchi"
                },
                {
                    "authorId": "2346085934",
                    "name": "Yusuke Sasaki"
                },
                {
                    "authorId": "2345925100",
                    "name": "Yoshihide Nomura"
                }
            ],
            "abstract": "Advancements in Natural Language Processing have enabled specialized language models, but integrating domain-specific knowledge into general-purpose models in multilingual settings remains challenging, particularly for technical vocabulary. This paper investigates the integration of technical vocabulary in merged language models and explores the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.",
            "corpus_id": 276422131,
            "sentences": [
                {
                    "corpus_id": "276422131",
                    "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
                    "text": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
                    "score": 0.6154559750402884,
                    "section_title": "Model Merging for LLM Domain Adaptation",
                    "char_start_offset": 4872,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 867
                        },
                        {
                            "start": 870,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1821
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81884765625
                }
            ],
            "relevance_judgement": 0.81884765625,
            "relevance_judgment_input_expanded": "# Title: Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition\n# Venue: arXiv.org\n# Authors: Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura\n## Abstract\nAdvancements in Natural Language Processing have enabled specialized language models, but integrating domain-specific knowledge into general-purpose models in multilingual settings remains challenging, particularly for technical vocabulary. This paper investigates the integration of technical vocabulary in merged language models and explores the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.\n## Model Merging for LLM Domain Adaptation\nModel merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
            "reference_string": "[276422131 | Rousset et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 193,
            "citation_count": 27,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81758928",
                    "name": "Jinliang Lu"
                },
                {
                    "authorId": "2310334594",
                    "name": "Ziliang Pang"
                },
                {
                    "authorId": "2310342808",
                    "name": "Min Xiao"
                },
                {
                    "authorId": "2290018211",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2258711381",
                    "name": "Rui Xia"
                },
                {
                    "authorId": "2290006077",
                    "name": "Jiajun Zhang"
                }
            ],
            "abstract": "The remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.",
            "corpus_id": 271050386,
            "sentences": [
                {
                    "corpus_id": "271050386",
                    "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
                    "text": "The remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.",
                    "score": 0.4137787195153735,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78955078125
                }
            ],
            "relevance_judgement": 0.78955078125,
            "relevance_judgment_input_expanded": "# Title: Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models\n# Venue: arXiv.org\n# Authors: Jinliang Lu, Ziliang Pang, Min Xiao, Yaochen Zhu, Rui Xia, Jiajun Zhang\n## Abstract\nThe remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.\n",
            "reference_string": "[271050386 | Lu et al. | 2024 | Citations: 27]"
        },
        {
            "title": "What Matters for Model Merging at Scale?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 89,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46841632",
                    "name": "Prateek Yadav"
                },
                {
                    "authorId": "144244743",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "2325489176",
                    "name": "Jonathan Lai"
                },
                {
                    "authorId": "2324583448",
                    "name": "Alexandra Chronopoulou"
                },
                {
                    "authorId": "1779225",
                    "name": "Manaal Faruqui"
                },
                {
                    "authorId": "2253396640",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2227827",
                    "name": "Tsendsuren Munkhdalai"
                }
            ],
            "abstract": "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.",
            "corpus_id": 273162841,
            "sentences": [
                {
                    "corpus_id": "273162841",
                    "title": "What Matters for Model Merging at Scale?",
                    "text": "Model merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
                    "score": 0.4677948803476974,
                    "section_title": "BACKGROUND",
                    "char_start_offset": 6956,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 563
                        },
                        {
                            "start": 566,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1289
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 257,
                            "end": 276,
                            "matchedPaperCorpusId": "270843326"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7861328125
                }
            ],
            "relevance_judgement": 0.7861328125,
            "relevance_judgment_input_expanded": "# Title: What Matters for Model Merging at Scale?\n# Venue: arXiv.org\n# Authors: Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai\n## Abstract\nModel merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.\n## BACKGROUND\nModel merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
            "reference_string": "[273162841 | Yadav et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i21.34405",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i21.34405?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i21.34405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363937",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2302773646",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2336739354",
                    "name": "Ganggui Ding"
                },
                {
                    "authorId": "2062580704",
                    "name": "Linlin Ou"
                },
                {
                    "authorId": "3263719",
                    "name": "Xinyi Yu"
                },
                {
                    "authorId": "3194022",
                    "name": "Bohan Zhuang"
                }
            ],
            "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "corpus_id": 277762515,
            "sentences": [
                {
                    "corpus_id": "277762515",
                    "title": "Channel Merging: Preserving Specialization for Merged Experts",
                    "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
                    "score": 0.5669850679075676,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78076171875
                }
            ],
            "relevance_judgement": 0.78076171875,
            "relevance_judgment_input_expanded": "# Title: Channel Merging: Preserving Specialization for Merged Experts\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Mingyang Zhang, Jing Liu, Ganggui Ding, Linlin Ou, Xinyi Yu, Bohan Zhuang\n## Abstract\nLately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.\n",
            "reference_string": "[277762515 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363937",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2302773646",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2336739354",
                    "name": "Ganggui Ding"
                },
                {
                    "authorId": "3263719",
                    "name": "Xinyi Yu"
                },
                {
                    "authorId": "2062580704",
                    "name": "Linlin Ou"
                },
                {
                    "authorId": "3194022",
                    "name": "Bohan Zhuang"
                }
            ],
            "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "corpus_id": 274965385,
            "sentences": [
                {
                    "corpus_id": "274965385",
                    "title": "Channel Merging: Preserving Specialization for Merged Experts",
                    "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
                    "score": 0.5672610211293997,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.779296875
                }
            ],
            "relevance_judgement": 0.779296875,
            "relevance_judgment_input_expanded": "# Title: Channel Merging: Preserving Specialization for Merged Experts\n# Venue: arXiv.org\n# Authors: Mingyang Zhang, Jing Liu, Ganggui Ding, Xinyi Yu, Linlin Ou, Bohan Zhuang\n## Abstract\nLately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.\n",
            "reference_string": "[274965385 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279090109",
                    "name": "Ziyu Zhao"
                },
                {
                    "authorId": "2342456608",
                    "name": "Yixiao Zhou"
                },
                {
                    "authorId": "2148404332",
                    "name": "Didi Zhu"
                },
                {
                    "authorId": "2317179711",
                    "name": "Tao Shen"
                },
                {
                    "authorId": "2279096273",
                    "name": "Xuwu Wang"
                },
                {
                    "authorId": "2279157215",
                    "name": "Jing Su"
                },
                {
                    "authorId": "2316947831",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2342458852",
                    "name": "Zhongyu Wei"
                },
                {
                    "authorId": "2275491998",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2343173027",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank as an independent expert}. With a \\textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.",
            "corpus_id": 275921828,
            "sentences": [
                {
                    "corpus_id": "275921828",
                    "title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning",
                    "text": "Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank as an independent expert}. With a \\textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.",
                    "score": 0.418148315854035,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.779296875
                }
            ],
            "relevance_judgement": 0.779296875,
            "relevance_judgment_input_expanded": "# Title: Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning\n# Venue: arXiv.org\n# Authors: Ziyu Zhao, Yixiao Zhou, Didi Zhu, Tao Shen, Xuwu Wang, Jing Su, Kun Kuang, Zhongyu Wei, Fei Wu, Yu Cheng\n## Abstract\nLow-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank as an independent expert}. With a \\textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.\n",
            "reference_string": "[275921828 | Zhao et al. | 2025 | Citations: 3]"
        },
        {
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 55,
            "citation_count": 3,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04222, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287297309",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "2217614543",
                    "name": "Fanqi Wan"
                },
                {
                    "authorId": "2286975236",
                    "name": "Longguang Zhong"
                },
                {
                    "authorId": "2258677979",
                    "name": "Canbin Huang"
                },
                {
                    "authorId": "2348918304",
                    "name": "Guosheng Liang"
                },
                {
                    "authorId": "2258552983",
                    "name": "Xiaojun Quan"
                }
            ],
            "abstract": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.",
            "corpus_id": 276813020,
            "sentences": [
                {
                    "corpus_id": "276813020",
                    "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
                    "text": "Combining the strengths of multiple large language models (LLMs) provides a powerful means to enhance performance, robustness, and generalization across diverse tasks by leveraging the unique expertise and knowledge each model offers. Individual LLMs, particularly those constrained by size or training data, may perform well in specific areas but struggle in others due to specialization gaps. For instance, one model might excel at generating creative content but lack precision in technical explanations, while another delivers technical accuracy but struggles with conversational fluency. By integrating multiple models, their collective strengths can bridge these gaps, leading to improved overall performance. This collaborative approach also improves robustness, as the system can compensate for individual model errors-when one model underperforms, others can intervene to support the response. Furthermore, this integration enhances task generalization by exposing the system to diverse patterns, allowing it to adapt more effectively to new or unseen challenges. \n\nVarious strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023b;Wang et al., 2025) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024;Hu et al., 2024;Ong et al., 2025) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors.",
                    "score": 0.5551862228373105,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1421
                        },
                        {
                            "start": 1422,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1944
                        },
                        {
                            "start": 1945,
                            "end": 2156
                        },
                        {
                            "start": 2157,
                            "end": 2348
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1177,
                            "end": 1198,
                            "matchedPaperCorpusId": "259075564"
                        },
                        {
                            "start": 1198,
                            "end": 1216,
                            "matchedPaperCorpusId": "270357878"
                        },
                        {
                            "start": 1434,
                            "end": 1453,
                            "matchedPaperCorpusId": "269303119"
                        },
                        {
                            "start": 1469,
                            "end": 1486,
                            "matchedPaperCorpusId": "270764307"
                        },
                        {
                            "start": 1746,
                            "end": 1769,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77587890625
                }
            ],
            "relevance_judgement": 0.77587890625,
            "relevance_judgment_input_expanded": "# Title: FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion\n# Venue: arXiv.org\n# Authors: Ziyi Yang, Fanqi Wan, Longguang Zhong, Canbin Huang, Guosheng Liang, Xiaojun Quan\n## Abstract\nWe introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.\n## INTRODUCTION\nCombining the strengths of multiple large language models (LLMs) provides a powerful means to enhance performance, robustness, and generalization across diverse tasks by leveraging the unique expertise and knowledge each model offers. Individual LLMs, particularly those constrained by size or training data, may perform well in specific areas but struggle in others due to specialization gaps. For instance, one model might excel at generating creative content but lack precision in technical explanations, while another delivers technical accuracy but struggles with conversational fluency. By integrating multiple models, their collective strengths can bridge these gaps, leading to improved overall performance. This collaborative approach also improves robustness, as the system can compensate for individual model errors-when one model underperforms, others can intervene to support the response. Furthermore, this integration enhances task generalization by exposing the system to diverse patterns, allowing it to adapt more effectively to new or unseen challenges. \n\nVarious strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023b;Wang et al., 2025) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024;Hu et al., 2024;Ong et al., 2025) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors.",
            "reference_string": "[276813020 | Yang et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Multi-task learning with multi-gate mixture of transformer-based experts for predictive process monitoring in manufacturing",
            "venue": "Science in progress",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1177/00368504241292196",
                "status": "GOLD",
                "license": "CCBYNC",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11788810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2144529923",
                    "name": "Jiaojiao Wang"
                },
                {
                    "authorId": "2334541673",
                    "name": "Yao Sui"
                },
                {
                    "authorId": "2118486263",
                    "name": "Chang Liu"
                },
                {
                    "authorId": "2334563656",
                    "name": "Xuewen Shen"
                },
                {
                    "authorId": "2334563718",
                    "name": "Zhongjin Li"
                },
                {
                    "authorId": "2335093638",
                    "name": "Dingguo Yu"
                }
            ],
            "abstract": "Manufacturing industries involve both business processes and complex manufacturing processes. Predictive process monitoring techniques are effective for managing process executions by making multi-perspective real-time predictions, preventing issues such as delivery delays. Conventional predictive process monitoring for business processes focuses on predicting the next activity, next event time, and remaining time using single-task learning, which is costly and complex. For complex manufacturing processes, predictive process monitoring primarily aims to predict the remaining time, that is, product cycle time. However, single-task learning methods fail to capture all the variations within the historical process executions. To address them, we propose the multi-gate mixture of transformer-based experts framework, which leverages a transformer network within the multi-gate mixture-of-experts multi-task learning architecture to extract sequential features and employs gated expert networks to model task commonalities and differences. Empirical results demonstrate that multi-gate mixture of transformer-based experts outperforms three alternative architectures across five real-life event logs, highlighting its generalization, effectiveness, and efficiency in predictive process monitoring.",
            "corpus_id": 274609601,
            "sentences": [
                {
                    "corpus_id": "274609601",
                    "title": "Multi-task learning with multi-gate mixture of transformer-based experts for predictive process monitoring in manufacturing",
                    "text": "Soft parameter sharing involves using separate models for each task, but incorporating parametric relationships or differences into a joint objective function. Mechanisms like regularization or constraints encourage similarity or distance between task models, which aids knowledge transfer and efficient parameter use. This approach allows the multi-task model to leverage both commonalities and differences among tasks, improving performance and model quality for each task. MoE represents a significant advancement in flexible parameter sharing. Initially proposed by Jacobs et al., it divides a system into independent networks, each handling a portion of the data. 16 Shazeer et al. enhanced this concept with the Sparsely-Gated MoE layer, which integrates multiple experts and a trainable gating network. 47 This approach uses a divide-and-conquer strategy to address complex problems, improving efficiency and model generalization. Furthermore, Shazeer et al. applied MoE to natural language modeling and machine translation, while Riquelme et al. introduced the visual mixed expert (V-MoE) model for image classification. 48 he MoE model performs well in single-task scenarios but faces challenges in MTL due to complex inter-task relationships such as correlation and conflicts. 49 In MoE's MTL framework (as shown in Figure 1(c)), multiple tasks share a common set of experts and a single gating network, which may lead to conflicts and inefficiencies. To address these issues, MMoE 15 was introduced, utilizing multiple gating networks to enable task-specific expert selection and better capture task relationships. 50 Unlike MoE, which relies on one gating network for all tasks, MMoE allows for different expert selections for distinct tasks. 51 As indicated by Wang et al., 52 MMoE enhances MTL by allowing task-specific adjustments to expert networks and improving the modeling of task relationships, thereby boosting overall performance. As a form of soft parameter sharing, MMoE uses soft gating networks to aggregate experts learned from Wang et al. \n\ndifferent tasks, addressing negative migration problems effectively. It outperforms other methods, such as cross-stitch networks, 53 particularly in content recommendation. Various novel approaches based on the MMoE have emerged. For instance, Qin et al. 54 introduced the mixture of sequential experts (MoSE), which utilizes LSTM within an advanced MMoE framework to capture sequential user behavior.",
                    "score": 0.3868479926989754,
                    "section_title": "Multi-task learning",
                    "char_start_offset": 16084,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 318
                        },
                        {
                            "start": 319,
                            "end": 475
                        },
                        {
                            "start": 476,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 671
                        },
                        {
                            "start": 672,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1952
                        },
                        {
                            "start": 1953,
                            "end": 2066
                        },
                        {
                            "start": 2069,
                            "end": 2137
                        },
                        {
                            "start": 2138,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2298
                        },
                        {
                            "start": 2299,
                            "end": 2470
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 669,
                            "end": 671,
                            "matchedPaperCorpusId": "572361"
                        },
                        {
                            "start": 810,
                            "end": 812,
                            "matchedPaperCorpusId": "12462234"
                        },
                        {
                            "start": 1129,
                            "end": 1131,
                            "matchedPaperCorpusId": "235417196"
                        },
                        {
                            "start": 1287,
                            "end": 1289,
                            "matchedPaperCorpusId": "253116858"
                        },
                        {
                            "start": 1492,
                            "end": 1494,
                            "matchedPaperCorpusId": "50770252"
                        },
                        {
                            "start": 1626,
                            "end": 1628,
                            "matchedPaperCorpusId": "248367384"
                        },
                        {
                            "start": 1755,
                            "end": 1757,
                            "matchedPaperCorpusId": "275955445"
                        },
                        {
                            "start": 1787,
                            "end": 1789,
                            "matchedPaperCorpusId": "251302720"
                        },
                        {
                            "start": 2199,
                            "end": 2201,
                            "matchedPaperCorpusId": "1923223"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75
                }
            ],
            "relevance_judgement": 0.75,
            "relevance_judgment_input_expanded": "# Title: Multi-task learning with multi-gate mixture of transformer-based experts for predictive process monitoring in manufacturing\n# Venue: Science in progress\n# Authors: Jiaojiao Wang, Yao Sui, Chang Liu, Xuewen Shen, Zhongjin Li, Dingguo Yu\n## Abstract\nManufacturing industries involve both business processes and complex manufacturing processes. Predictive process monitoring techniques are effective for managing process executions by making multi-perspective real-time predictions, preventing issues such as delivery delays. Conventional predictive process monitoring for business processes focuses on predicting the next activity, next event time, and remaining time using single-task learning, which is costly and complex. For complex manufacturing processes, predictive process monitoring primarily aims to predict the remaining time, that is, product cycle time. However, single-task learning methods fail to capture all the variations within the historical process executions. To address them, we propose the multi-gate mixture of transformer-based experts framework, which leverages a transformer network within the multi-gate mixture-of-experts multi-task learning architecture to extract sequential features and employs gated expert networks to model task commonalities and differences. Empirical results demonstrate that multi-gate mixture of transformer-based experts outperforms three alternative architectures across five real-life event logs, highlighting its generalization, effectiveness, and efficiency in predictive process monitoring.\n## Multi-task learning\nSoft parameter sharing involves using separate models for each task, but incorporating parametric relationships or differences into a joint objective function. Mechanisms like regularization or constraints encourage similarity or distance between task models, which aids knowledge transfer and efficient parameter use. This approach allows the multi-task model to leverage both commonalities and differences among tasks, improving performance and model quality for each task. MoE represents a significant advancement in flexible parameter sharing. Initially proposed by Jacobs et al., it divides a system into independent networks, each handling a portion of the data. 16 Shazeer et al. enhanced this concept with the Sparsely-Gated MoE layer, which integrates multiple experts and a trainable gating network. 47 This approach uses a divide-and-conquer strategy to address complex problems, improving efficiency and model generalization. Furthermore, Shazeer et al. applied MoE to natural language modeling and machine translation, while Riquelme et al. introduced the visual mixed expert (V-MoE) model for image classification. 48 he MoE model performs well in single-task scenarios but faces challenges in MTL due to complex inter-task relationships such as correlation and conflicts. 49 In MoE's MTL framework (as shown in Figure 1(c)), multiple tasks share a common set of experts and a single gating network, which may lead to conflicts and inefficiencies. To address these issues, MMoE 15 was introduced, utilizing multiple gating networks to enable task-specific expert selection and better capture task relationships. 50 Unlike MoE, which relies on one gating network for all tasks, MMoE allows for different expert selections for distinct tasks. 51 As indicated by Wang et al., 52 MMoE enhances MTL by allowing task-specific adjustments to expert networks and improving the modeling of task relationships, thereby boosting overall performance. As a form of soft parameter sharing, MMoE uses soft gating networks to aggregate experts learned from Wang et al. \n\ndifferent tasks, addressing negative migration problems effectively. It outperforms other methods, such as cross-stitch networks, 53 particularly in content recommendation. Various novel approaches based on the MMoE have emerged. For instance, Qin et al. 54 introduced the mixture of sequential experts (MoSE), which utilizes LSTM within an advanced MMoE framework to capture sequential user behavior.",
            "reference_string": "[274609601 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257014771",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2217614543",
                    "name": "Fanqi Wan"
                },
                {
                    "authorId": "2258677979",
                    "name": "Canbin Huang"
                },
                {
                    "authorId": "2261363209",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "143971529",
                    "name": "Chenliang Li"
                },
                {
                    "authorId": "2114009661",
                    "name": "Mingshi Yan"
                },
                {
                    "authorId": "2281901929",
                    "name": "Ji Zhang"
                }
            ],
            "abstract": "While fusing the capacities and advantages of various large language models (LLMs) offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during the training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.",
            "corpus_id": 271843401,
            "sentences": [
                {
                    "corpus_id": "271843401",
                    "title": "ProFuser: Progressive Fusion of Large Language Models",
                    "text": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
                    "score": 0.44405659672154085,
                    "section_title": "Model Merging",
                    "char_start_offset": 5998,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1534
                        },
                        {
                            "start": 1537,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1863
                        },
                        {
                            "start": 1864,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 311,
                            "end": 334,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1339,
                            "end": 1359,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.744140625
                }
            ],
            "relevance_judgement": 0.744140625,
            "relevance_judgment_input_expanded": "# Title: ProFuser: Progressive Fusion of Large Language Models\n# Venue: arXiv.org\n# Authors: Tianyuan Shi, Fanqi Wan, Canbin Huang, Xiaojun Quan, Chenliang Li, Mingshi Yan, Ji Zhang\n## Abstract\nWhile fusing the capacities and advantages of various large language models (LLMs) offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during the training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.\n## Model Merging\nModel merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
            "reference_string": "[271843401 | Shi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 73,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09445, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308224151",
                    "name": "Xiaoda Yang"
                },
                {
                    "authorId": "2350336954",
                    "name": "JunYu Lu"
                },
                {
                    "authorId": "2220747584",
                    "name": "Hongshun Qiu"
                },
                {
                    "authorId": "2350180388",
                    "name": "Sijing Li"
                },
                {
                    "authorId": "2349632427",
                    "name": "Hao Li"
                },
                {
                    "authorId": "72890649",
                    "name": "Shengpeng Ji"
                },
                {
                    "authorId": "2349737557",
                    "name": "Xudong Tang"
                },
                {
                    "authorId": "2349670795",
                    "name": "Jiayang Xu"
                },
                {
                    "authorId": "2329894630",
                    "name": "Jiaqi Duan"
                },
                {
                    "authorId": "2112347676",
                    "name": "Ziyue Jiang"
                },
                {
                    "authorId": "2349737916",
                    "name": "Cong Lin"
                },
                {
                    "authorId": "2328348412",
                    "name": "Sihang Cai"
                },
                {
                    "authorId": "2266912737",
                    "name": "Zejian Xie"
                },
                {
                    "authorId": "2352067468",
                    "name": "Zhuoyang Song"
                },
                {
                    "authorId": "2266803682",
                    "name": "Songxin Zhang"
                }
            ],
            "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.",
            "corpus_id": 276938164,
            "sentences": [
                {
                    "corpus_id": "276938164",
                    "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                    "text": "Visual-Language Models (VLMs), by integrating visual and linguistic modalities, have become a core paradigm for multimodal understanding tasks. VLMs aim to achieve complex understanding tasks through the simultaneous processing of image and text data. The key advantage of VLMs lies in their ability to unify multimodal and multitask scenarios within a single framework. \n\nExisting VLM research mainly follows two paradigms: 1) constructing a unified multitask training framework [8,54], which achieves cross-task versatility through shared models and joint training, characterized by model simplicity and support for knowledge transfer between tasks; 2) utilizing task-specific expert models for feature fusion [31,69], which optimizes task performance through specialized design and flexible integration, characterized by strong taskspecificity and high modality complementarity. \n\nHowever, both approaches face the challenge of balancing task heterogeneity and model generality. Specifically, while the former emphasizes model generality through a unified framework, it tends to encounter representation conflict issues in scenarios where cross-task correlations are insufficient. For instance, studies [11] have shown that when handling spatial localization and semantic description tasks simultaneously, due to significant differences in task objectives, the model can easily confuse its concentration on vision features, leading to performance degradation. This phenomenon is particularly evident in complex multitask settings, limiting the model's generalization ability. The latter approach designs different expert models for heterogeneous tasks, which alleviates task conflicts to some extent. However, due to the lack of generality among models, the efficiency of expert collaboration is low. Empirical studies [4,72] indicate that multi-model architectures lead to parameter redundancy and knowledge-loss problems. \n\nTo address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms. \n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms.",
                    "score": 0.4039059703586536,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 251
                        },
                        {
                            "start": 252,
                            "end": 370
                        },
                        {
                            "start": 373,
                            "end": 881
                        },
                        {
                            "start": 884,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1183
                        },
                        {
                            "start": 1184,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1578
                        },
                        {
                            "start": 1579,
                            "end": 1703
                        },
                        {
                            "start": 1704,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1926
                        },
                        {
                            "start": 1929,
                            "end": 2033
                        },
                        {
                            "start": 2034,
                            "end": 2194
                        },
                        {
                            "start": 2197,
                            "end": 2513
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 480,
                            "end": 483,
                            "matchedPaperCorpusId": "216080982"
                        },
                        {
                            "start": 712,
                            "end": 716,
                            "matchedPaperCorpusId": "215754208"
                        },
                        {
                            "start": 716,
                            "end": 719,
                            "matchedPaperCorpusId": "235692795"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73486328125
                }
            ],
            "relevance_judgement": 0.73486328125,
            "relevance_judgment_input_expanded": "# Title: Astrea: A MOE-based Visual Understanding Model with Progressive Alignment\n# Venue: arXiv.org\n# Authors: Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang\n## Abstract\nVision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.\n## Introduction\nVisual-Language Models (VLMs), by integrating visual and linguistic modalities, have become a core paradigm for multimodal understanding tasks. VLMs aim to achieve complex understanding tasks through the simultaneous processing of image and text data. The key advantage of VLMs lies in their ability to unify multimodal and multitask scenarios within a single framework. \n\nExisting VLM research mainly follows two paradigms: 1) constructing a unified multitask training framework [8,54], which achieves cross-task versatility through shared models and joint training, characterized by model simplicity and support for knowledge transfer between tasks; 2) utilizing task-specific expert models for feature fusion [31,69], which optimizes task performance through specialized design and flexible integration, characterized by strong taskspecificity and high modality complementarity. \n\nHowever, both approaches face the challenge of balancing task heterogeneity and model generality. Specifically, while the former emphasizes model generality through a unified framework, it tends to encounter representation conflict issues in scenarios where cross-task correlations are insufficient. For instance, studies [11] have shown that when handling spatial localization and semantic description tasks simultaneously, due to significant differences in task objectives, the model can easily confuse its concentration on vision features, leading to performance degradation. This phenomenon is particularly evident in complex multitask settings, limiting the model's generalization ability. The latter approach designs different expert models for heterogeneous tasks, which alleviates task conflicts to some extent. However, due to the lack of generality among models, the efficiency of expert collaboration is low. Empirical studies [4,72] indicate that multi-model architectures lead to parameter redundancy and knowledge-loss problems. \n\nTo address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms. \n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms.",
            "reference_string": "[276938164 | Yang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.15518, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2322611014",
                    "name": "Zesen Zhao"
                },
                {
                    "authorId": "2244738638",
                    "name": "Shuowei Jin"
                },
                {
                    "authorId": "2321511953",
                    "name": "Z. M. Mao"
                }
            ],
            "abstract": "The proliferation of Large Language Models (LLMs) with varying capabilities and costs has created a need for efficient model selection in AI systems. LLM routers address this need by dynamically choosing the most suitable model for a given query based on task requirements and budget constraints. However, existing routers face challenges in scalability and real-time adaptation, particularly in high-volume online environments. We present Eagle, a novel LLM routing approach that combines global and local ELO ranking modules to overcome these limitations. By evaluating both general and specialized LLM abilities, Eagle provides a scalable, training-free solution that enhances model selection quality while reducing computational overhead. Our experiments across multiple datasets show Eagle consistently outperforms baseline methods, with improvements of up to 23.52 percent in Area Under Curve (AUC) scores. Moreover, Eagle demonstrates remarkable efficiency, requiring only 1/20 of baseline methods' time for initialization and 100 to 200 times faster incremental updates in online scenarios, making it well-suited for dynamic, high-volume online serving environments.",
            "corpus_id": 272832307,
            "sentences": [
                {
                    "corpus_id": "272832307",
                    "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference",
                    "text": "Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, training and serving a single massive model is both costly and inefficient. Additionally, recent findings show that larger models do not consistently outperform smaller or specialized LLMs for all tasks. To address these issues, researchers are exploring multi-LLM approaches to enhance system performance while maintaining cost efficiency. \n\nMixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models. To facilitate fair comparisons between routing strategies, benchmarks like RouterBench [10] and Large Language Model Routing with Benchmark Datasets [20] provide standardized metrics that assess performance, efficiency, and resource consumption.",
                    "score": 0.4225595569284069,
                    "section_title": "C Related Works",
                    "char_start_offset": 11951,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 186
                        },
                        {
                            "start": 187,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 451
                        },
                        {
                            "start": 454,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 862
                        },
                        {
                            "start": 863,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1220
                        },
                        {
                            "start": 1221,
                            "end": 1358
                        },
                        {
                            "start": 1361,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 2088
                        },
                        {
                            "start": 2089,
                            "end": 2264
                        },
                        {
                            "start": 2265,
                            "end": 2510
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73388671875
                }
            ],
            "relevance_judgement": 0.73388671875,
            "relevance_judgment_input_expanded": "# Title: Eagle: Efficient Training-Free Router for Multi-LLM Inference\n# Venue: arXiv.org\n# Authors: Zesen Zhao, Shuowei Jin, Z. M. Mao\n## Abstract\nThe proliferation of Large Language Models (LLMs) with varying capabilities and costs has created a need for efficient model selection in AI systems. LLM routers address this need by dynamically choosing the most suitable model for a given query based on task requirements and budget constraints. However, existing routers face challenges in scalability and real-time adaptation, particularly in high-volume online environments. We present Eagle, a novel LLM routing approach that combines global and local ELO ranking modules to overcome these limitations. By evaluating both general and specialized LLM abilities, Eagle provides a scalable, training-free solution that enhances model selection quality while reducing computational overhead. Our experiments across multiple datasets show Eagle consistently outperforms baseline methods, with improvements of up to 23.52 percent in Area Under Curve (AUC) scores. Moreover, Eagle demonstrates remarkable efficiency, requiring only 1/20 of baseline methods' time for initialization and 100 to 200 times faster incremental updates in online scenarios, making it well-suited for dynamic, high-volume online serving environments.\n## C Related Works\nLarge language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, training and serving a single massive model is both costly and inefficient. Additionally, recent findings show that larger models do not consistently outperform smaller or specialized LLMs for all tasks. To address these issues, researchers are exploring multi-LLM approaches to enhance system performance while maintaining cost efficiency. \n\nMixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models. To facilitate fair comparisons between routing strategies, benchmarks like RouterBench [10] and Large Language Model Routing with Benchmark Datasets [20] provide standardized metrics that assess performance, efficiency, and resource consumption.",
            "reference_string": "[272832307 | Zhao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 63,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262512474",
                    "name": "Zhenyi Lu"
                },
                {
                    "authorId": "2277238906",
                    "name": "Chenghao Fan"
                },
                {
                    "authorId": "2284721764",
                    "name": "Wei Wei"
                },
                {
                    "authorId": "2262446609",
                    "name": "Xiaoye Qu"
                },
                {
                    "authorId": "2182623368",
                    "name": "Dangyang Chen"
                },
                {
                    "authorId": "2284687448",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
            "corpus_id": 270702345,
            "sentences": [
                {
                    "corpus_id": "270702345",
                    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                    "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
                    "score": 0.62423505107026,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1865
                        },
                        {
                            "start": 1868,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2200
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 340,
                            "end": 343,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 550,
                            "end": 554,
                            "matchedPaperCorpusId": "232110907"
                        },
                        {
                            "start": 677,
                            "end": 681,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 681,
                            "end": 684,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 684,
                            "end": 687,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 687,
                            "end": 690,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1592,
                            "end": 1596,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 1596,
                            "end": 1599,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1646,
                            "end": 1650,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1650,
                            "end": 1653,
                            "matchedPaperCorpusId": "258832777"
                        },
                        {
                            "start": 1653,
                            "end": 1656,
                            "matchedPaperCorpusId": "263831551"
                        },
                        {
                            "start": 1656,
                            "end": 1659,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 2028,
                            "end": 2032,
                            "matchedPaperCorpusId": "258865647"
                        },
                        {
                            "start": 2032,
                            "end": 2035,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.732421875
                }
            ],
            "relevance_judgement": 0.732421875,
            "relevance_judgment_input_expanded": "# Title: Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging\n# Venue: Neural Information Processing Systems\n# Authors: Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, Yu Cheng\n## Abstract\nIn the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}\n## Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "reference_string": "[270702345 | Lu et al. | 2024 | Citations: 63]"
        },
        {
            "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2263268557",
                    "name": "Luyang Fang"
                },
                {
                    "authorId": "2258714804",
                    "name": "Ehsan Latif"
                },
                {
                    "authorId": "2298332870",
                    "name": "Haoran Lu"
                },
                {
                    "authorId": "2325891087",
                    "name": "Yifan Zhou"
                },
                {
                    "authorId": "2276478007",
                    "name": "Ping Ma"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                }
            ],
            "abstract": "Automatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance.",
            "corpus_id": 276961298,
            "sentences": [
                {
                    "corpus_id": "276961298",
                    "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
                    "text": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
                    "score": 0.41605524622036594,
                    "section_title": "Model Merging",
                    "char_start_offset": 8245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 794
                        },
                        {
                            "start": 797,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1377
                        },
                        {
                            "start": 1380,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1787
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72705078125
                }
            ],
            "relevance_judgement": 0.72705078125,
            "relevance_judgment_input_expanded": "# Title: Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment\n# Venue: arXiv.org\n# Authors: Luyang Fang, Ehsan Latif, Haoran Lu, Yifan Zhou, Ping Ma, Xiaoming Zhai\n## Abstract\nAutomatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance.\n## Model Merging\nIn our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
            "reference_string": "[276961298 | Fang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Delta Decompression for MoE-based LLMs Compression",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17298, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347571185",
                    "name": "Hao Gu"
                },
                {
                    "authorId": "2331681523",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2331723310",
                    "name": "Lujun Li"
                },
                {
                    "authorId": "2313367125",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2331702843",
                    "name": "Mark Lee"
                },
                {
                    "authorId": "2331691577",
                    "name": "Shengjie Sun"
                },
                {
                    "authorId": "2239201089",
                    "name": "Wei Xue"
                },
                {
                    "authorId": "2118270918",
                    "name": "Yi-Ting Guo"
                }
            ],
            "abstract": "Mixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes are available in https://github.com/lliai/D2MoE.",
            "corpus_id": 276575054,
            "sentences": [
                {
                    "corpus_id": "276575054",
                    "title": "Delta Decompression for MoE-based LLMs Compression",
                    "text": "Recent advances in Large Language Models (LLMs) increasingly favor Mixture of Experts (MoE) (Cai et al., 2024) architectures for their ability to scale model capacity through specialized expert networks while maintaining computational efficiency via sparse activation. The success of MoE is evident in recent LLMs like DeepSeek-V3 (DeepSeek-AI et al., 2024) and MiniMax-01 (MiniMax et al., 2025), which demonstrate unprecedented capabilities in language understanding and generation tasks. Despite their compelling advantages, MoE LLMs face critical challenges in practical deployment scenarios (Tang et al., 2024;Zhong et al., 2024;Hwang et al., 2024). Their substantial parameter footprint, coupled with considerable memory overhead from storing multiple expert weights (Song et al., 2023), creates significant barriers to resource-constrained environments. \n\nTo address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods. \n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance. \n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations.",
                    "score": 0.42989740743818033,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 489
                        },
                        {
                            "start": 490,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 859
                        },
                        {
                            "start": 862,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1072
                        },
                        {
                            "start": 1075,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1933
                        },
                        {
                            "start": 1934,
                            "end": 2033
                        },
                        {
                            "start": 2034,
                            "end": 2188
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 633,
                            "end": 652,
                            "matchedPaperCorpusId": "261076133"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72314453125
                }
            ],
            "relevance_judgement": 0.72314453125,
            "relevance_judgment_input_expanded": "# Title: Delta Decompression for MoE-based LLMs Compression\n# Venue: arXiv.org\n# Authors: Hao Gu, Wei Li, Lujun Li, Qi Zhu, Mark Lee, Shengjie Sun, Wei Xue, Yi-Ting Guo\n## Abstract\nMixture-of-Experts (MoE) architectures in large language models (LLMs) achieve exceptional performance, but face prohibitive storage and memory requirements. To address these challenges, we present $D^2$-MoE, a new delta decompression compressor for reducing the parameters of MoE LLMs. Based on observations of expert diversity, we decompose their weights into a shared base weight and unique delta weights. Specifically, our method first merges each expert's weight into the base weight using the Fisher information matrix to capture shared components. Then, we compress delta weights through Singular Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we introduce a semi-dynamical structured pruning strategy for the base weights, combining static and dynamic redundancy analysis to achieve further parameter reduction while maintaining input adaptivity. In this way, our $D^2$-MoE successfully compact MoE LLMs to high compression ratios without additional training. Extensive experiments highlight the superiority of our approach, with over 13% performance gains than other compressors on Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\\sim$60% compression rates. Codes are available in https://github.com/lliai/D2MoE.\n## Introduction\nRecent advances in Large Language Models (LLMs) increasingly favor Mixture of Experts (MoE) (Cai et al., 2024) architectures for their ability to scale model capacity through specialized expert networks while maintaining computational efficiency via sparse activation. The success of MoE is evident in recent LLMs like DeepSeek-V3 (DeepSeek-AI et al., 2024) and MiniMax-01 (MiniMax et al., 2025), which demonstrate unprecedented capabilities in language understanding and generation tasks. Despite their compelling advantages, MoE LLMs face critical challenges in practical deployment scenarios (Tang et al., 2024;Zhong et al., 2024;Hwang et al., 2024). Their substantial parameter footprint, coupled with considerable memory overhead from storing multiple expert weights (Song et al., 2023), creates significant barriers to resource-constrained environments. \n\nTo address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods. \n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance. \n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations.",
            "reference_string": "[276575054 | Gu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2345985527",
                    "name": "Yuxuan Yao"
                },
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2221337060",
                    "name": "Xiaojin Fu"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2344902525",
                    "name": "Xing Li"
                },
                {
                    "authorId": "2267558779",
                    "name": "Hui-Ling Zhen"
                },
                {
                    "authorId": "2332348570",
                    "name": "Tao Zhong"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                }
            ],
            "abstract": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.",
            "corpus_id": 277322544,
            "sentences": [
                {
                    "corpus_id": "277322544",
                    "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                    "text": "Existing merging methods rely on sparsely estimated task vectors but face two key limitations: dependence on base model parameters and task vector interference. LoRE-Merging (Liu et al., 2025d), a low-rank estimation framework, is proposed to address such issues. It constructs an approximate base model and low-rank task vectors via optimization, minimizing discrepancies between fine-tuned and merged models. Using coordinate descent and singular value thresholding, LoRE-Merging reduces task vector interference, demonstrating the effectiveness of low-rank estimation in model merging. \n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments. In our study, we eliminate the router training and directly utilize its singular value decomposition (SVD) merging part. \n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
                    "score": 0.409618580173466,
                    "section_title": "LoRE-Merging",
                    "char_start_offset": 12716,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 588
                        },
                        {
                            "start": 591,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 937
                        },
                        {
                            "start": 938,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1225
                        },
                        {
                            "start": 1228,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1763
                        },
                        {
                            "start": 1764,
                            "end": 1878
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 730,
                            "end": 747,
                            "matchedPaperCorpusId": "270702345"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72021484375
                },
                {
                    "corpus_id": "277322544",
                    "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                    "text": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
                    "score": 0.5148324038871748,
                    "section_title": "MODEL MERGING",
                    "char_start_offset": 9525,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 342
                        },
                        {
                            "start": 345,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 998
                        },
                        {
                            "start": 999,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1225
                        },
                        {
                            "start": 1228,
                            "end": 1357
                        },
                        {
                            "start": 1360,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2000
                        },
                        {
                            "start": 2003,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 377,
                            "end": 400,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 904,
                            "end": 926,
                            "matchedPaperCorpusId": "254408495"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66845703125
                }
            ],
            "relevance_judgement": 0.72021484375,
            "relevance_judgment_input_expanded": "# Title: Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging\n# Venue: arXiv.org\n# Authors: Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan\n## Abstract\nThe transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.\n## MODEL MERGING\nModel merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.\n\n## LoRE-Merging\nExisting merging methods rely on sparsely estimated task vectors but face two key limitations: dependence on base model parameters and task vector interference. LoRE-Merging (Liu et al., 2025d), a low-rank estimation framework, is proposed to address such issues. It constructs an approximate base model and low-rank task vectors via optimization, minimizing discrepancies between fine-tuned and merged models. Using coordinate descent and singular value thresholding, LoRE-Merging reduces task vector interference, demonstrating the effectiveness of low-rank estimation in model merging. \n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments. In our study, we eliminate the router training and directly utilize its singular value decomposition (SVD) merging part. \n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
            "reference_string": "[277322544 | Wu et al. | 2025 | Citations: 14]"
        },
        {
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 54,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.00433, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "2248152216",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2279402395",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "2237424891",
                    "name": "Nan Yin"
                },
                {
                    "authorId": "2282189838",
                    "name": "Lefei Zhang"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://github.com/tanganke/weight-ensembling_MoE",
            "corpus_id": 267365047,
            "sentences": [
                {
                    "corpus_id": "267365047",
                    "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
                    "text": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://github.com/tanganke/weight-ensembling_MoE",
                    "score": 0.47056039526524157,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7197265625
                }
            ],
            "relevance_judgement": 0.7197265625,
            "relevance_judgment_input_expanded": "# Title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts\n# Venue: International Conference on Machine Learning\n# Authors: A. Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, D. Tao\n## Abstract\nMerging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://github.com/tanganke/weight-ensembling_MoE\n",
            "reference_string": "[267365047 | Tang et al. | 2024 | Citations: 54]"
        },
        {
            "title": "Cognitive Memory in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 88,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02441, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350351496",
                    "name": "Lianlei Shan"
                },
                {
                    "authorId": "2355000238",
                    "name": "Shixian Luo"
                },
                {
                    "authorId": "2353569735",
                    "name": "Zezhou Zhu"
                },
                {
                    "authorId": "2354002282",
                    "name": "Yu Yuan"
                },
                {
                    "authorId": "2354309916",
                    "name": "Yong Wu"
                }
            ],
            "abstract": "This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.",
            "corpus_id": 277510128,
            "sentences": [
                {
                    "corpus_id": "277510128",
                    "title": "Cognitive Memory in Large Language Models",
                    "text": "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing. \n\nThe advantages of the BTX method include the parallel and asynchronous nature of the expert training phase, which reduces communication costs and enhances training efficiency. The final BTX model is a unified neural network that can be fine-tuned like any standard LLM. Additionally, the model's FLOPs (floating-point operations) during inference do not increase significantly because it is sparsely activated, despite the increase in parameter count. The paper also explores various variants of BTX, such as load balancing, different routing methods, and strategies for splitting and merging experts, to further improve model performance and efficiency. \n\nas an additional instruction-aware query to extract and retain task-relevant information. Task-specific responses are generated using this key-value cache.",
                    "score": 0.4339181847522248,
                    "section_title": "MoE For Memory Parameterization",
                    "char_start_offset": 102329,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 375
                        },
                        {
                            "start": 378,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 821
                        },
                        {
                            "start": 822,
                            "end": 959
                        },
                        {
                            "start": 960,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1415
                        },
                        {
                            "start": 1418,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 1869
                        },
                        {
                            "start": 1870,
                            "end": 2072
                        },
                        {
                            "start": 2075,
                            "end": 2164
                        },
                        {
                            "start": 2165,
                            "end": 2230
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7177734375
                }
            ],
            "relevance_judgement": 0.7177734375,
            "relevance_judgment_input_expanded": "# Title: Cognitive Memory in Large Language Models\n# Venue: arXiv.org\n# Authors: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu\n## Abstract\nThis paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.\n## MoE For Memory Parameterization\nSukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing. \n\nThe advantages of the BTX method include the parallel and asynchronous nature of the expert training phase, which reduces communication costs and enhances training efficiency. The final BTX model is a unified neural network that can be fine-tuned like any standard LLM. Additionally, the model's FLOPs (floating-point operations) during inference do not increase significantly because it is sparsely activated, despite the increase in parameter count. The paper also explores various variants of BTX, such as load balancing, different routing methods, and strategies for splitting and merging experts, to further improve model performance and efficiency. \n\nas an additional instruction-aware query to extract and retain task-relevant information. Task-specific responses are generated using this key-value cache.",
            "reference_string": "[277510128 | Shan et al. | 2025 | Citations: 3]"
        },
        {
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "venue": "",
            "year": 2025,
            "reference_count": 64,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.06977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35640834",
                    "name": "Wenju Sun"
                },
                {
                    "authorId": "2262408434",
                    "name": "Qingyong Li"
                },
                {
                    "authorId": "8010931",
                    "name": "Yangli-ao Geng"
                },
                {
                    "authorId": "2342563128",
                    "name": "Boyang Li"
                }
            ],
            "abstract": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
            "corpus_id": 278501405,
            "sentences": [
                {
                    "corpus_id": "278501405",
                    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                    "text": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
                    "score": 0.5671491527792101,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71630859375
                }
            ],
            "relevance_judgement": 0.71630859375,
            "relevance_judgment_input_expanded": "# Title: CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging\n# Venue: \n# Authors: Wenju Sun, Qingyong Li, Yangli-ao Geng, Boyang Li\n## Abstract\nMulti-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.\n",
            "reference_string": "[278501405 | Sun et al. | 2025 | Citations: 2]"
        },
        {
            "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.12733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2192284724",
                    "name": "Mohammadreza Pourreza"
                },
                {
                    "authorId": "2068169921",
                    "name": "Ruoxi Sun"
                },
                {
                    "authorId": "2316892037",
                    "name": "Hailong Li"
                },
                {
                    "authorId": "71436125",
                    "name": "Lesly Miculicich"
                },
                {
                    "authorId": "2264567300",
                    "name": "Tomas Pfister"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                }
            ],
            "abstract": "Recent advances in Text-to-SQL have largely focused on the SQLite dialect, neglecting the diverse landscape of SQL dialects like BigQuery and PostgreSQL. This limitation is due to the diversity in SQL syntaxes and functions, along with the high cost of collecting and curating SQL-specific training data. To address this, we introduce SQL-GEN, a framework for generating high-quality synthetic training data for any SQL dialect, guided by readily available dialect-specific tutorials. SQL-GEN significantly improves cross-dialect Text-to-SQL performance, boosting execution accuracy by up to 20\\% over existing methods. This performance gain narrows the gap with models trained on large-scale human-annotated data. Furthermore, combining synthetic data from SQL-GEN with human-annotated data yields additional improvements of up to 5.6\\%. To unify multi-dialect capabilities within a single model, we propose a novel Mixture-of-Experts (MoE) initialization that leverages the shared knowledge across dialects. Our approach merges self-attention layers from dialect-specific models and initializes expert gates using dialect-specific keywords. This leads to a versatile model optimized for multiple SQL dialects, outperforming single-dialect models and significantly enhancing overall performance.",
            "corpus_id": 271947337,
            "sentences": [
                {
                    "corpus_id": "271947337",
                    "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
                    "text": "Training specialized, task-specific models presents several challenges, including the storage costs associated with maintaining multiple models, the substantial memory requirements for deploying these models, and the rapid obsolescence of models as training datasets age. One proposed solution to mitigate these issues is model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;Yadav et al., 2024;Yu et al., 2024) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values. \n\nAdditionally, DARE introduces random pruning to align more closely with the base model's performance (Goddard et al., 2024). More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures.",
                    "score": 0.45870934426893195,
                    "section_title": "A.1.2 Model Merging",
                    "char_start_offset": 33952,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 667
                        },
                        {
                            "start": 668,
                            "end": 964
                        },
                        {
                            "start": 967,
                            "end": 1091
                        },
                        {
                            "start": 1092,
                            "end": 1202
                        },
                        {
                            "start": 1203,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1528
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 759,
                            "end": 778,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 778,
                            "end": 794,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71240234375
                }
            ],
            "relevance_judgement": 0.71240234375,
            "relevance_judgment_input_expanded": "# Title: SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging\n# Venue: arXiv.org\n# Authors: Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, Sercan \u00d6. Arik\n## Abstract\nRecent advances in Text-to-SQL have largely focused on the SQLite dialect, neglecting the diverse landscape of SQL dialects like BigQuery and PostgreSQL. This limitation is due to the diversity in SQL syntaxes and functions, along with the high cost of collecting and curating SQL-specific training data. To address this, we introduce SQL-GEN, a framework for generating high-quality synthetic training data for any SQL dialect, guided by readily available dialect-specific tutorials. SQL-GEN significantly improves cross-dialect Text-to-SQL performance, boosting execution accuracy by up to 20\\% over existing methods. This performance gain narrows the gap with models trained on large-scale human-annotated data. Furthermore, combining synthetic data from SQL-GEN with human-annotated data yields additional improvements of up to 5.6\\%. To unify multi-dialect capabilities within a single model, we propose a novel Mixture-of-Experts (MoE) initialization that leverages the shared knowledge across dialects. Our approach merges self-attention layers from dialect-specific models and initializes expert gates using dialect-specific keywords. This leads to a versatile model optimized for multiple SQL dialects, outperforming single-dialect models and significantly enhancing overall performance.\n## A.1.2 Model Merging\nTraining specialized, task-specific models presents several challenges, including the storage costs associated with maintaining multiple models, the substantial memory requirements for deploying these models, and the rapid obsolescence of models as training datasets age. One proposed solution to mitigate these issues is model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;Yadav et al., 2024;Yu et al., 2024) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values. \n\nAdditionally, DARE introduces random pruning to align more closely with the base model's performance (Goddard et al., 2024). More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures.",
            "reference_string": "[271947337 | Pourreza et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 46,
            "citation_count": 50,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.07689",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.07689, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152953535",
                    "name": "Shashank Gupta"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "2043231778",
                    "name": "K. Subudhi"
                },
                {
                    "authorId": "2162804727",
                    "name": "Eduardo Gonzalez"
                },
                {
                    "authorId": "144430856",
                    "name": "Damien Jose"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ],
            "abstract": "Traditional multi-task learning (MTL) methods use dense networks that use the same set of shared weights across several different tasks. This often creates interference where two or more tasks compete to pull model parameters in different directions. In this work, we study whether sparsely activated Mixture-of-Experts (MoE) improve multi-task learning by specializing some weights for learning shared representations and using the others for learning task-specific information. To this end, we devise task-aware gating functions to route examples from different tasks to specialized experts which share subsets of network weights conditioned on the task. This results in a sparsely activated multi-task model with a large number of parameters, but with the same computational cost as that of a dense model. We demonstrate such sparse networks to improve multi-task learning along three key dimensions: (i) transfer to low-resource tasks from related tasks in the training mixture; (ii) sample-efficient generalization to tasks not seen during training by making use of task-aware routing from seen related tasks; (iii) robustness to the addition of unrelated tasks by avoiding catastrophic forgetting of existing tasks.",
            "corpus_id": 248227728,
            "sentences": [
                {
                    "corpus_id": "248227728",
                    "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners",
                    "text": "The traditional mechanism of using large-scale pre-trained language models PLMs (Devlin et al., 2019;He et al., 2021) involve fine-tuning them for each task individually. This approach fails to benefit from interactions between tasks that could be related to each other. For instance, the task of predicting if one text entails or contradicts another can benefit from tasks that predict whether two texts are semantically similar or not. To address these limitations, Multi-Task Learning (MTL) methods like MT-DNN (Liu et al., 2019) and Mup- * work done while at Microsoft. pet (Aghajanyan et al., 2021a) instead train a single model jointly on a multi-task mixture consisting of multiple tasks. The typical mechanism is to facilitate transfer between the tasks by encoding the examples using a task-agnostic network shared between all the tasks, and then using taskspecific layers on top to optimize individual task objectives. The dominant choice for the network is a Transformer-based PLM such as BERT (Devlin et al., 2019). However, such dense (fullyconnected) task-agnostic networks have the limitation that they use all the weights of the network for every example, including those coming from very different tasks. This creates interference among different tasks, e.g., the tug-of-war phenomenon (Hadsell et al., 2020) where two or more tasks pull the model parameters in different directions, thus impacting the multi-task learning performance. \n\nA possible mechanism to alleviate this problem is to devise a task-aware network that can capture specialized information about individual tasks, as well as information that can be shared among multiple tasks. Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;Lepikhin et al., 2021) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant.",
                    "score": 0.39214073077993916,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 437
                        },
                        {
                            "start": 438,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1027
                        },
                        {
                            "start": 1028,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1452
                        },
                        {
                            "start": 1455,
                            "end": 1664
                        },
                        {
                            "start": 1665,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 1974
                        },
                        {
                            "start": 1975,
                            "end": 2134
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 514,
                            "end": 532,
                            "matchedPaperCorpusId": "50770252"
                        },
                        {
                            "start": 1303,
                            "end": 1325,
                            "matchedPaperCorpusId": "226240885"
                        },
                        {
                            "start": 1741,
                            "end": 1763,
                            "matchedPaperCorpusId": "220265858"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70751953125
                }
            ],
            "relevance_judgement": 0.70751953125,
            "relevance_judgment_input_expanded": "# Title: Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners\n# Venue: arXiv.org\n# Authors: Shashank Gupta, Subhabrata Mukherjee, K. Subudhi, Eduardo Gonzalez, Damien Jose, A. Awadallah, Jianfeng Gao\n## Abstract\nTraditional multi-task learning (MTL) methods use dense networks that use the same set of shared weights across several different tasks. This often creates interference where two or more tasks compete to pull model parameters in different directions. In this work, we study whether sparsely activated Mixture-of-Experts (MoE) improve multi-task learning by specializing some weights for learning shared representations and using the others for learning task-specific information. To this end, we devise task-aware gating functions to route examples from different tasks to specialized experts which share subsets of network weights conditioned on the task. This results in a sparsely activated multi-task model with a large number of parameters, but with the same computational cost as that of a dense model. We demonstrate such sparse networks to improve multi-task learning along three key dimensions: (i) transfer to low-resource tasks from related tasks in the training mixture; (ii) sample-efficient generalization to tasks not seen during training by making use of task-aware routing from seen related tasks; (iii) robustness to the addition of unrelated tasks by avoiding catastrophic forgetting of existing tasks.\n## Introduction\nThe traditional mechanism of using large-scale pre-trained language models PLMs (Devlin et al., 2019;He et al., 2021) involve fine-tuning them for each task individually. This approach fails to benefit from interactions between tasks that could be related to each other. For instance, the task of predicting if one text entails or contradicts another can benefit from tasks that predict whether two texts are semantically similar or not. To address these limitations, Multi-Task Learning (MTL) methods like MT-DNN (Liu et al., 2019) and Mup- * work done while at Microsoft. pet (Aghajanyan et al., 2021a) instead train a single model jointly on a multi-task mixture consisting of multiple tasks. The typical mechanism is to facilitate transfer between the tasks by encoding the examples using a task-agnostic network shared between all the tasks, and then using taskspecific layers on top to optimize individual task objectives. The dominant choice for the network is a Transformer-based PLM such as BERT (Devlin et al., 2019). However, such dense (fullyconnected) task-agnostic networks have the limitation that they use all the weights of the network for every example, including those coming from very different tasks. This creates interference among different tasks, e.g., the tug-of-war phenomenon (Hadsell et al., 2020) where two or more tasks pull the model parameters in different directions, thus impacting the multi-task learning performance. \n\nA possible mechanism to alleviate this problem is to devise a task-aware network that can capture specialized information about individual tasks, as well as information that can be shared among multiple tasks. Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;Lepikhin et al., 2021) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant.",
            "reference_string": "[248227728 | Gupta et al. | 2022 | Citations: 50]"
        },
        {
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2276605422",
                    "name": "Bowei He"
                },
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "2257556686",
                    "name": "Linqi Song"
                }
            ],
            "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that 1bit-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.",
            "corpus_id": 276409347,
            "sentences": [
                {
                    "corpus_id": "276409347",
                    "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                    "text": "Large language models have achieved remarkable progress, demonstrating strong performance on a wide range of tasks (Touvron et al., 2023;Zhao et al., 2023). As researchers continue to fine-tune these models for specific domains, there is a growing need to combine their specialized capabilities into a single model (Yang et al., 2024;Goddard et al., 2024). While multi-task learning offers one solution (Sanh et al., 2022;Fifty et al., 2021), it \u2020 Corresponding author. requires extensive computational resources and simultaneous access to all task-specific datasets. Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining. \n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments. \n\nTo effectively balance performance and storage efficiency, we introduce 1bit-Merging, a novel dynamic merging framework that integrates taskspecific routing with 1-bit quantized task vectors.",
                    "score": 0.4428413926572353,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 859
                        },
                        {
                            "start": 862,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1306
                        },
                        {
                            "start": 1307,
                            "end": 1531
                        },
                        {
                            "start": 1532,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1913
                        },
                        {
                            "start": 1916,
                            "end": 2107
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 334,
                            "end": 355,
                            "matchedPaperCorpusId": "268537132"
                        },
                        {
                            "start": 422,
                            "end": 441,
                            "matchedPaperCorpusId": "237485414"
                        },
                        {
                            "start": 617,
                            "end": 640,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 640,
                            "end": 661,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 661,
                            "end": 680,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 680,
                            "end": 697,
                            "matchedPaperCorpusId": "265034087"
                        },
                        {
                            "start": 899,
                            "end": 921,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 921,
                            "end": 940,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 940,
                            "end": 957,
                            "matchedPaperCorpusId": "265034087"
                        },
                        {
                            "start": 1355,
                            "end": 1377,
                            "matchedPaperCorpusId": "259088823"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69580078125
                }
            ],
            "relevance_judgement": 0.69580078125,
            "relevance_judgment_input_expanded": "# Title: 1bit-Merging: Dynamic Quantized Merging for Large Language Models\n# Venue: arXiv.org\n# Authors: Shuqi Liu, Han Wu, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Linqi Song\n## Abstract\nRecent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that 1bit-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.\n## Introduction\nLarge language models have achieved remarkable progress, demonstrating strong performance on a wide range of tasks (Touvron et al., 2023;Zhao et al., 2023). As researchers continue to fine-tune these models for specific domains, there is a growing need to combine their specialized capabilities into a single model (Yang et al., 2024;Goddard et al., 2024). While multi-task learning offers one solution (Sanh et al., 2022;Fifty et al., 2021), it \u2020 Corresponding author. requires extensive computational resources and simultaneous access to all task-specific datasets. Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining. \n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments. \n\nTo effectively balance performance and storage efficiency, we introduce 1bit-Merging, a novel dynamic merging framework that integrates taskspecific routing with 1-bit quantized task vectors.",
            "reference_string": "[276409347 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Parameter Competition Balancing for Model Merging",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 24,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304551899",
                    "name": "Guodong Du"
                },
                {
                    "authorId": "2304707602",
                    "name": "Junlin Lee"
                },
                {
                    "authorId": "2307592281",
                    "name": "Jing Li"
                },
                {
                    "authorId": "2277489109",
                    "name": "Runhua Jiang"
                },
                {
                    "authorId": "2304744632",
                    "name": "Yifei Guo"
                },
                {
                    "authorId": "2304776697",
                    "name": "Shuyang Yu"
                },
                {
                    "authorId": "2307331727",
                    "name": "Hanting Liu"
                },
                {
                    "authorId": "2678283",
                    "name": "Sim Kuan Goh"
                },
                {
                    "authorId": "2277702989",
                    "name": "Ho-Kin Tang"
                },
                {
                    "authorId": "2325816146",
                    "name": "Daojing He"
                },
                {
                    "authorId": "2324304769",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
            "corpus_id": 273098230,
            "sentences": [
                {
                    "corpus_id": "273098230",
                    "title": "Parameter Competition Balancing for Model Merging",
                    "text": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
                    "score": 0.43639774709486284,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69287109375
                }
            ],
            "relevance_judgement": 0.69287109375,
            "relevance_judgment_input_expanded": "# Title: Parameter Competition Balancing for Model Merging\n# Venue: Neural Information Processing Systems\n# Authors: Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang\n## Abstract\nWhile fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.\n",
            "reference_string": "[273098230 | Du et al. | 2024 | Citations: 24]"
        },
        {
            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 145,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.13822, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2244619371",
                    "name": "Eric Nuertey Coleman"
                },
                {
                    "authorId": "2223689402",
                    "name": "Luigi Quarantiello"
                },
                {
                    "authorId": "2356575441",
                    "name": "Ziyue Liu"
                },
                {
                    "authorId": "2356596632",
                    "name": "Qinwen Yang"
                },
                {
                    "authorId": "2356499006",
                    "name": "Samrat Mukherjee"
                },
                {
                    "authorId": "2064859104",
                    "name": "J. Hurtado"
                },
                {
                    "authorId": "2285835294",
                    "name": "Vincenzo Lomonaco"
                }
            ],
            "abstract": "The emergence of large pre-trained networks has revolutionized the AI field, unlocking new possibilities and achieving unprecedented performance. However, these models inherit a fundamental limitation from traditional Machine Learning approaches: their strong dependence on the \\textit{i.i.d.} assumption hinders their adaptability to dynamic learning scenarios. We believe the next breakthrough in AI lies in enabling efficient adaptation to evolving environments -- such as the real world -- where new data and tasks arrive sequentially. This challenge defines the field of Continual Learning (CL), a Machine Learning paradigm focused on developing lifelong learning neural models. One alternative to efficiently adapt these large-scale models is known Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of adapting the model to a particular data or scenario by performing small and efficient modifications, achieving similar performance to full fine-tuning. However, these techniques still lack the ability to adjust the model to multiple tasks continually, as they suffer from the issue of Catastrophic Forgetting. In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine-Tuning (PECFT). We examine various approaches, discuss evaluation metrics, and explore potential future research directions. Our goal is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning, guide researchers in this field, and pave the way for novel future research directions.",
            "corpus_id": 277940324,
            "sentences": [
                {
                    "corpus_id": "277940324",
                    "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
                    "text": "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss. \n\nTypical model merging scenarios often require combining pre-existing expert models-each specialized in a specific task-into one unified system. However, this static approach falls short in scenarios where new tasks emerge over time. In continual learning, we face the challenge of incrementally integrating new task-specific models without retraining the entire system. Recent advances in dynamic model merging address this by tackling issues such as parameter interference, memory efficiency, and sequential integration, enabling systems that adapt more effectively as new tasks are encountered. For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios.",
                    "score": 0.4902337505669856,
                    "section_title": "Model Merging",
                    "char_start_offset": 74079,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 456
                        },
                        {
                            "start": 457,
                            "end": 679
                        },
                        {
                            "start": 680,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1231
                        },
                        {
                            "start": 1234,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1603
                        },
                        {
                            "start": 1604,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2123
                        },
                        {
                            "start": 2124,
                            "end": 2227
                        },
                        {
                            "start": 2228,
                            "end": 2345
                        },
                        {
                            "start": 2346,
                            "end": 2574
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69091796875
                }
            ],
            "relevance_judgement": 0.69091796875,
            "relevance_judgment_input_expanded": "# Title: Parameter-Efficient Continual Fine-Tuning: A Survey\n# Venue: arXiv.org\n# Authors: Eric Nuertey Coleman, Luigi Quarantiello, Ziyue Liu, Qinwen Yang, Samrat Mukherjee, J. Hurtado, Vincenzo Lomonaco\n## Abstract\nThe emergence of large pre-trained networks has revolutionized the AI field, unlocking new possibilities and achieving unprecedented performance. However, these models inherit a fundamental limitation from traditional Machine Learning approaches: their strong dependence on the \\textit{i.i.d.} assumption hinders their adaptability to dynamic learning scenarios. We believe the next breakthrough in AI lies in enabling efficient adaptation to evolving environments -- such as the real world -- where new data and tasks arrive sequentially. This challenge defines the field of Continual Learning (CL), a Machine Learning paradigm focused on developing lifelong learning neural models. One alternative to efficiently adapt these large-scale models is known Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of adapting the model to a particular data or scenario by performing small and efficient modifications, achieving similar performance to full fine-tuning. However, these techniques still lack the ability to adjust the model to multiple tasks continually, as they suffer from the issue of Catastrophic Forgetting. In this survey, we first provide an overview of CL algorithms and PEFT methods before reviewing the state-of-the-art on Parameter-Efficient Continual Fine-Tuning (PECFT). We examine various approaches, discuss evaluation metrics, and explore potential future research directions. Our goal is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning, guide researchers in this field, and pave the way for novel future research directions.\n## Model Merging\nModel merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss. \n\nTypical model merging scenarios often require combining pre-existing expert models-each specialized in a specific task-into one unified system. However, this static approach falls short in scenarios where new tasks emerge over time. In continual learning, we face the challenge of incrementally integrating new task-specific models without retraining the entire system. Recent advances in dynamic model merging address this by tackling issues such as parameter interference, memory efficiency, and sequential integration, enabling systems that adapt more effectively as new tasks are encountered. For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios.",
            "reference_string": "[277940324 | Coleman et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2241702793",
                    "name": "Qiuming Zhao"
                },
                {
                    "authorId": "2107310187",
                    "name": "Guangzhi Sun"
                },
                {
                    "authorId": "2256775692",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2241950375",
                    "name": "Mingxing Xu"
                },
                {
                    "authorId": "2241350908",
                    "name": "Thomas Fang Zheng"
                }
            ],
            "abstract": "Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.",
            "corpus_id": 276575632,
            "sentences": [
                {
                    "corpus_id": "276575632",
                    "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                    "text": "Model merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
                    "score": 0.4204377417244117,
                    "section_title": "Introduction",
                    "char_start_offset": 1979,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 716
                        },
                        {
                            "start": 719,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1282
                        },
                        {
                            "start": 1285,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1508
                        },
                        {
                            "start": 1511,
                            "end": 1823
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 436,
                            "end": 455,
                            "matchedPaperCorpusId": "53748459"
                        },
                        {
                            "start": 455,
                            "end": 475,
                            "matchedPaperCorpusId": "235790783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68603515625
                }
            ],
            "relevance_judgement": 0.68603515625,
            "relevance_judgment_input_expanded": "# Title: Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation\n# Venue: arXiv.org\n# Authors: Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng\n## Abstract\nLanguage diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.\n## Introduction\nModel merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
            "reference_string": "[276575632 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2276605422",
                    "name": "Bowei He"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "2257556686",
                    "name": "Linqi Song"
                }
            ],
            "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.",
            "corpus_id": 276422064,
            "sentences": [
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "The rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.",
                    "score": 0.48454864161783306,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 804
                        },
                        {
                            "start": 807,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1682
                        },
                        {
                            "start": 1685,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2040
                        },
                        {
                            "start": 2041,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2319
                        },
                        {
                            "start": 2320,
                            "end": 2451
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 349,
                            "end": 370,
                            "matchedPaperCorpusId": "268537132"
                        },
                        {
                            "start": 553,
                            "end": 572,
                            "matchedPaperCorpusId": "237485414"
                        },
                        {
                            "start": 767,
                            "end": 787,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 787,
                            "end": 803,
                            "matchedPaperCorpusId": "265034087"
                        },
                        {
                            "start": 1173,
                            "end": 1193,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1193,
                            "end": 1209,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68359375
                },
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.",
                    "score": 0.4499454454555756,
                    "section_title": "Introduction",
                    "char_start_offset": 2335,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 441
                        },
                        {
                            "start": 444,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 798
                        },
                        {
                            "start": 799,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1321
                        },
                        {
                            "start": 1324,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1631
                        },
                        {
                            "start": 1632,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 2102
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 230,
                            "end": 253,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 263,
                            "end": 280,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6650390625
                }
            ],
            "relevance_judgement": 0.68359375,
            "relevance_judgment_input_expanded": "# Title: Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models\n# Venue: arXiv.org\n# Authors: Shuqi Liu, Han Wu, Bowei He, Xiongwei Han, Mingxuan Yuan, Linqi Song\n## Abstract\nRecent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.\n## Introduction\nThe rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.\n...\nBy combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.",
            "reference_string": "[276422064 | Liu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.07089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310437466",
                    "name": "Ruochen Jin"
                },
                {
                    "authorId": "2248244711",
                    "name": "Bojian Hou"
                },
                {
                    "authorId": "2327839292",
                    "name": "Jiancong Xiao"
                },
                {
                    "authorId": "2311908363",
                    "name": "Weijie J. Su"
                },
                {
                    "authorId": "2248152254",
                    "name": "Li Shen"
                }
            ],
            "abstract": "In recent years, task arithmetic has garnered increasing attention. This approach edits pre-trained models directly in weight space by combining the fine-tuned weights of various tasks into a unified model. Its efficiency and cost-effectiveness stem from its training-free combination, contrasting with traditional methods that require model training on large datasets for multiple tasks. However, applying such a unified model to individual tasks can lead to interference from other tasks (lack of weight disentanglement). To address this issue, Neural Tangent Kernel (NTK) linearization has been employed to leverage a\"kernel behavior\", facilitating weight disentanglement and mitigating adverse effects from unrelated tasks. Despite its benefits, NTK linearization presents drawbacks, including doubled training costs, as well as reduced performance of individual models. To tackle this problem, we propose a simple yet effective and efficient method that is to finetune the attention modules only in the Transformer. Our study reveals that the attention modules exhibit kernel behavior, and fine-tuning the attention modules only significantly improves weight disentanglement. To further understand how our method improves the weight disentanglement of task arithmetic, we present a comprehensive study of task arithmetic by differentiating the role of the representation module and task-specific module. In particular, we find that the representation module plays an important role in improving weight disentanglement whereas the task-specific modules such as the classification heads can degenerate the weight disentanglement performance. (The code is available at https://github.com/kyrie-23/task_arithmetic_tangent)",
            "corpus_id": 271064761,
            "sentences": [
                {
                    "corpus_id": "271064761",
                    "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic",
                    "text": "Existing model merging techniques can be broadly categorized into two main types (Yang et al., 2024): (i) Pre-Merging Methods: These methods focus on enhancing the conditions necessary for effective model merging by optimizing the fine-tuning process of individual models. (ii) During Merging Methods: These approaches address task conflicts and interference through various strategies before executing the parameter merging operations. \n\nPre-Merging Methods. To establish better conditions for model merging, a significant body of work has focused on the fine-tuning processes of independent models. For instance, Ortiz-Jimenez et al. ( 2024) propose fine-tuning within the tangent space of the pre-trained model, leveraging the NTK framework to enhance performance during the fine-tuning stage. However, fine-tuning all parameters in the linearized model can be computationally intensive compared to nonlinear finetuning. To mitigate this issue, some studies advocate for selectively linearizing only certain layers; for example, Tang et al. (2023) suggest partially linearizing Adapter modules prior to merging them. Additionally, TAFT (Liu et al., 2024) introduces an efficient linearization method specifically for Transformer architectures, deriving closed-form linearized solutions that facilitate smoother integration of models. Overall, fine-tuning in the tangent space aids in disentangling both input and weight spaces, thereby reducing potential interference during subsequent model merging. \n\nDuring Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018;Ilharco et al., 2023;Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023).",
                    "score": 0.4975283969804201,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 23891,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 436
                        },
                        {
                            "start": 439,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1503
                        },
                        {
                            "start": 1506,
                            "end": 1529
                        },
                        {
                            "start": 1530,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2308
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1032,
                            "end": 1050,
                            "matchedPaperCorpusId": "263831551"
                        },
                        {
                            "start": 1862,
                            "end": 1884,
                            "matchedPaperCorpusId": "4055784"
                        },
                        {
                            "start": 1884,
                            "end": 1905,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1905,
                            "end": 1927,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6806640625
                }
            ],
            "relevance_judgement": 0.6806640625,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic\n# Venue: International Conference on Learning Representations\n# Authors: Ruochen Jin, Bojian Hou, Jiancong Xiao, Weijie J. Su, Li Shen\n## Abstract\nIn recent years, task arithmetic has garnered increasing attention. This approach edits pre-trained models directly in weight space by combining the fine-tuned weights of various tasks into a unified model. Its efficiency and cost-effectiveness stem from its training-free combination, contrasting with traditional methods that require model training on large datasets for multiple tasks. However, applying such a unified model to individual tasks can lead to interference from other tasks (lack of weight disentanglement). To address this issue, Neural Tangent Kernel (NTK) linearization has been employed to leverage a\"kernel behavior\", facilitating weight disentanglement and mitigating adverse effects from unrelated tasks. Despite its benefits, NTK linearization presents drawbacks, including doubled training costs, as well as reduced performance of individual models. To tackle this problem, we propose a simple yet effective and efficient method that is to finetune the attention modules only in the Transformer. Our study reveals that the attention modules exhibit kernel behavior, and fine-tuning the attention modules only significantly improves weight disentanglement. To further understand how our method improves the weight disentanglement of task arithmetic, we present a comprehensive study of task arithmetic by differentiating the role of the representation module and task-specific module. In particular, we find that the representation module plays an important role in improving weight disentanglement whereas the task-specific modules such as the classification heads can degenerate the weight disentanglement performance. (The code is available at https://github.com/kyrie-23/task_arithmetic_tangent)\n## RELATED WORK\nExisting model merging techniques can be broadly categorized into two main types (Yang et al., 2024): (i) Pre-Merging Methods: These methods focus on enhancing the conditions necessary for effective model merging by optimizing the fine-tuning process of individual models. (ii) During Merging Methods: These approaches address task conflicts and interference through various strategies before executing the parameter merging operations. \n\nPre-Merging Methods. To establish better conditions for model merging, a significant body of work has focused on the fine-tuning processes of independent models. For instance, Ortiz-Jimenez et al. ( 2024) propose fine-tuning within the tangent space of the pre-trained model, leveraging the NTK framework to enhance performance during the fine-tuning stage. However, fine-tuning all parameters in the linearized model can be computationally intensive compared to nonlinear finetuning. To mitigate this issue, some studies advocate for selectively linearizing only certain layers; for example, Tang et al. (2023) suggest partially linearizing Adapter modules prior to merging them. Additionally, TAFT (Liu et al., 2024) introduces an efficient linearization method specifically for Transformer architectures, deriving closed-form linearized solutions that facilitate smoother integration of models. Overall, fine-tuning in the tangent space aids in disentangling both input and weight spaces, thereby reducing potential interference during subsequent model merging. \n\nDuring Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018;Ilharco et al., 2023;Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023).",
            "reference_string": "[271064761 | Jin et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 90,
            "citation_count": 25,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.13656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182670937",
                    "name": "Yifei He"
                },
                {
                    "authorId": "2317078449",
                    "name": "Yuzheng Hu"
                },
                {
                    "authorId": "2292270783",
                    "name": "Yong Lin"
                },
                {
                    "authorId": "2306841244",
                    "name": "Tong Zhang"
                },
                {
                    "authorId": "2283183420",
                    "name": "Han Zhao"
                }
            ],
            "abstract": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.",
            "corpus_id": 271957310,
            "sentences": [
                {
                    "corpus_id": "271957310",
                    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                    "text": "Model merging. Model merging aims at efficiently integrating multiple finetuned models into a single model that retains the capabilities of each. This approach enhances the efficiency, generalization and multi-task capabilities of finetuned models. In scenarios where models are trained on the same task with different training configurations, Singh & Jaggi (2020); Ainsworth et al. (2022);Jolicoeur-Martineau et al. (2024) show that merged models perform comparably to ensemble models but with significantly lower deployment costs. Additionally, Wortsman et al. (2022a;b) demonstrate that the merged model improves the out-of-distribution (OOD) robustness. When merging finetuned models from different tasks, the merged model can provide better initialization for new tasks (Choshen et al., 2022;Gueta et al., 2023). Finetuned models with different specialized skills can also be combined to enhance multi-task capabilities (Ilharco et al., 2023;Tam et al., 2023;Matena & Raffel, 2022;Jin et al., 2022;Yang et al., 2023;Yu et al., 2023;Wang et al., 2024b;a). More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020;Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem. \n\nOur approach stands out with four key advantages: i) Localized merging: Instead of global merging, we localize merging to specific regions with finetuned skills, effectively decreasing task conflicts. ii) Simplified process: Existing works often require computationally intensive grid search or optimization to determine the optimal merging coefficients, while our stitching procedure does not have the requirement.",
                    "score": 0.508408085680556,
                    "section_title": "Related works",
                    "char_start_offset": 30230,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 14
                        },
                        {
                            "start": 15,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 657
                        },
                        {
                            "start": 658,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1319
                        },
                        {
                            "start": 1320,
                            "end": 1448
                        },
                        {
                            "start": 1449,
                            "end": 1673
                        },
                        {
                            "start": 1676,
                            "end": 1876
                        },
                        {
                            "start": 1877,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 390,
                            "end": 423,
                            "matchedPaperCorpusId": "257984998"
                        },
                        {
                            "start": 547,
                            "end": 570,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 925,
                            "end": 947,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 964,
                            "end": 986,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1037,
                            "end": 1056,
                            "matchedPaperCorpusId": "269757600"
                        },
                        {
                            "start": 1490,
                            "end": 1507,
                            "matchedPaperCorpusId": "210839011"
                        },
                        {
                            "start": 1507,
                            "end": 1524,
                            "matchedPaperCorpusId": "239998731"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67431640625
                }
            ],
            "relevance_judgement": 0.67431640625,
            "relevance_judgment_input_expanded": "# Title: Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Yifei He, Yuzheng Hu, Yong Lin, Tong Zhang, Han Zhao\n## Abstract\nModel merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.\n## Related works\nModel merging. Model merging aims at efficiently integrating multiple finetuned models into a single model that retains the capabilities of each. This approach enhances the efficiency, generalization and multi-task capabilities of finetuned models. In scenarios where models are trained on the same task with different training configurations, Singh & Jaggi (2020); Ainsworth et al. (2022);Jolicoeur-Martineau et al. (2024) show that merged models perform comparably to ensemble models but with significantly lower deployment costs. Additionally, Wortsman et al. (2022a;b) demonstrate that the merged model improves the out-of-distribution (OOD) robustness. When merging finetuned models from different tasks, the merged model can provide better initialization for new tasks (Choshen et al., 2022;Gueta et al., 2023). Finetuned models with different specialized skills can also be combined to enhance multi-task capabilities (Ilharco et al., 2023;Tam et al., 2023;Matena & Raffel, 2022;Jin et al., 2022;Yang et al., 2023;Yu et al., 2023;Wang et al., 2024b;a). More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020;Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem. \n\nOur approach stands out with four key advantages: i) Localized merging: Instead of global merging, we localize merging to specific regions with finetuned skills, effectively decreasing task conflicts. ii) Simplified process: Existing works often require computationally intensive grid search or optimization to determine the optimal merging coefficients, while our stitching procedure does not have the requirement.",
            "reference_string": "[271957310 | He et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Merging by Matching Models in Task Parameter Subspaces",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1390031652",
                    "name": "Derek Tam"
                },
                {
                    "authorId": "2253762115",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Model merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task parameter subspace'' in which models are matched before being merged. We connect the task parameter subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task parameter subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Parameter Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats.",
            "corpus_id": 266053657,
            "sentences": [
                {
                    "corpus_id": "266053657",
                    "title": "Merging by Matching Models in Task Parameter Subspaces",
                    "text": "The widespread fine-tuning of public pre-trained models has produced a huge number of specialized models. These specialized models may be trained on different tasks, where a \"task\" is simply the input-output relationship that we aim to train a model to perform (e.g. sentiment analysis of text, object recognition in images, etc.). Alternatively, the Stable Diffusion XL model (Podell et al., 2023) forms the basis of over a thousand specialized image generation models on the Hugging Face Model Hub that are specialized to different styles or content types. How can we recycle these specialized models to create better base models (Choshen et al., 2022;Ram\u00e9 et al., 2022)? Model merging (Wortsman et al., 2022b;Matena & Raffel, 2022) aims to tackle this problem by combining specialized models into a single model that retains the individual models' capabilities. A common example application of merging is constructing a multitask model from individual-task models, which is the primary application we explore in our paper. Compared to multitask learning, merging does not require simultaneous access to the individual-task datasets. Compared to outputspace ensembling of M models, merging produces a model that is M times cheaper to run. \n\nWhile merging via simple parameter averaging can work well for models that share an architecture and initialization (McMahan et al., 2017;Stich, 2018), recent merging methods improve over simple averaging by considering parameter importance (Matena & Raffel, 2022), matching activations (Jin et al., 2022), omitting the contribution of the pre-trained model (Ilharco et al., 2022), or resolving interference across models (Yadav et al., 2023). \n\nIn our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task.",
                    "score": 0.39037893526278894,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 331
                        },
                        {
                            "start": 332,
                            "end": 558
                        },
                        {
                            "start": 559,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1240
                        },
                        {
                            "start": 1243,
                            "end": 1686
                        },
                        {
                            "start": 1689,
                            "end": 1866
                        },
                        {
                            "start": 1867,
                            "end": 2045
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 688,
                            "end": 712,
                            "matchedPaperCorpusId": "237420687"
                        },
                        {
                            "start": 712,
                            "end": 734,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1359,
                            "end": 1381,
                            "matchedPaperCorpusId": "14955348"
                        },
                        {
                            "start": 1484,
                            "end": 1507,
                            "matchedPaperCorpusId": "244345933"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6708984375
                }
            ],
            "relevance_judgement": 0.6708984375,
            "relevance_judgment_input_expanded": "# Title: Merging by Matching Models in Task Parameter Subspaces\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Derek Tam, Mohit Bansal, Colin Raffel\n## Abstract\nModel merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task parameter subspace'' in which models are matched before being merged. We connect the task parameter subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task parameter subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Parameter Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats.\n## Introduction\nThe widespread fine-tuning of public pre-trained models has produced a huge number of specialized models. These specialized models may be trained on different tasks, where a \"task\" is simply the input-output relationship that we aim to train a model to perform (e.g. sentiment analysis of text, object recognition in images, etc.). Alternatively, the Stable Diffusion XL model (Podell et al., 2023) forms the basis of over a thousand specialized image generation models on the Hugging Face Model Hub that are specialized to different styles or content types. How can we recycle these specialized models to create better base models (Choshen et al., 2022;Ram\u00e9 et al., 2022)? Model merging (Wortsman et al., 2022b;Matena & Raffel, 2022) aims to tackle this problem by combining specialized models into a single model that retains the individual models' capabilities. A common example application of merging is constructing a multitask model from individual-task models, which is the primary application we explore in our paper. Compared to multitask learning, merging does not require simultaneous access to the individual-task datasets. Compared to outputspace ensembling of M models, merging produces a model that is M times cheaper to run. \n\nWhile merging via simple parameter averaging can work well for models that share an architecture and initialization (McMahan et al., 2017;Stich, 2018), recent merging methods improve over simple averaging by considering parameter importance (Matena & Raffel, 2022), matching activations (Jin et al., 2022), omitting the contribution of the pre-trained model (Ilharco et al., 2022), or resolving interference across models (Yadav et al., 2023). \n\nIn our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task.",
            "reference_string": "[266053657 | Tam et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08371, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325729401",
                    "name": "Thomas Gauthier-Caron"
                },
                {
                    "authorId": "51516859",
                    "name": "Shamane Siriwardhana"
                },
                {
                    "authorId": "2325730893",
                    "name": "Elliot Stein"
                },
                {
                    "authorId": "2175482685",
                    "name": "Malikeh Ehghaghi"
                },
                {
                    "authorId": "2292260669",
                    "name": "Charles Goddard"
                },
                {
                    "authorId": "2292260070",
                    "name": "Mark McQuade"
                },
                {
                    "authorId": "2047397646",
                    "name": "Jacob Solawetz"
                },
                {
                    "authorId": "2325729518",
                    "name": "Maxime Labonne"
                }
            ],
            "abstract": "By merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.",
            "corpus_id": 273323680,
            "sentences": [
                {
                    "corpus_id": "273323680",
                    "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
                    "text": "As the demand for versatile and powerful AI systems grows, the need to merge Large Language Models (LLMs) with specialized capabilities, such as multilingual skills or domain-specific knowledge, has become increasingly pressing. Effective model merging enables systems to leverage the unique strengths of individual models without necessitating extensive retraining. Merging also offers the potential to reduce catastrophic forgetting, a significant advantage in maintaining learned knowledge from each model (Sukhbaatar et al., 2024;Siriwardhana et al., 2024;Labrak et al., 2024). However, model merging remains inherently complex due to differences in training and fine-tuning processes, often requiring deep expertise and iterative tuning to achieve a balanced integration of the models' contributions. \n\nModel merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2024) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale. To gain deeper insight into the strengths and weaknesses of these approaches, we performed an in-depth comparative analysis of model merging techniques, spanning from basic averaging methods to more sophisticated automated approaches. \n\nBuilding on these insights, we introduce Differentiable Adaptive Merging (DAM), a new approach developed as a more efficient alternative to compute-heavy evolutionary strategies.",
                    "score": 0.4455569868585606,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 228
                        },
                        {
                            "start": 229,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 805
                        },
                        {
                            "start": 808,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1317
                        },
                        {
                            "start": 1320,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1797
                        },
                        {
                            "start": 1798,
                            "end": 2032
                        },
                        {
                            "start": 2035,
                            "end": 2213
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1018,
                            "end": 1041,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1085,
                            "end": 1105,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6640625
                }
            ],
            "relevance_judgement": 0.6640625,
            "relevance_judgment_input_expanded": "# Title: Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation\n# Venue: arXiv.org\n# Authors: Thomas Gauthier-Caron, Shamane Siriwardhana, Elliot Stein, Malikeh Ehghaghi, Charles Goddard, Mark McQuade, Jacob Solawetz, Maxime Labonne\n## Abstract\nBy merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.\n## Introduction\nAs the demand for versatile and powerful AI systems grows, the need to merge Large Language Models (LLMs) with specialized capabilities, such as multilingual skills or domain-specific knowledge, has become increasingly pressing. Effective model merging enables systems to leverage the unique strengths of individual models without necessitating extensive retraining. Merging also offers the potential to reduce catastrophic forgetting, a significant advantage in maintaining learned knowledge from each model (Sukhbaatar et al., 2024;Siriwardhana et al., 2024;Labrak et al., 2024). However, model merging remains inherently complex due to differences in training and fine-tuning processes, often requiring deep expertise and iterative tuning to achieve a balanced integration of the models' contributions. \n\nModel merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2024) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale. To gain deeper insight into the strengths and weaknesses of these approaches, we performed an in-depth comparative analysis of model merging techniques, spanning from basic averaging methods to more sophisticated automated approaches. \n\nBuilding on these insights, we introduce Differentiable Adaptive Merging (DAM), a new approach developed as a more efficient alternative to compute-heavy evolutionary strategies.",
            "reference_string": "[273323680 | Gauthier-Caron et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 64,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05357, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290239259",
                    "name": "Xinyu Zhao"
                },
                {
                    "authorId": "2299920928",
                    "name": "Guoheng Sun"
                },
                {
                    "authorId": "2209882676",
                    "name": "Ruisi Cai"
                },
                {
                    "authorId": "2325110987",
                    "name": "Yukun Zhou"
                },
                {
                    "authorId": "2253560631",
                    "name": "Pingzhi Li"
                },
                {
                    "authorId": "2118952622",
                    "name": "Peihao Wang"
                },
                {
                    "authorId": "2325098801",
                    "name": "Bowen Tan"
                },
                {
                    "authorId": "2299323303",
                    "name": "Yexiao He"
                },
                {
                    "authorId": "2325398358",
                    "name": "Li Chen"
                },
                {
                    "authorId": "2269703508",
                    "name": "Yingbin Liang"
                },
                {
                    "authorId": "2249538643",
                    "name": "Beidi Chen"
                },
                {
                    "authorId": "2325089272",
                    "name": "Binhang Yuan"
                },
                {
                    "authorId": "2319177861",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "2258555606",
                    "name": "Ang Li"
                },
                {
                    "authorId": "2307032205",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2290219041",
                    "name": "Tianlong Chen"
                }
            ],
            "abstract": "As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has garnered significant attention, which faces the challenge of decreasing performance when combining disparate models. Various techniques have been proposed for the aggregation of pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate an optimal strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization.Our methodology involves the clustering of mergeable models and optimal merging strategy selection, and the integration of clusters through a model mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at: https://github.com/Model-GLUE/Model-GLUE.",
            "corpus_id": 273228210,
            "sentences": [
                {
                    "corpus_id": "273228210",
                    "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
                    "text": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49,24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization. Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54,55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes.",
                    "score": 0.4574449156473954,
                    "section_title": "Related Works",
                    "char_start_offset": 5095,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 14
                        },
                        {
                            "start": 15,
                            "end": 101
                        },
                        {
                            "start": 102,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 515
                        },
                        {
                            "start": 516,
                            "end": 623
                        },
                        {
                            "start": 624,
                            "end": 775
                        },
                        {
                            "start": 776,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 985
                        },
                        {
                            "start": 986,
                            "end": 1085
                        },
                        {
                            "start": 1086,
                            "end": 1222
                        },
                        {
                            "start": 1223,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1391
                        },
                        {
                            "start": 1392,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 1962
                        },
                        {
                            "start": 1963,
                            "end": 2123
                        },
                        {
                            "start": 2124,
                            "end": 2248
                        },
                        {
                            "start": 2249,
                            "end": 2344
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 507,
                            "end": 511,
                            "matchedPaperCorpusId": "11290566"
                        },
                        {
                            "start": 1855,
                            "end": 1859,
                            "matchedPaperCorpusId": "267061245"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6611328125
                }
            ],
            "relevance_judgement": 0.6611328125,
            "relevance_judgment_input_expanded": "# Title: Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild\n# Venue: Neural Information Processing Systems\n# Authors: Xinyu Zhao, Guoheng Sun, Ruisi Cai, Yukun Zhou, Pingzhi Li, Peihao Wang, Bowen Tan, Yexiao He, Li Chen, Yingbin Liang, Beidi Chen, Binhang Yuan, Hongyi Wang, Ang Li, Zhangyang Wang, Tianlong Chen\n## Abstract\nAs Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has garnered significant attention, which faces the challenge of decreasing performance when combining disparate models. Various techniques have been proposed for the aggregation of pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed. In light of this research gap, this paper introduces Model-GLUE, a holistic LLM scaling guideline. First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. Utilizing the insights from the benchmark results, we formulate an optimal strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization.Our methodology involves the clustering of mergeable models and optimal merging strategy selection, and the integration of clusters through a model mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, Model-GLUE shows an average performance enhancement of 5.61%, achieved without additional training. Codes are available at: https://github.com/Model-GLUE/Model-GLUE.\n## Related Works\nModel Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49,24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization. Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54,55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes.",
            "reference_string": "[273228210 | Zhao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261661979",
                    "name": "Yujie Feng"
                },
                {
                    "authorId": "2347170204",
                    "name": "Xujia Wang"
                },
                {
                    "authorId": "2220034673",
                    "name": "Zexin Lu"
                },
                {
                    "authorId": "2347197165",
                    "name": "Shenghong Fu"
                },
                {
                    "authorId": "144218801",
                    "name": "Guangyuan Shi"
                },
                {
                    "authorId": "2215472732",
                    "name": "Yongxin Xu"
                },
                {
                    "authorId": "2253831765",
                    "name": "Yasha Wang"
                },
                {
                    "authorId": "2348327959",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "2315809919",
                    "name": "Xu Chu"
                },
                {
                    "authorId": "2261456593",
                    "name": "Xiao-Ming Wu"
                }
            ],
            "abstract": "Continual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.",
            "corpus_id": 276580914,
            "sentences": [
                {
                    "corpus_id": "276580914",
                    "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
                    "text": "Incorporating continual learning (CL) capability into large language models (LLMs) is essential for enabling them to acquire knowledge from diverse tasks sequentially, a critical requirement for adapting to ever-changing environments without extensive retraining (Wang et al., 2024b;Jiang et al., 2024;Yu et al., 2024;Chang et al., 2024). An effective CL system must address two key challenges: (1) Catastrophic Forgetting (CF) (McCloskey and Cohen, 1989), where previously acquired knowledge is lost when learning new tasks, and (2) Knowledge Transfer (KT) (Ke et al., 2021), which involves leveraging new, related tasks to improve performance on prior tasks, and vice versa. \n\nRecently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023;Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024;Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024;Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024).",
                    "score": 0.4708736364305526,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 676
                        },
                        {
                            "start": 679,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1440
                        },
                        {
                            "start": 1443,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1804
                        },
                        {
                            "start": 1805,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 263,
                            "end": 283,
                            "matchedPaperCorpusId": "256459333"
                        },
                        {
                            "start": 318,
                            "end": 336,
                            "matchedPaperCorpusId": "259360395"
                        },
                        {
                            "start": 428,
                            "end": 455,
                            "matchedPaperCorpusId": "61019113"
                        },
                        {
                            "start": 558,
                            "end": 575,
                            "matchedPaperCorpusId": "244908578"
                        },
                        {
                            "start": 770,
                            "end": 789,
                            "matchedPaperCorpusId": "258833488"
                        },
                        {
                            "start": 1201,
                            "end": 1219,
                            "matchedPaperCorpusId": "270703371"
                        },
                        {
                            "start": 1504,
                            "end": 1522,
                            "matchedPaperCorpusId": "271915471"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6591796875
                }
            ],
            "relevance_judgement": 0.6591796875,
            "relevance_judgment_input_expanded": "# Title: Recurrent Knowledge Identification and Fusion for Language Model Continual Learning\n# Venue: arXiv.org\n# Authors: Yujie Feng, Xujia Wang, Zexin Lu, Shenghong Fu, Guangyuan Shi, Yongxin Xu, Yasha Wang, Philip S. Yu, Xu Chu, Xiao-Ming Wu\n## Abstract\nContinual learning (CL) is crucial for deploying large language models (LLMs) in dynamic real-world environments without costly retraining. While recent model ensemble and model merging methods guided by parameter importance have gained popularity, they often struggle to balance knowledge transfer and forgetting, mainly due to the reliance on static importance estimates during sequential training. In this paper, we present Recurrent-KIF, a novel CL framework for Recurrent Knowledge Identification and Fusion, which enables dynamic estimation of parameter importance distributions to enhance knowledge transfer. Inspired by human continual learning, Recurrent-KIF employs an inner loop that rapidly adapts to new tasks while identifying important parameters, coupled with an outer loop that globally manages the fusion of new and historical knowledge through redundant knowledge pruning and key knowledge merging. These inner-outer loops iteratively perform multiple rounds of fusion, allowing Recurrent-KIF to leverage intermediate training information and adaptively adjust fusion strategies based on evolving importance distributions. Extensive experiments on two CL benchmarks with various model sizes (from 770M to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic forgetting and enhances knowledge transfer.\n## Introduction\nIncorporating continual learning (CL) capability into large language models (LLMs) is essential for enabling them to acquire knowledge from diverse tasks sequentially, a critical requirement for adapting to ever-changing environments without extensive retraining (Wang et al., 2024b;Jiang et al., 2024;Yu et al., 2024;Chang et al., 2024). An effective CL system must address two key challenges: (1) Catastrophic Forgetting (CF) (McCloskey and Cohen, 1989), where previously acquired knowledge is lost when learning new tasks, and (2) Knowledge Transfer (KT) (Ke et al., 2021), which involves leveraging new, related tasks to improve performance on prior tasks, and vice versa. \n\nRecently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023;Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024;Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024;Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024).",
            "reference_string": "[276580914 | Feng et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "venue": "",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.19098, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305817448",
                    "name": "Aecheon Jung"
                },
                {
                    "authorId": "2261794770",
                    "name": "Seunghwan Lee"
                },
                {
                    "authorId": "2338395404",
                    "name": "Dongyoon Han"
                },
                {
                    "authorId": "2261902726",
                    "name": "Sungeun Hong"
                }
            ],
            "abstract": "Model merging integrates independently fine-tuned models into a single multi-task model, offering a flexible alternative to joint training. However, many existing model merging methods introduce additional task-specific components, increasing complexity and requiring extra modifications. We propose Model Tinting, a lightweight yet highly effective approach that improves model merging by updating just a single layer, accounting for as low as 0.5% of total parameters. Our key observation is that explicit task-specific modules are not necessary; instead, subtle adjustments to a single layer can effectively capture task-specific variations within the merged model while maintaining generalization. We introduce a confidence-based filtering mechanism to alleviate the impact of unreliable predictions from individual models on the merged model. Extensive experiments across vision and NLP tasks demonstrate that Model Tinting achieves state-of-the-art performance, even in challenging dense prediction tasks. Our code is available at https://github.com/AIM-SKKU/ModelTinting",
            "corpus_id": 275119334,
            "sentences": [
                {
                    "corpus_id": "275119334",
                    "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
                    "text": "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge [4,60,78]. However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference [37,40,50]. Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts [6,76]. Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating [17,20,28,34,51]. Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework [13,25,70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework.",
                    "score": 0.5228712519382425,
                    "section_title": "Multi-task Learning",
                    "char_start_offset": 5007,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 892
                        },
                        {
                            "start": 893,
                            "end": 1063
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 116,
                            "matchedPaperCorpusId": "45998148"
                        },
                        {
                            "start": 116,
                            "end": 119,
                            "matchedPaperCorpusId": "221771219"
                        },
                        {
                            "start": 119,
                            "end": 122,
                            "matchedPaperCorpusId": "235790783"
                        },
                        {
                            "start": 357,
                            "end": 361,
                            "matchedPaperCorpusId": "50770252"
                        },
                        {
                            "start": 361,
                            "end": 364,
                            "matchedPaperCorpusId": "1923223"
                        },
                        {
                            "start": 364,
                            "end": 367,
                            "matchedPaperCorpusId": "22014305"
                        },
                        {
                            "start": 480,
                            "end": 483,
                            "matchedPaperCorpusId": "4703661"
                        },
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "210839011"
                        },
                        {
                            "start": 617,
                            "end": 621,
                            "matchedPaperCorpusId": "52952193"
                        },
                        {
                            "start": 621,
                            "end": 624,
                            "matchedPaperCorpusId": "261243229"
                        },
                        {
                            "start": 624,
                            "end": 627,
                            "matchedPaperCorpusId": "4800342"
                        },
                        {
                            "start": 627,
                            "end": 630,
                            "matchedPaperCorpusId": "4389348"
                        },
                        {
                            "start": 630,
                            "end": 633,
                            "matchedPaperCorpusId": "52957972"
                        },
                        {
                            "start": 751,
                            "end": 755,
                            "matchedPaperCorpusId": "237291521"
                        },
                        {
                            "start": 755,
                            "end": 758,
                            "matchedPaperCorpusId": "256658804"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.658203125
                }
            ],
            "relevance_judgement": 0.658203125,
            "relevance_judgment_input_expanded": "# Title: Why Train Everything? Tint a Single Layer for Multi-task Model Merging\n# Venue: \n# Authors: Aecheon Jung, Seunghwan Lee, Dongyoon Han, Sungeun Hong\n## Abstract\nModel merging integrates independently fine-tuned models into a single multi-task model, offering a flexible alternative to joint training. However, many existing model merging methods introduce additional task-specific components, increasing complexity and requiring extra modifications. We propose Model Tinting, a lightweight yet highly effective approach that improves model merging by updating just a single layer, accounting for as low as 0.5% of total parameters. Our key observation is that explicit task-specific modules are not necessary; instead, subtle adjustments to a single layer can effectively capture task-specific variations within the merged model while maintaining generalization. We introduce a confidence-based filtering mechanism to alleviate the impact of unreliable predictions from individual models on the merged model. Extensive experiments across vision and NLP tasks demonstrate that Model Tinting achieves state-of-the-art performance, even in challenging dense prediction tasks. Our code is available at https://github.com/AIM-SKKU/ModelTinting\n## Multi-task Learning\nMulti-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge [4,60,78]. However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference [37,40,50]. Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts [6,76]. Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating [17,20,28,34,51]. Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework [13,25,70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework.",
            "reference_string": "[275119334 | Jung et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 241,
            "citation_count": 92,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.07666, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2279871211",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2237427680",
                    "name": "Guibing Guo"
                },
                {
                    "authorId": "2237603955",
                    "name": "Xingwei Wang"
                },
                {
                    "authorId": "2316150631",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "2316176138",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "2135519749",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
            "corpus_id": 271865581,
            "sentences": [
                {
                    "corpus_id": "271865581",
                    "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
                    "text": "As shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models. For example, model merging in large language models can help mitigate untruthfulness and toxicity output, accomplish knowledge unlearning, and speed up training. Moreover, model merging also arises in different machine learning subfields, such as continual learning, multi-task/multi-domain learning, few-shot learning, and other subfields, to solve a variety of challenges. For instance, in continual learning, model merging can mitigate catastrophic forgetting of old tasks. In multi-task learning, multi-objective learning and multidomain learning, it facilitates knowledge transfer. Additionally, in adversarial learning, model merging can be employed for both attack and defense strategies. \n\nThird, what are the remaining challenges and future research opportunities for model merging? Despite the advancements in merging methods and their well-developed applications, there are still numerous open challenges and future research directions in the field ( \u00a75). For example, as the number of tasks increases, the performance gap between existing methods and independent expert models becomes signifi-Figure 2: The taxonomy of model merging in machine learning. This general framework covers advanced model merging methods and theories (top part), as well as practical applications of model merging techniques to foundation models and more than 10 machine learning subfields (bottom part). cantly larger. Additionally, current model merging methods incur enormous memory costs during merging and lack trust guarantees as well as in-depth theoretical analysis. Addressing these gaps will require substantial efforts from researchers to further advance the flourishing development of this field. \n\nTo summarize, the main contributions of this paper include the following three aspects: \n\n\u2022 Methodology Overview: We provide a comprehensive summary of the technical aspects of model merging. Specifically, we propose a new taxonomy that divides existing model merging methods into two stages and further subdivides the methods in each stage according to key techniques. Additionally, we discuss theoretical analysis work related to model merging. \n\n\u2022 Application Overview: We offer a comprehensive summary of the application aspects of model merging. Specifically, we explore the application of model merging to foundation models and 10+ machine learning subfields, demonstrating how model merging can address existing challenges in these areas.",
                    "score": 0.5610532141493767,
                    "section_title": "Introduction",
                    "char_start_offset": 4479,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 357
                        },
                        {
                            "start": 358,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 891
                        },
                        {
                            "start": 894,
                            "end": 987
                        },
                        {
                            "start": 988,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1589
                        },
                        {
                            "start": 1590,
                            "end": 1604
                        },
                        {
                            "start": 1605,
                            "end": 1759
                        },
                        {
                            "start": 1760,
                            "end": 1893
                        },
                        {
                            "start": 1896,
                            "end": 1983
                        },
                        {
                            "start": 1986,
                            "end": 2087
                        },
                        {
                            "start": 2088,
                            "end": 2265
                        },
                        {
                            "start": 2266,
                            "end": 2342
                        },
                        {
                            "start": 2345,
                            "end": 2446
                        },
                        {
                            "start": 2447,
                            "end": 2641
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65673828125
                }
            ],
            "relevance_judgement": 0.65673828125,
            "relevance_judgment_input_expanded": "# Title: Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities\n# Venue: arXiv.org\n# Authors: Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao\n## Abstract\nModel merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.\n## Introduction\nAs shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models. For example, model merging in large language models can help mitigate untruthfulness and toxicity output, accomplish knowledge unlearning, and speed up training. Moreover, model merging also arises in different machine learning subfields, such as continual learning, multi-task/multi-domain learning, few-shot learning, and other subfields, to solve a variety of challenges. For instance, in continual learning, model merging can mitigate catastrophic forgetting of old tasks. In multi-task learning, multi-objective learning and multidomain learning, it facilitates knowledge transfer. Additionally, in adversarial learning, model merging can be employed for both attack and defense strategies. \n\nThird, what are the remaining challenges and future research opportunities for model merging? Despite the advancements in merging methods and their well-developed applications, there are still numerous open challenges and future research directions in the field ( \u00a75). For example, as the number of tasks increases, the performance gap between existing methods and independent expert models becomes signifi-Figure 2: The taxonomy of model merging in machine learning. This general framework covers advanced model merging methods and theories (top part), as well as practical applications of model merging techniques to foundation models and more than 10 machine learning subfields (bottom part). cantly larger. Additionally, current model merging methods incur enormous memory costs during merging and lack trust guarantees as well as in-depth theoretical analysis. Addressing these gaps will require substantial efforts from researchers to further advance the flourishing development of this field. \n\nTo summarize, the main contributions of this paper include the following three aspects: \n\n\u2022 Methodology Overview: We provide a comprehensive summary of the technical aspects of model merging. Specifically, we propose a new taxonomy that divides existing model merging methods into two stages and further subdivides the methods in each stage according to key techniques. Additionally, we discuss theoretical analysis work related to model merging. \n\n\u2022 Application Overview: We offer a comprehensive summary of the application aspects of model merging. Specifically, we explore the application of model merging to foundation models and 10+ machine learning subfields, demonstrating how model merging can address existing challenges in these areas.",
            "reference_string": "[271865581 | Yang et al. | 2024 | Citations: 92]"
        },
        {
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "venue": "",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.16815, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153597925",
                    "name": "Shenghe Zheng"
                },
                {
                    "authorId": "2299488665",
                    "name": "Hongzhi Wang"
                }
            ],
            "abstract": "With the rapid growth of deep learning, there is an increasing availability of open-source models for various tasks. However, single fine-tuned models often fall short of meeting the diverse needs of users. Model merging has thus emerged as an efficient method to integrate the capabilities of existing models into a unified model. Nevertheless, existing model merging methods face challenging trade-offs between performance and deployment costs, primarily due to task interference. For the first time, we reveal that task interference is evident in the frequency domain of model parameters, yet current efforts only focus on spatial domain solutions, which are largely ineffective in addressing frequency domain interference. To mitigate the impact of frequency domain interference, we propose FR-Merging, an innovative method that effectively filters harmful frequency domain interference on the backbone with minimal computational overhead. Since performance loss is inevitable with cost-free methods, we propose a lightweight task-specific expert module that dynamically compensates for information loss during merging. This proposed framework, FREE-Merging (FR-Merging with experts), strikes a balanced trade-off between training cost, inference latency, storage requirements, and performance. We demonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple tasks across CV, NLP, and Multi-Modal domains and show that they can be flexibly adapted to specific needs.",
            "corpus_id": 277313159,
            "sentences": [
                {
                    "corpus_id": "277313159",
                    "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
                    "text": "With the rapid growth of deep learning, there is an increasing availability of open-source models for various tasks. However, single fine-tuned models often fall short of meeting the diverse needs of users. Model merging has thus emerged as an efficient method to integrate the capabilities of existing models into a unified model. Nevertheless, existing model merging methods face challenging trade-offs between performance and deployment costs, primarily due to task interference. For the first time, we reveal that task interference is evident in the frequency domain of model parameters, yet current efforts only focus on spatial domain solutions, which are largely ineffective in addressing frequency domain interference. To mitigate the impact of frequency domain interference, we propose FR-Merging, an innovative method that effectively filters harmful frequency domain interference on the backbone with minimal computational overhead. Since performance loss is inevitable with cost-free methods, we propose a lightweight task-specific expert module that dynamically compensates for information loss during merging. This proposed framework, FREE-Merging (FR-Merging with experts), strikes a balanced trade-off between training cost, inference latency, storage requirements, and performance. We demonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple tasks across CV, NLP, and Multi-Modal domains and show that they can be flexibly adapted to specific needs.",
                    "score": 0.43148521516765326,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65625
                }
            ],
            "relevance_judgement": 0.65625,
            "relevance_judgment_input_expanded": "# Title: FREE-Merging: Fourier Transform for Efficient Model Merging\n# Venue: \n# Authors: Shenghe Zheng, Hongzhi Wang\n## Abstract\nWith the rapid growth of deep learning, there is an increasing availability of open-source models for various tasks. However, single fine-tuned models often fall short of meeting the diverse needs of users. Model merging has thus emerged as an efficient method to integrate the capabilities of existing models into a unified model. Nevertheless, existing model merging methods face challenging trade-offs between performance and deployment costs, primarily due to task interference. For the first time, we reveal that task interference is evident in the frequency domain of model parameters, yet current efforts only focus on spatial domain solutions, which are largely ineffective in addressing frequency domain interference. To mitigate the impact of frequency domain interference, we propose FR-Merging, an innovative method that effectively filters harmful frequency domain interference on the backbone with minimal computational overhead. Since performance loss is inevitable with cost-free methods, we propose a lightweight task-specific expert module that dynamically compensates for information loss during merging. This proposed framework, FREE-Merging (FR-Merging with experts), strikes a balanced trade-off between training cost, inference latency, storage requirements, and performance. We demonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple tasks across CV, NLP, and Multi-Modal domains and show that they can be flexibly adapted to specific needs.\n",
            "reference_string": "[277313159 | Zheng et al. | 2024 | Citations: 0]"
        },
        {
            "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2345985527",
                    "name": "Yuxuan Yao"
                },
                {
                    "authorId": "2320312026",
                    "name": "Ruifeng She"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2332348570",
                    "name": "Tao Zhong"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                }
            ],
            "abstract": "While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.",
            "corpus_id": 276408756,
            "sentences": [
                {
                    "corpus_id": "276408756",
                    "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
                    "text": "Large Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
                    "score": 0.4391156090626096,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1570
                        },
                        {
                            "start": 1573,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2235
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 112,
                            "end": 132,
                            "matchedPaperCorpusId": "207847753"
                        },
                        {
                            "start": 358,
                            "end": 380,
                            "matchedPaperCorpusId": "4055784"
                        },
                        {
                            "start": 380,
                            "end": 402,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 734,
                            "end": 753,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1322,
                            "end": 1341,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 2050,
                            "end": 2068,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging\n# Venue: arXiv.org\n# Authors: Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan\n## Abstract\nWhile most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.\n## Introduction\nLarge Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
            "reference_string": "[276408756 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2242179319",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "145498976",
                    "name": "S. Hu"
                },
                {
                    "authorId": "2303652170",
                    "name": "Wayne Luk"
                },
                {
                    "authorId": "2253592926",
                    "name": "Timothy M. Hospedales"
                },
                {
                    "authorId": "10001427",
                    "name": "Hongxiang Fan"
                }
            ],
            "abstract": "Model merging has emerged as a promising approach for multi-task learning (MTL), offering a data-efficient alternative to conventional fine-tuning. However, with the rapid development of the open-source AI ecosystem and the increasing availability of fine-tuned foundation models, existing model merging methods face two key limitations: (i) They are primarily designed for in-house fine-tuned models, making them less adaptable to diverse model sources with partially unknown model and task information, (ii) They struggle to scale effectively when merging numerous model checkpoints. To address these challenges, we formulate model merging as a constrained optimization problem and introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by Frank-Wolfe optimization, our approach iteratively selects the most relevant model in the pool to minimize a linear approximation of the objective function and then executes a local merging similar to the Frank-Wolfe update. The objective function is designed to capture the desired behavior of the target-merged model, while the fine-tuned candidate models define the constraint set. More importantly, FW-Merging serves as an orthogonal technique for existing merging methods, seamlessly integrating with them to further enhance accuracy performance. Our experiments show that FW-Merging scales across diverse model sources, remaining stable with 16 irrelevant models and improving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining constant memory overhead, unlike the linear overhead of data-informed merging methods. Compared with the state-of-the-art approaches, FW-Merging surpasses the data-free merging method by 32.8% and outperforms the data-informed Adamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at github.com/hmarkc/FW-Merging.",
            "corpus_id": 277065877,
            "sentences": [
                {
                    "corpus_id": "277065877",
                    "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
                    "text": "Efficient Multi-Task Learning. In traditional Multi-Task Learning (MTL), a single model is trained on a dataset containing multiple tasks to enable the model to acquire diverse capabilities [3]. However, a significant challenge in traditional MTL is the issue of negative transfer [24]. To mitigate this, architecture-based approaches have been developed, such as parameter sparsification [36,55] and shared structure modularization [39,40]. On the optimization side, methods to resolve gradient conflicts [7,73] and domination of gradient or learning rate [6,34] have been proposed. With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient. \n\nModel Merging. While pre-merging methods prepare favorable conditions for merging, during-merging techniques combine multiple neural networks into a single model while retaining or enhancing their capabilities [68]. In this work, we focus on during-merging methods. Early insights into neural network landscapes [17] revealed that linear interpolation between models exposes useful loss surface properties, laying the foundation for weight averaging-a core merging technique. Simple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
                    "score": 0.5125883116090487,
                    "section_title": "Related Work",
                    "char_start_offset": 5224,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 30
                        },
                        {
                            "start": 31,
                            "end": 194
                        },
                        {
                            "start": 195,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 583
                        },
                        {
                            "start": 584,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1026
                        },
                        {
                            "start": 1029,
                            "end": 1043
                        },
                        {
                            "start": 1044,
                            "end": 1244
                        },
                        {
                            "start": 1245,
                            "end": 1294
                        },
                        {
                            "start": 1295,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2094
                        },
                        {
                            "start": 2095,
                            "end": 2318
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 190,
                            "end": 193,
                            "matchedPaperCorpusId": "45998148"
                        },
                        {
                            "start": 281,
                            "end": 285,
                            "matchedPaperCorpusId": "258865647"
                        },
                        {
                            "start": 389,
                            "end": 393,
                            "matchedPaperCorpusId": "4389348"
                        },
                        {
                            "start": 393,
                            "end": 396,
                            "matchedPaperCorpusId": "208513386"
                        },
                        {
                            "start": 433,
                            "end": 437,
                            "matchedPaperCorpusId": "50770252"
                        },
                        {
                            "start": 437,
                            "end": 440,
                            "matchedPaperCorpusId": "58145688"
                        },
                        {
                            "start": 506,
                            "end": 509,
                            "matchedPaperCorpusId": "222341884"
                        },
                        {
                            "start": 509,
                            "end": 512,
                            "matchedPaperCorpusId": "210839011"
                        },
                        {
                            "start": 557,
                            "end": 560,
                            "matchedPaperCorpusId": "4703661"
                        },
                        {
                            "start": 827,
                            "end": 831,
                            "matchedPaperCorpusId": "258841328"
                        },
                        {
                            "start": 834,
                            "end": 837,
                            "matchedPaperCorpusId": "258959274"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64794921875
                }
            ],
            "relevance_judgement": 0.64794921875,
            "relevance_judgment_input_expanded": "# Title: FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization\n# Venue: arXiv.org\n# Authors: Hao Chen, S. Hu, Wayne Luk, Timothy M. Hospedales, Hongxiang Fan\n## Abstract\nModel merging has emerged as a promising approach for multi-task learning (MTL), offering a data-efficient alternative to conventional fine-tuning. However, with the rapid development of the open-source AI ecosystem and the increasing availability of fine-tuned foundation models, existing model merging methods face two key limitations: (i) They are primarily designed for in-house fine-tuned models, making them less adaptable to diverse model sources with partially unknown model and task information, (ii) They struggle to scale effectively when merging numerous model checkpoints. To address these challenges, we formulate model merging as a constrained optimization problem and introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by Frank-Wolfe optimization, our approach iteratively selects the most relevant model in the pool to minimize a linear approximation of the objective function and then executes a local merging similar to the Frank-Wolfe update. The objective function is designed to capture the desired behavior of the target-merged model, while the fine-tuned candidate models define the constraint set. More importantly, FW-Merging serves as an orthogonal technique for existing merging methods, seamlessly integrating with them to further enhance accuracy performance. Our experiments show that FW-Merging scales across diverse model sources, remaining stable with 16 irrelevant models and improving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining constant memory overhead, unlike the linear overhead of data-informed merging methods. Compared with the state-of-the-art approaches, FW-Merging surpasses the data-free merging method by 32.8% and outperforms the data-informed Adamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at github.com/hmarkc/FW-Merging.\n## Related Work\nEfficient Multi-Task Learning. In traditional Multi-Task Learning (MTL), a single model is trained on a dataset containing multiple tasks to enable the model to acquire diverse capabilities [3]. However, a significant challenge in traditional MTL is the issue of negative transfer [24]. To mitigate this, architecture-based approaches have been developed, such as parameter sparsification [36,55] and shared structure modularization [39,40]. On the optimization side, methods to resolve gradient conflicts [7,73] and domination of gradient or learning rate [6,34] have been proposed. With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient. \n\nModel Merging. While pre-merging methods prepare favorable conditions for merging, during-merging techniques combine multiple neural networks into a single model while retaining or enhancing their capabilities [68]. In this work, we focus on during-merging methods. Early insights into neural network landscapes [17] revealed that linear interpolation between models exposes useful loss surface properties, laying the foundation for weight averaging-a core merging technique. Simple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
            "reference_string": "[277065877 | Chen et al. | 2025 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "score": 0.62423505107026,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "232110907"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1592,
                    "end": 1596,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1596,
                    "end": 1599,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1650,
                    "end": 1653,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1656,
                    "end": 1659,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 2028,
                    "end": 2032,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 2032,
                    "end": 2035,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.732421875
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
            "score": 0.6154559750402884,
            "section_title": "Model Merging for LLM Domain Adaptation",
            "char_start_offset": 4872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
            "score": 0.609232120836815,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "Model merging, while gaining traction and demonstrating significant potential, still encounters several key challenges that must be addressed to achieve broader adoption and improved efficiency. Firstly, as the number of tasks increases, merged models often underperform compared to independent expert models. Maintaining consistent performance across diverse tasks without extensive task-specific tuning remains a significant hurdle. Moreover, merging models trained on different tasks or domains can result in conflicts where knowledge from one model interferes with another. This issue is particularly pronounced in scenarios like multi-task learning and continual learning, where tasks often have distinct requirements. \n\nAdditionally, the lack of comprehensive theoretical frameworks for model merging limits the ability to predict and guarantee performance. Many current methods rely heavily on heuristic or empirical strategies, leaving room for improvement through deeper theoretical exploration. \n\nLastly, identifying optimal merging strategies, such as determining appropriate weight coefficients for models or parameters, is difficult. Fine-grained optimization approaches often come with high computational and resource demands, making them less feasible in practice. \n\nAddressing these challenges will require innovative methodologies, advanced computational tools, and a stronger theoretical foundation to support the efficient and reliable implementation of model merging across diverse applications. \n\nCombining model compression with model merging represents one of the promising future directions for advancing model merging techniques [Wang et al., 2024;Lu et al., 2024b]. Model compression, which involves reducing the size and complexity of a model while preserving its performance, helps mitigate interference between models during the merging process. By streamlining individual models before merging, compression can enhance compatibility and lead to more effective and seamless integration of model parameters, ultimately improving the overall merging outcomes. \n\nApart from leveraging model merging techniques, task merging or classification can also be explored as potential approaches. For instance, the Disperse-Then-Merge (DTM) method provides an innovative framework for addressing alignment tax in supervised fine-tuning of large language models [Fu et al., 2024]. This method tackles the issue of data biases by dividing the instruction-following dataset into several clusters, training separate sub-models on these clusters, and subsequently merging the sub-models into a single model. By doing so, DTM effectively distributes and mitigates dataset-specific biases, maintaining the model's instruction-following capacity while reducing the detrimental effects of such biases on knowledge and reasoning benchmarks.",
            "score": 0.5962320521704888,
            "section_title": "Challenges and Future Perspectives of Model Merging",
            "char_start_offset": 29181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2396
                },
                {
                    "start": 2397,
                    "end": 2619
                },
                {
                    "start": 2620,
                    "end": 2847
                }
            ],
            "ref_mentions": [
                {
                    "start": 1654,
                    "end": 1673,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1673,
                    "end": 1690,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "The rapid advancement of Natural Language Processing (NLP) has led to the widespread adoption of Large Language Models (LLMs) across diverse applications. While these models exhibit remarkable versatility [1,2], their effectiveness in specialized domains remains limited due to insufficient exposure to domain-specific knowledge during training. This issue becomes particularly pronounced in fields such as medicine, law, and engineering, where precise understanding and accurate generation of technical terminology are essential. Even minor misunderstandings in these domains can lead to significant misinterpretations, impacting decision-making and real-world applications. Therefore, developing methods to effectively incorporate domain-specific knowledge into LLMs is vital for enhancing their applicability and reliability in specialized contexts. \n\nOne promising approach to addressing this limitation is model merging, which integrates the strengths of multiple LLMs to enhance domain adaptation. Model merging presents a costeffective alternative to full-scale retraining or fine-tuning, allowing the integration of new knowledge without requiring large amounts of additional data or computational resources. However, the extent to which model merging facilitates domain-specific knowledge integration, particularly in multilingual settings, remains an open question. This limitation is particularly problematic for applications that require precise understanding and generation of technical language. An accurate interpretation of terms and concepts is essential in these fields, as even minor misunderstandings can lead to significant errors or miscommunications. \n\nThis study explores the potential of model merging for cross-lingual knowledge transfer, with a particular focus on integrating domain-specific technical vocabulary. The primary challenge lies in ensuring effective knowledge transfer without interference so that newly acquired domainspecific information enhances the model's proficiency while preserving its general linguistic capabilities. Another key issue is whether merging enables the model to retain and accurately utilize domain-specific terminology across different languages, maintaining both contextual meaning and usability in a multilingual setting. To investigate this, we conduct a comprehensive experiment, merging a general-purpose Japanese-specific model with an English medical domain-specific model and assessing various merging strategies. Through quantitative analysis, we evaluate the effectiveness of different approaches in transferring domain-specific terminology knowledge and improving the model's ability to understand technical language, particularly medical jargon.",
            "score": 0.5886999996229648,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2722
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "In this study, we explored the potential of model merging methods to enhance the integration of technical vocabulary in language models, particularly focusing on cross-lingual knowledge transfer. Our findings reveal that while model merging can facilitate knowledge transfer at a mono-lingual level, as evidenced by satisfactory performance on general benchmarks, it struggles with the effective acquisition and integration of technical terminology across languages. The merged models' performance remained similar to that of the non-expert Japanese model, with a tendency towards decline, highlighting the challenges of incorporating specialized vocabulary. \n\nThe results underscore the complexity of achieving effective cross-lingual knowledge transfer, particularly in domains requiring precise technical language comprehension. The observed performance suggests that current merging methods may introduce complexities that hinder the integration of domain-specific terminology. Despite these challenges, the study provides valuable insights into the strengths and limitations of model merging, offering a foundation for future research aimed at developing more sophisticated methods for domain adaptation and cross-lingual knowledge transfer. \n\nResearch should then focus on refining existing merging methods to better handle technical vocabulary and explore alternative strategies that enhance the integration of specialized knowledge without compromising the models' general capabilities. By addressing these challenges, we can advance the development of more versatile and capable language models suitable for specialized applications across diverse linguistic contexts. \n\nWhile this study provides valuable insights into the potential of model merging for cross-lingual technical vocabulary acquisition, several limitations must be acknowledged. Firstly, the reliance on judge LLMs for evaluating definition accuracy introduces a degree of uncertainty, as these models may not fully capture the nuances of human judgment. Secondly, the study's focus on the Japanese-English language pair and the medical domain may limit the generalizability of the findings. The significant linguistic differences between these languages likely influence the results, and effectiveness could vary with other language pairs or domains. Thirdly, the investigation is limited to six specific model merging methods, and exploring alternative approaches could reveal more effective strategies. Finally, the minimal impact of hyperparameter tuning suggests robustness, but further optimization could potentially yield improved results. Addressing these limitations is crucial for future research to achieve a comprehensive and detailed understanding of these mechanisms and to develop new, more performant techniques.",
            "score": 0.5880334826375098,
            "section_title": "CONCLUSION",
            "char_start_offset": 23524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2326
                },
                {
                    "start": 2327,
                    "end": 2480
                },
                {
                    "start": 2481,
                    "end": 2621
                },
                {
                    "start": 2622,
                    "end": 2803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "In addition, the conclusion drawn from a single model family may not generalize to other models. \n\nOther works explore specialized settings, such as temporal merging [13], multilingual merging [1], and domain-specific merging in material science [39]. A recent LLM merging competition [60,82] has also emerged, though its evaluation details remain undisclosed. \n\nMergeBench addresses these limitations by incorporating diverse model families, including Llama-3 and Gemma-2, and evaluating models up to 9B parameters. It focuses on domain-specific tasks beyond conventional NLP benchmarks and includes advanced merging methods. Both the specialized models and the evaluation pipeline are open-sourced, facilitating reproducibility and further research. \n\n6 Discussion and Future Directions \n\nOpportunities for improving merging efficiency. Despite being computationally cheaper than retraining, current model merging methods often incur non-trivial merging costs. Hyperparameter tuning, especially for scaling and sparsity, remains inefficient and largely trial-and-error, limiting the practicality of applying these methods to large-scale models. \n\nMix data or merge models? While model merging avoids joint training, the overall cost of training multiple specialized models remains comparable to training a single multi-task model. Our results show that multi-task models generally achieve stronger in-domain performance, particularly when the tasks are non-conflicting and a balanced data mixture can be constructed. This raises questions about the fundamental limitations of model merging compared to MTL in such settings. Nevertheless, model merging shows clear benefits in low-resource or imbalanced settings, such as fine-grained safety alignment [77] and multilingual language models [1], where data mixing is inherently challenging [14,21]. A deeper understanding of the trade-offs between data mixing and model merging remains an important future direction. \n\nPositioning model merging in LLM Pipelines. Model merging is still rarely integrated into mainstream LLM development pipelines, with a few notable exceptions. For example, Llama-3 employs model soup to average models trained with different hyperparameter settings for improved robustness [12]. Command A [11] applies merging similarly to our setting, combining separately trained specialized models. However, the potential applications of model merging could extend beyond these use cases.",
            "score": 0.5876789116642556,
            "section_title": "Related Works",
            "char_start_offset": 25831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2368
                },
                {
                    "start": 2369,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "272423492"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "275949902"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "257038048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
            "score": 0.5856320754004208,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60546875
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
            "score": 0.5742730316668667,
            "section_title": "E. Model Merging",
            "char_start_offset": 24254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1326
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "274965385",
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "score": 0.5672610211293997,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
            "score": 0.5671491527792101,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "277762515",
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "score": 0.5669850679075676,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "273482450",
            "title": "Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace",
            "text": "The rapid advancement of artificial intelligence has led to the emergence of pre-trained models that demonstrate exceptional performance across various tasks (Yang et al., 2024a). However, training and deploying individual models for each specific task not only incurs substantial computational costs but also results in knowledge redundancy and storage inefficiencies. To address these challenges, multi-task model merging, as a promising solution, integrates parameters from multiple single-task models into a unified model (Tang et al., 2024a), which not only enhances task-specific performance but also significantly improves computational efficiency and cost-effectiveness (Izmailov et al., 2018;Frankle et al., 2020;Ilharco et al., 2022b). \n\nCurrent research in model merging primarily focuses on resolving conflicts among task-specific models to achieve effective knowledge transfer and inheritance. Pioneering merging strategies based on task vectors include gradient conflict-based methods (Yadav et al., 2024) and subspace-based approaches (Tang et al., Clean Model Backdoor Model 0",
            "score": 0.5640445135291818,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1092
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 722,
                    "end": 744,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.501953125
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "In this study, we consider a collection of n existing Large Language Models (LLMs), denoted as {M 1 , . . . , M n }, which have been fine-tuned on diverse corpora. Our objective is to outline a systematic approach towards producing one stronger aggregated model across all knowledge domains. Specifically, the unified LLM incorporates single LLMs mainly through merging and mixture. The concept of Model Merging Model merging is integrating multiple models into one unified model in the weight space, compatible with LLMs of the same initialization [16]. Popular merging methods can be divided into two types: \u2776 Merging entire model weights represented by Model Soup [59] (Linear), SLERP [49], and Model Stock [24]; \u2777 Task-vector based merging represented by Task Arithmetic [22], TIES [63], and DARE [64]. The former method directly interpolates model weights, while the latter subtracts the pre-trained model from the fine-tuned model to obtain task vectors and utilizes sparsity and consistency of parameters for refined merging. The basic Linear interpolation merging is defined as w u = n i=1 s i \u2022 w i , where w i and s i are the corresponding model weights and merging coefficient of M i \u2208 {M 1 , . . . M n }. Selective Merging Pipeline Merging can be easily applied to models with the same architecture, but does not guarantee better results. Therefore, before searching for the merging coefficient, we first pre-process the models by clustering all the models using cosine similarity and then searching for the optimal merging coefficient and method within each cluster. Details are explained in Appendix A. 5.",
            "score": 0.5635833345908542,
            "section_title": "Preliminaries",
            "char_start_offset": 9133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "11290566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "Model merging [26,40,69,70,76] uses arithmetic operations on model parameters to combine the strengths of multiple models. It efficiently produces a single model with multi-task capabilities without necessitating joint training on data across all tasks. This significantly saves storage and maintenance costs compared with deploying multiple finetuned models independently. Moreover, model merging enables asynchronous development of model capabilities [11], allowing different teams to independently apply the most suitable optimization strategies for their target tasks. For instance, reasoning capabilities can be enhanced with RL tuning [56], while instruction following benefits from preference learning [43]. Those optimization procedures are non-trivial to integrate directly, and post-hoc merging provides a viable solution. \n\nDespite recent progress in model merging algorithms [22,26,29,40,66,69,73,79], existing evaluations [59,61,74] remain constrained in two critical dimensions: model size and task scale, making it difficult to quantify and compare the performance of different merging methods in real-world applications. On the model side, most evaluations rely on relatively small language models, such as GPT-2 (124M) [49], RoBERTa-base (125M) [37] and mT5 (2.85B) [51]. These choices inherently constrain the complexity and capability of the merged models, making it unclear whether Starting from open-source base models (Llama and Gemma), we perform task-specific post-training on five diverse domains: mathematics, coding, multilinguality, instruction following, and safety. This process produces five task-specialized models that perform well on their respective domains but likely poorly on others. We then apply a range of model merging algorithms to combine these specialized models into a single multi-task model. MergeBench evaluates the effectiveness of these merging approaches along three key dimensions: multi-task performance, retention of pretrained knowledge (forgetting), and runtime efficiency. \n\nobserved trends generalize to modern, large-scale language models. On the task side, evaluations typically focus on conventional NLP benchmarks such as sentiment classification and natural language inference. These tasks are narrow in scope, often solvable via shallow pattern recognition or memorization.",
            "score": 0.5629815043606126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2030
                },
                {
                    "start": 2033,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 18,
                    "end": 21,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "271957310"
                },
                {
                    "start": 891,
                    "end": 894,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "As shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models. For example, model merging in large language models can help mitigate untruthfulness and toxicity output, accomplish knowledge unlearning, and speed up training. Moreover, model merging also arises in different machine learning subfields, such as continual learning, multi-task/multi-domain learning, few-shot learning, and other subfields, to solve a variety of challenges. For instance, in continual learning, model merging can mitigate catastrophic forgetting of old tasks. In multi-task learning, multi-objective learning and multidomain learning, it facilitates knowledge transfer. Additionally, in adversarial learning, model merging can be employed for both attack and defense strategies. \n\nThird, what are the remaining challenges and future research opportunities for model merging? Despite the advancements in merging methods and their well-developed applications, there are still numerous open challenges and future research directions in the field ( \u00a75). For example, as the number of tasks increases, the performance gap between existing methods and independent expert models becomes signifi-Figure 2: The taxonomy of model merging in machine learning. This general framework covers advanced model merging methods and theories (top part), as well as practical applications of model merging techniques to foundation models and more than 10 machine learning subfields (bottom part). cantly larger. Additionally, current model merging methods incur enormous memory costs during merging and lack trust guarantees as well as in-depth theoretical analysis. Addressing these gaps will require substantial efforts from researchers to further advance the flourishing development of this field. \n\nTo summarize, the main contributions of this paper include the following three aspects: \n\n\u2022 Methodology Overview: We provide a comprehensive summary of the technical aspects of model merging. Specifically, we propose a new taxonomy that divides existing model merging methods into two stages and further subdivides the methods in each stage according to key techniques. Additionally, we discuss theoretical analysis work related to model merging. \n\n\u2022 Application Overview: We offer a comprehensive summary of the application aspects of model merging. Specifically, we explore the application of model merging to foundation models and 10+ machine learning subfields, demonstrating how model merging can address existing challenges in these areas.",
            "score": 0.5610532141493767,
            "section_title": "Introduction",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2265
                },
                {
                    "start": 2266,
                    "end": 2342
                },
                {
                    "start": 2345,
                    "end": 2446
                },
                {
                    "start": 2447,
                    "end": 2641
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65673828125
        },
        {
            "corpus_id": "276813020",
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "text": "Combining the strengths of multiple large language models (LLMs) provides a powerful means to enhance performance, robustness, and generalization across diverse tasks by leveraging the unique expertise and knowledge each model offers. Individual LLMs, particularly those constrained by size or training data, may perform well in specific areas but struggle in others due to specialization gaps. For instance, one model might excel at generating creative content but lack precision in technical explanations, while another delivers technical accuracy but struggles with conversational fluency. By integrating multiple models, their collective strengths can bridge these gaps, leading to improved overall performance. This collaborative approach also improves robustness, as the system can compensate for individual model errors-when one model underperforms, others can intervene to support the response. Furthermore, this integration enhances task generalization by exposing the system to diverse patterns, allowing it to adapt more effectively to new or unseen challenges. \n\nVarious strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023b;Wang et al., 2025) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024;Hu et al., 2024;Ong et al., 2025) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors.",
            "score": 0.5551862228373105,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2156
                },
                {
                    "start": 2157,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 1177,
                    "end": 1198,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1198,
                    "end": 1216,
                    "matchedPaperCorpusId": "270357878"
                },
                {
                    "start": 1434,
                    "end": 1453,
                    "matchedPaperCorpusId": "269303119"
                },
                {
                    "start": 1469,
                    "end": 1486,
                    "matchedPaperCorpusId": "270764307"
                },
                {
                    "start": 1746,
                    "end": 1769,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "277787291",
            "title": "Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs",
            "text": "Weight Interpolation and Task Arithmetic. In recent years, the increasing scale of large language models has significantly heightened the demand for data and training costs associated with finetuning multi-task models. To merge the capabilities of various existing single-task models into a unified framework, several model merging techniques (Daheim et al., 2024;Jin et al., 2023;Wan et al., 2024;Yang et al., 2024b) have been developed. The simplest and most intuitive approach is weight interpolation (Frankle et al., 2020;Izmailov et al., 2018;Ram\u00e9 et al., 2023;2022), which has been applied to enhance model generalization (Wortsman et al., 2022b), improve specific singletask performance (Wortsman et al., 2022a), and boost multi-task effectiveness (Ilharco et al., 2022;Li et al., 2022). Subsequently, Task Arithmetic (Ilharco et al., 2023) was introduced, enabling the merging of multiple models through a weighted operation on the parameter difference \u03c4 . Many subsequent methods (Yang et al., 2024c;Yu et al., 2024;Yadav et al., 2023;Zhang et al., 2023) have been proposed based on the principles of Task Arithmetic. In conclusion, modifying fine-tuning methods to achieve a more linear model can reduce interference during model merging. However, these methods still require training data and retraining, which is often challenging in practical scenarios.",
            "score": 0.547943321386011,
            "section_title": "RELATED WORK",
            "char_start_offset": 23237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 364,
                    "matchedPaperCorpusId": "264306115"
                },
                {
                    "start": 364,
                    "end": 381,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 381,
                    "end": 398,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 504,
                    "end": 526,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 526,
                    "end": 548,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "254877458"
                },
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 694,
                    "end": 718,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 777,
                    "end": 793,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 825,
                    "end": 847,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 989,
                    "end": 1009,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1009,
                    "end": 1025,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1025,
                    "end": 1044,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418212890625
        },
        {
            "corpus_id": "276574617",
            "title": "Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation",
            "text": "Rapid development of foundation models has facilitated the construction of expert model from custom data. Modern models like large language model (LLMs) are pre-trained on various datasets to obtain general knowledge and employing pre-trained models typically involves fine-tuning on task- specific data to gain ability on specific areas. When dealing with tasks of different domains, multi-task learning (Sanh et al., 2022) is a common paradigm to mitigate performance variations. However, particular knowledge may be required progressively over time. As the model becomes larger (Dehghani et al., 2023;Wang et al., 2024b), once the model is specialized on specific datasets, it is time consuming and resource intensive to retrain models to get knowledge of another area, even encountering catastrophic forgetting (Zeng et al., 2024). Furthermore, issues regarding data privacy may obstacle its practical application. To address these issues, model merging (Stoica et al., 2024) has been proposed to integrate multiple separate models of specific knowledge off-the-shelf into one model with multi-task ability without the demand of training or accessing data. Its effectiveness and convenience show great potential in various downstream tasks (Guo et al., 2024;Shah et al., 2024). \n\nDespite its popularity, crucial problems for model merging remain unsolved, hindering its real-world application. First, with larger model size like multimodal large language models (MLLMs) and massive data, parameter efficient tuning (PEFT) (Hu et al., 2022) has become the most popular and effective tuning method for large models. However, existing model merging method focuses on combining full fine-tuning (FFT) models (Yadav et al., 2024;Guodong et al., 2024), which is challenging in distribution shift and undergoes performance drop when directly applied to parameter efficient model merging, as is illustrated in Figure 1. Another issue lies in that current high-performance methods rely on extra information of seen tasks (e.g., validation data (Yang et al., 2024b), extra storage (Huang et al., 2024)) to boost the performance.",
            "score": 0.5393306814110375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 581,
                    "end": 604,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "265351656"
                },
                {
                    "start": 1708,
                    "end": 1728,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1728,
                    "end": 1749,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5009765625
        },
        {
            "corpus_id": "267938070",
            "title": "Knowledge Fusion of Chat LLMs: A Preliminary Technical Report",
            "text": "Large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series have demonstrated remarkable success across a wide range of natural language processing (NLP) tasks. However, the computational resources and time costs associated with LLM development remain prohibitively high for most entities. Despite the structural and functional differences among LLMs, they often exhibit similar capabilities across various tasks. Therefore, moving beyond the traditional approach of training a LLM from scratch, an alternative option is to combine existing LLMs into a new, more powerful one, which is termed knowledge fusion of LLMs by Wan et al. (2024). If successful, this fusion not only reduces the initial training costs but also enables the combined model to leverage the strengths of multiple LLMs. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone and Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of different models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models and executing each during inference, which is inefficient for LLMs due to their substantial memory and inference time requirements. Another approach is to directly merge several neural networks into a single network through arithmetic operations in the parameter space (Gupta et al., 2020). This approach typically assumes uniform network architectures and seeks to merge the parameters of different neural networks either through manual merging weights (Wortsman et al., 2022;Yadav et al., 2023) or by automatically obtaining merging weights based on model gradients or representations of additional data (Matena and Raffel, 2022;Jin et al., 2022). Recently, FUSELLM (Wan et al., 2024) introduced a new paradigm for integrating the capabilities of multiple LLMs. This approach externalizes the knowledge of multiple source LLMs using their generated probability distribution matrices and transfers their collective knowledge into a target LLM through lightweight continual training. Consequently, FUSELLM facilitates the fusion of multiple pre-trained LLMs with distinct architectures into a unified LLM.",
            "score": 0.53857833748814,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 961,
                    "end": 992,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1635,
                    "end": 1658,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1787,
                    "end": 1812,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1812,
                    "end": 1829,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "These tasks are narrow in scope, often solvable via shallow pattern recognition or memorization. As such, they fail to surface the generalization, compositionality and interference challenges that arise when merging stronger and more specialized models for real-world applications. \n\nTo address the limitations of existing model merging evaluations, we introduce MergeBench, a scalable and comprehensive benchmark designed to rigorously assess merging performance, illustrated in Figure 1. First, MergeBench improves model selection by adopting state-of-the-art, open-source language models as base models. Specifically, we include both pretrained and instruction-tuned versions of Llama-3.2-3B, Llama-3.1-8B [17], Gemma-2-2B, and Gemma-2-9B [62], resulting in a total of eight base models. Second, we construct a more challenging and representative task suite for evaluating merged models. Each base model is further finetuned on one of five carefully selected task categories, including instruction following, mathematics, multilingual understanding, coding and safety, to produce specialized models with minimal skill overlap 1 . By standardizing the finetuning and evaluation procedures, MergeBench ensures a fair and reproducible platform for comparing model merging algorithms. In addition to multi-task performance, MergeBench evaluates retention of pretrained generalization through forgetting analysis and reports runtime efficiency, offering a comprehensive view of both utility and computational cost of existing merging algorithms. \n\nOur extensive experiments reveal that model merging tends to perform better on stronger base models, and techniques such as scaling coefficient tuning and sparsification help preserve pretrained knowledge, often improving generalization compared to multi-task models. However, the computational cost of the merging process is non-trivial, leaving room for further optimization. In addition, when tasks are non-conflicting and relatively balanced, multi-task models still achieve stronger in-domain performance. The broader role of model merging in standard LLM training pipelines also remains underexplored. We hope MergeBench provides a foundation for future work to advance the understanding and practical adoption of model merging.",
            "score": 0.5263926513167299,
            "section_title": "Introduction",
            "char_start_offset": 2257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5361328125
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Fine-tuning pretrained foundation models has become a standard paradigm for addressing downstream applications (Brown et al., 2020). However, as the number of applications grows, managing and deploying numerous finetuned models introduces significant costs and operational complexity. To address this issue, multi-task model merging has emerged as a promising solution by consolidating multiple While masking low-magnitude components (horizontal axis, Task 2) mitigates some interference, high-magnitude dimensions (vertical axis, Task 1) can still induce significant conflicts. In contrast, our proposed CAT Merging proactively identifies and trims highconflict components, ensuring a more stable model merging. \n\nexpert models into a single unified model without additional training (Matena & Raffel, 2022). \n\nA notable advancement in model merging is Task Arithmetic (Ilharco et al., 2023b), which introduces the concept of task vectors-defined as the difference vector between pretrained and finetuned models in the parameter space. Task Arithmetic demonstrates that task-specific knowledge can be effectively integrated into the pretrained model through simple arithmetic operations, such as model merging by adding task vectors to the pretrained parameters. Despite its effectiveness, the technique is vulnerable to performance degradation caused by knowledge conflict (Sun et al., 2025;Ortiz-Jimenez et al., 2023), or the imbalance and contradiction among the accumulated task vectors. \n\nTo address knowledge conflict, prior research has explored enhancing Task Arithmetic by trimming unimportant components within task vectors. One typical importance metric is the parameter magnitude. For instance, Ties-Merging (Yadav et al., 2023) trims small-magnitude elements in task vectors. PCBMerging (DU et al., 2024) refines this importance measure by considering inter-task correlations. Similarly, Twin-Merging (Lu et al., 2024) applies singular value decomposition to task vectors; leveraging singular values as the importance metric, it retains components corresponding to large singular values. \n\nDespite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration.",
            "score": 0.5235852044582512,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2101
                },
                {
                    "start": 2104,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 870,
                    "end": 893,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1393,
                    "end": 1420,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1721,
                    "end": 1741,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1915,
                    "end": 1932,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50732421875
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge [4,60,78]. However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference [37,40,50]. Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts [6,76]. Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating [17,20,28,34,51]. Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework [13,25,70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework.",
            "score": 0.5228712519382425,
            "section_title": "Multi-task Learning",
            "char_start_offset": 5007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "235790783"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "22014305"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "52952193"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 627,
                    "end": 630,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 755,
                    "end": 758,
                    "matchedPaperCorpusId": "256658804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "277322544",
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "text": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
            "score": 0.5148324038871748,
            "section_title": "MODEL MERGING",
            "char_start_offset": 9525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2000
                },
                {
                    "start": 2003,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 400,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 904,
                    "end": 926,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66845703125
        },
        {
            "corpus_id": "277065877",
            "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
            "text": "Efficient Multi-Task Learning. In traditional Multi-Task Learning (MTL), a single model is trained on a dataset containing multiple tasks to enable the model to acquire diverse capabilities [3]. However, a significant challenge in traditional MTL is the issue of negative transfer [24]. To mitigate this, architecture-based approaches have been developed, such as parameter sparsification [36,55] and shared structure modularization [39,40]. On the optimization side, methods to resolve gradient conflicts [7,73] and domination of gradient or learning rate [6,34] have been proposed. With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient. \n\nModel Merging. While pre-merging methods prepare favorable conditions for merging, during-merging techniques combine multiple neural networks into a single model while retaining or enhancing their capabilities [68]. In this work, we focus on during-merging methods. Early insights into neural network landscapes [17] revealed that linear interpolation between models exposes useful loss surface properties, laying the foundation for weight averaging-a core merging technique. Simple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
            "score": 0.5125883116090487,
            "section_title": "Related Work",
            "char_start_offset": 5224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "58145688"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "222341884"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "258959274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "276409347",
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "text": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.",
            "score": 0.5114081856107027,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55517578125
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "Model merging. Model merging aims at efficiently integrating multiple finetuned models into a single model that retains the capabilities of each. This approach enhances the efficiency, generalization and multi-task capabilities of finetuned models. In scenarios where models are trained on the same task with different training configurations, Singh & Jaggi (2020); Ainsworth et al. (2022);Jolicoeur-Martineau et al. (2024) show that merged models perform comparably to ensemble models but with significantly lower deployment costs. Additionally, Wortsman et al. (2022a;b) demonstrate that the merged model improves the out-of-distribution (OOD) robustness. When merging finetuned models from different tasks, the merged model can provide better initialization for new tasks (Choshen et al., 2022;Gueta et al., 2023). Finetuned models with different specialized skills can also be combined to enhance multi-task capabilities (Ilharco et al., 2023;Tam et al., 2023;Matena & Raffel, 2022;Jin et al., 2022;Yang et al., 2023;Yu et al., 2023;Wang et al., 2024b;a). More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020;Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem. \n\nOur approach stands out with four key advantages: i) Localized merging: Instead of global merging, we localize merging to specific regions with finetuned skills, effectively decreasing task conflicts. ii) Simplified process: Existing works often require computationally intensive grid search or optimization to determine the optimal merging coefficients, while our stitching procedure does not have the requirement.",
            "score": 0.508408085680556,
            "section_title": "Related works",
            "char_start_offset": 30230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 423,
                    "matchedPaperCorpusId": "257984998"
                },
                {
                    "start": 547,
                    "end": 570,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 925,
                    "end": 947,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 964,
                    "end": 986,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1037,
                    "end": 1056,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1490,
                    "end": 1507,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1507,
                    "end": 1524,
                    "matchedPaperCorpusId": "239998731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "263608674",
            "title": "BYOM: Building Your Own Multi-Task Model For Free",
            "text": "As discussed in the Introduction and Figure 1, existing merging methods suffer a large performance gap compared with the Single-Task method. In this section, we introduce the task interference issue and conduct empirical analysis to show that interference cannot be resolved by simply merging without task-specific knowledge. \n\nIn merging models, task interference means merging one task-specific model can negatively impact the performance of other tasks. One consequence of task interference is the overall performance of a merged model is much worse than using multiple task-specific models. \n\nTask interference is more serious when merging more tasks. We conduct an experiment to study the performance of existing merging methods when varying the number of tasks being merged. For a merging method A, we normalize its accuracy on each task by the accuracy on the task-specific model \u03b8 t and report the average normalized accuracy, i.e.,",
            "score": 0.5080721413307081,
            "section_title": "Task Interference in Merging Models",
            "char_start_offset": 8125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331787109375
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "Moreover, weight merging may lead to suboptimal results when substantial differences exist in the parameter space (Li et al., 2022). \n\nIn this paper, we explore the fusion of LLMs from a probabilistic distribution perspective. For an input text, we argue that the probabilistic distributions generated by different source LLMs can reflect their inherent knowledge in understanding this text. Therefore, the proposed FUSELLM leverages the generative distributions of source LLMs to externalize both their collective knowledge and individual strengths and transfer them to the target LLM through lightweight continual training. \n\nTo achieve this, we develop a new strategy for aligning tokenizations originating from different LLMs and explore two methods for fusing the probability distributions generated by these diverse LLMs. During the continual training, FUSELLM places significant emphasis on minimizing the divergence between the target LLM's probabilistic distributions and those of the source LLMs. \n\nTo empirically demonstrate the effectiveness of FUSELLM, we examine a challenging yet general scenario of LLMs fusion, where the source models share minimal commonalities. Specifically, we focus on three popular open-source LLMs that possess distinct architectures and functionalities: Llama-2 (Touvron et al., 2023), OpenLLaMA (Geng & Liu, 2023), and MPT (Team, 2023). Evaluations across three benchmarks, which consist of a total of 42 tasks spanning reasoning, commonsense, and code generation, confirm that the target model trained by our method outperforms each source LLM and the baseline in most tasks. Moreover, we simulate the existence of functionally distinct LLMs with identical architecture by continually training a single base model on several domain-specific corpora. When evaluated based on perplexity, our method demonstrates superior potential in combining the capabilities of these structurally identical LLMs compared to traditional ensemble and weight merging methods. \n\nTo sum up, this paper explores a novel challenge called LLMs fusion, with the goal of creating a unified model that effectively utilizes the collective capabilities and unique strengths of diverse LLMs. Illustrated in Figure 1, our proposed approach distinguishes itself from traditional ensemble and weight merging techniques by prioritizing the fusion of multiple LLMs through knowledge externalization and transfer.",
            "score": 0.5057082393995787,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1999
                },
                {
                    "start": 2002,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 131,
                    "matchedPaperCorpusId": "251371375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3701171875
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "Large language models (LLMs) have enabled the development of numerous specialized, task-specific variants. However, the maintenance and deployment of these individual models present substantial challenges in terms of resource utilization and operational efficiency. In this work, we propose the \\textit{Mixture of Distributions (MoD)} framework, a novel approach for merging LLMs that operates directly on their output probability distributions, rather than on model weights. Unlike traditional weight-averaging methods, MoD effectively preserves the specialized capabilities of individual models while enabling efficient knowledge sharing across tasks. Through extensive experimentation on mathematical reasoning benchmarks using Qwen2.5 models, we demonstrate that MoD significantly outperforms existing model merging techniques across multiple benchmarks. All code, data, and experimental materials are published at https://github.com/knovel-eng/mod.",
            "score": 0.5044949931725596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "273403613",
            "title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
            "text": "Recent advancements in building domain-specific large language models (LLMs) have shown remarkable success, especially in tasks requiring reasoning abilities like logical inference over complex relationships and multi-step problem solving. However, creating a powerful all-in-one LLM remains challenging due to the need for proprietary data and vast computational resources. As a resource-friendly alternative, we explore the potential of merging multiple expert models into a single LLM. Existing studies on model merging mainly focus on generalist LLMs instead of domain experts, or the LLMs under the same architecture and size. In this work, we propose an unconstrained model merging framework that accommodates both homogeneous and heterogeneous model architectures with a focus on reasoning tasks. A fine-grained layer-wise weight merging strategy is designed for homogeneous models merging, while heterogeneous model merging is built upon the probabilistic distribution knowledge derived from instruction-response fine-tuning data. Across 7 benchmarks and 9 reasoning-optimized LLMs, we reveal key findings that combinatorial reasoning emerges from merging which surpasses simple additive effects. We propose that unconstrained model merging could serve as a foundation for decentralized LLMs, marking a notable progression from the existing centralized LLM framework. This evolution could enhance wider participation and stimulate additional advancement in the field of artificial intelligence, effectively addressing the constraints posed by centralized models.",
            "score": 0.49818769010517205,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47265625
        },
        {
            "corpus_id": "271064761",
            "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic",
            "text": "Existing model merging techniques can be broadly categorized into two main types (Yang et al., 2024): (i) Pre-Merging Methods: These methods focus on enhancing the conditions necessary for effective model merging by optimizing the fine-tuning process of individual models. (ii) During Merging Methods: These approaches address task conflicts and interference through various strategies before executing the parameter merging operations. \n\nPre-Merging Methods. To establish better conditions for model merging, a significant body of work has focused on the fine-tuning processes of independent models. For instance, Ortiz-Jimenez et al. ( 2024) propose fine-tuning within the tangent space of the pre-trained model, leveraging the NTK framework to enhance performance during the fine-tuning stage. However, fine-tuning all parameters in the linearized model can be computationally intensive compared to nonlinear finetuning. To mitigate this issue, some studies advocate for selectively linearizing only certain layers; for example, Tang et al. (2023) suggest partially linearizing Adapter modules prior to merging them. Additionally, TAFT (Liu et al., 2024) introduces an efficient linearization method specifically for Transformer architectures, deriving closed-form linearized solutions that facilitate smoother integration of models. Overall, fine-tuning in the tangent space aids in disentangling both input and weight spaces, thereby reducing potential interference during subsequent model merging. \n\nDuring Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018;Ilharco et al., 2023;Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023).",
            "score": 0.4975283969804201,
            "section_title": "RELATED WORK",
            "char_start_offset": 23891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 1032,
                    "end": 1050,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1862,
                    "end": 1884,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1884,
                    "end": 1905,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1905,
                    "end": 1927,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "275336686",
            "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion",
            "text": "Large Language Models (LLMs) have achieved remarkable results across tasks such as open-domain dialogue, mathematical reasoning, and code generation. However, no single model consistently outperforms others in all domains, making it essential to fuse domain-specialized source models into * Corresponding author. a unified pivot model that preserves the strengths of each. \n\nThis fusion process is complex, as source models differ in tokenization, vocabulary, and representational assumptions. Simple aggregation or standard knowledge distillation methods (Hinton et al., 2015) often fail to effectively capture the domainspecific expertise and uncertainty of source models, leading to pivot models that cannot fully utilize the combined knowledge of their sources. \n\nTo address the challenge of model fusion, we explore two distinct paradigms. The first is a pairwise fusion approach, inspired by FuseChat (Wan et al., 2024c), which incrementally integrates each source model into a pivot model. This approach has demonstrated strong performance by effectively preserving the strengths of individual source models. However, it requires K separate fusion training stages for K source models, making it computationally expensive and resource-intensive. Furthermore, the sequential nature of the process increases the risk of error propagation across stages, which can impact the final performance in certain cases. In contrast, the second paradigm is a unified pipeline approach that aims to fuse all source models simultaneously. \n\nSome studies, such as (Timiryasov and Tastet, 2023), adopt a naive method that directly sums the KD losses from multiple teachers for optimization. Others, such as (Wan et al., 2024a), employ the MinCE method, which selects a distribution matrix based on the lowest cross-entropy score. While these methods reduce the complexity of training, they often fail to match the performance of pairwise approaches, particularly in leveraging the domainspecific strengths of individual models. \n\nBuilding on these paradigms, we propose an uncertainty-based distribution fusion (UDF) method. By dynamically weighting the contributions of source models based on their predictive uncertainties, UDF achieves a balance between the efficiency of unified approaches and the effectiveness of pairwise fusion, offering a more cohesive integration of diverse model outputs.",
            "score": 0.4969247739647914,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2016
                },
                {
                    "start": 2019,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 1696,
                    "end": 1715,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "274981481",
            "title": "Parameter-Efficient Interventions for Enhanced Model Merging",
            "text": "Multi-task learning (MTL) assumes simultaneous accommodation of knowledge from multiple tasks using a common backbone [2,34]. This approach offers several benefits, including better performance, eliminating the need for separate task-specific models, and facilitating knowledge transfer between related tasks [39]. However, this approach is challenging, as it can lead to negative transfer or task interference. Furthermore, MTL necessitates collecting data from all tasks for joint training, which can be costly in terms of larger model size, extended training times, and usage of computational resources. Additionally, data privacy concerns may complicate the implementation of MTL, especially in sensitive domains. MTL also lacks the flexibility to incorporate new tasks without retraining the entire model [47]. \n\nModel merging has emerged as a promising alternative to traditional joint MTL to address these limitations [20]. Model merging aims to fuse the abilities of multiple task-specific models into a single multi-task model while keeping the parameter size the same as that of the individual models. Practitioners now follow a standard workflow where they fine-tune a pre-trained foundational model for various target tasks. This approach corresponds to model merging more than tradi- IntervMerge consistently demonstrates superior performance compared to the state-of-the-art Surgery approach in multi-task model merging. This advantage is particularly evident when utilizing our efficient mini-intervention mechanism, which achieves better results than Surgery while employing three times fewer parameters. It is important to note that Inter-vMerge may exhibit more additional parameters than Surgery for certain ranks, as it is applied across many network blocks. \n\ntional multi-task learning. Consequently, large language models (LLMs) have effectively utilized the benefits of this procedure [17,37,42,12,48,1,29,28]. Merging offers several advantages. It eliminates the need to collect and manage data from all tasks, reducing training costs and data privacy concerns. Model merging enables greater flexibility in incorporating new tasks, supporting continual learning [22]. Furthermore, model merging can enhance the robustness and improve the handling of distribution shifts [27,38]. To achieve these benefits, merging methods interpolate between the parameters of task-specific models [15,14,37].",
            "score": 0.4957095738062755,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 121,
                    "end": 124,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1922,
                    "end": 1925,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1930,
                    "end": 1933,
                    "matchedPaperCorpusId": "259096117"
                },
                {
                    "start": 2295,
                    "end": 2299,
                    "matchedPaperCorpusId": "254877458"
                },
                {
                    "start": 2299,
                    "end": 2302,
                    "matchedPaperCorpusId": "237420687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "276884783",
            "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
            "text": "Recently, the DeepSeek-R1 model has achieved great success, and its released R1-Distill models (DeepSeek-AI, 2025) demonstrated that distilled small models can be superior in reasoning. Building smaller-scale models is also beneficial for deployment and reducing inference costs. However, developing smaller yet powerful models is a key challenge in Large Language Models (LLMs). \n\nThe most effective method, to our knowledge, is distilling a smaller model from a bigger teacher model across various domains (Jiao et al., 2020;DeepSeek-AI, 2025;Team, 2025a;Muennighoff et al., 2025). However, this method has a fundamental limitation: it requires carefully selecting the most relevant data/domains and tuning their proportions for joint training, which is typically time-consuming and error-prone (Guo et al., 2019;Ji et al., 2024). Furthermore, optimizing many domains simultaneously can lead to conflicting gradients, where tasks interfere, impeding overall learning progress (Yu et al., 2020;Jiang et al., 2024). These problems limit the effectiveness and efficiency of naive data mixed distillation, often resulting in models that cannot achieve the performance levels desired for specialized tasks. \n\nTo address these issues and optimize performance across multiple areas, we propose an approach, namely branch-merge, which integrates a model-merging technique during the distillation. Our branch-merge distillation approach contains two phases as follows. \n\n\u2022 Branch Phase: Knowledge is selectively distilled from a unified large teacher model (e.g., DeepSeek-R1 671B) to instruct several specialized student models (e.g., math, coding, science) through domain-specific SFT. \n\n\u2022 Merge Phase: The specialized models are combined into a single unified model, enabling crossdomain knowledge transfer while preserving their original specialized capabilities.",
            "score": 0.49480858397141086,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 815,
                    "matchedPaperCorpusId": "102353198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "271903078",
            "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
            "text": "Merging models involves combining the parameters of two or more specialized models to create a unified multi-task model [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [38] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27], Fisher-Weighted averaging [20], Git Re-Basin [21], TIES-Merging [28], and DARE [22], to efficiently merge models trained on distinct domains. Considering the trend of scaling models to billions of parameters and the cost of fully fine-tuning these models, there is an increasing need to explore the feasibility of merging multiple adapters within a single language model when adapters are used to fine-tune the models. Works such as Adaptersoup [23] have investigated this approach for both in-domain and out-of-domain evaluation of mixed adapters. Another study has conducted a comprehensive analysis of the effectiveness of different adapter architectures for the indomain generalizability of merged adapters [24]. In this work, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
            "score": 0.4929400484656804,
            "section_title": "C. Merging",
            "char_start_offset": 7852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "256846453"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.607421875
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "In this paper, we introduce a novel approach for addressing the multi-objective optimization problem in large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human knowledge or intuition. Second, it's difficult to obtain the great model merging configuration in limited evaluations. To address these challenges, we formalize model merging as a multi-objective optimization problem and propose an automated optimization approach named MM-MO. This method leverages multi-objective optimization algorithms to autonomously search for optimal merging configurations across various tasks, alleviating the need for human intervention. In MM-MO, a weak-to-strong method is employed to enhance the acquisition function, allowing previously evaluated superior configurations to guide the search for new ones. Meanwhile, Fisher information is applied to screen these configurations, increasing the possibility of identifying high-quality merging configuration. Additionally, we designed a sparsity metric as an additional optimization objective to enhance the model's generalization performance across different tasks. We conducted comprehensive experiments with other mainstream model merging methods, demonstrating that the proposed MM-MO algorithm is competitive and effective in achieving high-quality model merging.",
            "score": 0.4925692697954701,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "273403613",
            "title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities, attributed to emergent abilities that arise with scale and diverse training data. Different organizations are developing their own LLMs, each excelling in specific reasoning tasks. In the context of LLMs, reasoning refers to the capability to perform logical inference, understand complex relationships, and solve problems that require multi-step thought processes, e.g., natural language reasoning, code programming, mathematical problem-solving with or without tools. However, applications in many other domains might require a combination of these abilities. For instance, in educational platforms or automated theorem proving, an LLM needs to comprehend complex mathematical concepts, reason through problems, and generate correct and efficient code solutions. An intuitive solution is to aggregate the datasets used to train these specialized LLMs and develop a more comprehensive, generalpurpose model. However, this approach could be more practical due to the enormous computational resources and human labor required. Moreover, the proprietary or sensitive nature of the data used to train individual models frequently restricts access, limiting the feasibility of data-centric methods. Therefore, there is a pressing need for efficiently building large models over off-the-shelf models (MoM), combining abilities without retraining or access to the original training data. Model merging has emerged as a promising avenue to address this challenge. Early approaches, such as weight averaging (Utans, 1996;Smith and Gashler, 2017), laid the foundation for techniques like Linear Mode Connectivity (LMC) (Garipov et al., 2018), which facilitates the merging of models trained from a common base. Methods like Model Soups (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2022) further exploit these principles by averaging weights or performing arithmetic operations on task-specific vectors. More advanced strategies such as TIES (Yadav et al., 2023) and Git-Rebasin (Ainsworth et al., 2022) address challenges related to permutation symmetries, enabling the alignment of models with Figure 1: The framework on unconstrained model merging. We first establish a robust evaluator and select the top-ranking domain-specific small models (DSSMs) with the strongest math or coding abilities.",
            "score": 0.49168044098060915,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 1585,
                    "end": 1609,
                    "matchedPaperCorpusId": "263858956"
                },
                {
                    "start": 1682,
                    "end": 1704,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1799,
                    "end": 1822,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "277940324",
            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
            "text": "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss. \n\nTypical model merging scenarios often require combining pre-existing expert models-each specialized in a specific task-into one unified system. However, this static approach falls short in scenarios where new tasks emerge over time. In continual learning, we face the challenge of incrementally integrating new task-specific models without retraining the entire system. Recent advances in dynamic model merging address this by tackling issues such as parameter interference, memory efficiency, and sequential integration, enabling systems that adapt more effectively as new tasks are encountered. For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios.",
            "score": 0.4902337505669856,
            "section_title": "Model Merging",
            "char_start_offset": 74079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2574
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "277043340",
            "title": "Taming Knowledge Conflicts in Language Models",
            "text": "Subs-Conflict Coh-Conflict contextual information-a phenomenon known as knowledge conflict (Xu et al., 2024). In such cases, the model may become uncertain about which source of knowledge to trust. These conflicts are particularly prevalent in real-world applications, especially in context-heavy Large Language Models (LLMs) systems like retrieval-augmented generation (RAG) (Gao et al., 2023), LLM agents (Xi et al., 2023), and tool-augmented LLMs (Qu et al., 2024). Depending on the application, user may require an LLM to either remain faithful to its parametric memory or prioritize contextual reliance for accurate and reliable outputs. \n\nPrior works have explored the behavior of LMs under knowledge conflicts, either by treating the model as an oracle to analyze how different contexts influence its predictions (Xie et al., 2023) or by treating the context as an oracle to evaluate how effectively the model follows it (Longpre et al., 2021). While these studies provide valuable insights into knowledge conflicts, the intrinsic mechanisms underlying these conflicts and corresponding mitigation strategies largely remain unexplored. Some studies have taken important steps to characterize (Yu et al., 2023) and intervene (Jin et al., 2024b) in knowledge conflicts, primarily focusing on a single conflict type (e.g., substitution-based conflicts). While pioneering, these efforts leave opportunities for more comprehensive understanding of diverse conflict types and the development of fine-grained approaches to address knowledge conflicts. In addition, much of the existing literature predominantly adopts a single-sided perspective on knowledge conflict, focusing on enhancing contextual reliance and addressing issues commonly referred to as \"RAG hallu-cination\" (Goyal et al., 2024;Huang et al., 2023;Shi et al., 2023b). In contrast, we advocate for a unified method capable of flexibly steering the model toward either parametric or contextual knowledge, offering broader utility. \n\nIn this paper, we begin by treating LMs as an oracle and considering the setting of factual recall, a task requiring pure memorization.",
            "score": 0.489799402095327,
            "section_title": "Clean",
            "char_start_offset": 301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 1199,
                    "end": 1216,
                    "matchedPaperCorpusId": "265067168"
                },
                {
                    "start": 1797,
                    "end": 1816,
                    "matchedPaperCorpusId": "265067168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2305908203125
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "By merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.",
            "score": 0.4892143694501269,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "276409294",
            "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
            "text": "Large Language Models (LLMs) have demonstrated remarkable success in various tasks such as natural language understanding, text summarization, and machine translation. However, their general-purpose nature often limits their effectiveness in domain-specific applications that require specialized knowledge, such as healthcare, chemistry, or legal analysis. To address this, researchers have explored diverse methods to enhance LLMs by integrating domain-specific knowledge. In this survey, we provide a comprehensive overview of these methods, which we categorize into four key approaches: dynamic knowledge injection, static knowledge embedding, modular adapters, and prompt optimization. Each approach offers unique mechanisms to equip LLMs with domain expertise, balancing trade-offs between flexibility, scalability, and efficiency. We discuss how these methods enable LLMs to tackle specialized tasks, compare their advantages and disadvantages, evaluate domain-specific LLMs against general LLMs, and highlight the challenges and opportunities in this emerging field. For those interested in delving deeper into this area, we also summarize the commonly used datasets and benchmarks. To keep researchers updated on the latest studies, we maintain an open-source at: https://github.com/abilliyb/Knowledge_Injection_Survey_Papers, dedicated to documenting research in the field of specialized LLM.",
            "score": 0.4883432802700345,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43994140625
        },
        {
            "corpus_id": "276574910",
            "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
            "text": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.",
            "score": 0.48789396991463185,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32275390625
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "We experiment with five model merging methods: \n\n\u2022 Linear (Model Soups) [12]. Model Soups improves model accuracy by averaging the weights of multiple models fine-tuned with different hyperparameters. This method enhances robustness without additional inference or memory costs, making it effective for large pre-trained models in tasks like image classification and natural language processing. \n\n\u2022 Task Arithmetic [13]. Task Arithmetic uses task vectors, which represent directions in the weight space of a pre-trained model, to modify and combine model behaviors. By performing arithmetic operations on these vectors, the model's performance can be steered across multiple tasks efficiently and effectively, without requiring additional training data for each task. \n\n\u2022 TIES [17]. TIES (TRIM, ELECT SIGN & MERGE) addresses parameter interference in model merging by resetting minimally changed parameters, resolving sign conflicts, and merging only aligned parameters. This method outperforms existing techniques in various settings, maintaining valuable information and ensuring better performance in multi-task models. \n\n\u2022 DARE [21]. DARE (Drop And Rescale) sparsifies delta parameters by setting most to zero and rescaling the rest. This approach mitigates parameter interference when merging multiple fine-tuned models into one, enhancing capabilities without retraining. DARE proves particularly effective for large-scale language models, often surpassing the performance of individual source models. \n\n\u2022 Model Breadcrumbs [41]. Model Breadcrumbs enhance task performance by defining a sparse set of weights that create a trajectory within the weight space of a pre-trained model. This method improves efficiency and performance across multiple tasks without needing hyperparameter tuning.",
            "score": 0.4876736665681787,
            "section_title": "B.4 Descriptions of Model Merging Methods",
            "char_start_offset": 36923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "269187724",
            "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
            "text": "Multilingual Interference.Multilingual training enables knowledge transfer but also causes interference, largely due to optimization conflicts among various languages or tasks (Wang and Zhang, 2022).Methods addressing conflicts between tasks hold promise to reduce interference (Wang et al., 2020), yet they show limited effectiveness in practical applications (Xin et al., 2022).Scaling up model size reduces interference directly but may lead to overly large models (Chang et al., 2023), with risks of overfitting (Aharoni et al., 2019).\n\nLanguage-Specific Modeling.Modular-based approaches enhance the unified model by adding language-dependent modules such as adapters (Bapna and Firat, 2019) or languageaware layers (Zhang et al., 2020b).Although the unified model serves as a common foundation, these approaches struggle to facilitate knowledge transfer among isolated modules due to a lack of clear inductive biases and thus heavy reliance on heuristics.For instance, Chronopoulou et al. (2023) rely on priori knowledge to control parameter sharing in language family adapters, Bapna and Firat (2019); Pires et al. (2023) isolate modules per language, hindering knowledge sharing.\n\nAdditionally, these modular-based methods substantially increase the number of parameters, thereby leading to increased memory demands and slower inference times (Liao et al., 2023a,b).Despite adapters normally being lightweight, they can easily accumulate to a significant parameter growth when dealing with many languages.In contrast, our method leverages the model's intrinsic modularity signals to promote task separation, without adding extra parameters.\n\nSub-networks in Multi-task Models.The lottery ticket hypothesis (Frankle and Carbin, 2018) states that within dense neural networks, sparse subnetworks can be found with iterative pruning to achieve the original network's performance.Following this premise, recent studies attempt to isolate sub-networks of a pre-trained unified model that captures task-specific features (Lin et al., 2021;He et al., 2023;Choenni et al., 2023a).",
            "score": 0.4861240729749855,
            "section_title": "Related Work",
            "char_start_offset": 4126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 26,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 539
                },
                {
                    "start": 541,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1187
                },
                {
                    "start": 1189,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1648
                },
                {
                    "start": 1650,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 198,
                    "matchedPaperCorpusId": "245502478"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "222291168"
                },
                {
                    "start": 516,
                    "end": 538,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 673,
                    "end": 696,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 721,
                    "end": 742,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 975,
                    "end": 1001,
                    "matchedPaperCorpusId": "257833499"
                },
                {
                    "start": 1085,
                    "end": 1107,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 1109,
                    "end": 1128,
                    "matchedPaperCorpusId": "258479712"
                },
                {
                    "start": 1714,
                    "end": 1740,
                    "matchedPaperCorpusId": "53388625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30029296875
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "The rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.",
            "score": 0.48454864161783306,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 370,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 553,
                    "end": 572,
                    "matchedPaperCorpusId": "237485414"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 787,
                    "end": 803,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1193,
                    "end": 1209,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "254877510",
            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
            "text": "The dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021). This results in specialized models that are explicitly trained to run inference over a single domain and task. Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022). Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models. \n\nHowever, the multi-task learning setup suffers from two practical limitations. First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy or to guard intellectual property to annotations. Second, because a significant amount of data or task combinations are not beneficial to performance (Poth et al., 2021), building a single model requires training on all data set combinations to identify the optimal one, which can be prohibitive especially if there are many available source data sets or models. \n\nModel merging is defined as combining multiple models into a single one in parameter space without access to data (Matena & Raffel, 2021). This technique provides an alternative to building a single model while satisfying data privacy constraints. Weight merging algorithms usually also have a closed-form solution, making them very efficient as no retraining is necessary, thus enabling usage even when a large number of data sets or model combinations are available. Merging can be considered as an alternative to model ensembling (Opitz & Maclin, 1999;Rokach, 2010), where the outputs of individual models are combined to produce the final prediction.",
            "score": 0.4828481691727965,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 201,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 515,
                    "end": 542,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 560,
                    "end": 580,
                    "matchedPaperCorpusId": "248780114"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 764,
                    "end": 784,
                    "matchedPaperCorpusId": "220045358"
                },
                {
                    "start": 1355,
                    "end": 1374,
                    "matchedPaperCorpusId": "233289699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50537109375
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In this paper, following the settings of model merging [29,76,79], we consider the case of T tasks, where training for each task t starts from pre-trained model weight \u03b8 0 and fine-tunes on D \n\nHere F represents an arbitrary merging function. For example, in Task Arithmetic [28],  Although existing merging methods, like Task Arithmetic, can combine multiple task-specific models efficiently, they often exhibit significant performance gaps compared to single-task models. Prior research, such as Ties Merging [76], attributes this phenomenon to parameter interference. This term refers to the redundancy or sign discrepancies found in parameters located at the same position (e.g., self-attention weights) across different task models, which in turn result in information conflicting and performance loss. Additionally, task interference, as noted in multi-task learning literature [13,31], arises from the inherent differences between tasks. For instance, tasks such as summarization, mathematical reasoning, and code generation require the model to process information in distinct ways. These differences worsen interference when models trained on different tasks are merged. \n\nTo understand these performance drops, we conducted two experiments using Task Arithmetic. First, we fine-tuned Qwen-14B with LoRA, assigning non-overlapping modules to avoid parameter interference. Despite this, a 3.21% drop in performance occurred, indicating persistent interference. Second, using two similar summarization tasks (XSUM and DailyMail), we observed an 8.42% drop compared to individually fine-tuned models, confirming that interference persists even between similar tasks. These results suggest that interference in model merging is not limited to parameter-wise and task-wise issues.",
            "score": 0.4818511181340906,
            "section_title": "Analysis of the Performance Gap in Model Merging",
            "char_start_offset": 7535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "260611732"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "258865647"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "In the past year, we have witnessed significant advancements in open-source large language models (LLMs), many of which are available on the Hugging Face model hub (Wolf et al., 2020). These models are trained on datasets containing trillions of tokens and range from 1 to 70 billion parameters (Minaee et al., 2024;Zhang et al., 2024). The diversity of open-source checkpoints is remarkable, with a broad classification into pretrained models (Zhuang et al., 2021) and models fine-tuned for instruction-following across a range of domains, such as coding (Rozi\u00e8re et al., 2024) and medical applications (Wu et al., 2023). However, fine-(*) English Name is Andrew Dang tuning separate models for each specific task poses two key challenges: \n\n1. Each task-specific model must be stored and deployed independently, leading to increased storage and deployment costs. \n\n2. Independently trained models are unable to share insights across tasks, limiting their ability to enhance task-specific performance or generalize to other domains (Sanh et al., 2022;Ram\u00e9 et al., 2022;Yu et al., 2024). \n\nTraining these models from scratch is resourceintensive, as illustrated by the Mistral-7B model, which incurred costs between 2 to 3 million USD (Jiang et al., 2023). Further fine-tuning of pretrained models often results in catastrophic forgetting (Wang et al., 2024), where the model's original generalization capabilities degrade, impairing its performance across multiple tasks (Cheng et al., 2024;Wu et al., 2024). Moreover, aligning models to human preferences demands substantial effort and data collection, making it impractical for most research teams to replicate (Wang et al., 2023;Rafailov et al., 2023). \n\nThese challenges bring into focus the critical question of how to best utilize existing pretrained checkpoints for research and practical applications. In this context, model merging has emerged as a promising approach, combining parameters from multiple task-specific models into a single, unified model.",
            "score": 0.47988661311129177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 183,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 444,
                    "end": 465,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 1070,
                    "end": 1086,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1339,
                    "end": 1358,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 1472,
                    "end": 1492,
                    "matchedPaperCorpusId": "271745635"
                },
                {
                    "start": 1492,
                    "end": 1508,
                    "matchedPaperCorpusId": "266755997"
                },
                {
                    "start": 1683,
                    "end": 1705,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "Large language models (LLMs) pretrained on a wide-variety of corpora have achieved notable success in multiple tasks (Touvron et al., 2023;Ope-nAI, 2023;Brown et al., 2020;Liu et al., 2024a). With significant progress, there is increasing interest in how to continuously improve the performance of LLMs in new domains, including math (Yu et al., 2023), code (Roziere et al., 2023), Wikipedia knowledge (Shao et al., 2024), or legal domains (Cui et al., 2023). One straightforward approach is through continual pretraining (CPT) on domain-specific data, which, however, is challenging for multiple target domains, as it can cause catastrophic forgetting on previously learned tasks (Luo et al., 2023). \n\nAn alternative approach is Mixture-of-Experts (MoE) merging, where dense experts are first CPTed in parallel for each domain and then merged into a unified MoE model, usually by keeping feedforward neural network (FFN) layers separate and averaging non-FFN layers (Sukhbaatar et al., 2024;Kang et al., 2024). Compared with dense models of similar size, the MoE model uses just a subset of parameters during inference by learning to route tokens to the top few experts, thus reducing inference costs. Unlike training an MoE model from scratch, MoE merging offers modularity, as individual experts are domain-specialized, and is substantially less expensive, as CPT-ing experts in parallel requires less compute than training the entire MoE on large datasets from the beginning (Sukhbaatar et al., 2024). \n\nIn this paper, we investigate how to effectively merge different domain expert models into a unified MoE model. The current state-of-the-art (SoTA) MoE merging approach, such as Branch-Train-Mix (BTX) (Sukhbaatar et al., 2024) assumes experts are branched from the same ancestor model and merges experts by simply unweighted averaging the non-FFN layers.",
            "score": 0.47749294501851636,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "Additionally, Zheng et al. [239] discuss the topic of \"learning from models\" and it only mentions model merging as a subsection (single page only) in the whole paper. The most related work to the \"model merging\" topic is [108], but in terms of application, it only discusses model merging in three scenarios: federated learning, fine-tuning, and distillation. It also ignores a lot of recently published articles due to the rapid evolution of the model merging direction. To address these gaps, this survey aims to elucidate the methods, theories, applications, and future trends in model merging direction, providing a comprehensive classification of relevant approaches. In particular, this paper enhances the comprehensive understanding of model merging by covering three main aspects: First, how are existing model merging methods classified? We first propose a new taxonomy in Figure 2 (upper part) that divides existing model merging methods into two phases ( \u00a72): pre-merging and during-merging. (i) Pre-merging methods aim to create better conditions for merging. It is further divided into using linearized fine-tuning to achieve weight space and input space disentanglement, performing architectural transformations to convert heterogeneous models into homogeneous models, and aligning weights to place them in the same basin. (ii) During-merging methods focus on designing sophisticated techniques to merge multiple models into one. These methods address task conflict and interference problems when merging models. They can be further divided into basic merging methods that perform the simplest parameter merging strategy; weighted merging methods that merge multiple models according to the importance calculated by specific rules; subspace merging methods that project multiple models into sparse subspaces for merging; routing-based methods that dynamically merge models according to input samples during inference; and the post-calibration based method that corrects the merged model. In addition to these methods, we also discuss the theoretical or empirical analysis of model merging. \n\nSecond, which applications can benefit from model merging? We discuss in detail the various use cases of model merging in foundation models ( \u00a73) and over ten subfields of machine learning ( \u00a74). As shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models.",
            "score": 0.47592142450192587,
            "section_title": "Introduction",
            "char_start_offset": 2177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "Advancements in Natural Language Processing have enabled specialized language models, but integrating domain-specific knowledge into general-purpose models in multilingual settings remains challenging, particularly for technical vocabulary. This paper investigates the integration of technical vocabulary in merged language models and explores the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.",
            "score": 0.4757768092357937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "276287431",
            "title": "Speculate, then Collaborate: Fusing Knowledge of Language Models during Decoding",
            "text": "State-of-the-art large language models (LLMs), such as GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024), have demonstrated impressive capabilities in generating high-quality text across a variety of domains. These models are trained on vast datasets, allowing them to perform well on a wide range of tasks. However, despite their general effectiveness, no single LLM excels uniformly across all domains. Different models tend to have complementary knowledge, with each model specializing in certain areas. For example, one model may be more proficient in \u2020Work started and partially done during Ziyao's internship at IBM Research. 1 Department of Electrical and Computer Engineering, University of Maryland, College Park, College Park, MD, USA. 2 IBM Research, Yorktown Heights, NY, USA. 3 MIT-IBM Watson AI Lab, Cambridge, MA, USA. Correspondence to: Ziyao Wang <ziyaow@umd.edu>. \n\ntechnical writing, while another may outperform in creative tasks. This heterogeneity has led to an increasing interest in developing methods that can fuse the knowledge of multiple LLMs, enabling users to harness their collective strengths for more robust and versatile applications. \n\nTo address these challenges, recent research has shifted focus to test-time knowledge fusion, which eliminates the need for retraining by combining model outputs during inference. This approach allows users to leverage the complementary knowledge of multiple LLMs without the overhead of additional training. For example, Wang et al. (2023) proposed a method that selects expert models dynamically at inference time using supervised learning, while Ong et al. (2024) introduced a router model that optimizes the selection of models based on performance and cost. Other approaches focus on integrating outputs through the decoding process, such as token-wise decoding (Shen et al., 2024) and characterwise decoding (Gu et al., 2024), which combine outputs at a fine-grained level. Although these methods offer potential, they often struggle to balance strong knowledge integration with efficiency, which limits their practicality in real-world applications.",
            "score": 0.47555735858706866,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In this section, we focus on model merging research, for additional related work on multi-task learning and Mixture of Experts, please see Appendix B. Model merging aims to fuse multiple fine-tuned taskspecific models into one comprehensive multi-task model without additional training. FisherMerging [46] and RegMean [33], use straightforward weight averaging but require extra data and computation. Some works [1,21,58,60,70] bring models into a single low-loss basin and interpolate between them based on the linear mode connectivity (LMC) theory [15,18,20]. The weight permutations [1] and optimal transport [58] are utilized to better interpolate neural networks. However, recent studies [83] suggest that LMC might not always hold for fine-tuned models. Task-Arithmetic [28,51] extends averaging to arithmetic operations in the parameter space for finer control over model behaviors, but the interference between the multiple models can be an issue. To tackle this challenge, advanced merging methods like Ties-Merging [76], AdaMerging [78] and DARE [79] have been proposed. These methods aim to reduce task conflicts by addressing parameter redundancy or disagreements in signs, finding optimal merging coefficients, and reducing weight density, respectively. Jiang et al. [32] assume that test tasks are known and use task-specific knowledge to improve performance. However, this assumption is often unrealistic since real-world data distributions are unpredictable. In contrast, our method addresses merging interference by modularizing shared and task-specific knowledge. We handle heterogeneous test data scenarios by introducing dynamic merging techniques.",
            "score": 0.47490001449554975,
            "section_title": "Related Work",
            "char_start_offset": 5801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 305,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 415,
                    "end": 418,
                    "matchedPaperCorpusId": "256697368"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "211132598"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "3845139"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1042,
                    "end": 1046,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "277103728",
            "title": "Survey of Adversarial Robustness in Multimodal Large Language Models",
            "text": "MLLMs integrate multiple modalities into a unified framework, enabling cross-modal reasoning for tasks like captioning, retrieval, and translation [Carlini et al., 2024]. These models leverage large language model backbones, such as GPT-based models, to process textual data, while specialized encoders extract features from other modalities [Qi et al., 2023]. MLLMs enhance their ability to understand and generate information across diverse data sources by capturing intricate interdependencies among modalities [Zhu et al., 2024]. To achieve integration, MLLMs rely on fusion modules that align and combine multimodal features, often mapping them into shared latent spaces for unified representation. This facilitates interaction and alignment between modalities, enabling cohesive understanding and generation. The ability to unify and process diverse modalities not only broadens the scope of achievable tasks but also establishes MLLMs as a crucial technology for advancing cross-modal AI applications [Rafailov et al., 2024].",
            "score": 0.4744792427892288,
            "section_title": "Overview of MLLMs",
            "char_start_offset": 4582,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1032
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 169,
                    "matchedPaperCorpusId": "268296800"
                },
                {
                    "start": 342,
                    "end": 359,
                    "matchedPaperCorpusId": "267750414"
                },
                {
                    "start": 1008,
                    "end": 1031,
                    "matchedPaperCorpusId": "267750414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41162109375
        },
        {
            "corpus_id": "273228908",
            "title": "Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes",
            "text": "Model fusion, which involves the integration of capabilities from different models, can be mainly divided into two categories. Firstly, ensemble approaches combine the output of multiple models to enhance the overall prediction performance (Littlestone and Warmuth, 1994;Sagi and Rokach, 2018). Outputs are typically combined by weight averaging (Littlestone and Warmuth, 1994) or majority voting (Monteith et al., 2011). These ensemble approaches can improve the prediction performance of large-scale language models (Jiang et al., 2023). Secondly, weight merging approaches enable model fusion at the parameter level. Wortsman et al. (2022) show that weight averaging of multiple models fine-tuned with different hyperparameters improves prediction accuracy and robustness. Task vectors derived from model weights can be modified and combined together through arithmetic operations to steer the behavior of a resulting model (Ilharco et al., 2023). This approach can be enhanced by trimming task vectors and resolving sign conflicts before merging them (Yadav et al., 2023). In addition, Drop And Rescale (DARE) can be used as a general preprocessing technique for existing model merging methods to merge multiple task-specific fine-tuned models into a single model with diverse abilities (Yu et al., 2023).",
            "score": 0.47356095212822275,
            "section_title": "Model Fusion",
            "char_start_offset": 6437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 271,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 271,
                    "end": 293,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 346,
                    "end": 377,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 518,
                    "end": 538,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 620,
                    "end": 642,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 927,
                    "end": 949,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5234375
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "Model Merging. Model merging integrates existing models to handle multiple tasks without training [21,23,49], but it faces task interference challenges [36,48]. Simple averaging [44] results in great performance degradation. Methods like Fisher-Merging [33] and RegMean [23] determine merging coefficients using various matrices but incur high computational costs or data requirements, limiting their usage in edge deployment. Task Arithmetic [21] merges task vectors instead of weights, while Ties-Merging [48] resolves parameter conflicts, and AdaMerging [52] automates coefficient selection. However, these methods have requirements on data distribution and exhibit limited applicability. Techniques such as DARE [53] and PCB-Merging [12] analyze parameters to mitigate performance drops but still face task conflicts. EMR-Merging [20] and Twin-Merging [29] store task-specific knowledge but require large storage and overlook backbone optimization. Our method applies highpass filtering to reduce task interference and lightweight experts to enhance performance, achieving a balance between merging costs and effectiveness across various tasks. For further details, please refer to Appendix C.2. Model Ensemble. Model ensemble combines outputs from multiple models [11,15,22], but with large models, it faces storage and inference challenges. In contrast, Model merging enables a single model to solve multiple tasks, significantly reducing storage and inference costs. Multi-task Learning. Multi-task learning (MTL) aims to solve multiple tasks using a single model [4,28,41], but in the era of large models, it faces challenges like high training costs, data privacy issues, and expertise requirements [52]. In contrast, model merging uses existing open-source models to create a multi-task model with little or no training, significantly reducing deployment costs. b) The correlation between the merging performance of ViT-B-32 and the variance of frequency domain amplitude power. Normalized accuracy is defined as the ratio of merged model performance to fine-tuned model performance on the same task.",
            "score": 0.4730186459591378,
            "section_title": "Related Work",
            "char_start_offset": 5320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1269,
                    "end": 1273,
                    "matchedPaperCorpusId": "201667785"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "233033495"
                },
                {
                    "start": 1276,
                    "end": 1279,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "237571793"
                },
                {
                    "start": 1574,
                    "end": 1577,
                    "matchedPaperCorpusId": "261080843"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "276580914",
            "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
            "text": "Incorporating continual learning (CL) capability into large language models (LLMs) is essential for enabling them to acquire knowledge from diverse tasks sequentially, a critical requirement for adapting to ever-changing environments without extensive retraining (Wang et al., 2024b;Jiang et al., 2024;Yu et al., 2024;Chang et al., 2024). An effective CL system must address two key challenges: (1) Catastrophic Forgetting (CF) (McCloskey and Cohen, 1989), where previously acquired knowledge is lost when learning new tasks, and (2) Knowledge Transfer (KT) (Ke et al., 2021), which involves leveraging new, related tasks to improve performance on prior tasks, and vice versa. \n\nRecently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023;Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024;Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024;Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024).",
            "score": 0.4708736364305526,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 428,
                    "end": 455,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 558,
                    "end": 575,
                    "matchedPaperCorpusId": "244908578"
                },
                {
                    "start": 770,
                    "end": 789,
                    "matchedPaperCorpusId": "258833488"
                },
                {
                    "start": 1201,
                    "end": 1219,
                    "matchedPaperCorpusId": "270703371"
                },
                {
                    "start": 1504,
                    "end": 1522,
                    "matchedPaperCorpusId": "271915471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6591796875
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://github.com/tanganke/weight-ensembling_MoE",
            "score": 0.47056039526524157,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7197265625
        },
        {
            "corpus_id": "276557749",
            "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation",
            "text": "In the field of Natural Language Processing (NLP), the emergence of large language models (LLMs) (Brown et al., 2020;Touvron et al., 2023;OpenAI, 2023;Chowdhery et al., 2023) represents a revolutionary breakthrough. With their remarkable capabilities, these models have demonstrated outstanding performance across various tasks (Jiao et al., 2023;Chang et al., 2024b;Nam et al., 2024;Xing, 2024;Guo et al., 2024)  Supervised Fine-Tuning (SFT) is a crucial technique for adapting LLMs to specific tasks, refining their performance by training on domain-specific data (Hu et al., 2021;Ding et al., 2023;Xia et al., 2024). However, SFT requires substantial computational resources and long training times (Brown et al., 2020;Chang et al., 2024a). To address this challenge, Model Merging has emerged as an efficient solution, fusing the parameters of multiple fine-tuned LLMs into a unified model with diverse capabilities, without the need for additional training or computational costs (Yang et al., 2024;Akiba et al., 2024). It effectively reduces the resourceintensive demands of SFT while preserving and even enhancing model performance. \n\nA simple analogy for model merging is the Super Mario game, where the protagonist gains special abilities by absorbing power-up items. Similarly, merging model parameters integrates the strengths of different models, enabling more effective multi-task learning (Yu et al., 2024). However, existing model merging methods have some limitations, these methods heavily rely on predefined or heuristic parameter fusion strategies (Wortsman et al., 2022;Ilharco et al., 2022;Matena and Raffel, 2022;Jin et al., 2022;Yadav et al., 2023;Yu et al., 2024) such that they fail to fully explore the parameter space, thereby restricting the potential of the merged model. \n\nTo address this issue, we draw inspiration from Mixup (Zhang, 2017) and propose a novel technique called Mixup Model Merge (M 3 ).",
            "score": 0.47033056282389485,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 151,
                    "end": 174,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 347,
                    "end": 367,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 367,
                    "end": 384,
                    "matchedPaperCorpusId": "259937834"
                },
                {
                    "start": 384,
                    "end": 395,
                    "matchedPaperCorpusId": "266933455"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "257316425"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1403,
                    "end": 1420,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1567,
                    "end": 1590,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1611,
                    "end": 1635,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1671,
                    "end": 1687,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of natural language tasks, encompassing commonsense reasoning, question answering, and specialized domains such as mathematics and programming [39,43,52]. The effectiveness of LLMs is based on the scaling law, which posits that proportionally increasing model and training data size leads to enhanced model performance [27]. Nevertheless, the computation overhead and data requirement surge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs, aggregating existing models to construct a more versatile LLM emerges as an economical alternative to training a larger LLM from scratch [13,16,54]. This not only mitigates the computation cost but also leverages the collective advancements of previous efforts in building LLMs. Within different methods to combine existing LLMs, a major class is merging [2,4,22,24,35,59,63,64]. Model merging combines multiple models into a single one of the same size through weight-space transformation. Wortsman et al. [59] first propose to merge a few fine-tuned models as a training trick for the flat loss-landscape, and Ilharco et al. [22] extends it to multi-task scenario, both of which employ the simple averaging. Other works propose more complicated merging methods, leveraging weight sparsity [63,64] and non-uniform coefficient [4,35]. However, they assume that all candidate models are \"useful\" when merging. While this may hold for small-sized designed model collections, it may not be the case in real-world scenarios given a large and divergent model zoo. How to ensure the benefits of merging different model zoo sizes and similarities, and exclude \"harmful\" candidates, remains underexplored. Since merging is limited to the same model structures and initial weights, another alternative is Mixture-of-Experts (MoE) [16]. MoE is a conditional computation architecture that activates only a subset of model parameters for each specific input example [47]. MoE LLMs have already demonstrated performance and computational efficiency advantages over their dense counterparts [15,25,30,68].",
            "score": 0.4686255690333193,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.546875
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "First, we investigate the merging of finetuned models for different languages on the same task, which corresponds to multi-lingual single-task learning. \n\nLanguage knowledge interference yields imbalanced improvements: Table 1 shows the multilingual results of the ASR task with the highresource language set. On average, multi-lingual training slightly improves the pretrained model but significantly underperforms the finetuned models and merging methods. This may be due to negative interference between the knowledge of different languages, leading to gradient conflicts during training (Wang et al., 2020b). From a per-language perspective, it is observed that ca and fr achieve the largest improvements during fine-tuning while still showing significant improvements in multi-lingual training, whereas languages with smaller improvements during finetuning exhibit a substantial performance drop in multi-lingual training, even worse than the pretrained model. This indicates a strong language conflict in multi-lingual training, with ca and fr dominating. Additionally, we observe that the optimal learning rates for finetuned models vary significantly across languages (see Appendix A), while the unified learning rate configuration required by multi-lingual training prevents each language from reaching its optimal performance. \n\nModel merging mitigates language conflicts: In contrast, model merging methods show obvious improvements across almost all languages, demonstrating reduced conflict and better stability. Among model merging methods, TA outperforms WA due to its flexible scaling factor. Both MP-Merging and SVP-Merging further improve the performance of TA by reducing redundancy, and MP-Merging slightly outperforms SVP-Merging due to its finer-grained pruning. Combining the advantages of SVP and MP, LoRS-Merging achieves the best performance. \n\nTable 2 provides the multi-lingual results on ST task with the high-resource language set. The main conclusion is consistent with the ASR task: model merging methods still significantly outperform multi-lingual training, with LoRS-Merging achieving the best performance.",
            "score": 0.46817617055104815,
            "section_title": "Multi-Lingual Model Merging",
            "char_start_offset": 16495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "273162841",
            "title": "What Matters for Model Merging at Scale?",
            "text": "Model merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
            "score": 0.4677948803476974,
            "section_title": "BACKGROUND",
            "char_start_offset": 6956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "270843326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "276574910",
            "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks (Brown et al., 2020;OpenAI, 2024;Cai et al., 2024;Touvron et al., 2023;Bai et al., 2023). Al-technique that combines parameters from multiple fine-tuned models into a unified model, has emerged as a promising solution. \n\nPrevious research has shown that merging methods can lead to safety-utility conflicts, where improvements in general ability (e.g., mathematical reasoning) degrade safety safeguards (Hammoud et al., 2024). For instance, merging safety-aligned and math-specific fine-tuned models get an unsafe mathematical AI expert (left conversation in Fig. 1a), reducing safety capabilities by over 30%, as shown in Fig. 1b. To address this problem, additional alignment training has been employed to improve the safety capabilities of the merged model (Thakkar et al., 2024;Aakanksha et al., 2024). However, such consequential safety-specific training requires labeled data and training costs, limiting their applicability in privacy-sensitive or resource-constrained scenarios. More critically, these methods address symptoms rather than root causes-they neither analyze neuron-level conflicts nor resolve interference mechanisms. \n\nThe safety-utility conflicts stem from two fundamental limitations in existing methods: (i) Neuron misidentification: Previous merging methods rely on simplistic metrics like parameter magnitude to select neurons, failing to distinguish safety-related regions from LLMs and impair safety capacity (ii) Neuron interference: Neurons optimized for different tasks (e.g., safety and code generation) exhibit antagonistic updates during merging, causing destructive parameter collisions and severely reduced performance, as shown in Fig. 1b, and Fig. 1c. \n\nIn this paper, we propose LED-Merging, a simple and effective merging method to address the above problems. Specifically, LED-Merging has three steps, including Location, Election, and Disjoint Merging. For the Location, LED-Merging identifies critical neurons in both base and fine-tuned models using gradient-based attribution scores to avoid neuron misidentification.",
            "score": 0.466209702052102,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Multi-task learning (MTL) [4] enables models to handle multiple tasks efficiently by facilitating knowledge transfer. Model merging [22,67,68] emerged as an alternative to conventional MTL [20,28,40,76] by combining independently fine-tuned models into a single model. This approach allows for flexible adaptation, reduces computational overhead, and better preserves task-specific knowledge. A key challenge in model merging is balancing generalization and task specialization. On one hand, a merged model should retain shared knowledge across tasks [72,74], but on the other hand, it must preserve task-specific adaptations that are crucial for individual task performance [21,73]. \n\nEarly methods [23,71,72,74,75] primarily focused on constructing task-agnostic models by averaging weights or aligning parameters across models, assuming that similar models in weight space would naturally form a meaningful shared representation. However, this generalization-first * Co-corresponding author. T a s k A r i t h m e t i c [ICLR `23] RegMean [ICLR `23] T i e s M e r g i n g [NeurIPS `23] MagMax [ECCV `24] AdaMerging [ICLR `24] Consensus TA [ICML `24] Surgery [ICML `24] LiNeS [ICLR `25] Ours strategy often led to a loss of task-specific information, reducing performance on individual tasks. This issue became more pronounced when merging models trained on diverse or conflicting tasks, as simple averaging often failed to preserve specialized adaptations essential for high accuracy. \n\nRecent approaches [21,36,58,73] explored task-specific parameterization, with some incorporating additional components, in some cases, are trainable to enhance adaptation. These methods typically added task-specific modules or learnable merging coefficients, selectively adjusting how different tasks influence the final model. While this enhanced task adaptation, it also introduced challenges such as increased model complexity, higher computational costs in both training and inference, and scalability issues when applied to many tasks. Furthermore, many merging strategies still relied on a fixed shared model [11,21,23,36,58,72,73,75], often formed by merging encoders without adaptation.",
            "score": 0.465531902041138,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 29,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1512,
                    "end": 1515,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1515,
                    "end": 1518,
                    "matchedPaperCorpusId": "267365047"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "267412030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "254591386",
            "title": "Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?",
            "text": "Traditional multi-task learning architectures train a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a range of diverse tasks. Does this new architecture suffer from task-conflicts that require specialized training algorithms? We study how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures.",
            "score": 0.46532069643166263,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "Model merge refers to combining the parameters and features of multiple large pretrained models to generate a unified model that can perform better in multiple tasks. Through model merge, the advantages of different models can be utilized to enhance the model's generalization and multitask processing capabilities. In the general setting of model merge, given a set of K tasks and the corresponding pretrained or fine-tuned models, whose parameters are denoted as {\u03b8 \u03b8 \u03b8 1 , \u03b8 \u03b8 \u03b8 2 , . . . , \u03b8 \u03b8 \u03b8 K }. The goal of model merging is to combine these K models into a single model that can effectively handle all K tasks. It is important to note that these models are fine-tuned from the same base model with parameters \u03b8 \u03b8 \u03b8 base . The merging process can be represented as (Cong et al. 2024): \n\nwhere \u03b8 \u03b8 \u03b8 merge is the parameters of the merged model that can efficiently perform all K tasks; and g merge represents the model merging method. The illustration of the mathematical representation of model merging is shown in Fig. 1(b). Among existing model merge methods, average merging (Matena and Raffel 2022) is a common approach, which constructs merged models by averaging parameters expressed as \u03b8 \u03b8 \u03b8 merge = K k=1 \u03b8 \u03b8 \u03b8 k K . Model soups (Wortsman et al. 2022) generates a multifunctional composite merged pretrained model by simply linearly combining the parameters of multiple fine-tuned models, denoted as \u03b8 \u03b8 \u03b8 merge = K k=1 \u03bb k \u03b8 \u03b8 \u03b8 k . Task arithmetic (Ilharco et al. 2023) uses predefined scaling coefficients to differentiate the importance of various models, which is described as \n\n. Ties merging method (Yadav et al. 2024) addresses the task conflict problem in task arithmetic by pruning low-magnitude parameters, resolving sign discrepancies, and non-overlappingly merging parameters with consistent signs. DARE (Drop And REscale) merge method (Yu et al. 2024) sets most delta parameters denoted as \u03b4 k = \u03b8 \u03b8 \u03b8 k \u2212\u03b8 \u03b8 \u03b8 base to zero and rescales the remaining embeddings to approximate the original embeddings.",
            "score": 0.4636483237524883,
            "section_title": "Background",
            "char_start_offset": 6107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 1467,
                    "end": 1487,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1623,
                    "end": 1642,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.479736328125
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Integrating retrieved knowledge into LLM is a challenging task that demands seamless alignment between external knowledge and the LLM's internal knowledge [78]. Such integration is crucial for generating outputs that are not only semantically relevant but also coherent and contextually appropriate. One of the significant challenges is ensuring that the LLM can effectively interpret and utilize the retrieved information without introducing inconsistencies or factual inaccuracies, which requires strategies for merging diverse data types, such as text chunks, and structured data into a unified representation that LLM can process. \n\nFurthermore, the integration process needs to handle potential conflicts between retrieved knowledge and the LLM's pre-existing knowledge base. For instance, discrepancies or contradictions between newly retrieved information and the LLM's training data can lead to confusion and reduced output quality, which requires advanced techniques for conflict resolution and knowledge validation to ensure that the integrated information enhances rather than detracts from the generation process. Additionally, the temporal relevance of retrieved knowledge needs to be considered, particularly for applications requiring up-to-date information. This necessitates flexible integration strategies that prioritize recent and contextually relevant data, thereby maintaining the relevance and accuracy of the generated content over time.",
            "score": 0.4625257305699433,
            "section_title": "Seamless Knowledge Integration",
            "char_start_offset": 17752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1461
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "982868"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292724609375
        },
        {
            "corpus_id": "268704840",
            "title": "Amalgamating Multi-Task Models with Heterogeneous Architectures",
            "text": "Single-Task Learning Multi-Task Learning covered across multiple pre-trained models. Utilizing many of these models concurrently is not ideal, due to the computational cost of using multiple models as well as the issue of potential conflicts between model predictions. \n\nRecently, Knowledge Amalgamation (KA) (Shen et al. 2019a) has become a popular approach to combine the knowledge of multiple pre-trained models (teachers) into one unified compact student using only unlabeled data. The student's objective is to become a master of all tasks solved across all teachers. This unified student not only solves the potential scalability and conflict issue mentioned above but also mitigates costs in collecting the labeled data and in reusing the pre-trained teachers. Yet, an effective strategy for extracting and combining knowledge from disparate multitask teachers needs to be developed. An example of how KA could be used for MTL is shown in Figure 1. \n\nUnfortunately, as depicted in Figure 2, most existing KA works (Shen et al. 2019a;Luo et al. 2019;Thadajarassiri et al. 2021Thadajarassiri et al. , 2023) ) focus on amalgamating knowledge for only a single task. There are few initial works that began to study KA for multiple tasks (Ye et al. 2019;Shen et al. 2019b), though they all make the unrealistic assumption that the teachers have identical architectures. This is too restrictive in practice, as models that specialize on different task sets are rarely identical. \n\nProblem Definition. We propose to study the open problem of Amalgamating Multi-Task Models with Heterogeneous Architectures (AmalMTH) as illustrated in Figure 1. The goal is to train a multi-task student model using only unlabeled data and pre-trained multi-task models (teachers). The teachers may have different architectures and may each handle different sets of tasks. The student is trained to master the union of the teachers' task sets. \n\nChallenges. Three challenges arise for AmalMTH: \n\n\u2022 No labeled data. Traditional MTL methods are developed under the standard supervised setting, which requires labeled data for training. Without labels, these existing MTL methods are not applicable. Therefore, a solution that does not need labeled data must be developed.",
            "score": 0.4614566943444339,
            "section_title": "Heterogeneous Architectures",
            "char_start_offset": 2541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 268
                },
                {
                    "start": 271,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2178
                },
                {
                    "start": 2179,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 327,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 1021,
                    "end": 1040,
                    "matchedPaperCorpusId": "53231387"
                },
                {
                    "start": 1040,
                    "end": 1056,
                    "matchedPaperCorpusId": "195584436"
                },
                {
                    "start": 1056,
                    "end": 1082,
                    "matchedPaperCorpusId": "231658990"
                },
                {
                    "start": 1082,
                    "end": 1113,
                    "matchedPaperCorpusId": "259746042"
                },
                {
                    "start": 1256,
                    "end": 1274,
                    "matchedPaperCorpusId": "201107180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36279296875
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "To tackle the challenge of interference, we examine the merging process at a finer-grained knowledge perspective. We identify two types of critical knowledge: (1) Shared knowledge, which benefits multiple tasks, and (2) Exclusive knowledge, which is useful only for a specific task. Single-task models often contain both types, complicating the merging process and leading to interference. To validate our hypotheses, we conduct experiments that vary the ratio of task-specific and shared knowledge. \n\nTo examine the impact of shared knowledge, we conducted full fine-tuning on each model for its specific task. Excessive fine-tuning epochs can lead to catastrophic forgetting [19], a phenomenon where the model retains task-specific knowledge but loses general knowledge. As the fine-tuning epochs increase, the shared knowledge gradually decreases. The top section of Figure 3 illustrates that as the epoch count increases, merging performance significantly deteriorates, even though the fine-tuned model performs well on its task. This underscores the crucial role of shared knowledge in merging performance. \n\nTo explore the impact of exclusive knowledge, we merge a single task-specific model into the base model. We apply a sparsity method (e.g., SVD) to reduce the ratios of task-specific weights in the merging model from 100% (standard merging) to 0% (base model). As shown in the lower part of Figure 3, performance remains stable up to 90% sparsity. Notably, even with a 99% sparsity rate, a single-merged model outperforms multi-model merging, confirming the existence of exclusive knowledge, which is more pronounced with more models. This also underscores the value of unmerged task-specific knowledge, since the fine-tuning performance can be effectively restored by preserving unmerged task-specific information. \n\nTo summarize, both shared knowledge and un-merged task-specific knowledge play a vital role in merging performance. The exclusive nature of task-specific knowledge hinders the effectiveness of merging methods. Different types of knowledge need to be separated and modularized to achieve optimal performance. Thus, the first step of our Twin-Merging approach is to explicitly partition the weights into an expert containing shared knowledge and weights holding task-exclusive knowledge before merging.",
            "score": 0.46059542101775763,
            "section_title": "Interpreting Interference From the Perspective of Knowledge",
            "char_start_offset": 9383,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "2691726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488525390625
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "In this paper, we delve deep into the critical challenge of knowledge conflict in multi-task model merging with a focus on task arithmetic. We began by formalizing the concept of knowledge conflict as the degradation in model performance caused by the interference between task vectors. Our analysis and empirical findings suggest that components of task vectors orthogonal to the gradient direction exhibit minimal knowledge conflict. This insight motivates us to define a trust region based on orthogonality and propose Task Arithmetic in the Trust Region (TATR). Extensive experiments across eight diverse datasets demonstrate that TATR effectively mitigates the knowledge conflict, enhancing the overall multi-task performance of task arithmetic-based methods. \n\n\u2022 AdaMerging (Yang et al., 2024b) leverages an unlabeled test set to adaptively learn the merging coefficients at either a layer-wise or task-wise level in Task Arithmetic. \n\n\u2022 AdaMerging++ (Yang et al., 2024b) an enhanced version of AdaMerging, integrates the principles of Ties-Merging (Yadav et al., 2023). \n\n\u2022 Surgery (Yang et al., 2024a) introduces a feature transformation module, trained to align features during the merging process. In this work, we adopt the basic version of Surgery combined with task arithmetic for evaluation iii) Training-free methods: \n\n\u2022 Weight Averaging directly averages model parameters from multiple tasks into a single model, enabling multi-task learning without additional training. \n\n\u2022 Fisher Merging (Matena & Raffel, 2022) leverages the Fisher information matrix to assess parameter importance, merging model parameters based on this importance. \n\n\u2022 RegMean (Jin et al., 2023) refines weight matrices by adjusting and linearly combining rows, utilizing statistical information derived from the training data. \n\n\u2022 Task Arithmetic (Ilharco et al., 2023b) introduces the concept of a \"task vector,\" defined as the difference between fine-tuned model parameters and pre-trained model parameters. \n\nMultiple task vectors are then combined and added to the pre-trained model to facilitate multi-task learning. \n\n\u2022 Ties-Merging (Yadav et al., 2023) eliminates unimportant parameters from the task vector and resolves sign conflicts among parameters, reducing interference during the final task vector merging process. \n\niv) Our methods:",
            "score": 0.4605027011848495,
            "section_title": "CONCLUSION",
            "char_start_offset": 24124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1999
                },
                {
                    "start": 2002,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2318
                },
                {
                    "start": 2321,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 799,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 957,
                    "end": 977,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1089,
                    "end": 1109,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1666,
                    "end": 1684,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1837,
                    "end": 1860,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 2129,
                    "end": 2149,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "With the continuous success of large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series across a wide range of natural language processing (NLP) tasks, it has become a strategic imperative for corporations to create their own LLMs. However, the costs associated with LLM development are astronomical. In addition to requiring vast amounts of training data, advanced techniques, substantial computational resources, and skilled labor, the development process also exerts significant pressure on energy consumption and the environment (Rillig et al., 2023). While these LLMs exhibit structural and functional differences, they share similar capabilities across a spectrum of NLP tasks. Consequently, beyond the traditional approach of training an LLM from scratch, an alternative option is to combine existing LLMs into a new, more powerful one, which is termed knowledge fusion of LLMs in this paper. If successful, this fusion not only cuts the cost of initial training but also allows the integrated model to benefit from the strengths of all the LLMs. This new model can also be fine-tuned and adapted for various downstream tasks. Moreover, the fusion can also happen among fine-tuned LLMs that specialize in a specific task. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone & Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of different models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models and executing each during inference, which is impractical for LLMs due to their substantial memory and inference time requirements. Likewise, this approach doesn't facilitate fine-tuning, which is essential for many LLMs. Another approach is to directly merge several neural networks into a single network through parameter-wise arithmetic operations (Wortsman et al., 2022;Jin et al., 2022). This approach typically assumes uniform network architectures and attempts to establish mappings between the weights of distinct neural net- works, which is often unattainable in the context of LLMs. Moreover, weight merging may lead to suboptimal results when substantial differences exist in the parameter space (Li et al., 2022).",
            "score": 0.4600342186949592,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 92,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "257098877"
                },
                {
                    "start": 1402,
                    "end": 1431,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1971,
                    "end": 1994,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1994,
                    "end": 2011,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "249538647",
            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
            "text": "Specialized Models. Previous research focuses on building specialized models for specific tasks. \n\nCNNs [47,26,70] and ViTs [20,53,76,80] are developed for image classification. Subsequent works re-design them to adapt to diverse downstream visual tasks, e.g., object detection [63] and segmentation [15,48]. In NLP, different architectures are specifically designed for neural machine translation [77], natural language understanding [19], and natural language generation [51]. As for vision-language tasks, previous works usually combined modality-specific encoders and representation fusion modules together [13,54]. Recently, [89,65,71] integrate several specialized models into a single one to handle diverse tasks. Such integrated specialized models are equipped with multiple task-specific modules to adapt to as many downstream tasks as possible. However, these methods still follow the task-specific paradigm, which conflicts with the objective of generalist models. \n\nVanilla Generalist Models. Vanilla generalist models handle different tasks and modalities with shared parameters. Uni-Perceiver [93] formulates various perception tasks as finding the maximum likelihood target for each input through the similarity of their representations. OFA [79], Flamingo [3] and SimVLM [84] attempt to unify different tasks into sequence-to-sequence generation. UniCORN [86] and Gato [62] further incorporate bounding box and reinforcement learning tasks into the unified formulation, respectively. These generalist models not only achieve competitive performance on pre-training tasks with shared parameters, but also can perform zero-shot inference on new tasks [62,93]. However, these methods rarely investigate the potential interference among different modalities and tasks, which could result in the performance degradation of generalist models. \n\nMulti-Task Learning. Multi-task learning [8,17] has been widely studied in the community of vision [27,74,72], language [25,16,50] and vision-language learning [10,55,29]. While multi-task training enables collaboration between tasks, it may also introduce the task interference problem [81,83,28,36,72].",
            "score": 0.4597955919204352,
            "section_title": "Related Works",
            "char_start_offset": 4709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 134,
                    "end": 137,
                    "matchedPaperCorpusId": "232035922"
                },
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1896,
                    "end": 1899,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 1954,
                    "end": 1958,
                    "matchedPaperCorpusId": "54465873"
                },
                {
                    "start": 1958,
                    "end": 1961,
                    "matchedPaperCorpusId": "85544221"
                },
                {
                    "start": 1961,
                    "end": 1964,
                    "matchedPaperCorpusId": "159040666"
                },
                {
                    "start": 2019,
                    "end": 2022,
                    "matchedPaperCorpusId": "208637516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426513671875
        },
        {
            "corpus_id": "271947337",
            "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
            "text": "Training specialized, task-specific models presents several challenges, including the storage costs associated with maintaining multiple models, the substantial memory requirements for deploying these models, and the rapid obsolescence of models as training datasets age. One proposed solution to mitigate these issues is model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;Yadav et al., 2024;Yu et al., 2024) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values. \n\nAdditionally, DARE introduces random pruning to align more closely with the base model's performance (Goddard et al., 2024). More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures.",
            "score": 0.45870934426893195,
            "section_title": "A.1.2 Model Merging",
            "char_start_offset": 33952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 759,
                    "end": 778,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 778,
                    "end": 794,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "274610810",
            "title": "How to Merge Your Multimodal Models Over Time?",
            "text": "Foundation models consolidate a wide range of capabilities and knowledge into a single, large model [5]. Consequently, model merging [24,34,63,87] has emerged as a key technique for unifying multiple task-specific fine-tuned models derived from a shared base into a single model, effectively preserving the strengths of each specialized expert. Current model merging approaches typically assume a fixed base model that is fine-tuned independently on k diverse tasks and domains to produce a set of independent experts [17,24,34,64,89], which are then merged simultaneously. Research in this field has therefore focused primarily on improving merging techniques for larger or structurally differing k-sets, exploring the impact of the diversity and scale of finetuning domains, tasks and experts [27,69,86,89]. \n\nHowever, the world is constantly evolving, leading to continuous shifts over data distributions, domains, and tasks, with new concepts emerging [45,68] that may have been insufficiently covered during large-scale pretraining [20,22,32,45,57,65,66,[81][82][83]98]. This dynamic nature of realworld applications motivates a hitherto missing systematic exploration into temporal model merging (see Fig. 1) to better understand model merging along an additional, overlooked [13,100] axis: time. Specifically, in this work, we ask: (1) Is temporal model merging significantly influenced by the choice of initialization for expert training? (2) When evaluated over time, which model merging techniques emerge as most suitable? (3) Should merging strategies differ between initialization and model deployment? To answer these questions, we propose a unified framework for studying temporal model merging-TIME (Temporal Integration of Model Expertise)-structured around three key axes spanning the design space of temporal merging solutions (as shown in Fig. 2): 1. Initialization Phase. As expert models are trained and created continuously over time, initialization becomes a crucial design element. \n\n2. Deployment Phase. After training an expert on each task, the next step is to deploy a suitable final model. For temporal model merging, this process has to account for a varying number of past expert models and deployed variants, striking a balance between retaining past knowledge and incorporating new task-specific knowledge.",
            "score": 0.4583480195389339,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 522,
                    "end": 525,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "268733341"
                },
                {
                    "start": 799,
                    "end": 802,
                    "matchedPaperCorpusId": "259089314"
                },
                {
                    "start": 802,
                    "end": 805,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "252872997"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1044,
                    "end": 1047,
                    "matchedPaperCorpusId": "229156320"
                },
                {
                    "start": 1047,
                    "end": 1050,
                    "matchedPaperCorpusId": "252872997"
                },
                {
                    "start": 1050,
                    "end": 1053,
                    "matchedPaperCorpusId": "252111028"
                },
                {
                    "start": 1053,
                    "end": 1056,
                    "matchedPaperCorpusId": "259137560"
                },
                {
                    "start": 1059,
                    "end": 1063,
                    "matchedPaperCorpusId": "254069670"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49,24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization. Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54,55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes.",
            "score": 0.4574449156473954,
            "section_title": "Related Works",
            "char_start_offset": 5095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "11290566"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "275336686",
            "title": "InfiFusion: A Unified Framework for Enhanced Cross-Model Reasoning via LLM Fusion",
            "text": "We introduce InfiFusion, an efficient training pipeline designed to integrate multiple domain-specialized Large Language Models (LLMs) into a single pivot model, effectively harnessing the strengths of each source model. Traditional fusion methods either merge model parameters directly or rely on knowledge distillation with rigid assumptions, limiting their flexibility and efficiency. InfiFusion overcomes these limitations by enhancing Universal Logit Distillation (ULD) with Top-K selection and Logits Standardization. We propose two fusion strategies: Pairwise Fusion (InfiFusion$_p$), where each source model knowledge is distilled individually into the pivot model followed by merging and Unified Fusion (InfiFusion$_u$), where knowledge from all source models is distilled simultaneously into the pivot model. InfiFusion outperforms the state-of-the-art models, such as Qwen-2.5-14B-Instruct and Phi-4, across 11 widely applied benchmarks covering reasoning, coding, mathematics, and instruction-following tasks. Notably, InfiFusion achieves this superior performance while significantly reduces computational costs, completing full training with only 160 H800 GPU hours compared to the millions typically required for traditional LLM training.",
            "score": 0.45720327270487915,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.476806640625
        },
        {
            "corpus_id": "271874793",
            "title": "FuseChat: Knowledge Fusion of Chat Models",
            "text": "Large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series have demonstrated remarkable success across a wide range of natural language processing (NLP) tasks. Currently, it has become prevalent and imperative for individuals and corporations to build their own LLMs. However, the computational resources and time costs associated with LLM development remain prohibitively high. Furthermore, despite the structural and functional differences among LLMs, they often exhibit similar capabilities across various tasks. Therefore, besides training an LLM from scratch, another option is to combine the distinct advantages of existing LLMs into a more potent LLM, which is termed knowledge fusion of LLMs (Wan et al., 2024). Figure 1(a) illustrates the results of our preliminary experiment conducted on AlpacaEval 2.0 and MT-Bench, where we plot the percentage of questions each LLM answers best (measured by PairRM (Jiang et al., 2023)) among six prominent chat LLMs. These established LLMs, regardless of their potency, exhibit distinct strengths. Therefore, knowledge fusion not only reduces the developmental costs of creating a new LLM but also has the potential to integrate the diverse strengths of existing models. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone and Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of multiple models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models during inference, which is inefficient for LLMs due to their substantial memory and inference time requirements. Another approach is to directly merge several neural networks into a single network through arithmetic operations in the parameter space (Gupta et al., 2020), whereas this approach typically assumes uniform network architectures and requires manually-tuned (Wortsman et al., 2022;Yadav et al., 2024) or automatically-learned (Matena and Raffel, 2022;Jin et al., 2023) coefficients to merge the parameters of different neural networks.",
            "score": 0.4564311684403571,
            "section_title": "Introduction",
            "char_start_offset": 259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 743,
                    "end": 761,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1390,
                    "end": 1421,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1421,
                    "end": 1440,
                    "matchedPaperCorpusId": "259075564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44384765625
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "Recent advancements in large pretrained models have demonstrated remarkable performance and strong generalization abilities across various domains, such as natural language processing (Chang et al. 2024;Zhou et al. 2024b;Wu et al. 2024) and computer vision (Wang et al. 2024;Radford et al. 2021). Open-source communities have provided many pretrained models for various data types, as well as finetuned versions tailored to specific tasks. However, large finetuning models is often a complex process that requires vast amounts of high-quality data and computational resources. To address the challenge of building foundational models capable of handling diverse tasks under limited computational resources, model merging has gained increasing attention (Jang, Yun, and Han 2024). Model merging leverages existing pretrained models to flexibly transfer and inte-grate knowledge without requiring the original training data or additional model training (White 2016). This approach enables the creation of new models with higher generalization capabilities, suited to multiple tasks and scenarios. In recent years, model merging has become a simple yet popular method in pretrained models development, as illustrated in Fig. 1(a), with merged models showing significant potential on the Open LLM leaderboard (Myrzakhan, Bsharat, and Shen 2024). Current model merging methods primarily focus on merging models with the same architecture in the parameter space. In recent years, research in the parameter space has become quite extensive, including approaches like weight averaging (Matena and Raffel 2022), Model Soup (Wortsman et al. 2022), Ties merging (Yadav et al. 2024), and DARE (Drop And REscale) method (Yu et al. 2024). \n\nHowever, focusing solely on merging models within the parameter space significantly limits their practical utility. Models with different architectures exhibit broader diversity in representation capabilities and task types (Mellor et al. 2021;Zoph et al. 2018), potentially expanding the performance boundaries of merged models beyond those of a single architecture. Nevertheless, merging models across different architectures presents several practical challenges (Dong et al. 2021), leading to limited research in this area.",
            "score": 0.4561543169661399,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 257,
                    "end": 275,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1651,
                    "end": 1670,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1971,
                    "end": 1988,
                    "matchedPaperCorpusId": "12227989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358154296875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Despite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration. As illustrated in Figure 1, methods like Ties-Merging prioritize dimensions with larger magnitudes (e.g., the vertical axis in Task 1's vector), which can unintentionally overwrite critical information from Task 2. This imbalance skews the merged model towards Task 1, substantially degrading performance on Task 2 and undermining the overall multi-task capability. In contrast, the lowmagnitude horizontal axis associated with Task 2 carries valuable information while exerting minimal negative impact on Task 1's performance. Trimming these components is therefore counterproductive. This example highlights the dual objectives of effective model merging: (1) suppressing conflicts where dominant knowledge from one task undermines the performance of others, and (2) preserving the unique and essential knowledge required by each task. \n\nMotivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars. We evaluate CAT Merging on diverse visual, language, and visual-language datasets, demonstrating its superiority over state-of-the-art methods while maintaining robustness with limited exemplars.",
            "score": 0.45533673058826735,
            "section_title": "Introduction",
            "char_start_offset": 2119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "The goal of multi-task learning (MTL) is to enable a single model to perform multiple tasks simultaneously, thereby facilitating knowledge transfer between these tasks [20,123,162,172,218,228]. As shown in Figure 1(c), to avoid the high cost of joint training, a straightforward approach is to merge multiple independently trained models on different tasks to accomplish MTL. Almost all of the model merging methods discussed in \u00a72.3 can be used to merge multiple models trained on different tasks to perform MTL. In this section, we take some representative tasks as examples. For MTL tasks in computer vision, Task Arithmetic [71], Ties-Merging [216], AdaMerging [220] and other studies [176,219,223] proposed to combine ViT models trained on different visual classification tasks, and the obtained model can complete the object classification of multiple tasks. The results of Task Arithmetic [71] demonstrate that merging independently trained models from any two datasets yields a merged model whose performance is comparable to that of a singletask model. Similarly, ZipIt [170], which merges ResNet architectures trained on different tasks, achieves comparable results. For MTL tasks in natural language processing, DARE [226] introduces a method to assimilate homologous models, augmenting LLMs as a \"free lunch\". For instance, merging WizardLM with WizardMath significantly boosts WizardLM's performance on GSM8K (a benchmark for evaluating the mathematical reasoning ability of LLMs) from 2.2 to 66.3. Akiba et al. [6] suggest that directly merging an LLM with mathematical capabilities and an LLM with Japanese language proficiency results in a model capable of solving Japanese mathematical problems. Furthermore, numerous studies have demonstrated that combining PEFT modules (such as Adapter, LoRA or Soft-prompts) trained on different tasks can also achieve MTL [11,180,234].",
            "score": 0.45447939787032265,
            "section_title": "Knowledge Transfer in Multi-Task Learning",
            "char_start_offset": 58669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "254043876"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1228,
                    "end": 1233,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392578125
        },
        {
            "corpus_id": "277322544",
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "text": "Special Characteristics of Model Merging on Long-to-Short Reasoning In traditional model merging, the merging objectives can be categorized into two main branches: merging models from different domains into a unified model to integrate multi-domain capabilities, and merging checkpoints from the training process to enhance the final performance on a single task. Unlike multidomain model merging, which can typically decompose domain-specific knowledge from the task model, identifying feature vectors that represent quick-thinking or slow-thinking capacities poses a significant challenge. Moreover, the models being merged are often general-purpose models and are evaluated on general reasoning benchmarks rather than domain-specific datasets. In contrast to checkpoint merging, where the candidate models exhibit minimal parameter shifts, long-to-short merging involves models with substantial parameter differences, as well as variations in response style and downstream task performance, making the merging considerably more challenging. \n\nFrom Long-to-Short to Short-to-Long Unlike the training-based methods (Ma et al., 2025;Xia et al., 2025) which need to change the base model for training, model merging offers a more straightforward approach to achieving short-to-long adjustments in quick-thinking models by simply tuning the merging weights or selecting an appropriate base merging model, without the need for further training. For instance, in a simple attempt to achieve short-to-long reasoning, we assigned a small weight (e.g., 0.2) to the slow-thinking model while using the quick-thinking model as the base. This approach resulted in an approximate improvement of over 20 points in the overall score, accompanied by a 25%+ increase in response length. model merging provides a more effective and efficient solution for enabling System 1 models to acquire System 2 reasoning abilities compared to model distillation (Yu et al., 2024b). \n\nFailure Experience and Future Directions Apart from the successful attempts, we also encountered numerous failure cases in this study. Here, we share some key insights from these critical failures and provide potential future directions in this field. \n\nSensitivity of merging hyper-parameters. Most merging methods are sensitive to hyper-parameters. \n\nFor instance, task-vector-based merging methods involve a coefficient hyperparameter \u03b1 \u2208 [0, 1], which determines the contribution proportions of different merging models to the final model.",
            "score": 0.4523599992806915,
            "section_title": "Method",
            "char_start_offset": 30107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2208
                },
                {
                    "start": 2211,
                    "end": 2251
                },
                {
                    "start": 2252,
                    "end": 2307
                },
                {
                    "start": 2310,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 1133,
                    "end": 1150,
                    "matchedPaperCorpusId": "276421423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31005859375
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "The growing adoption of large foundation models is accompanied by significant practical challenges in terms of computational and storage demands (Kaplan et al., 2020). To address these challenges, multi-task model merging (Matena & Raffel, 2022) has emerged as a promising solution. For example, Task Arithmetic (Ilharco et al., 2023b) merges models by summing the task vectors from multiple tasks and applying them to the pre-trained model. Here task vectors are the difference in model parameters between the pre-trained foundation model and its fine-tuned version on a specific task. This approach builds a high-performance multi-task model by simple arithmetic operations in the model parameter space, thereby reducing computational overheads associated with fine-tuning on multiple tasks. \n\nDespite their successes, task arithmetic and its variants (Yadav et al., 2023;Wang et al., 2024;Yang et al., 2024b;a) still suffer from conflicts between task vectors. As illustrated in Figure 1, adding task vectors pointing to largely opposite directions may lead to catastrophic forgetting, and inconsistent task vector magnitudes may cause unbalanced merging, allowing the resulting model to be disproportionately influenced by a small subset of tasks. We refer to this issue as knowledge conflicts, represented as the expected performance variation of one task observed before and after merging another task vector. Knowledge conflicts differ from the typical notion of negative transfer (Yang et al., 2022;Meng et al., 2021;Liu et al., 2021b;Wang et al., 2023), as the former specifically refers to conflicts between predetermined, static task vectors, whereas the latter typically describes dynamic interference among tasks during training. Although current methods like sign alignment and test-time adaptation partially address knowledge conflicts, a thorough analysis of the root causes and a dedicated solution remain elusive. In scenario (a), the two task vectors contain largemagnitude components in opposite directions. In scenario (b), the difference in vector magnitudes causes the merged model to be dominated by one task. Both lead to suboptimal performance in one or more tasks. \n\nIn this paper, we propose a novel trust-region criterion for model merging, Task Arithmetic in the Trust Region (TATR), which addresses the knowledge conflict problem.",
            "score": 0.45207935413152656,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 245,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 312,
                    "end": 335,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 854,
                    "end": 874,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 874,
                    "end": 892,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 892,
                    "end": 911,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1488,
                    "end": 1507,
                    "matchedPaperCorpusId": "247011924"
                },
                {
                    "start": 1507,
                    "end": 1525,
                    "matchedPaperCorpusId": "238656924"
                },
                {
                    "start": 1525,
                    "end": 1543,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 1543,
                    "end": 1561,
                    "matchedPaperCorpusId": "266163541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Negative transfer remains a fundamental challenge in model merging for MTL, as merging often inherits task interference within the shared representation. The goal is to construct a single model that generalizes well across all tasks while minimizing such interference. Existing weight alignment methods [1,55,71] enable merging fine-tuned weights obtained from different pre-trained models. However, these approaches face scalability limitations as the number of tasks increases. A foundational work in model merging for MTL is Task Arithmetic [23], which introduces task vectors, defined as the difference between pre-trained and fine-tuned models. This enables arithmetic-based weight combinations across multiple tasks, allowing the merging of a large number of individual models. Subsequent methods refine this approach by explicitly addressing task conflict within the merged model. Ties-Merging [72] mitigates redundancy and sign conflicts, while DARE [75] removes redundant components and rescales key parameters. AdaMerging [74] learns task vector importance at test time, and LiNeS [64] reduces the impact of shallow layers to preserve general representations. While merged models have traditionally been considered task-agnostic, recent findings [19,63] suggest that they inherently encode task-specific knowledge. Motivated by these observations, recent approaches incorporate task-specific modules into the merged encoder to better align with the performance of individual models. Surgery [73] refines merged encoders by introducing taskspecific adapters, while EMR-Merging [21] applies taskspecific masking to a pre-defined merged model. Routerbased methods [36,58] determine task vector coefficients via routing mechanisms that depend on each task's input. However, these methods introduce additional parameters and often overlook the role of a well-structured shared expert in maximizing task-specific module effectiveness. \n\nModel Tinting explicitly refines both shared and taskspecific layers in a complementary manner, which allows it to easily capture hidden task-specific knowledge. This balanced design improves task adaptation and often surpasses individual model performance.",
            "score": 0.45185132788661697,
            "section_title": "Model Merging for Multi-task Learning",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "271957310"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1671,
                    "end": 1675,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "267365047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "273404117",
            "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
            "text": "In our initial experiments, we trained a separate LLAMA-3.1-8B-I model for each language, resulting in six distinct models. Recent research has investigated model merging, where models trained on different tasks or languages are combined to form a unified model. We apply linear (Wortsman et al., 2022) and TIES (Yadav et al., 2023) merging techniques to create a single Evaluator LLM for all six languages (Goddard et al., 2024), comparing these methods to joint fine-tuning, which combines data from all languages for training. Notably, all methods utilize the same total GPU hours across languages. The results in Table 6 show that model merging generally outperforms joint training and achieves performance comparable to individually trained models, particularly for high-resource languages like German and French. However, individually trained models still excel in low-resource languages. Overall, model merging proves to be a promising approach for developing unified multilingual evaluator LLMs, especially when balancing performance across high-resource languages. We also examined the rationales generated by the merged model and found them to be coherent, effectively justifying the assigned scores. Examples are provided in Appendix G.",
            "score": 0.4509459347399625,
            "section_title": "Single / Joint training / Weight Merging",
            "char_start_offset": 26221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1247
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 332,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4345703125
        },
        {
            "corpus_id": "271218446",
            "title": "OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces",
            "text": "Fusing knowledge from different sources is a classical and wildly-used method to develop robust AI models.Traditional ensemble learning methods [68,71] train models with different sub-datasets, and combine the output of different models as the final prediction.Similar ideas are also employed by large language model research, recent works [4,54,65] propose to merge multiple language models tuned for different downstream tasks, and the resulting model excels at all aspects.Moreover, the Mixture-of-Experts (MoE) language model [17,26,8] is also developing a hybrid model consisting of multiple sub-models and obtains better performance or efficiency by integrating them together.\n\nFor multimodal representation learning, C-MCR [58] and Ex-MCR [57] first propose to fuse two bi-modality representation space via the shared modality, thereby building unified space with low data and computing resource requirements.FreeBind [56] is a high-level abstract of the above two methods, which employs bi-modality spaces to augment pre-trained unified space.Unlike these methods that only involve a few numbers of spaces, our goal is to obtain large-scale omni representations via binding extensive spaces, which face more severe risks of knowledge interference.3 OmniBind",
            "score": 0.45069375806174705,
            "section_title": "Knowledge Fusion",
            "char_start_offset": 8066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 106,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 682
                },
                {
                    "start": 684,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 537,
                    "end": 539,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "258866011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50927734375
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.",
            "score": 0.4499454454555756,
            "section_title": "Introduction",
            "char_start_offset": 2335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 253,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 263,
                    "end": 280,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "The increasing prevalence of open-source models trained on diverse tasks provides unprecedented opportunities to utilize pre-trained weights for various applications. However, access to the original training data is often restricted due to privacy concerns, proprietary limitations, or other constraints, posing significant challenges for tasks requiring cross-domain capabilities [Jin et al., 2022]. Model merging techniques address this issue by enabling the combination of model weights without relying on original data, thereby equipping models with the ability to handle multiple tasks effectively. Currently, model merging has emerged as a promising solution, enabling the combination of multiple models with similar architectures to harness complementary strengths. This approach not only enhances task-specific performance but also fosters greater adaptability across tasks [Yang et al., 2024a;Tam et al., 2024]. \n\nModel merging provides several key advantages [Yang et al., 2024a;Yu et al., 2024b;Zhao et al., 2024]. Firstly, it allows for the aggregation of knowledge across multiple models without requiring extensive retraining, thereby offering a more resource-efficient alternative to traditional fine-tuning and transfer learning. Additionally, model merging can mitigate issues like catastrophic forgetting and offers a pathway to create models that encapsulate the strengths of multiple training regimes. For instance, weight average [Wortsman et al., 2022;Choshen et al., 2022] and task arithmetic merging [Ilharco et al., 2022] are widely adopted methods for retaining model capabilities while maintaining robustness across varied domains. \n\nIn recent years, model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements. These advancements underscore the potential of model merging as a versatile solution, capable of adapting to a range of tasks and minimizing resource demands.",
            "score": 0.44994258670938425,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 969,
                    "end": 989,
                    "matchedPaperCorpusId": "267499590"
                },
                {
                    "start": 1006,
                    "end": 1024,
                    "matchedPaperCorpusId": "259937385"
                },
                {
                    "start": 1524,
                    "end": 1546,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "We categorize the baselines into three main groups: Nonmerging methods, Training-free methods, and Test-time Adaptation methods. Non-merging Methods: \n\n\u2022 Pretrained employs pre-trained models for each task without integrating task-specific information, thereby serving as a fundamental benchmark. \u2022 Individual utilizes models that have been fine-tuned specifically for each task. In this context, there is no interference between tasks. \u2022 Traditional MTL involves joint training using training data from multiple tasks. This approach necessitates access to the original data and poses challenges in scaling to new tasks. Training-free Methods: \u2022 Weight Averaging is the most straightforward approach to model merging, achieved by directly averaging the pa- rameters of several models. \u2022 Fisher Merging [19] utilizes the Fisher information matrix to evaluate the significance of each parameter, guiding the merging process based on this assessment. \u2022 RegMean [11] aims to minimize the predictive differences between the merged model and the individual taskspecific models. \u2022 Task Arithmetic [10] introduces the concept of task vectors and achieves model merging by performing arithmetic operations on these task vectors, integrating them into a pre-trained model to facilitate multi-task learning. \u2022 Ties-Merging [39] mitigates task interference by eliminating redundant parameters, resolving sign conflicts, and averaging parameters that are consistent with the dominant direction. \u2022 Consensus Merging [33] improves the effectiveness of existing model merging techniques by removing selfish and catastrophic weights, which are important exclusively to individual task and irrelevant to the other tasks but detrimental to model merging. Test-time Adaptation Methods: \u2022 TW AdaMerging [42] employs entropy optimization to determine the merging coefficients for task vectors by leveraging an unlabeled test set. \u2022 LW AdaMerging [42] adopts entropy optimization to independently deduce the merging coefficients for individual layers of task vectors. \u2022 TW AdaMerging++ [42] implements entropy optimization to identify the merging coefficients for task vectors in Ties-Merging autonomously. \u2022 LW AdaMerging++ [42] applies entropy optimization to compute the merging coefficients for each layer of task vectors in Ties-Merging.",
            "score": 0.4492616579661907,
            "section_title": "Baseline Details",
            "char_start_offset": 30956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1090,
                    "end": 1094,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1782,
                    "end": 1786,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1924,
                    "end": 1928,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 2063,
                    "end": 2067,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "276885421",
            "title": "To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging",
            "text": "Multi-task learning (MTL) (Fifty et al., 2021) leverages transferable knowledge to handle multiple related tasks simultaneously. Existing MTL approaches primarily rely on architectural design or optimization strategies. Architectural-based methods, such as Mixture of Experts (MoE) (Shazeer et al., 2017), introduce specialized subnetworks that dynamically route inputs to task-specific experts, effectively reducing interference. However, these methods require modifying the pretrained model structure, increasing computational complexity, and limiting scalability (Liu et al., 2019;Shen et al., 2024). Optimization-based approaches, on the other hand, focus on balancing task gradients or loss functions to mitigate task conflicts during (Rosenblatt, 1958) Figure 2: Illustration of our proposed framework. Neuronal Task Vector: Neuronal task vectors \u03c4 for the k th neuron are defined as the difference between the fine-tuned and pre-trained neuron for each task. Decomposition: The pre-trained k th neuron is decomposed into its parallel and orthogonal complementary subspaces, followed by the projection of neuronal task vectors onto these subspaces. NeuroMerging: Our proposed NeuroMerging operates within these complementary subspaces for neuronal model merging. \n\ntraining (Bai et al., 2023;Kendall et al., 2018). While these methods improve convergence, they still depend on task-specific training data, which may be impractical in real-world applications due to privacy concerns or data scarcity (Liang et al., 2020). In contrast, model merging offers an alternative paradigm by integrating knowledge from multiple fine-tuned models into a single unified model without requiring additional training data or architectural modifications (Wortsman et al., 2022;Ilharco et al., 2023). Notwithstanding the promising findings, a key challenge in model merging is task conflict (Yadav et al., 2024;Du et al., 2024), where different tasks compete for model capacity, potentially leading to suboptimal performance. \n\nTo resolve task conflicts, existing model merging methods can be categorized into three levels based on their granularity.",
            "score": 0.446476844803185,
            "section_title": "Related Work",
            "char_start_offset": 5030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 584,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "12781225"
                },
                {
                    "start": 1298,
                    "end": 1319,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 1744,
                    "end": 1767,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1767,
                    "end": 1788,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1880,
                    "end": 1900,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1900,
                    "end": 1916,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56982421875
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "Large language models demonstrate strong multitask capabilities, effectively addressing a wide range of tasks across diverse domains [Brown et al., 2020;Radford et al., 2019]. \"Safety\" in a model can be viewed as another \"task-solving\" ability that a model can learn. It is well established that equipping a model with any kind of capabilities with the standard paradigm of training requires copious amounts of data. Multi-tasking abilities typically arise from fine-tuning models on mixed datasets, which combine data from various sources and across many tasks [Raffel et al., 2023;Wang et al., 2019;\u00dcst\u00fcn et al., 2024]. However, determining the optimal strategy for mixing datasets in multi-task training is often complex and resource-intensive, as it must ensure that all tasks benefit from the shared training process -especially in the context of safety, where the general performance of models often gets cannibalized in exchange for safety [Tsipras et al., 2019;Bianchi et al., 2024;Ray & Bhalani, 2024;\u00dcst\u00fcn et al., 2024]. \n\nMore recently, an emerging approach for enabling multi-tasking has focused on training distinct models for specific tasks, followed by a weight-merging process governed by a pre-defined algorithm [Tam et al., 2023;Yang et al., 2024;Li et al., 2024a;Wan et al., 2024;Zhou et al., 2024;Davari & Belilovsky, 2024]. This method has shown great promise in building models with new capabilities without incurring additional costs and challenges that accompany training from scratch. However, a We analyze the differences in merging models on trained with specialized multilingual datasets, particularly in the context of safety, in contrast to those trained directly on mixtures of these datasets. We follow the LLM-as-ajudge approach for evaluating the performance of these models along two axes -general and safety. \n\nkey question remains -how does it compare to traditional data mixing and weighting approaches?",
            "score": 0.44629200058430013,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 153,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "As the demand for versatile and powerful AI systems grows, the need to merge Large Language Models (LLMs) with specialized capabilities, such as multilingual skills or domain-specific knowledge, has become increasingly pressing. Effective model merging enables systems to leverage the unique strengths of individual models without necessitating extensive retraining. Merging also offers the potential to reduce catastrophic forgetting, a significant advantage in maintaining learned knowledge from each model (Sukhbaatar et al., 2024;Siriwardhana et al., 2024;Labrak et al., 2024). However, model merging remains inherently complex due to differences in training and fine-tuning processes, often requiring deep expertise and iterative tuning to achieve a balanced integration of the models' contributions. \n\nModel merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2024) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale. To gain deeper insight into the strengths and weaknesses of these approaches, we performed an in-depth comparative analysis of model merging techniques, spanning from basic averaging methods to more sophisticated automated approaches. \n\nBuilding on these insights, we introduce Differentiable Adaptive Merging (DAM), a new approach developed as a more efficient alternative to compute-heavy evolutionary strategies.",
            "score": 0.4455569868585606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 1018,
                    "end": 1041,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1085,
                    "end": 1105,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6640625
        },
        {
            "corpus_id": "273228754",
            "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought",
            "text": "In practical scenarios, models often require the integration of multiple capabilities to address a single task effectively. To quantify how a large language model can be boosted by the cooperation of multiple capabilities through the CoT mechanism, we introduce the \"Combination Law of RB\", giving a concrete formula of the upper-bound of the CoT. The law estimates the unified reasoning boundary B Acc=K1 (t 1 , t 2 , . . . , t n |m) for n tasks within a model m, which is formulated as: \n\nwhere B Acc=K1 (t i |m) denotes the reasoning boundary of model m for task t i . N i , and b i are scaling factors, which are only affected by the related task. As shown in Figure 1 (b), Equation (2) provides a mathematical formula to estimate the combined RBs from the independent ones, enabling deeper insights into model behavior for intricate tasks. See Appendix A for detailed mathematical analysis. \n\nFurthermore, the combination law for reasoning boundary demonstrates favorable theoretical properties, with broad applicability across diverse scenarios and flexibility in accommodating various boundary segmentation methods. For detailed practical application, please refer to Appendix B.",
            "score": 0.44520701418129993,
            "section_title": "Combination Law of Reasoning Boundary",
            "char_start_offset": 3102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13818359375
        },
        {
            "corpus_id": "273098762",
            "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "text": "Large Language Models (LLMs) have significantly revolutionized the natural language processing area, showcasing unparalleled abilities across various tasks (Achiam et al., 2023). Despite their versatility, LLMs exhibit limitations in specialized areas such as mathematics, programming, etc., which * Work done during an internship at Alibaba Cloud. \u2020 Tao Gong is the corresponding author. hinder the potential of wide-ranging applications. \n\nTo address these gaps, existing work (Liu et al., 2023;Wang et al., 2023) has sought to enhance the diverse skills of pre-trained LLMs through customized data strategies. However, they require extensive computational efforts and massive data volumes, challenging the widespread accessibility of LLM research. Furthermore, while Parameter-Efficient Fine-Tuning (PEFT) techniques offer a reduction in training requirements, their effectiveness tends to diminish (Biderman et al., 2024;Wu et al., 2024) compared to traditional fine-tuning methods, especially as the size of the model and the dataset grows. \n\nSubsequently, another line of research emerged, focusing on methods such as model merging (Akiba et al., 2024) and model expansion (Wu et al., 2024;Choi and Gazeley, 2024;Kim et al., 2023). Model merging methods strive to synthesize a multifaceted model that amalgamates insights from various pre-trained domain-specific LLMs, potentially crafting a model adept at addressing a multitude of tasks concurrently. However, the process of training multiple domain-specific LLMs is resource-intensive. On the other hand, model expansion methods, exemplified by Llama Pro, seek to refine pre-trained models for domain-specific applications in the post-pretraining phase by only fine-tuning the expanded layers. Therefore, it can employ significantly fewer trainable parameters than full model fine-tuning. \n\nHowever, present model expansion methods generally treat each part of LLMs equally, although different layers may exhibit varying sensitivity to incorporated knowledge. This lack of differentiation can result in less-than-ideal knowledge injection results. An intuitive idea is to inject knowledge into the most important layers so that the LLM can more sufficiently leverage the new knowledge without the overhead of redundant adjustments across all layers.",
            "score": 0.44489070722532587,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270751953125
        },
        {
            "corpus_id": "272910894",
            "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
            "text": "Driven by the growth of datasets, the increase in model size, and advances in generative language foundation models [Achiam et al., 2023;Touvron et al., 2023], multi-modal large language models (MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are spurring innovation across various visual and linguistic tasks [Chen et al., 2023a;Lyu et al., 2023;Su et al., 2023]. While significant strides have been made in building a unified foundation model for natural scenery [Chen et al., 2022;Lu et al., 2022Lu et al., , 2023]], the development of generalist medical artificial intelligence is still in its early stages [Moor et al., 2023a]. \n\nThe goal of a unified and generalist medical foundation model is to enable joint training on massive medical datasets. This model aims to handle multiple tasks and modalities within a single architecture with shared parameters [Zhang et al., 2023;Li et al., 2024]. It seeks to eliminate the need for task-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specific approach to model development [Wu et al., 2023b;Tu et al., 2024]. However, existing open-source efforts have not yet fully achieved these ambitious goals. \n\nA key challenge in creating a unified medical foundation model is the complexity of multi-modal, multi-task learning, often exacerbated by the tug-of-war problem [Hadsell et al., 2020]. Inherent task conflicts and data imbalances can cause interference during the simultaneous learning of different tasks. This problem is particularly acute in the medical field, where tasks and modalities are highly specialized and diverse. As a result, the performance of each task may degrade compared to taskspecialized models [Yu et al., 2020;Zhu et al., 2022]. \n\nTo mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs et al., 1991] into MLLMs.",
            "score": 0.44466099504870793,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 527,
                    "end": 542,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 925,
                    "end": 941,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 1127,
                    "end": 1143,
                    "matchedPaperCorpusId": "260164663"
                },
                {
                    "start": 1398,
                    "end": 1420,
                    "matchedPaperCorpusId": "226240885"
                },
                {
                    "start": 1751,
                    "end": 1768,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1768,
                    "end": 1785,
                    "matchedPaperCorpusId": "249538647"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41845703125
        },
        {
            "corpus_id": "271843401",
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "text": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
            "score": 0.44405659672154085,
            "section_title": "Model Merging",
            "char_start_offset": 5998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 334,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1339,
                    "end": 1359,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "For example, the UP-RISE framework employs adapters to select the most appropriate prompts from a pool for a specific zero-shot task [36]. This flexibility is particularly useful for enterprise systems, where multiple domain-specific adaptations may be required. \n\nAnother promising method for improving scalability is multi-agent architectures, where distinct agents handle specific system tasks, such as data ingestion, knowledge retrieval, and response generation. This division of labor distributes the workload more efficiently and allows LLMs to integrate smoothly with complex systems [145]. \n\nTo further enhance the system's ability to scale, model merging can combine the parameters of multiple pre-trained models into a unified model, integrating different task-specific capabilities into a single, more versatile LLM. Additionally, tool augmentation extends the LLM abilities by integrating it with external tools, APIs, and realtime data streams for specialized tasks like scheduling, booking, or domain-specific reasoning.",
            "score": 0.44394692909499334,
            "section_title": "Seamless integration of LLMs with existing systems",
            "char_start_offset": 79296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "Another significant challenge of merging multi-task models is mitigating the interference between parameters of different models, which can substantially deteriorate the average performance (Yadav et al., 2023;Yu et al., 2023;Tang et al., 2023b). Existing methods, while effective in some scenarios, may not be flexible enough to handle the dynamic nature of multi-task learning, where the optimal solution can vary depending on the input. \n\nTo address these challenges, we propose a novel approach to merge vision Transformers (ViTs). Our method merges most of the parameters while upscaling the multilayer perceptron (MLP) of the Transformer layers to a weight-ensembling Mixture of Experts (MoE) module. This module can dynamically integrate shared and task-specific knowledge based on the input sample, thereby providing a more flexible solution that can adapt to the specific needs of each instance. \n\nOur primary realization is that the issue of parameter inter-  Here we outline the detailed structure of the Weight-Ensembling MoE module, composed of the router, pre-trained MLP weights, and a collection of task vectors. Collaboration between shared weights and task vectors is employed to create input-conditioned weights dynamically. In this way, we separate shared information and task-specific knowledge, which are then combined based on input in time. \n\nference can be significantly alleviated by identifying and separating shared and task-specific knowledge and then dynamically integrating them. So we can leverage the shared knowledge that is beneficial across all tasks, while also taking into account the unique requirements. By dynamically combining these two types of knowledge based on the specific input data, we can create a more flexible and adaptable model that can effectively handle a wide range of tasks. \n\nWe validate our method through conventional multi-task model merging experiments and evaluate its generalization and robustness. The results demonstrate the effectiveness of our method and provide a comprehensive understanding. \n\nTo summarize, our contributions are as follows: \n\n\u2022 We propose a novel method to merge Transformerbased models. Our method is effective in transferring knowledge from various task-specific fine-tuned models and constructing a unified multi-task model.",
            "score": 0.4429729529700102,
            "section_title": "Introduction",
            "char_start_offset": 1869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2062
                },
                {
                    "start": 2065,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2316
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55029296875
        },
        {
            "corpus_id": "276409347",
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "text": "Large language models have achieved remarkable progress, demonstrating strong performance on a wide range of tasks (Touvron et al., 2023;Zhao et al., 2023). As researchers continue to fine-tune these models for specific domains, there is a growing need to combine their specialized capabilities into a single model (Yang et al., 2024;Goddard et al., 2024). While multi-task learning offers one solution (Sanh et al., 2022;Fifty et al., 2021), it \u2020 Corresponding author. requires extensive computational resources and simultaneous access to all task-specific datasets. Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining. \n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments. \n\nTo effectively balance performance and storage efficiency, we introduce 1bit-Merging, a novel dynamic merging framework that integrates taskspecific routing with 1-bit quantized task vectors.",
            "score": 0.4428413926572353,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 355,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 422,
                    "end": 441,
                    "matchedPaperCorpusId": "237485414"
                },
                {
                    "start": 617,
                    "end": 640,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 640,
                    "end": 661,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 661,
                    "end": 680,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 680,
                    "end": 697,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 899,
                    "end": 921,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 921,
                    "end": 940,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 940,
                    "end": 957,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1355,
                    "end": 1377,
                    "matchedPaperCorpusId": "259088823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69580078125
        },
        {
            "corpus_id": "247450558",
            "title": "Do BERTs Learn to Use Browser User Interface? Exploring Multi-Step Tasks with Unified Vision-and-Language BERTs",
            "text": "Prior studies have attempted to unify models for processing natural language to facilitate the transfer of learned knowledge by reducing task-specific structures. For example, Radford et al. (2018); Devlin et al. (2019) suggest that language models with a generic structure, Transformer (Vaswani et al., 2017), are effective. Raffel et al. (2020) proposed a text-to-text framework which converts tasks into a problem where a model receives and generates (b) In our WoB-like framework, we make a task as web pages, which allow structured contents, hyperlinks and scripts. The page design decides how to submit an answer (e.g., choose a button or input text). A model completes a task in multiple steps using the browser user interface (BUI). The model take a screenshot to output an action for each step. (e.g., click or keystroke). \n\ntext. Cho et al. (2021) extended the input of the text-to-text framework to accommodate images. However, existing research on unified models remains limited. First, although the models proposed by Cho et al. (2021) use a linear sequence of text and several images as input, their models are not designed to handle input with a layout. Second, existing unified models assume singlestep tasks. Task-specific design still must be completed when applying these models to compound tasks, such as reading a single document and subsequently searching for missing information. The latter challenge is more difficult to address because methods for using a transformer in multiple-step tasks, have not yet been fully established. Although transformer-based models have been successful in many language-related tasks such as language understanding (Wang et al., 2019), question answering (Rajpurkar et al., 2016), visual question answering (Antol et al., 2015), and referring expression comprehension (Kazemzadeh et al., 2014), nevertheless, these are single-step tasks. \n\nIn this study, we investigate the following question to explore the possibility of performing tasks: Can language models complete tasks through graphical user interfaces expressed as visual input?",
            "score": 0.44283663568206244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 219,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 287,
                    "end": 309,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 326,
                    "end": 346,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 1031,
                    "end": 1048,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 1671,
                    "end": 1689,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1711,
                    "end": 1735,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 1763,
                    "end": 1783,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 1824,
                    "end": 1849,
                    "matchedPaperCorpusId": "6308361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1580810546875
        },
        {
            "corpus_id": "272831995",
            "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering",
            "text": "In this paper, we address the critical challenge of merging multiple LoRAs, each tailored for distinct tasks, into a unified and comprehensive LoRA. We identify parameter interference as a primary obstacle in merging, with parameter misalignment and knowledge conflict being significant contributors. denotes the output. The performance of the ensemble of LoRAs tends to be more stable, but it incurs additional computational overhead. In contrast to the ensemble method, model merging presents an alternative composition strategy. A typical strategy involves employing element-wise fusion of these parameters, represented as A \u2032 = 1 n n j=1 A j and B \u2032 = 1 n n j=1 B j . This formulation allows the merged parameters to function similarly to a single LoRA. However, directly merging parameters can lead to performance degradation due to parameter interference. \n\nOur proposed LoRA-LEGO method serves as a bridge between the two strategies, ensuring an optimal balance between computational efficiency and performance. By selectively aligning and fusing MSUs based on their semantic similarity, LoRA-LEGO effectively condenses the most relevant semantic features into fewer clusters. This process allows for the merging of parameters within each cluster, reducing the overall parameter count in a manner similar to the model merging method. By adjusting the number of clusters, LoRA-LEGO can accommodate more parameters for inference, much like the ensemble method. In this way, our method leverages the strengths of both methodologies, ultimately enhancing model performance and inference efficiency.",
            "score": 0.4419836467859258,
            "section_title": "CONCLUSION",
            "char_start_offset": 25740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "269187724",
            "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
            "text": "Jointly training multilingual data in a unified model with a shared architecture for different languages has been a trend (Conneau et al., 2020;Le Scao et al., 2022) encouraging knowledge transfer across languages, especially for low-resource languages (Johnson et al., 2017;Pires et al., 2019).However, such a training paradigm also leads to negative interference due to conflicting optimization demands (Wang et al., 2020).This interference often causes performance degradation for highresource languages (Li and Gong, 2021;Pfeiffer et al., 2022) and can be further exacerbated by limited model capacity (Shaham et al., 2023).specific modeling (Zhang et al., 2020b) and adapters (Bapna and Firat, 2019), aim to mitigate interference by balancing full parameter sharing with isolated or partially shared modules (Pfeiffer et al., 2023).However, they heavily depend on heuristics for allocating task-specific capacity and face challenges in enabling knowledge transfer between modules (Zhang et al., 2020a).Specifically, such methods rely on prior knowledge for managing parameter sharing such as language-family adapters (Chronopoulou et al., 2023) or directly isolate parameters per language, which impedes transfer (Pires et al., 2023).\n\nResearch in vision and cognitive science has shown that unified multi-task models may spontaneously develop task-specific functional specializations for distinct tasks (Yang et al., 2019;Dobs et al., 2022), a phenomenon also observed in mixture of experts Transformer systems (Zhang et al., 2023).These findings suggest that through multi-task training, networks naturally evolve towards specialized modularity to effectively manage diverse tasks, with the ablation of these specialized modules adversely affecting task performance (Pfeiffer et al., 2023).Despite these insights, exploiting the inherent structural signals for multitask optimization remains largely unexplored.\n\nIn this work, we explore the intrinsic taskspecific modularity within multi-task networks in Multilingual Machine Translation (MMT), treating each language pair as a separate task.",
            "score": 0.44164221361260114,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 295,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 837
                },
                {
                    "start": 837,
                    "end": 1007
                },
                {
                    "start": 1007,
                    "end": 1239
                },
                {
                    "start": 1241,
                    "end": 1538
                },
                {
                    "start": 1538,
                    "end": 1797
                },
                {
                    "start": 1797,
                    "end": 1918
                },
                {
                    "start": 1920,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 144,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 253,
                    "end": 275,
                    "matchedPaperCorpusId": "260464809"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "222291168"
                },
                {
                    "start": 507,
                    "end": 526,
                    "matchedPaperCorpusId": "235422129"
                },
                {
                    "start": 526,
                    "end": 548,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 606,
                    "end": 627,
                    "matchedPaperCorpusId": "254685798"
                },
                {
                    "start": 646,
                    "end": 667,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 681,
                    "end": 704,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 813,
                    "end": 836,
                    "matchedPaperCorpusId": "257078634"
                },
                {
                    "start": 985,
                    "end": 1006,
                    "matchedPaperCorpusId": "235613405"
                },
                {
                    "start": 1122,
                    "end": 1149,
                    "matchedPaperCorpusId": "257833499"
                },
                {
                    "start": 1218,
                    "end": 1238,
                    "matchedPaperCorpusId": "258479712"
                },
                {
                    "start": 1409,
                    "end": 1428,
                    "matchedPaperCorpusId": "58006968"
                },
                {
                    "start": 1428,
                    "end": 1446,
                    "matchedPaperCorpusId": "235770756"
                },
                {
                    "start": 1517,
                    "end": 1537,
                    "matchedPaperCorpusId": "258967629"
                },
                {
                    "start": 1773,
                    "end": 1796,
                    "matchedPaperCorpusId": "257078634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406005859375
        },
        {
            "corpus_id": "276557749",
            "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation",
            "text": "Model Merging Model merging is a technique that integrates the parameters of multiple models to create a unified model with enhanced or diverse capabilities (Wortsman et al., 2022;Ilharco et al., 2022;Matena and Raffel, 2022;Jin et al., 2022;Yadav et al., 2023;Yu et al., 2024;Lin et al., 2024). Task arithmetic (Ilharco et al., 2022) leverages task vectors for model merging through arithmetic operations, incorporating a predefined scaling term to weight the contribution of different models. Fisher Merging (Matena and Raffel, 2022) performs parameter fusion by applying weights derived from the Fisher information matrix (Fisher, 1922), resulting in more precise parameter integration. TIES-Merging (Yadav et al., 2023) addresses task conflicts by removing low-magnitude parameters, resolving sign disagreements, and merging only the parameters that align with the final agreed-upon sign. In (Yu et al., 2024), it is found that LLMs can enhance their capabilities through model merging. Additionally, it introduces DARE, a method for sparsifying the delta parameters of the model (Ilharco et al., 2022), significantly improving the performance of various model merging techniques. \n\nMixup Mixup is proposed to enhance the generalization ability of deep learning models by surpassing traditional Empirical Risk Minimization (ERM) (Zhang, 2017). It is a simple, data-agnostic augmentation technique that trains models using virtual examples created by linearly interpolating pairs of random examples and their corresponding labels. Rooted in the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000), this ap- proach improves generalization across a variety of datasets (Russakovsky et al., 2015;Krizhevsky et al., 2009;Warden, 2017;Asuncion et al., 2007), and helps reduce overfitting, sensitivity to adversarial examples, and training instability, all with minimal computational cost. Given two samples (x i , y i ) and (x j , y j ), Mixup generates a new sample using the following formulas:",
            "score": 0.4404090305649704,
            "section_title": "Related Works",
            "char_start_offset": 4024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 180,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 201,
                    "end": 225,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 261,
                    "end": 277,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 277,
                    "end": 294,
                    "matchedPaperCorpusId": "261697277"
                },
                {
                    "start": 896,
                    "end": 913,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1684,
                    "end": 1710,
                    "matchedPaperCorpusId": "2930547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.367919921875
        },
        {
            "corpus_id": "272987039",
            "title": "Surveying the MLLM Landscape: A Meta-Review of Current Surveys",
            "text": "Feng et al. [67] provides a thorough overview of how Large Language Models (LLMs) are being integrated with external knowledge to enhance their capabilities. The survey categorizes the integration methods into two main approaches: knowledge editing and retrieval augmentation. Knowledge editing involves modifying the input or the model itself to update the outdated or incorrect information, while retrieval augmentation fetches external information during inference without altering the model's core parameters. The authors present a taxonomy covering these methods, benchmarks for evaluation, and applications such as LangChain and ChatDoctor, which leverage these strategies to address domain-specific challenges. Additionally, the paper explores the handling of knowledge conflicts and suggests future research directions for improving LLM performance in complex, real-world tasks through better integration of multi-source knowledge. \n\nShi et al. [68] offer an extensive survey on the continual learning (CL) of Large Language Models (LLMs), addressing the critical challenges and methodologies in this field. presents an extensive overview of the challenges and techniques related to the continual learning (CL) of Large Language Models (LLMs). The authors focus on two primary directions of continuity: vertical continual learning (adapting models from general to specific domains) and horizontal continual learning (adapting models over time across various domains). They discuss the problems of \"catastrophic forgetting,\" where models lose knowledge of previous tasks, and the complexity of continually updating models to maintain performance on both old and new tasks. The survey outlines key CL methods, including continual pre-training, domain-adaptive pre-training, and continual fine-tuning. It also evaluates various CL techniques, such as replaybased, regularization-based, and architecture-based methods, to mitigate forgetting and ensure knowledge retention. \n\nThe authors call for more research into evaluation benchmarks and methodologies to counter forgetting and support knowledge transfer in LLMs, making this an underexplored yet crucial area of machine learning research",
            "score": 0.4396041450734348,
            "section_title": "Contunual Learning",
            "char_start_offset": 30568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "276408756",
            "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
            "text": "Large Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
            "score": 0.4391156090626096,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 358,
                    "end": 380,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 380,
                    "end": 402,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1322,
                    "end": 1341,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2050,
                    "end": 2068,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "271769807",
            "title": "Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation",
            "text": "Using SFT would necessitate storing and managing distinct models per each task, increasing complexity and storage demands. Additionally, models often fail to generalize beyond the data or domains they were trained on, presenting challenges in out-of-domain generalization. In contrast, multi-task learning, which strives to train a single model for multiple tasks, brings its own set of challenges. It offers a solution to the inefficiencies of SFT by integrating training across different tasks into a single model. However, this approach necessitates retraining with large and diverse datasets to achieve a balanced representation of each task within the model (Fifty et al., 2021). Such a balance is critical to ensure that all tasks are learned effectively. The need of providing balanced, extensive, and varied data adds complexity and potential costs of multi-task learning, making it a sophisticated and sometimes expensive endeavor. Model merging emerges as a response to these issues, offering a way to integrate the strengths of individual models trained on specific tasks or through multitask learning, while mitigating the limitations of each approach. \n\nBased on the challenges identified, we introduce Chamain, a novel approach that enables chatbots to acquire additional knowledge while maintaining their character and charm without additional extensive training (Figure 1). Chamain is based on the actively researched model merging method (Yadav et al., 2023;Ilharco et al., 2023), focusing on maintaining the character and style of the model. Chamain consists of three main stages: (1) preparing instruction-tuned models for merging, (2) combining task vectors and character vectors of instruction-tuned models, and (3) subsequently fusing the latter layers of the character model based on the layer selection method. It enhances the model's ability to generate utterances that embody the nuances of the character's persona. We merge three types of models, a conversation model trained on a self-created persona dataset, an instruction-tuned model on a domain-specific data, and a fine-tuned model for downstream tasks within the domain. To verify the integration of new knowledge, we selected law and finance as specific domains for testing.",
            "score": 0.4377531842812187,
            "section_title": "Introduction",
            "char_start_offset": 2078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 663,
                    "end": 683,
                    "matchedPaperCorpusId": "237485414"
                },
                {
                    "start": 1455,
                    "end": 1475,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1475,
                    "end": 1496,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "Model merging (Lu et al., 2024a) has become a pivotal technique in the realm of LLMs, enabling the integration of strengths from multiple pre-trained or fine-tuned models to boost performance, adaptability, and efficiency. This approach can be broadly divided into two paradigms: zero-shot merging, which fuses models without further training, and merge-then-train, which involves refining the combined model after integration. \n\nEarly zero-shot techniques, such as weight averaging (Nagarajan and Kolter, 2021; Wortsman et al., 2022) and Linear Mode Connectivity, laid the groundwork, evolving into more advanced methods like Task Arithmetic (Ilharco et al., 2023a)-where task vectors steer parameter adjustments-and TIES (Yadav et al., 2023a), which reduces interference through trimming and conflict resolution. Recent innovations, including DARE (Yu et al., 2024) and Evolutionary Model Merge (Akiba et al., 2024a), further refine this by optimizing selective parameters or inference pathways, all without additional training. On the other hand, merge-thentrain strategies, such as Fisher Merging (Matena and Raffel, 2022), utilize the Fisher information matrix to assign parameter weights, while Reg-Mean (Jin et al., 2023) fine-tunes linear merging on a per-layer basis, carefully balancing embeddings and biases. However, both approaches encounter difficulties when merging models with divergent initializations, prompting research into permutation symmetries (Ainsworth et al., 2022;Verma and Elbayad, 2024) to better align parameters. A notable distinction exists between model merging and model fusion. Merging typically aims for efficiency by creating a single, cohesive model (Singh and Jaggi, 2020), whereas fusion often combines multiple models to enhance quality, potentially at the expense of speed (Ravaut et al., 2022b;Jiang et al., 2023b).",
            "score": 0.43756986154887406,
            "section_title": "C.5 LLM Model Fusion",
            "char_start_offset": 42882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 534,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "273098230",
            "title": "Parameter Competition Balancing for Model Merging",
            "text": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
            "score": 0.43639774709486284,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "269005388",
            "title": "Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging",
            "text": "The objective of model merging is to integrate multiple expert models into a unified merged model which is capable of concurrently executing diverse tasks. \n\nIn this paper, we focus on merging homologous models. The homologous models are fine-tuned from the same base model, that is, the architectures of the homologous models are consistent. Given  homologous expert models,  1 , ...,   , which are all fine-tuned from the base model   , the process of merging method  could be denoted as: \n\nHere   is the final merged model that the attackers aim to generate. Promising model merging methods encompass Model Soups, Task Arithmetic, TIES-MERGING, and DARE. The specific technical details are as follows. \n\nModel Soups. Wortsman et al. [20] propose that simply performing linear combinations of the parameters from different expert models can generate a composite merged model with multifunctionality, i.e., \n\nHere   is the scaling factor, a hyper-parameter for generating   (For other model merging methods, we also use   to denote the scaling factor). Note that there is a special situation called Average Merging, that is, \n\nTask Arithmetic. Ilharco et al. [12] point out that the capability differences between the expert model and the base model can be reflected by the delta parameter \n\nwhere   is also called the task vector on the task . Task Arithmetic merging algorithm leverages the linear combinations of multiple task vectors to generate the merged model   as: \n\nNote that compared with the generation of   , the parameters of the base model   are necessary for generating   . TIES-MERGING. Yadav et al. [22] highlight that prevailing merging methodologies frequently disregard the interference among parameters originating from distinct models, thereby yielding substantial performance deterioration upon the merged model. Therefore, they propose a new merging method, TIES-MERGING, which aims to deal with (1) the interference caused by redundant parameter values and (2) disparity in the sign of a specific parameter's values among models. To generate the final merged model   , TIES-MERGING first generates a merged task vector   , and   can be calculated by \n\nThe generation process for   contains three steps: Trim, Elect, and Joint merge. DARE.",
            "score": 0.4362411277578923,
            "section_title": "Preliminaries 2.1 Model Merging",
            "char_start_offset": 4793,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2175
                },
                {
                    "start": 2178,
                    "end": 2258
                },
                {
                    "start": 2259,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1617,
                    "end": 1621,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "277781074",
            "title": "Efficient Multi-Task Modeling through Automated Fusion of Trained Models",
            "text": "Model fusion is a well-studied field, mainly divided into traditional model integration (combining predictions from multiple models to enhance performance [14]) and weight merging (fusing models at the parameter level, e.g., Gupta et al. [15] and Wortsman et al. [16] merging similar models). Researchers like Rame et al. [17], and Arpit et al. [18] explored merging models trained under different settings, analyzing data distribution and proposing strategies for better generalization. Jin et al. [19] aimed to unify models trained on different datasets into a robust model. Others, such as Huang et al. [20], and Zhang et al. [21], used linear operations to optimize adapter parameters, enhancing generalization. \n\nWhile current fusion methods integrate multiple models effectively, weight merging is usually limited to identicalstructured models for a single task. Our approach, however, offers flexibility, combining knowledge from diverse models regardless of structure or task, to create a versatile model capable of multiple tasks.",
            "score": 0.435170986522033,
            "section_title": "B. Model Fusion",
            "char_start_offset": 4644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1039
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "209315801"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 322,
                    "end": 326,
                    "matchedPaperCorpusId": "248887264"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "239049452"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Merging methods. The traditional approach to learning multiple skills/tasks simultaneously is joint training on a mixture of task datasets [7]. Over the years, several works have improved this multi-task learning approach [41,53,48,20,38,9,48]. As data collection for specialized tasks, and training large models from scratch get more expensive; coupled with the rapid expansion in the availability of well-trained open-source models -model merging has emerged as a convenient way of building powerful models from existing ones [17,32]. The richly studied simplest way of merging by averaging model weights [57,52,16,26] paved the way to linear weight averaging [60]. Some methods like Fisher Merging [39] and RegMean [27] need training data based pre-computations to measure individual parameter importance but these are highly memory and data intensive. Expanding on weight averaging, Task Arithmetic [25] involving the creation and combination of task vectors facilitated multi-task learning. While this weight interpolation was heavily used for merging image generation models, recent methods like TIES [67] and DARE [2] reset redundant parameters, resolve sign conflicts, and exclusively merge parameters that exhibit sign-consistency, and SLERP [59] by spherical linear interpolation build upon this for language models. In all these methods, the coefficients governing the model merging are determined by trial-error; while works in the vision domain demonstrate the pivotal role played by these coefficients [68]. In contrast to the cumbersome grid-searching, our method CAT learns these coefficients layer-wise with access to very few examples in the natural language domain. \n\nLoRA merging methods. PEM composition [73] adopts the task arithmetic framework to incorporate the merging of LoRAs. Recently, the vision community witnessed the widespread application LoRAs [5,14,37,43,62,75,69] as an effective approach to multi-task learning and composing styles and subjects [49]. Many of these utilize Mixture of Experts (MoE) [5,14,37,62] based architectures having input-dependent learnable routers. These models have been primarily used in the context of multitask learning.",
            "score": 0.435107903536733,
            "section_title": "Related Work",
            "char_start_offset": 4820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "219259832"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "263858956"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "259262373"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5205078125
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "The rapid development of Large Language Models (LLMs) has shaped the contours of AI. This has landed the models in areas where they can excel in generating natural text, never reaching any peak before.LLMs are based on deep learning structures, and can therefore perform effectively in many natural language processing tasks, such as text generation, sentiment analysis, or sophisticated dialogue systems. \n\nRecent surveys have explored diverse aspects of LLMs, such as their architectures, training methodologies, and performance evaluation benchmarks. Many focus on specific topics, such as detailed analyses of state-of-the-art models [1], scaling laws innovations((i.e. what a new law brings to make old predictions still work)) [2], and pretraining methods used with large datasets [3]. Others investigate domain-specific fine-tuning effects from reinforcement learning or human feedback [4,5], and transfer learning strategies [6]. Despite these valuable contributions, however, there remains a paucity of comprehensive perspectives that connect the foundational principles of LLMs with practical applications and the challenges faced as these models are implemented in real-world situations. \n\nThis survey addresses this gap by presenting a comprehensive analysis of LLMs' foundational principles and their applications across diverse domains. Although LLMs have achieved remarkable progress, \u2022 Providing a comprehensive analysis of LLMs, focusing on their foundational principles, architectures, and real-world applications, while highlighting their capabilities, as well as technical, operational, and ethical limitations, and their transformative impact across diverse domains. \u2022 Investigating the integration of LLMs with knowledge-based methods, analyzing their technical capabilities, hybrid integration strategies, and real-world case studies, while summarizing evaluation metrics used to assess the effectiveness of LLM-knowledge integration. \u2022 Critically evaluating the challenges of LLM-knowledge integration and presenting actionable recommendations to enhance efficiency, interpretability, and scalability in future AI research and applications. \n\nThis survey paper begins with an overview of Large Language Models (LLMs), detailing their evolution, underlying architecture, and diverse real-world applications. It then explores the challenges associated with implementing these models, categorizing them into technical, operational, and ethical/social dimensions and discussing potential solutions.",
            "score": 0.43468493755216964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2164
                },
                {
                    "start": 2167,
                    "end": 2330
                },
                {
                    "start": 2331,
                    "end": 2518
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "277510128",
            "title": "Cognitive Memory in Large Language Models",
            "text": "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing. \n\nThe advantages of the BTX method include the parallel and asynchronous nature of the expert training phase, which reduces communication costs and enhances training efficiency. The final BTX model is a unified neural network that can be fine-tuned like any standard LLM. Additionally, the model's FLOPs (floating-point operations) during inference do not increase significantly because it is sparsely activated, despite the increase in parameter count. The paper also explores various variants of BTX, such as load balancing, different routing methods, and strategies for splitting and merging experts, to further improve model performance and efficiency. \n\nas an additional instruction-aware query to extract and retain task-relevant information. Task-specific responses are generated using this key-value cache.",
            "score": 0.4339181847522248,
            "section_title": "MoE For Memory Parameterization",
            "char_start_offset": 102329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1415
                },
                {
                    "start": 1418,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2230
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7177734375
        },
        {
            "corpus_id": "276408321",
            "title": "Superpose Singular Features for Model Merging",
            "text": "Model merging is a technique that combines multiple models into a single model to enhance performance or enable the model to perform multiple tasks. Previous studies have shown that averaging the weights of multiple models fine-tuned from the same pre-trained initialization is a promising approach for model merging. Fisher Merging [Matena and Raffel, 2022] advances beyond simple averaging by utilizing the Fisher information matrix to assess the importance of individual parameters, which are then weighted accordingly during the merging process. Similarly, RegMean [Jin et al., 2022] forms a linear regression problem with extra data for each layer and offers a closed-form solution for the merged model's parameters by solving the regression problem. \n\nBeyond parameter averaging, Task Arithmetic [Ilharco et al., 2022a] introduces task vectors and adding the task vectors of individual tasks to merge model, demonstrating their effectiveness and lightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composition [Zhang et al., 2023a] extends the task arithmetic framework to merge LoRA [Hu et al., 2021], while Ties-Merging [Yadav et al., 2024] addresses task conflicts by resetting redundant parameters and resolving sign conflicts. These methods, however, use a single merging coefficient across all task vectors, which limits their flexibility. In contrast, Lorahub [Huang et al., 2023] and AdaMerging [Yang et al., 2023] use different coefficients for enhanced adaptability. Lorahub's performance is limited as it only searches for coefficients at the task level, while AdaMerging requires complex training and unlabeled test datasets, making it applicable solely to classification problems. DARE [Yu et al., 2024] proposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging [Du et al., 2024] is a lightweight, training-free technique for model merging that balances parameter competition by intra-balancing parameter significance within tasks and inter-balancing parameter similarities across tasks, effectively enhancing performance across various scenarios and domains.",
            "score": 0.43376657459288753,
            "section_title": "Model Merging of Fine-tuned Models",
            "char_start_offset": 5222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1077,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1168,
                    "end": 1188,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1745,
                    "end": 1762,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1854,
                    "end": 1871,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "271709544",
            "title": "UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model",
            "text": "In this paper, we propose UnifiedMLLM, a multimodal large language model that handles various multi-modal tasks using a unified representation. By introducing task tokens, grounding tokens, and a task router, we seamlessly integrate multiple tasks with excellent scalability and versatility. We construct a task-specific dataset and a multi-task multiturn instruction-tuning dataset, and employ a threestage training approach to enable the model to effectively perform diverse multi-modal tasks while avoiding degradation of general capabilities. Due to the powerful reasoning and grounding abilities of our model, a significant number of quantitative experiments and visual results demonstrate the effectiveness of our approach. \n\nModel Architecture Due to limited training resources and the complexity of tasks, our model primarily relies on external models to accomplish various multi-modal tasks. This approach ensures the effectiveness and scalability of completing visual tasks. However, the scope and effectiveness of the model are still constrained by the expert models. \n\nA future research direction is to construct an endto-end trainable multi-modal system. One possible approach is to discretize various modal information, following the methodology of AnyGPT (Zhan et al., 2024). \n\nMulti-modal Interleaving Currently, our model mainly focuses on processing single-modal inputs. \n\nEffectively handling multi-modal information simultaneously or interleaved is a challenge that needs to be addressed. CoDi-2 (Tang et al., 2024a) provides some insights, but due to the lack of this type of data, the number of tasks that can be handled is relatively limited. A future research direction is to explore how to achieve interleaved understanding and generation of inputs and outputs.",
            "score": 0.43170196310053616,
            "section_title": "Conclusion",
            "char_start_offset": 22771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 1516,
                    "end": 1536,
                    "matchedPaperCorpusId": "265506621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37060546875
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "With the rapid growth of deep learning, there is an increasing availability of open-source models for various tasks. However, single fine-tuned models often fall short of meeting the diverse needs of users. Model merging has thus emerged as an efficient method to integrate the capabilities of existing models into a unified model. Nevertheless, existing model merging methods face challenging trade-offs between performance and deployment costs, primarily due to task interference. For the first time, we reveal that task interference is evident in the frequency domain of model parameters, yet current efforts only focus on spatial domain solutions, which are largely ineffective in addressing frequency domain interference. To mitigate the impact of frequency domain interference, we propose FR-Merging, an innovative method that effectively filters harmful frequency domain interference on the backbone with minimal computational overhead. Since performance loss is inevitable with cost-free methods, we propose a lightweight task-specific expert module that dynamically compensates for information loss during merging. This proposed framework, FREE-Merging (FR-Merging with experts), strikes a balanced trade-off between training cost, inference latency, storage requirements, and performance. We demonstrate the effectiveness of both FR-Merging and FREE-Merging on multiple tasks across CV, NLP, and Multi-Modal domains and show that they can be flexibly adapted to specific needs.",
            "score": 0.43148521516765326,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65625
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "Model Merging Recent advances in large language models (LLMs) have highlighted model 1 https://github.com/knovel-eng/mod merging as a crucial strategy for combining the capabilities of multiple models into a unified system (Ainsworth et al., 2023;Goddard et al., 2024;Labrak et al., 2024). This approach has gained prominence for its ability to enhance multitask performance and enable continual learning without requiring costly retraining procedures. Initial investigations in this domain explored weight averaging techniques, which directly combined parameters of models sharing identical architectures and initializations (Matena and Raffel, 2022;Garipov et al., 2018). While these methods demonstrated promising results, they revealed significant limitations when applied to models trained on heterogeneous tasks or initialized differently, prompting the development of more sophisticated approaches.",
            "score": 0.4310919816026011,
            "section_title": "Related Work",
            "char_start_offset": 3640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 905
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "267740180"
                },
                {
                    "start": 626,
                    "end": 651,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 651,
                    "end": 672,
                    "matchedPaperCorpusId": "4055784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "249538647",
            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
            "text": "Generalist models that handle multiple modalities and numerous tasks have been long pursued by the machine learning community. However, previous researches [65,89,71] focus on developing specialized models with task-specific modules. When these models are applied to new tasks, the specifically-designed components need to be redesigned on demand and fine-tuned on sufficient downstream data. As a result, their model size increases with the number of diverse downstream tasks, conflicting with the goal of generalist models. \n\nRecently, some pioneers [93,79,3,84,86,62] have made preliminary attempts to build generalist models by modeling various tasks into a unified formulation. With the unified modeling, large-scale pre-training on various datasets enables the generalist models to process different downstream tasks using shared parameters. These generalist models not only achieve competitive performance on pre-training tasks [79,3,84,86], but also can perform zero-shot inference on novel tasks without introducing additional parameters [93,62]. \n\nHowever, compared to specialized models with specific parameters for each task, generalist models with shared parameters would suffer from the task-interference issue -different tasks with shared parameters may conflict with each other [88]. The same issue is also observed in multilingual NLP models [4,81,83]. We argue that the task-interference issue is mainly caused by the inconsistent optimization in multi-task learning. As shown in Tab. 1, during the training phase of generalist models, the gradient directions of different tasks would be inconsistent or even opposite. Thus, if multiple tasks share parameters, the optimal update direction of the shared parameters will be uncertain, resulting in sub-optimal performance. \n\nAllowing conflicting modalities and tasks to use separate parameters should effectively mitigate the interference issue in generalist models. Mixture of Experts (MoEs) [43,23] provides a potential solution, which learns to activate sub-networks dynamically without introducing any task-specific modules. Nevertheless, vanilla MoEs [67] select the experts according to token representations, which suffers from high training/inference cost and neglects the information of different tasks and modalities. In this work, we argue that routing strategies of MoEs require special design when applied to generalist models for mitigating the task-interference issue.",
            "score": 0.4309420145957372,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2450
                }
            ],
            "ref_mentions": [
                {
                    "start": 1294,
                    "end": 1298,
                    "matchedPaperCorpusId": "210839011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "271533631",
            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
            "text": "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.",
            "score": 0.43059950311922235,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2304
                },
                {
                    "start": 2307,
                    "end": 2613
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 637,
                    "end": 640,
                    "matchedPaperCorpusId": "211532645"
                },
                {
                    "start": 1384,
                    "end": 1387,
                    "matchedPaperCorpusId": "256808267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "259950775",
            "title": "Integration of large language models and federated learning",
            "text": "The advent of Large Language Models [173] (LLMs) has markedly influenced contemporary society. These models use deep learning strategies, principally the transformer architecture [102] to discern intricate patterns and structures inherent to data [2]. Presently, a vast amount of work [124,19,115] confirms that these models exhibit superior performance both in predefined tasks and practical applications. Impressively, given accurate instructions and demonstrations, these models are capable of adapting to specific contexts or addressing new tasks without additional fine-tuning, as corroborated by numerous studies [208,88,162]. Moreover, LLMs have made significant strides in specialized domains, delivering commendable outcomes in areas like healthcare [167], finance [198], law [72,111,38], scientific knowledge analysis [155], and code generation [113,93]. Considering the large parameter size and complex model structure of LLMs, common privacypreserving computation techniques, such as Secure Multi-party Computation [34] (SMPC), Differential Privacy [43] (DP), and Trusted Execution Environments [128] (TEE), struggle to juggle privacy protection and computational efficiency effectively. Unlike these methods, Federated Learning [106] (FL) offers a more practical approach by allowing collaborative model development. FL demonstrates a mature engineering execution method and strikes an ideal balance between efficiency and data privacy [17]. Therefore, a feasible solution to address the challenges of LLMs in practical applications is to introduce FL into the LLMs. Conversely, capitalizing on the strong task generalization capabilities of LLMs, they can also be employed within FL systems to help address challenges inherent to FL. Based on this complementarity, the combination of LLMs with FL has demonstrated exceptional performance benefits and mutual enhancement, a characteristic that has elicited widespread research interest. \n\nIn this paper, we focus on the promising direction of combining LLMs and FL. Previous studies present initial perspectives on this integration [26,210,196], providing preliminary insights into its motivations and future directions. Despite this, current research has not fully covered all areas related to the integration of LLMs and FL.",
            "score": 0.43042901603550143,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 184,
                    "matchedPaperCorpusId": "242590231"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "204741819"
                },
                {
                    "start": 285,
                    "end": 290,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 290,
                    "end": 293,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 293,
                    "end": 297,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 619,
                    "end": 624,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1061,
                    "end": 1065,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1107,
                    "end": 1112,
                    "matchedPaperCorpusId": "195917113"
                },
                {
                    "start": 1241,
                    "end": 1246,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "256268919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "276259373",
            "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
            "text": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release our models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for further investigations.",
            "score": 0.430067718899263,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "276575054",
            "title": "Delta Decompression for MoE-based LLMs Compression",
            "text": "Recent advances in Large Language Models (LLMs) increasingly favor Mixture of Experts (MoE) (Cai et al., 2024) architectures for their ability to scale model capacity through specialized expert networks while maintaining computational efficiency via sparse activation. The success of MoE is evident in recent LLMs like DeepSeek-V3 (DeepSeek-AI et al., 2024) and MiniMax-01 (MiniMax et al., 2025), which demonstrate unprecedented capabilities in language understanding and generation tasks. Despite their compelling advantages, MoE LLMs face critical challenges in practical deployment scenarios (Tang et al., 2024;Zhong et al., 2024;Hwang et al., 2024). Their substantial parameter footprint, coupled with considerable memory overhead from storing multiple expert weights (Song et al., 2023), creates significant barriers to resource-constrained environments. \n\nTo address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods. \n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance. \n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations.",
            "score": 0.42989740743818033,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1072
                },
                {
                    "start": 1075,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 633,
                    "end": 652,
                    "matchedPaperCorpusId": "261076133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "271902780",
            "title": "Activated Parameter Locating via Causal Intervention for Model Merging",
            "text": "Recently, large language models (LLM) have achieved significant breakthroughs across various NLP tasks, with numerous task-specific checkpoints, fine-tuned on open-source LLMs, now publicly available (Wolf et al. 2019). These fine-tuned models incorporate valuable, high-quality taskspecific information on top of the pre-trained models (Devlin et al. 2018). However, obtaining supervised task-specific data remains challenging due to security and privacy concerns, while training LLMs on extensive datasets is also costly (Hu et al. 2021). Thus, a method that directly leverages the capabilities of these fine-tuned models with minimal or no additional training would be highly beneficial. Another relevant concept is linear model connectivity (Frankle et al. 2020), which suggests that fine-tuned models originating from the same pre-trained model, even when using different hyperparameter settings, generally lie within a single low-error basin (Izmailov et al. 2018;Kuditipudi et al. 2019;Garipov et al. 2018). Merging these models in parameter space can identify points that provide improved generalization (Wortsman et al. 2022) multiple homologous models are combined to achieve better generalization without necessitating a large volume of highquality data to fully fine-tune LLMs (Goddard et al. 2024;Stoica et al. 2023;Huang et al. 2023). \n\nA backbone of model merging involves manipulating and merging the task vector (Ilharco et al. 2022), also known as delta parameters (see Eq. 1), which quantify the difference between a pre-trained model and its fine-tuned counterpart. Clearly, delta parameters contain task-specific knowledge learned during fine-tuning, reflected in the shifts within the parameter space. Furthermore, while (Yu et al. 2024;Yadav et al. 2024) highlight the critical role of delta parameters in model merging, they also observe a major challenge: resolving redundancies and conflicts among the delta parameters of multiple models, particularly as the number of models being merged increases, which tends to aggravate these conflicts.",
            "score": 0.4298908788610365,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 217,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 745,
                    "end": 766,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 970,
                    "end": 993,
                    "matchedPaperCorpusId": "189897792"
                },
                {
                    "start": 993,
                    "end": 1013,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1289,
                    "end": 1310,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55126953125
        },
        {
            "corpus_id": "276250228",
            "title": "UniMoD: Efficient Unified Multimodal Transformers with Mixture-of-Depths",
            "text": "Unified multimodal transformers have received growing attention due to their ability to handle both generation and un-Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). \n\nderstanding tasks within a shared parameter space. Recent studies (Xie et al., 2024;Team, 2024;Ge et al., 2024b;Wang et al., 2024;Sun et al., 2023d;Ye et al., 2024a;Ma et al., 2024;Zhou et al., 2024;Liu et al., 2024a) have explored different approaches for unified transformers. These models can be broadly categorized into two types: one where both generation and understanding tasks are handled using a fully autoregressive method (Ge et al., 2024b;Team, 2024;Wang et al., 2024), and another where generation tasks are addressed using diffusion or flow matching techniques, while autoregressive methods are used for understanding tasks (Xie et al., 2024;Zhou et al., 2024;Ma et al., 2024). However, regardless of the approach, training these unified transformers remains time and memory intensive, primarily due to token redundancy and the computationally heavy attention mechanisms. In the past, few studies have explored efficient training strategies for these models. Therefore, developing efficient methods for training unified transformers remains a significant challenge. \n\nThe Mixture of Depths (MoD) approach has been employed in previous studies on large language models (LLMs) (Raposo et al., 2024) and multimodal large language models (MLLMs) (Luo et al., 2024;Wu et al., 2024b;Zhang et al., 2024a) to prune tokens. MoD employs a router to assign weights to tokens based on their significance, enabling the model to selectively prune less important tokens and reduce computational overhead. However, applying MoD to unified transformers presents unique challenges that are worth exploring.",
            "score": 0.42983198736958983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31201171875
        },
        {
            "corpus_id": "270067704",
            "title": "Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs",
            "text": "The fusion of Large Language Models (LLMs) and Knowledge Graphs (KGs) has been a focal point of recent research, aimed at harnessing the complementary strengths of these technologies to address their individual limitations.\n\nRecent advancements highlighted by Yang et al. [46] have demonstrated that while LLMs like ChatGPT excel at predicting language patterns, their ability to recall factual, knowledge-grounded content remains limited without the support of KGs.This has led to the exploration of KGs as essential tools for enhancing the factual accuracy of LLM outputs.\n\nFurthermore, Alam et al. [47] have proposed innovative methods to improve KG embeddings for graph completion tasks, which help overcome traditional challenges of KGs, such as neglecting rich textual information.This approach underscores the evolving synergy between textual and knowledge-based AI systems.\n\nShirui Pan et al. [48] contribute to this dialogue by mapping out various strategies for integrating LLMs with KGs.Their work lays the groundwork for approaches like KG-enhanced LLM inference, which effectively merges textual data with structured knowledge to enrich both the comprehension and generation capabilities of AI models.\n\nDespite these innovations, the integration of LLMs with KGs faces challenges such as the need for frequent updates and the difficulty in generalizing to unseen data [49].Dynamic Knowledge Fusion methods like the Two-Tower Architecture [50] attempt to address these issues by processing text and KG inputs separately to enhance interaction, but often lack integrated feedback loops between the two data streams.\n\nMore advanced models like KagNet [51] and MHGRN [52] introduce mechanisms for augmenting text with KG inputs, providing a more nuanced interaction, albeit typically in one direction.Enhancements such as those introduced by GreaseLM [53], which facilitates deep interaction across all layers of the model, and the bi-directional attention mechanisms of QA-GNN [54] and JointLK [55], represent significant strides towards creating a more robust, interactive system.\n\nSakib et al. [56] utilize Large Language Models (LLMs) to generate task trees that are subsequently integrated into a unified graph structure to optimize task execution.This integration strategically eliminates paths that are less probable and resource-intensive by leveraging the predictive capabilities of LLMs:",
            "score": 0.4296074337448121,
            "section_title": "D. Integrating LLM s and (EKGs)",
            "char_start_offset": 12779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 225,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 574
                },
                {
                    "start": 576,
                    "end": 787
                },
                {
                    "start": 787,
                    "end": 881
                },
                {
                    "start": 883,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1214
                },
                {
                    "start": 1216,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1626
                },
                {
                    "start": 1628,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 2091
                },
                {
                    "start": 2093,
                    "end": 2262
                },
                {
                    "start": 2262,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 601,
                    "end": 605,
                    "matchedPaperCorpusId": "250658073"
                },
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 1451,
                    "end": 1455,
                    "matchedPaperCorpusId": "52291548"
                },
                {
                    "start": 1661,
                    "end": 1665,
                    "matchedPaperCorpusId": "202540096"
                },
                {
                    "start": 1676,
                    "end": 1680,
                    "matchedPaperCorpusId": "218486837"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "246240437"
                },
                {
                    "start": 1987,
                    "end": 1991,
                    "matchedPaperCorpusId": "233219869"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.288818359375
        },
        {
            "corpus_id": "273501535",
            "title": "Improving General Text Embedding Model: Tackling Task Conflict and Data Imbalance through Model Merging",
            "text": "Model merging [27][28][29][30] refers to the process of integrating the parameters of multiple models with the same architecture into a single unified model, enabling the merged model to possess the capabilities of multiple models. Model merging has been applied to many machine learning subfields, such as continual learning to mitigate catastrophic forgetting of old tasks, and multi-task/multi-domain learning to facilitate knowledge transfer [30]. \n\nThis work involves five popular model merging methods. Specifically, given N datasets {D i } N i=1 and the N models {M i } N i=1 trained on them, with corresponding model weights {\u03b8 i } N i=1 and task vectors {V i } N i=1 , the following five methods are used to obtain the merged task vector V m : (1) Average Merging [27] performs linear interpolation between different task vectors to obtain V m : \n\nwhere \n\n(2) SLERP1 performs Spherical Linear intERPolation successively between two task vectors for (N \u2212 1) times to obtain V m : \n\nwhere a i , a i \u2208 R are hyperparameters; (3) TIES Merging tackles the task conflicts in task vectors by trimming low-magnitude model weights, resolving sign disagreements, and disjointly merging parameters with consistent signs to obtain V TIES , which is scaled by hyperparameter \u03bb to obtain V m : \n\n(4) Fisher Merging [31] calculates merging coefficient F\u03b8i \u2208 R \u2225\u03b80\u2225 based on Fisher information matrix: \n\nwhere \n\nwhere \n\nto obtain V m , where {W k i } K k=1 are linear layers weights in \u03b8 i , each corresponding to an input matrix \n\nWe implement these methods based on the implementations from [29]. \n\nThe difference in model performance obtained from joint training compared to the best performance achieved when trained separately on STS and Retrieval tasks. 3 Problems Analysis",
            "score": 0.42920401340437375,
            "section_title": "Model Merging",
            "char_start_offset": 9613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1772
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 18,
                    "end": 22,
                    "matchedPaperCorpusId": "265351794"
                },
                {
                    "start": 22,
                    "end": 26,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 773,
                    "end": 777,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1310,
                    "end": 1314,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44775390625
        },
        {
            "corpus_id": "278534535",
            "title": "Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces",
            "text": "Various techniques have been proposed to mitigate these collisions, including N-LoRA methods, which attempt to preserve or restore orthogonality between subspaces associated with each training instance, thereby limiting the unintended rewriting of previously acquired knowledge [197]. \n\nLarge-scale language models with greater parametric capacity generally exhibit an increased tendency toward orthogonality [193,192]. Their expanded representational space allows for a finer separation of relevant dimensions, reducing interference noise. Consequently, this finer-grained separation of concepts and tasks translates into superior performance, particularly in complex tasks requiring intensive knowledge management [193]. However, larger models also face novel challenges in detecting and correcting internal collisions, as the multiplicity of parameters and potentially addressed tasks makes controlling residual orthogonality more difficult [192]. \n\nA deeper understanding of superposition in LLMs, whether manifested through quasi-orthogonality or parameter collisions, largely depends on disentanglement techniques. Concept Activation Vectors (CAVs) enable the projection of neural activity into semantically interpretable directions [194]. Sparse Autoencoders (SAEs) offer a dimensionality reduction method to isolate semantic features into distinct latent codes [190], making residual overlaps visible. Furthermore, causal intervention approaches, which involve selectively manipulating the activation of certain neurons, have demonstrated the importance of overlaps in determining model outputs : any targeted modification can simultaneously affect multiple tasks or concepts. For instance, [199] applied causal interventions to transformer models to analyze pronoun resolution, showing that targeted manipulation of specific attention heads influences the propagation of contextual information and simultaneously affects multiple possible interpretations. Similarly, [200] studied causal relationships across different layers of language models, revealing that specific interventions modulate neural activity hierarchically and affect various cognitive functions in parallel [199,200]. \n\nThis superposition has significant implications for the explainability of large language models (LLMs) and the localization of their internal knowledge. Probing methods indicate that neurons initially appearing specialized and monosemantic (e.g., concerning grammatical categories) actually participate in more abstract functions when considering the entire context. For instance, [50] demonstrated that the early layers of LLMs use sparse neuron combinations to represent multiple superimposed features, while intermediate layers contain neurons that appear less poly-semantic and more dedicated to high-level contextual characteristics.",
            "score": 0.4290521919633622,
            "section_title": "Superposition and Quasi-Orthogonality",
            "char_start_offset": 11391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2349
                },
                {
                    "start": 2350,
                    "end": 2563
                },
                {
                    "start": 2564,
                    "end": 2835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "275212824",
            "title": "Superposition in Transformers: A Novel Way of Building Mixture of Experts",
            "text": "The proposed method represents a significant advancement in enabling large language models (LLMs) to integrate knowledge from multiple domains or tasks without sacrificing foundational capabilities or requiring extensive parameter growth. By leveraging autoencoders and blending mechanisms, the approach facilitates efficient knowledge integration making it a practical and resource-efficient enhancement to LLMs. Experimental results highlight improvements in perplexity reduction, reconstruction accuracy, and hidden state alignment with base and fine-tuned models. Techniques like t-SNE visualizations further validate the model's ability to adapt representations dynamically. This adaptability enables seamless processing of diverse inputs and enhances multi-domain integration. Beyond merging two models, this method promises broader applications, such as multilingual processing, seamless language switching, and integrating symbolic reasoning with domain knowledge. Looking ahead, this approach paves the way for the development of models capable of integrating diverse skills and knowledge domains, representing a step toward more flexible, powerful, and general-purpose AI systems. The potential applications are vast and exciting. \n\n\u2022 Smoothness Loss: Ensures that adjacent control points are close to each other, minimizing abrupt changes in blending behavior. \n\nN represents the number of control points in the B-spline interpolation. \n\n\u2022 Centrality Loss: Penalizes deviations of the control points from a central value (e.g., 0). \n\n\u2022 Mean Bias Loss: Penalizes deviations of the mean layer-wise bias from zero to encourage balanced adjustments across layers. \n\nwhere \u00b5 2 b is the mean of the layer-wise bias. \u2022 Variance Bias Loss: Encourages the variance of the layer-wise bias to match a desired target variance \n\nwhere \u03c3 2 b is the variance of the layer-wise bias. \n\nThe total alpha regularization loss is: \n\nThe Total Losses The total 1D loss combines the language modeling loss and the reconstruction loss. Each component is weighted to balance their contributions during training. \n\nThe total 2D loss combines all the aferomentionned losses weighted to balance their contributions during training. Sample a minibatch of inputs x and targets y 13:",
            "score": 0.42815671160281016,
            "section_title": "Conclusion",
            "char_start_offset": 18969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1878
                },
                {
                    "start": 1881,
                    "end": 1920
                },
                {
                    "start": 1923,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2097
                },
                {
                    "start": 2100,
                    "end": 2214
                },
                {
                    "start": 2215,
                    "end": 2263
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.521484375
        },
        {
            "corpus_id": "273482155",
            "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
            "text": "The baselines we compare include three non-model merging methods, and seven representative model merging methods: \n\n(i) Non-model Merging Methods: \u2022 Pretrained directly uses the pre-trained model to test multiple downstream tasks. The performance of this method is generally less than acceptable due to the lack of task-related knowledge. \n\n\u2022 Individual involves full fine-tuning of the pretrained model with task-specific training data and often yields superior performance. However, it requires maintaining a copy of model parameters for each task, which is very costly when the number of tasks is large. \u2022 Traditional MTL collects data from multiple tasks and collaboratively trains a multi-task model. Due to potential task interference, the performance of a multi-task model is usually slightly lower than that of individual models, but the advantage is that it only has a single model. \n\n(ii) Model Merging Methods: \u2022 Weight Averaging is the simplest model merging method, which directly adds the parameters of well-trained multiple models according to average values. \u2022 Fisher Merging [19] measures the importance of each parameter based on the Fisher information matrix [46] and merges models based on that importance. \u2022 RegMean [24] utilizes pre-computed inner product matrices of inputs at each layer from the original training data to conduct a linear combination of the model's parameters. \u2022 Task Arithmetic [36] defines the parameter difference between the fine-tuned model and the pre-trained model as a 'task vector', and merges multiple task vectors into the pretrained model based on grid search coefficients for model merging. \n\n\u2022 Ties-Merging [21] eliminates sign conflicts between parameters of task vectors based on Task Arithmetic [36], thereby effectively alleviating interference during model merging. \n\n\u2022 AdaMerging [26] designed a layer-wise model merging strategy, which assigns a merging coefficient to each layer of the model to be merged and optimizes these coefficients by entropy minimization. \u2022 Concrete AdaMerging [52] performs model merging in the shared subspace of multiple models. Similar to AdaMerging [26], it employs entropy minimization to identify the shared subspace. \n\n(iii) Our Representation Surgery Methods: Our Surgery (in \u00a7IV-B) is orthogonal to the model merging methods in (ii).",
            "score": 0.42802029819871845,
            "section_title": "B. Baselines",
            "char_start_offset": 37143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2211
                },
                {
                    "start": 2214,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "15354499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "273098230",
            "title": "Parameter Competition Balancing for Model Merging",
            "text": "This section provides a detailed baseline description. Our experiments encompass seven comparison methods: \n\n\u2022 Individual means that each task uses an independent fine-tuned model, which has no interference between tasks, but cannot perform multiple tasks simultaneously. \u2022 Traditional MTL collects the original training data of all tasks together to train a multi-task model. It can be used as a reference upper bound for model merging \u2022 Weight Averaging is the simplest method of model merging, which directly averages the parameters of multiple models using \u03b8 m = n t=1 \u03b8 t /n, calculating the element-wise mean of all individual models. It can be used as a lower bound for model merging. [8,86]. \u2022 Fisher Merging [46] calculates the Fisher information matrix [17] Ft = E x\u223cDt E y\u223cp \u03b8 t (y|x) \u2207 \u03b8t (log p \u03b8t (y|x t )) 2 to measure the importance of each parameter when merging models for task t, where and model merging is performed according to the guidance of this importance. \u2022 RegMean [30] imposes a constraint when merging models, that is, the L 2 distance between the merged model's and the individual models' activations. It computes a least-squares solution as \n\n, where X t is the input activation of the corresponding layer. \n\n\u2022 Task Arithmetic [28] first defines the concept of \"task vectors\" and merges these vectors into a pre-trained model to execute multi-task learning. The model is produced by scaling and adding the task vectors to the initial model as \u03b8 m = \u03b8 init + \u03bb * n t=1 \u03c4 t . \u2022 Ties-Merging [89] further solves the task conflict problem in Task Arithmetic [28]. It eliminates redundant parameters and resolves symbol conflicts through three steps: Trim, Elect Sign, and Disjoint Merge. \u2022 AdaMerging automatically learns a merging coefficient for each layer of each task vector in Task Arithmetic [28]. \u2022 LoraHub [25] employs Low-rank Adaptations to dynamically combine task-specific modules for cross-task generalization, and adapts to new tasks by configuring \u03b8 \u2032 = K k=1 w k \u2022 \u03b8 k .",
            "score": 0.4279456443094273,
            "section_title": "E Baseline details",
            "char_start_offset": 37140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40478515625
        },
        {
            "corpus_id": "258865495",
            "title": "Few-shot Unified Question Answering: Tuning Models or Prompts?",
            "text": "In this study, our goal is to investigate the potential of soft prompt-tuning extensively and to better understand its benefits and drawbacks in comparison with model-tuning-based approaches for building a unified QA system grounded on the aforementioned four principles. In particular, we further center the study around understanding these tradeoffs in the few-shot learning scenarios, which is a realistic and more practical challenge. \n\nModel-tuning This paradigm involves the finetuning of all the parameters of a language model to cater to a specific task or a set of tasks. Although fine-tuning (FT) on a particular dataset is an effective strategy, it is not suitable for unified QA because it requires specialized models for each dataset during inference, which is counter-intuitive to the concept of a unified QA model. \n\nIn contrast, multi-task learning via fine-tuning (FT-MT) (Raffel et al., 2022;Aribandi et al., 2021) involves the joint learning of a single model on multiple datasets by sharing all the trainable model parameters across different tasks. By training on multiple datasets, FT-MT allows for knowledge transfer from relevant tasks during inference. However, sharing all the parameters often leads to negative transfer from unrelated tasks. Incorporating additional tasks into existing models requires retraining the model with all previous tasks and the new ones, making them computationally expensive to scale and more prone to negative interference. \n\nPrompt-tuning This paradigm involves learning soft-prompt tokens added to the input while the backbone language model remains frozen. We follow the approach proposed by Lester et al. (2021) to train soft prompts for each task, where prompts are initialized from random words in the vocabulary (PT-R). This vanilla prompt-tuning approach is parameter-efficient and easy to scale. Since taskspecific knowledge is captured in a different set of parameters (i.e., the prompts), this approach avoids negative interference to a great extent. With a single backbone model, we can use these prompts for different tasks. However, this approach does not leverage knowledge from other tasks not already captured in the backbone model.",
            "score": 0.4272086948310834,
            "section_title": "Related Work",
            "char_start_offset": 9609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2206
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1652,
                    "end": 1672,
                    "matchedPaperCorpusId": "233296808"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38671875
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "In this paper, we introduced Mixture of Distributions (MoD), a novel approach for merging Large Language Models that preserves and leverages the strengths of constituent models through probabilistic distribution combination. Our method demonstrates significant advantages over existing parameter-merging techniques by maintaining critical density characteristics while enabling selective integration of model capabilities. The experimental results across diverse mathematical benchmarks validate MoD's effectiveness, achieving state-ofthe-art performance on both fundamental and advanced tasks. Our findings suggest that distributionbased merging approaches offer a promising direc-tion for developing more capable and adaptable language models, particularly in specialized domains requiring precise knowledge integration.",
            "score": 0.4268795056642635,
            "section_title": "Conclusions",
            "char_start_offset": 15057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 822
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.509765625
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.Akiba et al. (2024) propose a merging method for LLMs, which not only employ TIE-MERGING for merging in parameter space, but also adopt evolutionary algorithms to optimize the data inference path inside the merge model.The above methods have been successfully adapted to LLMs.Kim et al. (2024) apply the above methods to fuse the LLMs obtain a stronger LLM evaluator.Hammoud et al. (2024) investigate the effects of above methods on LLM safety alignment.\n\nUnlike the aforementioned task vector based methods to resolve conflicting parameters, Stoica et al. (2024) propose ZIPIT that retains similar parameters from another perspective.ZIPIT first identifies highly correlated parameters between different models.It then merges these parameters while retaining significantly different layers, thus improving the merging flexibility.\n\nPruning Redundant Parameters Given that conflicts may exist in the parameters of different models, another solution is to employ pruning tech-niques to reduce these conflicts before merging.Such methods further enhances the relevance of parameters to the task, and we introduce these methods separately.DARE (Yu et al., 2023), a technique that efficiently reduces redundancy in finetuned language models by dropping and rescaling parameters.DELLA-MERGING (Deep et al., 2024) further selects important parameters for fusion on the basis of DARE.As domain-specific data and training techniques grow, the distinctions between fine-tuned models and their base models become more significant.However, DARE experiences significant performance drops, resulting in insufficient capability to process multiple domains effectively.DPPA (Zhu et al., 2024) presents a dual-stage pruning approach as Dynamic Pruning Partition Amplification (DPPA) for effectively merging divergent fine-tuned large language models across different domains.\n\nToolkit Recently, Goddard et al. (2024) have developed Arcee's MergeKit, an open-source toolkit that integrates various model merging methods, including Model Soups, DARE, and TIES-MERGING.This toolkit significantly advances the application of model merging strategies in LLMs3 .",
            "score": 0.4268795056642635,
            "section_title": "Methods based on Task Property",
            "char_start_offset": 23048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 927
                },
                {
                    "start": 929,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1955
                },
                {
                    "start": 1957,
                    "end": 2146
                },
                {
                    "start": 2146,
                    "end": 2236
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498291015625
        },
        {
            "corpus_id": "270559976",
            "title": "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
            "text": "Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations. Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization. The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications. Though challenges persist, integrating LLMs may revolutionize the way software systems are built.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357666015625
        },
        {
            "corpus_id": "269430497",
            "title": "Attacks on Third-Party APIs of Large Language Models",
            "text": "Recently, the advances in Large Language Models (LLMs) (such as GPT (Brown et al., 2020;OpenAI et al., 2023), Gemini, and Llama (Touvron et al., 2023a;b), etc.) have shown impressive outcomes and are expected to revolutionize various industrial sectors, such as finance, healthcare and marketing.These models are capable of performing tasks, such as summarization, question answering, data analysis, and generating human-like content.Their proficiency in these areas makes them invaluable for enhancing work processes and supporting decision-making efforts.\n\nIntegrating these models into practical real-world applications presents several challenges.First, there is the hazard of the models relying on outdated information or generating content that is inaccurate or potentially misleading (Schick et al., 2023;Qin et al., 2023), a critical issue in fields where up-to-date data is essential, such as weather forecasting, news broadcasting, and stock trading.Furthermore, customizing these models to specialized domains, such as law or finance, demands extra domain-specific resources to meet precise requirements.Additionally, although LLMs may achieve expert-level performance in certain tasks, broadening their application across various domains or for complex reasoning tasks remains difficult (Wei et al., 2022).Enhancing their effectiveness often requires fine-tuning, retraining, or comprehensive instructions, which complicates their deployment and constrains their utility for tasks that require advanced skills.\n\nTo address these limitations, one strategy is to integrate third-party Application Programming Interfaces (APIs) with the LLMs.By accessing real-time information (Yao et al., 2022), conducting complex calculations (Schick et al., 2023), and executing specialized tasks such as image recognition (Patil et al., 2023;Qin et al., 2023), this integration broadens the functional scope of LLMs.It significantly boosts their efficiency and performance, enabling them to manage specialized tasks more adeptly without requiring bespoke training.For example, OpenAI's GPT Store significantly expands the operational capabilities of LLMs by hosting over 3 million custom ChatGPT variants.",
            "score": 0.42655952372308426,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 557
                },
                {
                    "start": 559,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 960
                },
                {
                    "start": 960,
                    "end": 1115
                },
                {
                    "start": 1115,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1522
                },
                {
                    "start": 1524,
                    "end": 1651
                },
                {
                    "start": 1651,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2061
                },
                {
                    "start": 2061,
                    "end": 2202
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 88,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1299,
                    "end": 1317,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1686,
                    "end": 1704,
                    "matchedPaperCorpusId": "252762395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "271843401",
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, training these models often requires substantial resources, including thousands of GPUs and the processing of trillions of tokens (Sukhbaatar et al., 2024). To achieve a more powerful and efficient model, integrating the capabilities and advantages of various LLMs into a unified model presents a cost-effective solution. \n\nWhen considering the integration of multiple models' capabilities, ensemble methods often come to mind (Monteith et al., 2011;Jiang et al., 2023). These methods enhance system performance by combining the outputs from various trained models during inference. However, this process often involves the simultaneous deployment of multiple models, which can significantly increase memory and computational overhead, especially with LLMs. An alternative path seeks to merge multiple models into a single model by executing arithmetic operations (Gupta et al., 2020) on their parameters. This approach entails finding suitable combination coefficients, a task that can be either manually executed (Wortsman et al., 2022;Yadav et al., 2024) or carried out through automated learning mechanisms (Matena and Raffel, 2021;Jin et al., 2023). Yet, this method is constrained by the prerequisite that models maintain identical structures. Moving beyond these limitations, FuseLLM (Wan et al., 2024) offers a pioneering fusion approach capable of fusing LLMs with diverse architectures. Rooted in the theory of knowledge distillation, FuseLLM employs probability distribution matrices derived from multiple heterogeneous source LLMs to transfer their collective knowledge to a single target LLM. \n\nAlthough FuseLLM (Wan et al., 2024) shows significant potential, its approach to assessing the strengths of source models primarily based on Min-CE in a teacher-forcing training mode might offer a restricted view of the models' advantages. As depicted in Figure 1a, we evaluate models in both training and inference modes on the training data ( \u00a74.1.2). In inference mode, we employ reward models to assess the quality of source model outputs, with the best-performing output indicating the most advantageous model. Our findings indicate that while vicuna-7b-v1.5",
            "score": 0.4265378902872051,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2289
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 568,
                    "matchedPaperCorpusId": "18937"
                },
                {
                    "start": 1133,
                    "end": 1156,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1156,
                    "end": 1175,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427978515625
        },
        {
            "corpus_id": "270562274",
            "title": "Knowledge Fusion By Evolving Weights of Language Models",
            "text": "Due to the high training costs of large language models, it is common practice to fine-tune already pre-trained language models to adapt them for specific applications.This fine-tuning approach often allows us to achieve excellent performance in specific data domains or tasks at a relatively lower cost (Chen et al., 2021).However, the challenge lies in the fact that fine-tuning (Dodge et al., 2020) the same model in different task scenarios may result in performance variations, meaning that the results may not be satisfactory when testing the same model in different contexts.Therefore, our objective is to integrate knowledge from models trained in different scenarios to enhance the model's performance in cross-domain or cross-task scenarios (Wortsman et al., 2022b), without the need for further training or extra training data.\n\nMainstream knowledge fusion methods can be divided into two main categories.The first involves extensive training on large datasets across multiple tasks to learn new model parameters with shared representations, such as in multi-task learning.The second relies on fusing existing models from specific scenarios without requiring extensive data.While multi-task learning generally improves overall performance, it has significant drawbacks: the need for abundant annotated data for all tasks and the complexity and time consumption of the training phase, especially with dataset combinations (Ruder, 2017).In contrast, model merging methods do not require retraining models and do not raise concerns about data privacy.In this paper, we primarily delve into the second category of methods and introduce an innovative model evolution approach inspired by Darwinian evolution theory (Shafiee et al., 2018).In short, we compare our model evolution approach with other prevalent knowledge fusion methods, detailing their distinct features in Table 1.\n\nIn fact, the problem of model merging can be reformulated as an optimization problem that aims to find the most valuable information for knowledge fusion in order to achieve better outcomes than any individual model alone.For instance, (Jin et al., 2023) employed a simple linear regression approach for optimization, while model soups (Wortsman et al., 2022a) implemented a greedy search method.",
            "score": 0.4265300382771493,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 582
                },
                {
                    "start": 582,
                    "end": 838
                },
                {
                    "start": 840,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1559
                },
                {
                    "start": 1559,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1886
                },
                {
                    "start": 1888,
                    "end": 2110
                },
                {
                    "start": 2110,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 304,
                    "end": 323,
                    "matchedPaperCorpusId": "231602959"
                },
                {
                    "start": 751,
                    "end": 775,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 1432,
                    "end": 1445,
                    "matchedPaperCorpusId": "90063862"
                },
                {
                    "start": 1721,
                    "end": 1743,
                    "matchedPaperCorpusId": "8106771"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.434326171875
        },
        {
            "corpus_id": "278501696",
            "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration",
            "text": "However, these approaches focus on performance of a single models and do not consider a multi-model multi-task system. \n\nMulti-Task Model Merging Model merging integrates multiple models into a single model by performing weight interpolation at the parameter level, serving as an efficient alternative (Singh & Jaggi, 2020;Yang et al., 2024b;2023). Existing model merging methods include weighted merging (Matena & Raffel, 2022;Wortsman et al., 2022;Jin et al., 2022), which assigns varying importance to different models, and subspace merging (Du et al., 2024;Yadav et al., 2023), which eliminates unimportant neurons to reduce task interference. However, these approaches use static merging strategies, limiting adaptability. Moreover, techniques such as activation matching, and permutation invariance help minimize discrepancies. Although these approaches demonstrate improved performance on considerably smaller models, their applicability for larger MoE architectures remains unexplored. Additionally, unlike previous methods that rely solely on static merging, our approach enhances both efficiency and effectiveness by integrating dynamic runtime reconfigurability with static merging.",
            "score": 0.426269784963055,
            "section_title": "Related Work",
            "char_start_offset": 29470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 323,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435791015625
        },
        {
            "corpus_id": "277452617",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
            "text": "Recent researches on model merging techniques have contributed to building more capable models by providing an efficient approach to combine abilities on various tasks from different models that requires less data and compute. Some studies focus on merging homogeneous models with identical architecture, while others focus on tackle the challenge on heterogeneous models which have different architectures. In merging homogeneous models, Task Arithmetic [10] proposes the concept of task vectors, which subtracts fine-tuned weights from pre-train weights to obtain task-related weight difference as the object of merging. Ties-Merging [26] and DARE [29] further improve the performance by mitigating parameter interference during the merging process through parameter pruning and conflict resolving. MetaGPT [32] scales the task vectors with task-agnostic coefficients in closed-form by seperating data term and scaling coefficients in the optimization objective. Although these methods improves the performance of the merged models, they cannot be directly applied on models with architecture difference. In fusing heterogeneous models, DAMC [1] employs parameter decoupling and adaptive adjustment to enhance model merging strategies for fusing modalities on MLLMs with different modality encoders, but this work still focus on merging identical language model architecture. To consolidate LLMs with different architectures, FuseLLM [22] and FuseChat [23] applies token alignment and model fusion strategies with knowledge distillation before continue training the model, but they need labeled data and computation resources for continue training. In fact, the majority of previous works on model merging requires labeled data for validation search or supervised training [1,10,22,23,26,29]. In this work, we eliminate the need of labeled data by leveraging our unsupervised hyper-parameter selection method, and enable model merging strategies to be applied on heterogeneous MLLMs with architecture differences.",
            "score": 0.42607003088517037,
            "section_title": "Model Merging",
            "char_start_offset": 4643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "267760102"
                },
                {
                    "start": 1775,
                    "end": 1778,
                    "matchedPaperCorpusId": "267760102"
                },
                {
                    "start": 1787,
                    "end": 1790,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57958984375
        },
        {
            "corpus_id": "276742465",
            "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
            "text": "Model merging, widely explored in natural language processing and image classification, typically combines multiple fine-tuned task-specific models into a unified multitask model without additional training. Early methods [1,11,13,14,26,33] leverage interpolation techniques to integrate multi-task knowledge effectively. Task-Arithmetic [25] extends these approaches by enabling complex arithmetic operations in weight space, offering finer control over the merging process. Recent studies [30,39,61,63,66] have further reduced interference during merging. Beyond interpolation-based methods, recent works have shown that sparsifying [66] or amplifying delta weights [74] can enhance model knowledge in merging. Additionally, merging techniques have been introduced to multimodal understanding such as question-answering [9,47]. However, the video generation domain has remained unexplored until now. \n\nWe pioneer the introduction of model merging techniques into the video generation domain. Specifically, we leverage model merging to extrapolate motion degree knowledge and extract fine-grained decoupled knowledge, which are selectively merged at different diffusion stages.",
            "score": 0.42592274068133135,
            "section_title": "Model Merging",
            "char_start_offset": 5542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "3845139"
                },
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.425537109375
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.",
            "score": 0.4255969223253288,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.395263671875
        },
        {
            "corpus_id": "271769807",
            "title": "Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation",
            "text": "The weight merging technique has emerged as a significant application of NLP in recent years, aiming to combine multiple task-specific models into a unified model. This methodology has been widely adopted in various benchmarks (Kim et al., 2023) due to its ability to enhance performance not only on the target task but also on out-of-domain tasks. Unlike model ensemble methods, which utilizes the predictions of multiple models to generate a final output, weight merging yields a single model through techniques such as interpolating the weights of multiple models or employing task arithmetic (Ilharco et al., 2023). There are various methods for merging the weights of models fine-tuned on different datasets, with traditional approaches including weight averaging. For instance, TIES-Merging (Yadav et al., 2023) selectively incorporates changes from fine-tuned models by discarding low-magnitude alterations and merging only those values that align with designated sign, while Dare-TIES (Yu et al., 2023) reduces redundancy by converting the majority of delta parameters to zero. We leverage these merging techniques to develop a chatbot that, by accounting for the distinct traits of chit-chat and knowledge-grounded dialogues, seamlessly integrates knowledge, maintains its persona, and effectively engages in multi-turn conversations to ensure enjoyable interactions.",
            "score": 0.4247717838862052,
            "section_title": "Weight Merging",
            "char_start_offset": 8856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 618,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3896484375
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "We apply six different merging methods to combine the models. Each method is designed to integrate the strengths of both models, with specific parameters adjusted to optimize the knowledge transfer. The merging process involves aligning the models' representations and combining their knowledge bases to create a unified model capable of handling both general and technical data in both languages. \n\nDataset Preparation A curated list of technical medical terms is compiled from the Systematized Nomenclature of Medicine (SNOMED) [17], ensuring that only relevant and specialized vocabulary is included. These terms are then translated into Japanese using GPT-4o [2] to maintain consistency with the evaluation process and minimize translation bias.",
            "score": 0.42448436200351325,
            "section_title": "Merging Methods",
            "char_start_offset": 10606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 397
                },
                {
                    "start": 400,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 749
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "74793446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2059326171875
        },
        {
            "corpus_id": "273482450",
            "title": "Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace",
            "text": "Model merging has gained significant attention as a cost-effective approach to integrate multiple single-task fine-tuned models into a unified one that can perform well on multiple tasks. However, existing model merging techniques primarily focus on resolving conflicts between task-specific models, they often overlook potential security threats, particularly the risk of backdoor attacks in the open-source model ecosystem. In this paper, we first investigate the vulnerabilities of existing model merging methods to backdoor attacks, identifying two critical challenges: backdoor succession and backdoor transfer. To address these issues, we propose a novel Defense-Aware Merging (DAM) approach that simultaneously mitigates task interference and backdoor vulnerabilities. Specifically, DAM employs a meta-learning-based optimization method with dual masks to identify a shared and safety-aware subspace for model merging. These masks are alternately optimized: the Task-Shared mask identifies common beneficial parameters across tasks, aiming to preserve task-specific knowledge while reducing interference, while the Backdoor-Detection mask isolates potentially harmful parameters to neutralize security threats. This dual-mask design allows us to carefully balance the preservation of useful knowledge and the removal of potential vulnerabilities. Compared to existing merging methods, DAM achieves a more favorable balance between performance and security, reducing the attack success rate by 2-10 percentage points while sacrificing only about 1% in accuracy. Furthermore, DAM exhibits robust performance and broad applicability across various types of backdoor attacks and the number of compromised models involved in the merging process. Our codes and models are available at https://github.com/Yangjinluan/DAM.",
            "score": 0.4232880689581886,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.490478515625
        },
        {
            "corpus_id": "277787580",
            "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models",
            "text": "RAG represents a method which improves the capabilities of large language models by allowing them to fetch information from external knowledge bases throughout the generation phase. The approach fixes weaknesses in models which use static training data enhanced performance in multimodal task with integration through non-parametric knowledge infusion [5]. However, RAG also faces the challenge of knowledge integration. Primarily, the uncontrolled incorporation of external document prompts leads to conflicts between information extracted from external sources and the parameter-based knowledge of LLMs. The research shows that LLMs experience knowledge update failures during context-memory conflicts because of their parametric bias which leads to 41% changes in retrieval necessity and 35% error rate in multi-hop reasoning problems on account of incorrect parametric answers appearing in the context [7]. To alleviate these problems, recent research adopts query analysis which calculates query complexity through cosine similarity between current query and pre-defined knowledge domains. This method determines whether to activate information retrieval (IR) through static similarity thresholds to solve internal-external knowledge conflicts improving F1-score by at least 4.0% compared to full retrieval baselines [16]. However, static similarity thresholds exhibit limitations in dynamic query environments. The fixed threshold method produces errors in IR decisions when multiple queries from different domains exist in context environments. Moreover, this method merely depends on the query to decide if IR should proceed and can lead to deficient decisions by overlooking the quality of responses generated by LLMs [8]. \n\nTherefore, it demonstrates an urgent requirement to develop a unified framework to optimize information retrieval by considering and evaluating both query and response. This paper presents the Cognitive Convection of Self-Knowledge (CCSK) framework which provides a new solution to resolve the existing limitations within contemporary IR systems. CCSK implements a Siamese Network (SN) to identify queries requiring IR intervention and it also employs Response Quality Model (RQM) to evaluate response quality. We combined both analytical modules through an attention-based mechanism for dynamic fusion of their final outputs. The design improves its semantic relationship detection capability and its adaptability to diverse ambiguous queries. \n\nOur proposed framework received validation through experimental trials which involved benchmarked datasets and real-life conversational data collections.",
            "score": 0.4232337235727225,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 5070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2360
                },
                {
                    "start": 2361,
                    "end": 2478
                },
                {
                    "start": 2481,
                    "end": 2634
                }
            ],
            "ref_mentions": [
                {
                    "start": 1322,
                    "end": 1326,
                    "matchedPaperCorpusId": "263828724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251220703125
        },
        {
            "corpus_id": "264288692",
            "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
            "text": "Once external (non-parametric) knowledge is retrieved, it must be composed with the LLM generation process. Khandelwal et al. (2020) introduce a k-nearest neighbor method with interpolation; RETRO (Borgeaud et al., 2021) combines the prompt with retrieved documents through a specialized cross-attention mechanism; other LLM-IR variants include a fusion-in-decoder method (AT-LAS; Izacard et al., 2022) and TRIME (Zhong et al., 2022), all retrieval-based models that maintain the capacity for few-shot in-context learning. \n\nRecent work addresses the integration of LLMs and IR by learning to combine results from search engines into the context provided to the LLM to answer a specific question (Xu et al., 2023). SAIL (Luo et al., 2023) introduces an instruction fine-tuning method to ground language generation on search results and learn to select trustworthy sources. CooK (Feng et al., 2023) approaches the task of combining multiple curated modular knowledge sources into an integrative system, where sources are modeled as independent LMs and an integrator LLM combines the information from these modules to answer a question. \n\nIn all of these approaches, factual knowledge is stored outside of the parameters of the LLM and can be manipulated without retraining the LLM. These approaches have been shown to be scalable in the number of facts. Editing the fact store means the same as updating a database, thus simplifying the formulation of the task. \n\nRetrieval-based models, arguably, do not resolve all of the concerns we identify with model editing. The problem of identifying the provenance of a given generation span in these combined models remains acute: the text can be determined by the internal LLM parameters, by the external stores, or by their combination, even if they are not logically consistent with one another. Facts that have been seen more often during LLM training may have more weight in this interpolation even if they are wrong or when they become obsolete. Zhu et al. (2020) claim that \"modifying knowledge in explicit memory module networks like FaE (Verga et al., 2020) is not easier and demands changes in the LLM component as well.\"",
            "score": 0.42262536520818533,
            "section_title": "Alternatives to Model Editing",
            "char_start_offset": 12996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 220,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 413,
                    "end": 433,
                    "matchedPaperCorpusId": "249062699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191650390625
        },
        {
            "corpus_id": "272832307",
            "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference",
            "text": "Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, training and serving a single massive model is both costly and inefficient. Additionally, recent findings show that larger models do not consistently outperform smaller or specialized LLMs for all tasks. To address these issues, researchers are exploring multi-LLM approaches to enhance system performance while maintaining cost efficiency. \n\nMixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models. To facilitate fair comparisons between routing strategies, benchmarks like RouterBench [10] and Large Language Model Routing with Benchmark Datasets [20] provide standardized metrics that assess performance, efficiency, and resource consumption.",
            "score": 0.4225595569284069,
            "section_title": "C Related Works",
            "char_start_offset": 11951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2264
                },
                {
                    "start": 2265,
                    "end": 2510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "270521749",
            "title": "Towards Efficient Pareto Set Approximation via Mixture of Experts Based Model Fusion",
            "text": "A common challenge in model merging is task interference and parameter conflicts, which can be addressed through various approaches, such as subspace-based [53,44], representation-based [52,22], and parameter-based methods [51].\n\nOur proposed method differs from existing model fusion approaches that usually integrate individual models into a single multi-task model.Instead, we merge multiple expert models to approximate the entire Pareto set.Additionally, our approach is the first to approximate the Pareto set through model merging, which is more efficient and scalable compared to existing Pareto set learning methods.",
            "score": 0.4223492118373151,
            "section_title": "Related Work",
            "char_start_offset": 6169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 230,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "Model merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
            "score": 0.4204377417244117,
            "section_title": "Introduction",
            "char_start_offset": 1979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "53748459"
                },
                {
                    "start": 455,
                    "end": 475,
                    "matchedPaperCorpusId": "235790783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "276928602",
            "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors",
            "text": "With the widespread adoption of the pre-training and finetuning paradigm, a large number of pre-trained and finetuned checkpoints have been released in open-source communities. However, directly applying multiple individual \n\nCorresponding Input Coefficient T a s k Ve c t o r s  '  ' # Approximate Linear Subspace of The Corresponding Input \n\nFigure 1: The task vector constitude an approximate linear subspace of its corresponding input. models for multi-task problems incurs significant storage costs. While multi-task learning has been employed to address this issue, it typically requires costly training and poses potential privacy risks. Recently, model merging has emerged as a solution, enabling the integration of multiple expert models into a single unified multi-task model without the need for expensive retraining on multi-task datasets. \n\nHowever, due to interference among the expert models, the merged model exhibits a performance gap when compared to its corresponding expert models on specific tasks. Numerous model merging approaches have been proposed to address this issue. Test-time adaptation model merging methods, such as AdaMerging (Yang et al., 2024b) and Surgery (Yang et al., 2024a), leverage unlabeled test data to resolve interference through reweighting or model editing. \n\nMoE-like (Mixture-of-Experts) model merging methods, exemplified by EMR-Merging (Huang et al., 2024), retain additional task-specific parameters, such as task-specific masks, to prevent interference when integrating diverse expert models into a unified model. While these approaches effectively mitigate the interference, they require extra storage or access to test data. Conversely, data-free model merging methods, like Task Arithmetic (Ilharco et al., 2023), enable model merging without additional data or storage requirements, which is particularly advantageous when data privacy or availability is a concern. However, due to the absence of data, current data-free model merging techniques still exhibit a performance gap compared to the aforementioned methods and multi-task learning. \n\nIn this paper, we revisit the update process of fine-tuning phase. Consider a linear layer whose parameters are updated through gradient descent: each neuron's weight adjustment derives from the product of the learning rate, the gradient of its output, and the corresponding input vector.",
            "score": 0.42005631265245025,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2098
                },
                {
                    "start": 2101,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2389
                }
            ],
            "ref_mentions": [
                {
                    "start": 1159,
                    "end": 1179,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1192,
                    "end": 1212,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1746,
                    "end": 1768,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "The swift advancement of deep learning has fostered a shift towards fine-tuning large pre-trained models for downstream tasks, rather than training them from scratch. Having Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). \n\nbeen initially trained on large-scale datasets, pre-trained models have outstanding common sense and are adept at recognizing and processing diverse data patterns (Radford et al., 2019;He et al., 2021;Wang et al., 2023). These models can then be fine-tuned on downstream tasks to acquire task-specific knowledge (Chung et al., 2022;Zheng et al., 2023;Cao et al., 2024). In this context, merging multiple task-specific models into a single unified model has emerged as an effective and scalable strategy for knowledge transfer and multi-task learning (Li et al., 2023;Lin et al., 2023). \n\nThere are several state-of-the-art algorithms for merging models. A prominent example in this field is task arithmetic (Ilharco et al., 2023), which interpolates the parameters of models linearly. These methods excel in extracting knowledge from various models and synthesizing a unified model cheaply. Such approaches present a promising solution for constructing robust models (Izmailov et al., 2019;Wortsman et al., 2022), especially in the scenarios where the training data is decentralized, limited, or inaccessible due to privacy constraints (Tang et al., 2023a;Jin et al., 2023). \n\nExisting methods predominantly aim to find a static multitask optimal solution within the original parameter space. These methods do not introduce any new parameters, thus maintaining the original inference cost. This approach, however, imposes limitations on adaptability to the unique requirements of each instance, as the task-specific optimal solution varies. Another significant challenge of merging multi-task models is mitigating the interference between parameters of different models, which can substantially deteriorate the average performance (Yadav et al., 2023;Yu et al., 2023;Tang et al., 2023b).",
            "score": 0.4193609370551048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 532,
                    "matchedPaperCorpusId": "257038341"
                },
                {
                    "start": 664,
                    "end": 681,
                    "matchedPaperCorpusId": "257505035"
                },
                {
                    "start": 880,
                    "end": 897,
                    "matchedPaperCorpusId": "259022411"
                },
                {
                    "start": 1449,
                    "end": 1469,
                    "matchedPaperCorpusId": "258841789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4853515625
        },
        {
            "corpus_id": "263620126",
            "title": "AdaMerging: Adaptive Model Merging for Multi-Task Learning",
            "text": "Joint Training for Multi-Task Learning. The joint training method gathers training data from multiple tasks to learn these tasks simultaneously (Caruana, 1997) to achieve knowledge transfer (Wu et al., 2023). Existing works mainly focus on mitigating task conflicts from a architecture (Misra et al., 2016;Sun et al., 2020) or optimization (Sener & Koltun, 2018;Liu et al., 2021) perspective. \n\nArchitectural-based methods mitigate task interference by sparsifying (Liu et al., 2019a;Ding et al., 2021), branching (Lu et al., 2017;Guo et al., 2020) or modularizing (Ma et al., 2018;Hazimeh et al., 2021) shared structures. Optimization-based methods balance multiple tasks from the perspectives of task training weights (Sener & Koltun, 2018;Liu et al., 2019a), gradient dominance (Chen et al., 2018;He et al., 2022;Yang et al., 2023), and gradient conflicts (Yu et al., 2020;Chen et al., 2020;Liu et al., 2021). However, the conventional approaches for collecting raw data across multiple tasks for joint training face challenges that may render them unsuitable in the era of foundation models. This is primarily due to either (i) their computational inefficiency stemming from the high computation cost for updating the pre-trained models or (ii) numerous data owners refrain from releasing valuable or privacy-sensitive raw data. Instead, they opt to share models fine-tuned on these pre-trained models. \n\nModel Merging for Multi-task Learning. The practice of model merging has emerged as a promising solution to enhance model generalization and facilitate MTL. The first type of research involves merging multiple models, all initially trained on the same task, with the aim of enhancing the model's overall generalization (Gupta et al., 2020;Cha et al., 2021;Wortsman et al., 2022;Wang et al., 2022) or to perform federated learning (Li et al., 2019;Wang et al., 2020;Liu et al., 2022).",
            "score": 0.4193538364875079,
            "section_title": "RELATED WORK",
            "char_start_offset": 9468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 159,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 190,
                    "end": 207,
                    "matchedPaperCorpusId": "258352277"
                },
                {
                    "start": 286,
                    "end": 306,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 306,
                    "end": 323,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 340,
                    "end": 362,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 362,
                    "end": 379,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 465,
                    "end": 484,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 484,
                    "end": 502,
                    "matchedPaperCorpusId": "235792414"
                },
                {
                    "start": 514,
                    "end": 531,
                    "matchedPaperCorpusId": "2057685"
                },
                {
                    "start": 531,
                    "end": 548,
                    "matchedPaperCorpusId": "219259832"
                },
                {
                    "start": 565,
                    "end": 582,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 582,
                    "end": 603,
                    "matchedPaperCorpusId": "235358484"
                },
                {
                    "start": 720,
                    "end": 742,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 742,
                    "end": 759,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 816,
                    "end": 834,
                    "matchedPaperCorpusId": "254043876"
                },
                {
                    "start": 859,
                    "end": 876,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 894,
                    "end": 911,
                    "matchedPaperCorpusId": "239998731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "259950775",
            "title": "Integration of large language models and federated learning",
            "text": "Despite this, current research has not fully covered all areas related to the integration of LLMs and FL. Specifically, some studies focus on exploring the integration of sub-technologies within LLMs and FL [210], neglecting the importance of the overall concept of Federated Large Language Models (FedLLMs). In view of this, we adopt a more comprehensive research approach to organize existing work on combining LLMs and FL. By analyzing the current progress in research combining LLMs and FL, we offer unique insights into the benefits, challenges, and future development trends of their integration. Notably, while analyzing the combination of sub-technologies in LLMs and FL, we also explored sub-technologies shared by foundational models, extending beyond just language models to include multimodal and visual models. Since these shared sub-technologies can be easily adapted to language models, this broader perspective offers valuable insights into the integration of LLMs with FL. \n\nThe remainder of this paper is organized as follows: we first briefly introduce the technical backgrounds of FL and LLMs in Section 2. In Section 3, we present a comprehensive analysis of the current status, challenges, and future directions regarding the combination of LLMs and FL. This includes three sub-sections: i) the integration of sub-technologies in LLMs with FL, ii) the integration of sub-technologies in FL with LLMs, and iii) the overall integration of LLMs and FL. Section 4 analyzes the application scenarios where LLMs are combined with FL. Finally, in Section 5, we summarize the progress of research on the integration of LLMs and FL and present insights into the future development of this field.",
            "score": 0.4185848365485682,
            "section_title": "Introduction",
            "char_start_offset": 2199,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4189453125
        },
        {
            "corpus_id": "273375534",
            "title": "Exploring Model Kinship for Merging Large Language Models",
            "text": "Intuitively, understanding the task capabilities of models is crucial for effective model merging. However, the black-box nature of LLMs presents significant challenges in comprehending their advantages Shi et al. (2024) and task-specific capabilities. For fine-tuned models, task benchmark results (Gao et al., 2024;Li et al., 2023c) and the characteristics of the fine-tuning datasets are often used as indicators of task-related knowledge. Yet, these metrics often fall short in comprehensive comparisons. The internal relationships between tasks and datasets can arise confusion, particularly in complex merging scenarios that involve multiple tasks. \n\nMotivated by the parallel drawn between biological reproduction and the process of model evolution (as seen in Figure 1), we propose that the concept of kinship, which is central to evolutionary biology for understanding breeding relationships and human genealogies (Thompson, 1985), can be conceptually extended to the field of model merging, specifically to describe the kinship between models. To be noted, Ilharco et al. (2023) has proposed Task Arithmetic and offered new insights into how the merging process preserves and blends task information from the original models. (Ilharco et al., 2023) propose to use task vectors, defined as the differences in weight space between fine-tuned models and a pre-trained model, and demonstrate that arithmetic operations on these task vectors can serve as an effective method for model merging. \n\nInspired by this idea, we introduce the concept of Model Kinship, a metric designed to assess the degree of similarity or relatedness between large language models (LLMs) based on their \"genetic\" information (a.k.a. the changes in weights during model evolution). Considering two target models involved in a single merge, the weights of each target model are denoted as \u03b8 target \u2208 R d . In this paper, it is significant to claim that a target model may either be a fine-tuned model or a previously merged model. Similarly, \u03b8 base \u2208 R d represents the weights of the base model from which the two target models are derived. The difference of weights \u03b4 target between the target model and the base model is denoted as:",
            "score": 0.418274029333997,
            "section_title": "MODEL KINSHIP",
            "char_start_offset": 5174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1498
                },
                {
                    "start": 1501,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 220,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 1067,
                    "end": 1088,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1236,
                    "end": 1258,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2286376953125
        },
        {
            "corpus_id": "275921828",
            "title": "Each Rank Could be an Expert: Single-Ranked Mixture of Experts LoRA for Multi-Task Learning",
            "text": "Low-Rank Adaptation (LoRA) is widely used for adapting large language models (LLMs) to specific domains due to its efficiency and modularity. Meanwhile, vanilla LoRA struggles with task conflicts in multi-task scenarios. Recent works adopt Mixture of Experts (MoE) by treating each LoRA module as an expert, thereby mitigating task interference through multiple specialized LoRA modules. While effective, these methods often isolate knowledge within individual tasks, failing to fully exploit the shared knowledge across related tasks. In this paper, we establish a connection between single LoRA and multi-LoRA MoE, integrating them into a unified framework. We demonstrate that the dynamic routing of multiple LoRAs is functionally equivalent to rank partitioning and block-level activation within a single LoRA. We further empirically demonstrate that finer-grained LoRA partitioning, within the same total and activated parameter constraints, leads to better performance gains across heterogeneous tasks. Building on these findings, we propose Single-ranked Mixture of Experts LoRA (\\textbf{SMoRA}), which embeds MoE into LoRA by \\textit{treating each rank as an independent expert}. With a \\textit{dynamic rank-wise activation} mechanism, SMoRA promotes finer-grained knowledge sharing while mitigating task conflicts. Experiments demonstrate that SMoRA activates fewer parameters yet achieves better performance in multi-task scenarios.",
            "score": 0.418148315854035,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "273185626",
            "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
            "text": "Based on the sequence of inference and fusion processes, multiple model collaboration methods can be broadly classified into two categories: model ensembling (Jiang et al., 2023b;Yu et al., 2024b) and model merging (Yu et al., 2024a;Akiba et al., 2024). Model ensembling follows an inferencethen-fusion approach, aiming to integrate the outputs of various models to achieve a more refined response. Conversely, model merging adopts a fusion-then-inference strategy, wherein different models are combined into a single model before inference. While model merging is typically applicable only to homologous models, our focus in this study is on the more general approach, namely model ensembling. We discuss different model ensembling methods as follows: \n\nOutput-Level Model Ensembling methods involve selecting multiple candidate models and utilizing their complete outputs for aggregation. For instance, Jiang et al. (2023b) developed PAIR-RANKER, an additional ranking model, to evaluate and select the best candidate output. Similarly, Lu et al. (2024) and Shnitzer et al. (2023) designed a router that determines the most appropriate candidate model based on the given question. However, these approaches are restricted by the existing outputs and become ineffective if all options are incorrect. Some studies have addressed this by training fusion models to combine outputs (Jiang et al., 2023b;Wang et al., 2024b), which alleviates the limitation of relying solely on available candidates and often yields improved results. Nevertheless, achieving generalization with fusion models remains a significant challenge, as they may not fully utilize the probability information generated at each step. Probability-Level Model Ensembling methods focus on integrating outputs from different models by utilizing the probability distribution at each generation step. COOL-FUSION (Liu et al., 2024) let each source LLM generate tokens until reaching common word boundaries, then jointly reranks the segments. However, this method relies on common word boundaries, limiting generation flexibility and diversity. Additionally, managing multiple source models can increase complexity.",
            "score": 0.41771790436313644,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 179,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 905,
                    "end": 925,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1039,
                    "end": 1055,
                    "matchedPaperCorpusId": "265212821"
                },
                {
                    "start": 1379,
                    "end": 1400,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1400,
                    "end": 1419,
                    "matchedPaperCorpusId": "263608332"
                },
                {
                    "start": 1876,
                    "end": 1894,
                    "matchedPaperCorpusId": "262064307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "277634330",
            "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
            "text": "Standard full-model fine-tuning methods update all parameters when learning each new task, fully exploiting the model's expressive power but risking severe catastrophic forgetting due to conflicting updates (Luo et al., 2025). On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging. \n\nPositioning Our Work: Our approach introduces a novel constrained full-parameter update method that differs fundamentally from existing categories. Unlike parameter-efficient approaches, we leverage the entire parameter space, maximizing expressive capacity. Unlike isolation approaches, we don't partition parameters or require additional task-specific modules. Unlike constrained full fine-tuning, we explicitly mitigate interference through geometric constraints. Specifically, we dynamically identify low-rank subspaces via Singular Value Decomposition (SVD) and constrain updates to be orthogonal to previously learned task representations. This geometric approach to interference minimization ensures knowledge preservation while maintaining update flexibility. By operating in the full parameter space while enforcing orthogonality constraints, our method achieves a unique balance between knowledge retention and model plasticity, providing a theoretically grounded and practically scalable solution for continual learning in large language models.",
            "score": 0.4176548445747452,
            "section_title": "Full-Model Fine-Tuning and Model Merging:",
            "char_start_offset": 8495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 311,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 318,
                    "end": 338,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61181640625
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "Dense merging methods combine multiple dense models into one to achieve diverse capabilities (Wortsman et al., 2022;Ilharco et al., 2022;Goddard et al., 2024;Jin et al., 2022;Matena and Raffel, 2022;Roberts et al., 2024). Most approaches focus on merging homogeneous dense models into another dense model. For example, average merging (Wortsman et al., 2022) averages model parameters, while task vector merging (Ilharco et al., 2022) adds the unweighted sum of task vectors (the difference between base and expert parameters) back to the dense model with scaling. Other work determines task vector weights instead of using an unweighted sum (Jin et al., 2022;Matena and Raffel, 2022). SoTA methods like Dare and Ties (Yadav et al., 2024;Yu et al., 2024) trim the task vector to resolve parameter interference: Dare trims the task vector randomly and rescales, while Ties sets vector parameters to zero by magnitude and adjusts signs to reduce conflicts. \n\nIn addition to homogeneous model merging, Roberts et al. (2024) propose merging heterogeneous models into a dense model using projectors, while Wan et al. (2024) apply knowledge distillation to fuse heterogeneous models. In this work, we introduce a more efficient method for merging experts with limited or no further fine-tuning and, unlike previous work focusing on dense models, we explore merging homogeneous and heterogeneous experts into an MoE model.",
            "score": 0.4175412556894326,
            "section_title": "Dense Model Merging",
            "char_start_offset": 3993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1415
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 116,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 335,
                    "end": 358,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 660,
                    "end": 684,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 718,
                    "end": 738,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 738,
                    "end": 754,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "266573578",
            "title": "Machine Learning Approaches for Diagnostics and Prognostics of Industrial Systems Using Industrial Open Source Data",
            "text": "Moreover, the challenge of collecting a sufficiently large, labeled dataset is a significant barrier in practical applications. The development of unsupervised and semisupervised transfer learning techniques may help to address this issue. \n\n(5) Potential Utilization of Large Language Models (LLMs) and Industrial Large Knowledge Models (ILKMs). Recent advancements in large language model technologies have shown remarkable abilities in natural language processing and related tasks, hinting at the potential for general artificial intelligence applications [130]. Leveraging these cutting-edge technologies could lead to new changes in PHM domain. Yang et al. introduced a novel benchmark dataset focused on Question Answering (QA) in the industrial domain and proposed a new model interaction paradigm, aimed at enhancing the performance of LLMs in domain-specific QA tasks [123]. This approach signifies a substantial stride in customizing LLMs for more specialized, industry-oriented applications. Meanwhile, Li's team systematically reviewed the current progress and key components of ChatGPT-like large-scale foundation (LSF) models, and provided a comprehensive guide on adapting these models to meet the specific needs of PHM, underscoring the challenges and opportunities for future development [67]. Moreover, as shown in Figure 6, Lee's team proposed an Industrial Large Knowledge Model (ILKM) framework that aims to solve complex challenges in intelligent manufacturing by combining LLMs and domainspecific knowledge [63]. Therefore, integrating specialized domain knowledge with LLM technology presents a good opportunity to develop more effective ML models, potentially leading to better solutions for challenges in PHM.",
            "score": 0.4166393844063988,
            "section_title": "Prospects",
            "char_start_offset": 61443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1736
                }
            ],
            "ref_mentions": [
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "271432380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "276961298",
            "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
            "text": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
            "score": 0.41605524622036594,
            "section_title": "Model Merging",
            "char_start_offset": 8245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1787
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "Multi-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.",
            "score": 0.41602397374366834,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83935546875
        },
        {
            "corpus_id": "276885421",
            "title": "To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging",
            "text": "Fine-tuning pre-trained models on targeted datasets enhances task-specific performance but often comes at the expense of generalization. Model merging techniques, which integrate multiple fine-tuned models into a single multi-task model through task arithmetic at various levels: model, layer, or parameter, offer a promising solution. However, task interference remains a fundamental challenge, leading to performance degradation and suboptimal merged models. Existing approaches largely overlook the fundamental role of individual neurons and their connectivity, resulting in a lack of interpretability in both the merging process and the merged models. In this work, we present the first study on the impact of neuronal alignment in model merging. We decompose task-specific representations into two complementary neuronal subspaces that regulate neuron sensitivity and input adaptability. Leveraging this decomposition, we introduce NeuroMerging, a novel merging framework developed to mitigate task interference within neuronal subspaces, enabling training-free model fusion across diverse tasks. Through extensive experiments, we demonstrate that NeuroMerging achieves superior performance compared to existing methods on multi-task benchmarks across both vision and natural language domains. Our findings highlight the importance of aligning neuronal mechanisms in model merging, offering new insights into mitigating task interference and improving knowledge fusion.",
            "score": 0.41487274593634194,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "In the pretraining-finetuning paradigm, different model branches that share a common pretrained initialization exhibit specific linear properties. These properties, such as Cross-Task Linearity (CTL), allow for the linear interpolation of weights and corresponding feature spaces across tasks. This phenomenon enables effective model merging by ensuring consistency between parameter spaces and feature representations, even when the models are fine-tuned on different tasks. It provides a foundation for combining diverse taskspecific models into a unified framework [Zhou et al., 2024b]. \n\nDirect Merging type is a relatively simple model merging method that avoids the complexities of resolving conflicts and interferences during the merging of model weights. This approach does not require retraining, significantly reducing computational costs. Moreover, many advanced model merging methods are built upon these straightforward foundational approaches, highlighting their importance in the field. Despite their simplicity, these foundational methods serve as the cornerstone for further innovations and advancements in model merging techniques. Several model merging methods fall into this category. The following sections introduce these approaches. \n\nModel Soup ( Weight Averaging ): It is a method for merging fine-tuned models by averaging their weights, improving accuracy and robustness without additional training or inference costs. It works well when models share the same pretrained initialization but differ in hyperparameters [Wortsman et al., 2022]. Variants like uniform averaging and greedy soup selectively optimize performance. Model soup is efficient, flexible, and outperforms individual models, making it a robust alternative to ensembles for tasks like image and text classification. However, this method does not account for weight conflicts during model merging, which often results in performance that falls short of more sophisticated model merging approaches. \n\nTask arithmetic: It is a method for editing pre-trained models by leveraging task vectors, which are computed as the difference between a model's fine-tuned weights and its pre-trained weights (\u03c4 t = \u03b8 t \u2212 \u03b8 0 ). These vectors encapsulate the changes needed for a model to perform a specific task and can be applied to modify model weights through simple operations like addition, subtraction, and analogy-based reasoning [Ilharco et al., 2022].",
            "score": 0.4147317064157072,
            "section_title": "Direct Merging",
            "char_start_offset": 9842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 588,
                    "matchedPaperCorpusId": "267499590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "In this study, we have explored the realm of knowledge fusion for LLMs to create a unified model that combines the capabilities and distinctive strengths of multiple structurally diverse LLMs. We introduced a novel method, FUSELLM, which leverages the generative distributions of these source LLMs to externalize their knowledge and employs them in the continual training of the target LLM. Through a series of experiments, we have demonstrated the superiority of FUSELLM over individual source LLMs and established baselines. Notably, in a simulated experiment featuring multiple structurally identical LLMs, FUSELLM has showcased its competitive effectiveness compared to ensemble and weight merging methods. Hence, the domain of LLMs fusion emerges as a more promising avenue for exploration, particularly given the diverse structures and substantial model sizes of LLMs. We believe that these findings will inspire future research endeavors. \n\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. \n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791-4800, 2019. \n\nJinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operations. arXiv preprint arXiv:2306.14870, 2023.",
            "score": 0.41450130713776434,
            "section_title": "CONCLUSION",
            "char_start_offset": 27695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1565
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181640625
        },
        {
            "corpus_id": "277509943",
            "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables",
            "text": "Elementary Task decomposition within a Single Prompt: To test whether a single prompt can give reasonable outputs even when directed to do task decomposition as an intermediate step, we propose Align-Update Decomposition prompt. In this prompt, the model is instructed to first implicitly align all corresponding information between the two tables. Once these alignments are automatically generated, the model should carefully review each alignment to identify and remove any outdated information wherever necessary. Additionally, the model is explicitly instructed to add missing rows that could not be mapped during the alignment process. This prompt is inspired by the task decomposition of Khincha et al. (2023), which does the same with rule-based approach. \n\nHierarchical Task Decomposition Prompt. Instead of creating a single instruction set for the task of Information synchronization. We do a hierarchical decomposition of the task and create prompts for each step. These prompts are applied sequentially, with the output of the last prompt as input to the next prompt in the hierarchy. Different hierarchical steps for this prompt are: \n\n\u2022 Translation: All tables (T S , T R , and T G ) from different languages are converted to English. English is selected as the base language because most state-of-the-art LLMs are largely trained on curated English data, resulting in higher accuracy for complex reasoning and analysis tasks performed in English compared to other languages. \u2022 Knowledge graphs conversion: The translated source and reference tables are then converted into knowledge graphs. Our experiments indicate that the subsequent hierarchical steps are more effective when using knowledge graphs rather than infoboxes/tables. LLMs perform better reasoning over knowledge graphs. \u2022 Merging or alignment: The source(KG S ) and reference(KG R ) knowledge graphs are merged to create a unified knowledge graph that consolidates all the information from both sources. Merging of knowledge graph is equal to alignment step described in the section above. This merging process helps eliminate redundant information from unresolved conflicts and enhances the inclusion of missing details. During the merge step, the model first gathers all necessary information, and in the subsequent update step, it makes the relevant adjustments. \u2022 Update: The merged knowledge graphs is used to update information in the source knowledge graphs. Due to these being node operations, these are fast and have better interpretablity. \n\nAfter the update step, the revised knowledge graphs are converted back into tables.",
            "score": 0.41448298364540415,
            "section_title": "Proposed Methodology",
            "char_start_offset": 7599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2445
                },
                {
                    "start": 2446,
                    "end": 2529
                },
                {
                    "start": 2532,
                    "end": 2615
                }
            ],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 715,
                    "matchedPaperCorpusId": "259375897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12408447265625
        },
        {
            "corpus_id": "248496374",
            "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",
            "text": "Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced\"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.",
            "score": 0.41422417423168845,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "As the pretraining-finetuning paradigm gains increasing popularity [38], the research community has witnessed a proliferation of finetuned models [20], typically derived from foundational models such as T5 [23], and CLIP [22], among others. However, these models are often finetuned on task-specific training data, which limits their capacity for out-of-domain generalization [25,29,30]. In diverse realworld applications, the independent deployment of multiple fine-tuned models increases storage costs and computational demands. While traditional multi-task learning methods can mitigate these issues, they typically necessitate concurrent training across multiple task-specific datasets. Nonetheless, managing original datasets incurs significant expenses and potential privacy risks [16]. Moreover, when confronted with new tasks, traditional multi-task learning methods necessitate training from scratch. Consequently, exploiting existing models to construct efficient multi-task models has become a crucial challenge. \n\nFortunately, model merging [10] has garnered growing attention as an economical and efficient method for obtaining multi-task models. This approach aims to merge multiple task-specific models, requiring no original training data or only a small amount of unlabeled data, thereby enabling the merged model to perform efficiently across diverse tasks [40]. As a foundational technique in this field, Ilharco et al. [10] introduced the concept of Task Arithmetic. Specifically, Task Arithmetic combines task vectors through arithmetic operations, facilitating the efficient transfer of capabilities among different tasks and thus enabling the construction of multi-task models. Here, task vectors are derived by computing the difference between the weights of fine-tuned models and the pre-trained model. However, related studies have indicated that task interference has become the main challenge for this method [21,39]. Previ-ous studies have proposed various techniques to mitigate this issue [8,33,39,44]. Ties-Merging [39] addresses this challenge by pruning redundant parameters, resolving sign conflicts, and averaging parameters aligned along dominant directions. DARE [44] reduces merging conflicts through random parameter drop and maintains model performance via rescaling operations. Consensus Merging [33] aims to eliminate selfish and catastrophic weights. Despite these advancements, these methods often rely on empirical strategies to resolve conflicts.",
            "score": 0.41397225250371145,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2394
                },
                {
                    "start": 2395,
                    "end": 2493
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "257766557"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "267547973"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "258832820"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "52297310"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1937,
                    "end": 1941,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1941,
                    "end": 1944,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2023,
                    "end": 2026,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2026,
                    "end": 2029,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2029,
                    "end": 2032,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2047,
                    "end": 2051,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2201,
                    "end": 2205,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2338,
                    "end": 2342,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "The remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.",
            "score": 0.4137787195153735,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78955078125
        },
        {
            "corpus_id": "273821064",
            "title": "Collective Model Intelligence Requires Compatible Specialization",
            "text": "In this work, we explore the limitations of combining models by averaging intermediate features, referred to as model merging, and propose a new direction for achieving collective model intelligence through what we call compatible specialization. Current methods for model merging, such as parameter and feature averaging, struggle to effectively combine specialized models due to representational divergence during fine-tuning. As models specialize to their individual domains, their internal feature representations become increasingly incompatible, leading to poor performance when attempting to merge them for new tasks. We analyze this phenomenon using centered kernel alignment (CKA) and show that as models specialize, the similarity in their feature space structure diminishes, hindering their capacity for collective use. To address these challenges, we investigate routing-based merging strategies, which offer more flexible methods for combining specialized models by dynamically routing across different layers. This allows us to improve on existing methods by combining features from multiple layers rather than relying on fixed, layer-wise combinations. However, we find that these approaches still face limitations when layers within models are representationally incompatible. Our findings highlight the importance of designing new approaches for model merging that operate on well-defined input and output spaces, similar to how humans communicate through language rather than intermediate neural activations.",
            "score": 0.41339304493028095,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "266725285",
            "title": "Elastic Multi-Gradient Descent for Parallel Continual Learning",
            "text": "H OW to learn a single model for multiple tasks from multiple data sources has been a long-standing research topic [53], [39], [9], with applications in various fields, such as recommendation systems [85] and machine translation [81]. Two major paradigms (i.e., Multi-Task Learning (MTL) and Continual Learning (CL)) have emerged for learning a unified model, and they differ in terms of their approach to data access. Fig. 1(a) and (b) provide an overview of MTL and CL. \n\nMTL [76], [57], [31] receives a fixed number of data pipes (from one to many) for different tasks, where all tasks are trained in parallel at the same time. The challenge of MTL is the competition between tasks, i.e., task conflict. To address the task conflict, traditional MTL solutions can be mainly grouped into feature-based [55], [74] and parameterbased [72], [10], [36] approaches. In recent years, some studies focus on reweighting multi-task training for task balancing, which can be categorized into three kinds, including learningbased methods [19], [45], solving-based methods [66], and calculating-based methods [46], [20], [79], [33], [42]. \n\nDifferent from MTL, CL [37], [40], [50] aims to continuously learn new knowledge from a sequence of tasks with nonoverlapping data streams over a lifelong time. Because the data of each task comes in sequence, the challenge of CL is catastrophic forgetting of old tasks or finished tasks. The existing CL methods are grouped into three types [22], i.e., regularization-based methods [37], [15], [24], [80], [1], [28], [27], architecture-based methods [54], [77], [65], [64]. and rehearsal-based methods [50], [17], [34], [6], [68], [58], [52].",
            "score": 0.41085350055682085,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1674
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 119,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "247618786"
                },
                {
                    "start": 200,
                    "end": 204,
                    "matchedPaperCorpusId": "202640022"
                },
                {
                    "start": 484,
                    "end": 488,
                    "matchedPaperCorpusId": "7178598"
                },
                {
                    "start": 490,
                    "end": 494,
                    "matchedPaperCorpusId": "216561881"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "12324861"
                },
                {
                    "start": 810,
                    "end": 814,
                    "matchedPaperCorpusId": "3135197"
                },
                {
                    "start": 834,
                    "end": 838,
                    "matchedPaperCorpusId": "10420876"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "14236561"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "785634"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1166,
                    "end": 1170,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1473,
                    "end": 1477,
                    "matchedPaperCorpusId": "218889912"
                },
                {
                    "start": 1514,
                    "end": 1518,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "4047127"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "53776855"
                },
                {
                    "start": 1532,
                    "end": 1536,
                    "matchedPaperCorpusId": "10409742"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "4254748"
                },
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "247362808"
                },
                {
                    "start": 1582,
                    "end": 1586,
                    "matchedPaperCorpusId": "3977226"
                },
                {
                    "start": 1600,
                    "end": 1604,
                    "matchedPaperCorpusId": "3285974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31298828125
        },
        {
            "corpus_id": "274610810",
            "title": "How to Merge Your Multimodal Models Over Time?",
            "text": "Model merging combines multiple expert models - finetuned from a base foundation model on diverse tasks and domains - into a single, more capable model. However, most existing model merging approaches assume that all experts are available simultaneously. In reality, new tasks and domains emerge progressively over time, requiring strategies to integrate the knowledge of expert models as they become available: a process we call temporal model merging. The temporal dimension introduces unique challenges not addressed in prior work, raising new questions such as: when training for a new task, should the expert model start from the merged past experts or from the original base model? Should we merge all models at each time step? Which merging techniques are best suited for temporal merging? Should different strategies be used to initialize the training and deploy the model? To answer these questions, we propose a unified framework called TIME - Temporal Integration of Model Expertise - which defines temporal model merging across three axes: (1) Initialization Phase, (2) Deployment Phase, and (3) Merging Technique. Using TIME, we study temporal model merging across model sizes, compute budgets, and learning horizons on the FoMo-in-Flux benchmark. Our comprehensive suite of experiments across TIME allows us to uncover key insights for temporal model merging, offering a better understanding of current challenges and best practices for effective temporal model merging.",
            "score": 0.41043396917783104,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58935546875
        },
        {
            "corpus_id": "266755751",
            "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
            "text": "To address the training and the data challenges mentioned above, we propose and study a practical setting for model composition: (i) we are given access to one (or more) augmenting model(s) and an anchor model, (ii) we are not allowed to modify the weights of either models, and (iii) we only have access to a small amount of data, representing the \"combined skills\" of the given models, e.g., code generation with complex logical reasoning. \n\nPrior work has largely approached the question of composition from either a routing or a merging standpoint, neither of which provide an effective solution to capture this setting. Routing between the given models, i.e., choosing an output of one model over the other (Ma et al., 2019), or performing a soft ensemble (Muqeeth et al., 2023) is not effective when neither of the models can demonstrate the desired capability. Another body of work creates a combined model by an arithmetic combination of base model parameters (Wortsman et al., 2022;Ilharco et al., 2022;Matena & Raffel, 2022). However, these settings are naturally restrictive and their efficacy is unclear when combining models with different sizes and pre-training objectives (Yadav et al., 2023). \n\nIn this work, we propose a novel Composition to Augment Language Models (CALM) framework to address the general model composition setting mentioned above. Rather than a shallow combination of the augmenting and anchor LMs (Wortsman et al., 2022;Ilharco et al., 2022), CALM introduces a small number of trainable parameters over both augmenting and anchor models' intermediate layer representations. CALM finds an effective combination of the given models to perform new challenging tasks more accurately than either of the models alone, while preserving the capabilities of individual models. Figure 1 highlights few motivating scenarios for CALM. \n\nWe study key practical applications of CALM: language inclusivity and code generation. For language inclusivity ( \u00a74.2), we use a model that has been trained on a set of low-resource languages.",
            "score": 0.41034174109776367,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 729,
                    "matchedPaperCorpusId": "58145688"
                },
                {
                    "start": 968,
                    "end": 991,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1433,
                    "end": 1456,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204345703125
        },
        {
            "corpus_id": "270619909",
            "title": "APPL: A Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating texts across a broad spectrum of tasks (Raffel et al., 2020;Brown et al., 2020;Chowdhery et al., 2023;Touvron et al., 2023a;b;OpenAI, 2023).Their success has contributed to an emerging trend of regarding LLMs as novel operating systems (Andrej Karpathy, 2023;Ge et al., 2023;Packer et al., 2023).Compared with traditional computer systems that precisely execute structured and well-defined programs written in programming languages, LLMs can be guided through flexible natural language prompts to perform tasks that are beyond the reach of traditional programs.ming language that integrates conventional programs and natural language prompts to provide a unified interface for users to access computers and LLMs together.APPL also facilitates users fusing the strengths of computer programs and LLMs by providing convenient conventions between them.\n\nMeanwhile, there is a growing interest in harnessing the combined strengths of LLMs and conventional computing to address more complex challenges.Approaches like tree-of-thoughts (Yao et al., 2024), RAP (Hao et al., 2023) and LATS (Zhou et al., 2023) integrate search-based algorithms with LLM outputs, showing improved outcomes over using LLMs independently.Similarly, the creation of semi-autonomous agents such as AutoGPT (Richards, 2023) and Voyager (Wang et al., 2023) demonstrates the potential of LLMs to engage with external tools, like interpreters, file systems, and skill libraries, pushing the boundaries of what Figure 2: (a) The APPL program for answering multiple questions about a quotation by first extracting the author's name.In this APPL program, the @ppl marks the function to be APPL function, which provides a context that interactive expression statements (highlighted lines) can be interacted with.\n\nPassing the context across APPL functions is implicitly handled along with function calls, mimicking the process of stack frame management.The gen function requests an LLM generation using the prompts stored in the context, with other optional arguments following the OpenAI's API (OpenAI).",
            "score": 0.40995587125415534,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 399
                },
                {
                    "start": 399,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 952
                },
                {
                    "start": 954,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1877
                },
                {
                    "start": 1879,
                    "end": 2018
                },
                {
                    "start": 2018,
                    "end": 2169
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 163,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 163,
                    "end": 182,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 182,
                    "end": 205,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1133,
                    "end": 1151,
                    "matchedPaperCorpusId": "258762525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1300048828125
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "We compare our method against a diverse set of baselines, ranging from simple merging strategies to advanced methods leveraging task-specific or shared representations. Pretrained indicates a model that predicts multiple tasks without additional fine-tuning for task-specific requirements. However, the absence of task-specific information for downstream tasks generally leads to poor performance. Individual refers to the fine-tuning of individual pre-trained models for each task. Since there is no interference between tasks, it has been regarded as the upper bound of task-specific performance. Traditional MTL trains a multi-task model by combining the original training data for all tasks. While effective, this approach depends on access to raw data and labels for each task, which is not always practical. Weight Averaging merges multiple individual models by directly averaging their parameters to create a single model for multi-task learning. Although simple, this method lacks task-specific adjustments. Fisher Merging [39] uses the Fisher information matrix to estimate the importance of each parameter. Model parameters are merged based on their relative contributions. RegMean [27] merges models by aligning the inputs of linear layers to minimize the L2 distance between the individual models and the merged model. Task Arithmetic [23] defines the difference between the fine-tuned and pre-trained model parameters as a task vector. By combining multiple task vectors and adding them to the pre-trained model, it enables multi-task learning. MagMax [38] merges task vectors [23] by selecting the parameter with the largest magnitude for each position, consolidating knowledge into a single model without retaining task-specific data. Ties Merging [72] highlight the importance of addressing interference in task arithmetic-based merging. It involves removing redundant parameters from the task vector and resolving parameter sign conflicts. LiNeS [64] observes that reducing the influence of shallow layers helps prevent distortion of general representations, thereby simplifying merging coefficient selection by allowing them to increase linearly with layer depth. \n\nConsensus TA [63] enhances Task Arithmetic [23] by using Consensus Merging, which retains only weights beneficial to multiple tasks while eliminating irrelevant or taskspecific weights. This process uses task-specific binary masks to identify relevant weights and forms a consensus mask to minimize task interference.",
            "score": 0.4098456074320387,
            "section_title": "B.2. Baseline Details",
            "char_start_offset": 34982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2181
                },
                {
                    "start": 2184,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 2197,
                    "end": 2201,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2227,
                    "end": 2231,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427490234375
        },
        {
            "corpus_id": "276813020",
            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
            "text": "While enhancing robustness and generalization, they are limited to homogeneous model families. Xu et al. (2024c) introduced a strategy for merging models with differing depths and widths by aligning cohesive layer groups and applying elastic neuron zipping to project weights into a common space. While this approach allows for the merging of heterogeneous models, it encounters scalability challenges due to high computational complexity with large models. \n\nModel fusion methods seek to consolidate the capabilities of multiple source models into a single target model and can be categorized as either explicit or implicit. Explicit model fusion (EMF) techniques, such as FuseLLM (Wan et al., 2024a) and FuseChat (Wan et al., 2024b), transfer knowledge from multiple source LLMs to a target LLM through multi-teacher knowledge distillation. FuseChat implements a two-stage process: first, it performs pairwise fusion between each source model and a pivot model; next, it merges the resulting homologous models in parameter space. While EMF methods adapt to various architectures and model sizes, they face challenges like vocabulary alignment and merging distribution matrices. To address these issues, WRPO (Yang et al., 2025) introduces implicit model fusion (IMF), which uses source model responses as auxiliary signals during on-policy preference optimization. WRPO progressively shifts optimization from the target model's on-policy outputs to high-reward off-policy responses from the source models. \n\nPreference Alignment Aligning large language models (LLMs) with human preferences is crucial for their effectiveness. Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017;Ouyang et al., 2022) is a widely used approach for this purpose. However, RLHF relies on complex reinforcement learning techniques like proximal policy optimization (PPO) (Schulman et al., 2017), which can be challenging to implement and prone to instability during training. To address these challenges, Direct Preference Optimization (DPO) (Rafailov et al., 2023) offers a simplified alternative by directly optimizing the policy model based on a closed-form implicit reward function derived from human preference data.",
            "score": 0.4096941135701433,
            "section_title": "RELATED WORK",
            "char_start_offset": 8095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 734,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 1210,
                    "end": 1229,
                    "matchedPaperCorpusId": "274465199"
                },
                {
                    "start": 1678,
                    "end": 1703,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 1703,
                    "end": 1722,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415283203125
        },
        {
            "corpus_id": "276903216",
            "title": "Task Vector Quantization for Memory-Efficient Model Merging",
            "text": "We applied our quantization method against diverse merging strategies, from simple to advanced. Notably, we limited our investigation to approaches that leverage task vectors for the purpose of implementing our proposed quantization scheme. Furthermore, we reimplemented all merging methods and conducted extensive experiments to ensure a fair and comprehensive evaluation. Below, we briefly summarize the key insights of each approach: Individual fine-tunes the pretrained model separately for each task, which optimizes performance for the specific task. However, this approach cannot handle multiple tasks. \n\nTask Arithmetic [23] defines a task vector as the difference between a pretrained model and a fine-tuned model, then combines these vectors to form a multi-task model. Ties Merging [55] mitigates interference during merging by addressing redundant parameters and sign conflicts among task vectors. \n\nMagMax [34] merges task vectors by selecting, for each parameter, the one with the largest magnitude change. \n\nBreadcrumbs [12] applies layer-wise filtering to remove extreme weight changes, including both large outliers and negligible values, constructing a unified multi-task model. Consensus TA [48] retains general weights that are important across multiple tasks while removing selfish weights to reduce task interference. \n\nLiNeS [49] applies linear scaling to adjust layer-wise coefficients, which captures the relative importance of each task vector during the merging process. \n\nAdaMerging [58] employs an unsupervised approach at test time to optimize merge coefficients, rather than setting them empirically. EMR-Merging [20] constructs a unified model by electing a shared set of weights. It then applies task-specific binary masks and rescaling factors to adjust magnitudes.",
            "score": 0.4096902791948609,
            "section_title": "A.2. Merging method baseline",
            "char_start_offset": 27603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1799
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1035,
                    "end": 1039,
                    "matchedPaperCorpusId": "266174505"
                },
                {
                    "start": 1210,
                    "end": 1214,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1348,
                    "end": 1352,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1644,
                    "end": 1648,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432373046875
        },
        {
            "corpus_id": "277322544",
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "text": "Existing merging methods rely on sparsely estimated task vectors but face two key limitations: dependence on base model parameters and task vector interference. LoRE-Merging (Liu et al., 2025d), a low-rank estimation framework, is proposed to address such issues. It constructs an approximate base model and low-rank task vectors via optimization, minimizing discrepancies between fine-tuned and merged models. Using coordinate descent and singular value thresholding, LoRE-Merging reduces task vector interference, demonstrating the effectiveness of low-rank estimation in model merging. \n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments. In our study, we eliminate the router training and directly utilize its singular value decomposition (SVD) merging part. \n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
            "score": 0.409618580173466,
            "section_title": "LoRE-Merging",
            "char_start_offset": 12716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 747,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "This type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.",
            "score": 0.4095363344231575,
            "section_title": "Decomposing weight type",
            "char_start_offset": 19865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 772,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1255,
                    "end": 1275,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1762,
                    "end": 1780,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2126,
                    "end": 2148,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "271533631",
            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
            "text": "\"Mixtral of Experts\" by Jiang et al. [13] presents Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. The model dynamically selects two out of eight feedforward blocks per token at each layer, optimizing computational resource usage. Mixtral's transformer model incorporates MoE layers with a routing mechanism to allocate tokens to experts. Evaluations show superior performance in various benchmarks, highlighting the model's efficiency and effectiveness. \n\n\"Branch-Train-MiX\" by Sukhbaatar et al. [14] investigates methods for training LLMs across multiple specialized domains. The Branch-Train-MiX (BTX) method involves branching from a seed model, training domain-specific experts, and integrating them into a unified MoE model. This approach improves training efficiency and model performance by leveraging parallelism and specialization. BTX outperforms baselines like Llama-2 in accuracy and computational efficiency. \n\n\"Branch-Train-Merge\" by Li et al. [15] introduces the Branch-Train-Merge (BTM) algorithm, which enhances the efficiency of training large language models. BTM facilitates independent training of subparts of the model on different data subsets, reducing communication overhead. The approach involves three steps: branching, training, and merging. BTM achieves improved perplexities and higher updates per second due to reduced communication overhead, making it a scalable and efficient training paradigm. \n\nOur research integrates MoE with Knowledge Distillation (KD) to develop specialized multilingual models. Unlike Mixtral and BTX, which focus on token-level routing and parallel training of domain-specific experts, our work emphasizes sequence-level routing and the integration of KD with MoE. This approach aims to address multi-domain adaptability and reduce catastrophic forgetting, contributing to the development of modular and efficient language models.",
            "score": 0.40919233161605917,
            "section_title": "Mixture of Experts",
            "char_start_offset": 4092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1906
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82275390625
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Current retrieval techniques often struggle with accurately matching queries to knowledge sources, especially in highly specialized or ambiguous contexts. Additionally, knowledge-enhanced in-context reasoning presents a significant hurdle, as models must not only retrieve relevant knowledge but also effectively integrate and reason over it in a coherent and contextually appropriate manner. These challenges highlight the need for a deeper understanding and improved methodologies in the RAG space, making a comprehensive survey necessary to address both the current limitations and future opportunities in this rapidly evolving field. \n\nThe primary objective of this survey is to provide a comprehensive overview of RAG from a knowledge-centric perspective. By focusing on the integration of external knowledge sources into the generative process, this survey aims to review the recent advancements in RAG models, methods, and techniques. We will cover key models that have emerged, along with the fundamental approaches used in retrieval and generation, offering insights into how these methods address the unique challenges of knowledge selection, retrieval, and in-context reasoning. Additionally, we seek to highlight emerging trends and identify gaps in the existing research, particularly areas that require further exploration, such as multi-modal knowledge integration [16,25,195] and domain-specific applications [146,168,216,267]. This survey's unique contribution lies in its focus on RAG from a knowledge-centric viewpoint, providing a unified Fig. 1. A Framework for Organizing RAG Works. The timeline spans from 2020 to the present, categorizing RAG-related research into three main areas: Basic (including RAG Learning and RAG Framework), Advanced, and Evaluation. Key milestones in language models (GPT-3, ChatGPT, GPT-4) are marked along the timeline. \n\nfollowing the release of ChatGPT in late 2022, with numerous studies emerging to enhance large language models through retrieval-augmented approaches. These ongoing developments continue to shape the landscape of modern natural language processing.",
            "score": 0.4077181921979435,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1871
                },
                {
                    "start": 1874,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 1380,
                    "end": 1384,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 1384,
                    "end": 1387,
                    "matchedPaperCorpusId": "231879922"
                },
                {
                    "start": 1438,
                    "end": 1442,
                    "matchedPaperCorpusId": "272140261"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32275390625
        },
        {
            "corpus_id": "266163240",
            "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion",
            "text": "Pre-trained large models serve as foundational components in machine learning systems nowadays, playing a crucial role.They are adaptive to a wide range of downstream tasks through post fine-tuning, including image classification [He et al., 2016[He et al., , 2021]], natural language processing [Radford et al., 2019, Chung et al., 2022], and speech recognition [Gulati et al., 2020].These fine-tuned models are often specialized for different tasks, and it is desirable to merge them into a unified model that performs well across multiple tasks.\n\nMulti-task model fusion serves as a powerful and scalable methodology for extracting knowledge from individually fine-tuned models targeting different downstream tasks, enabling the construction of a unified multi-task model [Li et al., 2023, Zheng et al., 2023].This approach becomes particularly valuable when access to the underlying training data remains restricted and inaccessible, despite the availability of fine-tuned models [Wu et al., 2019, Tang et al., 2023a].In",
            "score": 0.4075700439532762,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 385
                },
                {
                    "start": 385,
                    "end": 548
                },
                {
                    "start": 550,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 1022
                },
                {
                    "start": 1022,
                    "end": 1024
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 246,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1000,
                    "end": 1021,
                    "matchedPaperCorpusId": "258841789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47265625
        },
        {
            "corpus_id": "273482155",
            "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
            "text": "Multi-task learning (MTL) leverages a single model to predict multiple related tasks, thereby reducing parameter costs and facilitating cross-task knowledge transfer [2,3,4,5]. MTL has been extensively applied across various domains, including computer vision [6,7,8,9,10,11], natural language processing [12,13], recommendation systems [14,15,16] and speech recognition [17,18]. However, the traditional MTL paradigm requires pre-collection of training data for all tasks and then collaboratively training an MTL model, which may lead to privacy leakage of valuable data and additional data management costs, particularly when dealing with large datasets or numerous tasks. \n\nRecently, a novel approach known as \"model merging\" or \"model fusion\" [19,20,21,22,23,24,25,26,27,28,29,30,31,32,33] has emerged within the machine learning community, demonstrating promise in addressing these challenges. The goal of model merging is to directly merge multiple expert models that have been trained individually on various tasks, to perform MTL without accessing their raw training data. In other words, model merging relies solely on the trained model parameters for each task, eliminating the need for centralized management of MTL training data and joint training. This innovative approach significantly broadens the potential applications of MTL. Despite these advancements ( \u00a7II), a notable performance gap persists between the most advanced model merging-based MTL methods [34,35,24,21,20,21,26] and traditional MTL or individual expert models. This gap motivates our investigation into the underlying issues of model merging and develops solutions to close this gap. \n\nIn this paper, we revisit several representative model merging schemes [34,36,21,26] from the perspective of the representation distribution, identifying a common challenge: \"representation bias\" ( \u00a7IV-A). Representation bias is defined as the gap between the representations extracted by the merged model and those extracted by the individual models. Recall that the primary goal of model merging is to create a unified model that retains the capabilities of all individual expert models.",
            "score": 0.4071569920667485,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2158
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 169,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 169,
                    "end": 171,
                    "matchedPaperCorpusId": "237571793"
                },
                {
                    "start": 171,
                    "end": 173,
                    "matchedPaperCorpusId": "740063"
                },
                {
                    "start": 173,
                    "end": 175,
                    "matchedPaperCorpusId": "51888887"
                },
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 265,
                    "end": 267,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 267,
                    "end": 269,
                    "matchedPaperCorpusId": "254043876"
                },
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 272,
                    "end": 275,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "2617020"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "3666937"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 341,
                    "end": 344,
                    "matchedPaperCorpusId": "221784966"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "239709636"
                },
                {
                    "start": 751,
                    "end": 754,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1485,
                    "end": 1488,
                    "matchedPaperCorpusId": "258832777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "276249702",
            "title": "Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources",
            "text": "Multi-source multi-hop question answering (QA) has emerged as a critical area in natural language processing (NLP), given its significance in knowledge-intensive tasks such as open-domain reasoning, knowledge retrieval, and decision support [1,2]. Unlike single-hop QA, which requires retrieving or reasoning over one piece of information, multi-hop QA involves connecting multiple reasoning steps, often over diverse sources of knowledge. This requirement makes it essential for systems to dynamically integrate information from heterogeneous sources such as structured databases, unstructured text, and even parametric knowledge stored in large language models (LLMs). Success in this domain not only advances the state of the art in QA but also contributes to the development of interpretable and robust AI systems capable of solving complex real-world problems [3,4]. \n\nHowever, multi-source multi-hop QA is inherently challenging due to three key factors: (1) Knowledge diversity and conflicts: Information from different sources often overlaps or contradicts, making it difficult for models to aggregate and reason over heterogeneous knowledge [5]. (2) Error propagation: In multi-hop settings, intermediate reasoning steps heavily influence the final answer, leading to cascading errors that amplify inaccuracies [6]. (3) Scalability: Existing methods typically rely on iterative retrieval or exhaustive reasoning over all possible paths, which is computationally expensive and impractical for realworld deployment. These challenges highlight the need for approaches that are not only accurate but also efficient and capable of resolving knowledge conflicts dynamically during reasoning. \n\nOur work is motivated by recent advancements in LLMs, which have demonstrated strong reasoning capabilities with techniques like chain-of-thought prompting [3]. While LLMs excel at leveraging implicit knowledge stored within their parameters, they are limited when faced with questions requiring external information retrieval, particularly in cases involving multi-hop reasoning. Inspired by these observations, we propose a novel framework, Adaptive Multi-source Knowledge-Oriented Reasoning (AMKOR), which combines the generalization ability of LLMs with a robust knowledge fusion mechanism. AMKOR is designed to dynamically aggregate multi-source knowledge, reason over intermediate steps effectively, and minimize cascading errors.",
            "score": 0.4070510554277248,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2291
                },
                {
                    "start": 2292,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "249712198"
                },
                {
                    "start": 244,
                    "end": 246,
                    "matchedPaperCorpusId": "254877030"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 868,
                    "end": 870,
                    "matchedPaperCorpusId": "236477422"
                },
                {
                    "start": 1155,
                    "end": 1158,
                    "matchedPaperCorpusId": "254877030"
                },
                {
                    "start": 1853,
                    "end": 1856,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.",
            "score": 0.4070309655753247,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "263605804",
            "title": "Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks",
            "text": "Before we get into the technical details of the pretraining and prefinetuning regime adopted in our work, we lay out some fundamental use-cases which primarily motivate the need to consolidate the pretraining phase of several tabular tasks (more details on these can be found in Section 1 of the Appendix). Pietruszka et al. 2022). In this paper, we evaluate our pretrained models on the first three use-cases with appropriate downstream tasks. The main contribution of our paper has been to revisit how individual language models had been trained for these use cases in silos and then design a unified pretraining stage so that a single model can transfer well to these tasks. As mentioned in the work done in UnifiedSKG (Xie et al. 2022), since these tasks leverage structured knowledge to complete user requests, the challenge lies in the heterogeneous nature of the inputs and the outputs. They have been studied separately by different communities. To handle that challenge, we follow the paradigm used in UnifiedSKG (which only contains a single/multi-task finetuning stage), albeit we add a pretraining stage and propose Unified Tabular Pretraining -UniTabPT. We follow the pretraining \u2192 multi-task prefinetuning \u2192 finetuning paradigm, however we run several ablation studies to show how our method generalizes better to downstream tasks than the paradigm in UnifiedSKG and previous task specific baselines.",
            "score": 0.40686239408469915,
            "section_title": "Unified Language Model Pretraining",
            "char_start_offset": 5872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1414
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 330,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33349609375
        },
        {
            "corpus_id": "277435010",
            "title": "AdaRank: Adaptive Rank Pruning for Enhanced Model Merging",
            "text": "Model Merging. Model merging seeks to integrate multiple independently trained models into a unified multi-task framework while preserving the performance of each individual component. Early approaches, such as Fisher Merging [33], which weights each model using the Fisher information matrix, and RegMean [22], which minimizes the L 2 norm discrepancy with respect to each model's parameters, were proposed as alternatives to simple weight averaging. More recently, Task Arithmetic (TA) [21] has emerged as a straightforward solution, defining task vectors as the difference between fine-tuned and pre-trained weights and merging them through simple arithmetic operations. However, TA suffers from cross-task interference, as task vectors are simply added together without addressing their potential interference, leading to suboptimal performance in multi-task scenarios. \n\nTask Arithmetic with Weight Sparsification. To address cross-task interference, several methods [19,55,60,63] proposed to sparsify task vectors by removing redundant components and preserving critical parameters in an element-wise manner. Notably, TIES-Merging [60] selects dominant parameters from task vectors based on their magnitude and constructs a sign vector reflecting the prevailing sign across models. Similarly, DARE [55] employs a Bernoulli distribution to randomly drop parameters and rescales the remaining ones to approximate the original task vector. Additionally, Consensus Merging [55] builds on prior methods by applying an additional fixed mask to task vectors, extracting task-specific information through the L 1 norm difference between the merged model and individual task vectors. Despite the efforts, these methods based on element-wise sparsification still exhibit noticable performance gap between fine-tuned models. \n\nTask Arithmetic in a Low-Rank Subspace. Instead, recent works [4,16,28,32] leverage Singular Value Decomposition (SVD) to address interference between task vectors. For example, CART [4] redefines task vectors as deviations from the average of fine-tuned weights rather than pre-trained weights, and showed applying low-rank approximation to these task vectors make merged model outperforms previous merging techniques without further modifications. Task Singular Vectors (TSV) [16] propose enforcing a low-rank structure on each task vector, followed by a whitening transformation to minimize interference among the truncated components.",
            "score": 0.40670003639270397,
            "section_title": "Related Work",
            "char_start_offset": 3810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1819
                },
                {
                    "start": 1822,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2271
                },
                {
                    "start": 2272,
                    "end": 2460
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 306,
                    "end": 310,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 488,
                    "end": 492,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 982,
                    "end": 985,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1137,
                    "end": 1141,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1475,
                    "end": 1479,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1893,
                    "end": 1896,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56787109375
        },
        {
            "corpus_id": "259064264",
            "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
            "text": "One possible solution for realizing a unified retriever is to leverage multiple single-source knowledge retrievers. When a query comes in, the QAP module first decomposes it into several smaller sub-queries, where each sub-query can be answered using one component knowledge retriever. The results from multiple knowledge retrievers can be integrated and then returned as the final output. However, several technical difficulties, including how to accurately decompose the question and how to join the retrieved results often hinder the success of this approach. Alternatively, unifying multiple sources of information in a standard representation, using text as a denominator representation, has been promoted recently (Oguz et al., 2022;Zeng et al., 2022). If all data items have a corresponding textual description, it is possible for the knowledge retriever to use only text-based retrieval techniques to find relevant data items once all input entities of non-textual modality have been mapped to their corresponding textual descriptions. \n\nSuch approach circumvents the complexity of managing multiple knowledge stores in different format. Moreover, with the success of large multilingual and multi-modal language models (Conneau and Lample, 2019; Aghajanyan et al., 2022), data of different structures or from different modalities can naturally share the same representation space. While unifying multiple sources of information through representation learning seems to be a promising direction, it should be noted that certain structured information may be lost in the process. For example, by flatting a knowledge graph to sequences of (subject, predicate, object) triples, the graph structure is then buried in the textual form. Whether the information loss limits the retriever's ability to handle certain highly relational queries remains to be seen. \n\n6 Provenance-aware answer generators",
            "score": 0.4063486399413986,
            "section_title": "Knowledge Retriever",
            "char_start_offset": 23746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 739,
                    "matchedPaperCorpusId": "235399987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281982421875
        },
        {
            "corpus_id": "259064039",
            "title": "TIES-Merging: Resolving Interference When Merging Models",
            "text": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
            "score": 0.4052816560860758,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "270257718",
            "title": "FusionBench: A Comprehensive Benchmark of Deep Model Fusion",
            "text": "In our benchmark, we have implemented 16 model fusion algorithms as the initial set.This includes 3 model ensemble methods, 8 model merging methods, and 5 model mixing methods.Our primary selection criterion for choosing among various algorithms is their applicability and effectiveness within the realm of deep learning architectures We have also considered the popularity of the algorithms in the literature and their practical applicability, such as their potential use in large-scale language models.We list the implemented algorithms in Table 1.EuroSAT [19], SVHN [40], GTSRB [47], MNIST [32], DTD [9] 8\u00d7CLIP-ViT-B/32 (Transformers [56]), 8\u00d7CLIP-ViT-B/32 (OpenCLIP [24]), 8\u00d7CLIP-ViT-B/16 (OpenCLIP [24]), 8\u00d7CLIP-ViT-L/14 (Transformers [56]), 8\u00d7CLIP-ViT-L/14 (OpenCLIP [24]) We use these distortions to assess the robustness and generalization capacity of the merged model as in [63,51].\n\nAs shown in Table 1, we implemented three kinds of model fusion algorithms.A brief introduction and formal definition of our taxonomy are provided in Section 2. Model ensemble methods are effective at enhancing the performance of a machine learning system, but they are computationally expensive to infer.Model merging methods aim to integrate the advantages of individual models, making them popular in multi-task model fusion and auxiliary learning.In these scenarios, multiple single-task models are merged to construct a multi-task model, or models focused on auxiliary tasks are combined to boost the performance of a primary task.Model mixing methods are frequently used to scale up a pre-trained model to a larger size or to combine multiple models into a new one.Consequently, model mixing methods often necessitate additional training after the fusion process.",
            "score": 0.40511905065082565,
            "section_title": "Implemented Algorithms",
            "char_start_offset": 8703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 891
                },
                {
                    "start": 893,
                    "end": 968
                },
                {
                    "start": 968,
                    "end": 1198
                },
                {
                    "start": 1198,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 558,
                    "end": 562,
                    "matchedPaperCorpusId": "53231412"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "14580063"
                },
                {
                    "start": 593,
                    "end": 597,
                    "matchedPaperCorpusId": "14542261"
                },
                {
                    "start": 603,
                    "end": 606,
                    "matchedPaperCorpusId": "4309276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27392578125
        },
        {
            "corpus_id": "277043475",
            "title": "Teamwork makes the dream work: LLMs-Based Agents for GitHub README.MD Summarization",
            "text": "Large Language Models (LLMs) have transformed how we approach different tasks such as natural language understanding, creative writing, and software engineering [19]. However, despite the individual strengths of LLMs, no single model can fully address the vast range of human language and problem domains. For example, while LLMs like GPT have excelled in natural language understanding, their performance on tasks requiring domain-specific expertise or complex multi-step reasoning remains limited [11]. A potential solution involves prompt-tuning, where prompts are iteratively refined to improve the performance of the pre-trained language model without modifying its internal design. Although this approach has shown promise, it faces challenges when the LLM is accessible only via an API. Furthermore, manual prompt engineering techniques, such as Chain-of-Thought (CoT) reasoning, require significant human effort to refine prompts iteratively. This labor-intensive process is prone to subjective bias and scalability issues, making it difficult to generalize across diverse tasks [29]. \n\nTo address the inherent limitations of single LLMs, researchers have proposed multi-agent systems that enable specialized LLMs to collaborate within a shared framework [26]. These systems capitalize on the unique strengths of different LLMs, where agents specialize in tasks such as code generation, debugging, or domain-specific problem-solving [11,31]. For instance, frameworks like TransAgent have demonstrated how task-specific agents can integrate seamlessly to tackle complex engineering challenges [10]. However, these systems are not without limitations. Challenges such as effective coordination, efficient communication, and the overhead of integrating multiple agents persist. Additionally, designing robust frameworks for agent interaction and feedback loops requires considerable engineering and computational resources. Despite these constraints, the collaborative potential of multi-agent systems offers an interesting alternative to relying on single model's capabilities [27]. \n\nTo illustrate a practical application of multi-agent LLM systems, we propose a novel framework targeting the problem of summarizing SE artifacts. This approach leverages the capabilities of specialized LLM agents to optimize task performance. The work supports a call for fundamentally new research directions with an initial evaluation on a real issue in SE, thus having the following contributions: \u22b2 Solution.",
            "score": 0.40509085613442297,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2088
                },
                {
                    "start": 2091,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2333
                },
                {
                    "start": 2334,
                    "end": 2503
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 165,
                    "matchedPaperCorpusId": "258366715"
                },
                {
                    "start": 499,
                    "end": 503,
                    "matchedPaperCorpusId": "274965784"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "274965784"
                },
                {
                    "start": 2083,
                    "end": 2087,
                    "matchedPaperCorpusId": "273471619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397216796875
        },
        {
            "corpus_id": "277509827",
            "title": "TeleMoM: Consensus-Driven Telecom Intelligence via Mixture of Models",
            "text": "Large language models (LLMs) face significant challenges in specialized domains like telecommunication (Telecom) due to technical complexity, specialized terminology, and rapidly evolving knowledge. Traditional methods, such as scaling model parameters or retraining on domain-specific corpora, are computationally expensive and yield diminishing returns, while existing approaches like retrieval-augmented generation, mixture of experts, and fine-tuning struggle with accuracy, efficiency, and coordination. To address this issue, we propose Telecom mixture of models (TeleMoM), a consensus-driven ensemble framework that integrates multiple LLMs for enhanced decision-making in Telecom. TeleMoM employs a two-stage process: proponent models generate justified responses, and an adjudicator finalizes decisions, supported by a quality-checking mechanism. This approach leverages strengths of diverse models to improve accuracy, reduce biases, and handle domain-specific complexities effectively. Evaluation results demonstrate that TeleMoM achieves a 9.7\\% increase in answer accuracy, highlighting its effectiveness in Telecom applications.",
            "score": 0.40480098845785756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "271769807",
            "title": "Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation",
            "text": "The recent advancements in large language models (LLMs) have been driving innovation across various fields like open-domain conversational models (Achiam et al., 2023;Touvron et al., 2023). LLMs demonstrate their capacity not just by solving com-plex computational problems in mathematics (Azerbayev et al., 2023) or programming (Roziere et al., 2023), but also by delivering expert-level performance in specialized knowledge areas (Singhal et al., 2023;Wu et al., 2023;Katz et al., 2024). \n\nResearches on open-domain chatbot focus on integrating personas to develop unique AI agents (Zheng et al., 2020). The efforts to make chatbots more human-like are not just for the purpose of obtaining knowledge and information, but to enhance the close interaction between humans and machines (Yin et al., 2023). Such efforts have achieved significant commercial applications, allowing users to craft custom AI agents with character-related information, enhancing user-AI interaction. However, it has been observed that relying solely on prompt design, without additional training, as seen in products like ChatGPT and Character.AI (Character.AI., 2022), presents challenges in displaying a consistent persona throughout dialogues (Wang et al., 2024). Furthermore, despite efforts to preserve style using character-related dialogue data, the necessity of assimilating new knowledge can lead to catastrophic forgetting (He et al., 2021), where the newly acquired information overshadows previously learned character traits (Liu and Mazumder, 2021). This indicates a need for a more robust approach to sustain both the acquisition of new knowledge and the preservation of unique character features in AI agents. \n\nThe emergence of model merging as a prominent area of interest is largely due to the challenges associated with supervised fine-tuning (SFT) and multi-task learning. For instance, while SFT is an effective method for optimizing language models for specific tasks (Dodge et al., 2020), it requires the storage and deployment of a separate model for each task. Using SFT would necessitate storing and managing distinct models per each task, increasing complexity and storage demands.",
            "score": 0.40425926838443244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "257572753"
                },
                {
                    "start": 584,
                    "end": 604,
                    "matchedPaperCorpusId": "207863734"
                },
                {
                    "start": 785,
                    "end": 803,
                    "matchedPaperCorpusId": "257279848"
                },
                {
                    "start": 1410,
                    "end": 1427,
                    "matchedPaperCorpusId": "233189563"
                },
                {
                    "start": 1514,
                    "end": 1538,
                    "matchedPaperCorpusId": "228095610"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30078125
        },
        {
            "corpus_id": "263671931",
            "title": "Procedural Text Mining with Large Language Models",
            "text": "In prior research, it's vital to examine the methods used for procedural text mining and the incorporation of Large Language Models (LLMs) for knowledge extraction. \n\nKnowledge Extraction from Unstructured Sources Extracting complex knowledge from unstructured sources presents several challenges in several domains. This variability complicates the accurate extraction and structuring of relevant information through knowledge extraction algorithms which are usually applied to specific domains [6]. The intricate nature of these documents requires manual review by domain experts after automated extraction, underscoring the limitations of machine-learning-based approaches [4]. Innovative methods, including interactive dialogues and language models, have emerged to address the lack of readily available training data for machine learning methods [3,10]. \n\nProcedural Text Mining and Large Language Models (LLMs) In response to the challenges mentioned earlier, our research delves into the field of procedural text mining, capitalizing on advancements in Natural Language Processing (NLP). Large Language Models (LLMs) have emerged as a pivotal tool in this endeavor, surpassing the capabilities of traditional symbolic AI and machine learning technologies [2,13]. These models offer a means to address the intricate and extensive nature of procedural documents, with the potential to enhance knowledge extraction efficiency. \n\nIntegration of LLMs in Knowledge Extraction Large Language Models (LLMs) demonstrate exceptional capabilities in natural language processing, surpassing what conventional symbolic AI and machine learning technologies can achieve [7]. These capabilities have sparked a substantial increase in proofs of concept and practical applications of LLMs, suggesting their potential utility in various knowledgerelated tasks [8]. Nevertheless, the exploration of methods for effectively integrating LLMs into structured, controllable, and repeatable approaches for the development and deployment of such applications in production is still in its early stages and requires further detailed consideration [9]. Similarly, our study centers on the integration of LLMs, notably the state-of-the-art GPT-4 model, in the context of extracting procedural knowledge from unstructured PDF documents.",
            "score": 0.4039760583375702,
            "section_title": "Related Work",
            "char_start_offset": 27776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 496,
                    "end": 499,
                    "matchedPaperCorpusId": "231985479"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "233313244"
                },
                {
                    "start": 851,
                    "end": 854,
                    "matchedPaperCorpusId": "247209181"
                },
                {
                    "start": 854,
                    "end": 857,
                    "matchedPaperCorpusId": "232307867"
                },
                {
                    "start": 1848,
                    "end": 1851,
                    "matchedPaperCorpusId": "240420063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.308837890625
        },
        {
            "corpus_id": "276938164",
            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
            "text": "Visual-Language Models (VLMs), by integrating visual and linguistic modalities, have become a core paradigm for multimodal understanding tasks. VLMs aim to achieve complex understanding tasks through the simultaneous processing of image and text data. The key advantage of VLMs lies in their ability to unify multimodal and multitask scenarios within a single framework. \n\nExisting VLM research mainly follows two paradigms: 1) constructing a unified multitask training framework [8,54], which achieves cross-task versatility through shared models and joint training, characterized by model simplicity and support for knowledge transfer between tasks; 2) utilizing task-specific expert models for feature fusion [31,69], which optimizes task performance through specialized design and flexible integration, characterized by strong taskspecificity and high modality complementarity. \n\nHowever, both approaches face the challenge of balancing task heterogeneity and model generality. Specifically, while the former emphasizes model generality through a unified framework, it tends to encounter representation conflict issues in scenarios where cross-task correlations are insufficient. For instance, studies [11] have shown that when handling spatial localization and semantic description tasks simultaneously, due to significant differences in task objectives, the model can easily confuse its concentration on vision features, leading to performance degradation. This phenomenon is particularly evident in complex multitask settings, limiting the model's generalization ability. The latter approach designs different expert models for heterogeneous tasks, which alleviates task conflicts to some extent. However, due to the lack of generality among models, the efficiency of expert collaboration is low. Empirical studies [4,72] indicate that multi-model architectures lead to parameter redundancy and knowledge-loss problems. \n\nTo address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms. \n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms.",
            "score": 0.4039059703586536,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 881
                },
                {
                    "start": 884,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2513
                }
            ],
            "ref_mentions": [
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "215754208"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "235692795"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "259243579",
            "title": "Knowledge-Infused Self Attention Transformers",
            "text": "Language modeling has witnessed significant advancements with the introduction of self-attention-based transformer architectures (e.g., GPT-3, ChatGPT, PaLM, etc.) [1,2]. These models have achieved remarkable success in a wide range of natural language processing tasks, demonstrating their ability to generate coherent and contextually relevant text. By utilizing self-attention mechanisms, transformers excel at capturing long-range dependencies and establishing meaningful relationships between words, enabling them to generate high-quality, context-aware text. However, despite their successes, self-attention-based transformer models have limitations when it comes to capturing all the necessary context solely from the available data. Language models often struggle with comprehending missing or implicit information, particularly in scenarios where the training data is incomplete or lack the desired context [3]. This limitation can lead to generated text that is plausible but semantically incorrect or inconsistent, diminishing the model's ability to fully understand and generate language with nuanced meaning [4]. To address these limitations, incorporating external knowledge into language models can provide the missing and implicit context required for accurate language generation. External knowledge, such as factual information, world knowledge, or domain-specific expertise, can supplement the training data by offering additional context that may not be explicitly present in the data alone. By integrating external knowledge, language models can enhance their understanding of complex concepts, disambiguate ambiguous statements, and generate more coherent and contextually accurate text. \n\nHowever, the existing methods used to incorporate external knowledge into language models often lack a systematic and welldefined approach. These methods seem rather ad hoc, as they introduce knowledge at various components of the transformer architecture based mainly on empirical justifications related to improved performance in downstream tasks. Transformers comprise several interconnected components, including input embedding matrices, encoder layers, and self-attention operations. One concern is that augmenting knowledge in an ad hoc manner may lead to the exploitation of statistical artifacts by the numerous moving parts of the transformer [5]. For instance, it could involve overfitting by utilizing additional parameters provided by the knowledge or fitting to task-specific hidden or spurious patterns to achieve high downstream performance scores.",
            "score": 0.40387982063444755,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2576
                }
            ],
            "ref_mentions": [
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "248779936"
                },
                {
                    "start": 1121,
                    "end": 1124,
                    "matchedPaperCorpusId": "208310123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36962890625
        },
        {
            "corpus_id": "271328836",
            "title": "Computer Audition: From Task-Specific Machine Learning to Foundation Models",
            "text": "Another way of unlocking new abilities in FMs, beyond instruction-finetuning, is that of merging.Merging FMs essentially involves the task of combining the weights of multiple models into a single, unified model.There are various techniques for achieving this, each aiming to leverage the strengths of individual models (either sharing the same architecture or not) to create a more robust and versatile system.This approach is particularly beneficial given the complexity and resource-intensity of training FMs.\n\nThe process of merging LLMs involves various techniques and has led to several state-of-the-art models [88].One fundamental approach is linear mode connectivity (LMC) [89], which combines models with identical architectures and initialisations through linear interpolation of weights.Linear averaging, as discussed by Utans [90] and expanded by Wortsman et al. [91] in the \"Model Soups\" approach, forms the basis of many merging techniques by leveraging the similarities in weight space.For models with identical architectures but different initialisations, the permutation symmetry of neural network checkpoints is crucial.Techniques like Git-Rebasin [88] align weights from independently trained models to achieve functionally equivalent configurations.This alignment facilitates effective merging despite differences in initial training paths.Similarly, approaches such as Tatro et al. [92] reduce the interpolation barrier by assigning correspondences between neurons in the models.Advanced methods also support merging models with different architectural configurations.For instance, the composition to augment language models (CALM) method [93] employs cross-attention mechanisms to integrate representations from diverse models, leveraging their combined strengths, whereas FUSELLM [4] aligns and fuses probabilistic distributions of source LLMs to enhance their generative capabilities; however, it requires additional pretraining.\n\nOverall, by merging off-the-shelf pretrained models specialised in different tasks, one can develop a single, comprehensive model capable of effectively handling a diverse range of tasks.This has been primarily explored for the case of LLMs, but is, in principle, also possible for audio FMs -that is, as an independent step before connecting audio and language models.",
            "score": 0.40308147011487794,
            "section_title": "Merging unimodal foundation models",
            "char_start_offset": 66083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 512
                },
                {
                    "start": 514,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1269
                },
                {
                    "start": 1269,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1500
                },
                {
                    "start": 1500,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1953
                },
                {
                    "start": 1955,
                    "end": 2142
                },
                {
                    "start": 2142,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "221516809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400634765625
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "Merging Models with Sense-Merging. We first evaluate the effectiveness of our Sens-Merging method by utilizing it as a plug-and-play module to enhance existing task-vector-based baselines. Table 1 presents the performance of the baseline methods alongside their Sens-Merging enhanced counterparts across seven datasets. Specifically, when merging fine-tuned models specialized in general knowledge (Chat1 ), mathematical reasoning (Math2 ), and code generation (Code3 ), all derived from LLaMA2-7B4 , Sens-Merging demonstrates a consistent improvement in the average performance across all domains. Specifically, when comparing the average scores of each method with and without Sens-Merging, we find that: \n\n(1) Superior Improvement in Task Arithmetic: Task Arithmetic exhibits a particularly notable increase from an average score of 29.03 without Sens-Merging to 34.78 with Sens-Merging, achieving a 19.22% relative improvement of 5.58 points. As both Ties-Merging and DARE have implemented drop strategies to mitigate parameter interference, the integration of scaling coefficient adjustments through Sens-Merging does not achieve as substantial an enhancement as seen with Task Arithmetic. Nevertheless, Sens-Merging still contributes to performance improvements in these methods, with Ties-Merging increasing from an average score  (47.69) and MATH (7.80), surpassing their respective baselines. In code generation, Task Arithmetic shows substantial improvements, increasing from 13.5 to 33.1 on MBPP and from 7.3 to 18.9 on HumanEval. \n\n(3) Enhanced Performance than Individual Fine-tuned Models: Sens-Merging enables the combined models to achieve higher performance on general knowledge and code generation tasks, even surpassing the original code fine-tuned model. For example, when integrating the Chat, Math, and Code models using Sens-Merging, performance on the MBPP and HumanEval datasets increases significantly. Specifically, accuracy improves from 26.8 to 32.3 on the MBPP dataset and from 12.8 to 19.5 on the HumanEval dataset. This demonstrates that model merging can overcome the challenges associated with training a single model for complex tasks by effectively integrating capabilities from other specialized fine-tuned models.",
            "score": 0.4025835981450489,
            "section_title": "Main Results",
            "char_start_offset": 16128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416259765625
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "While MoD demonstrates superior performance compared to existing methods, we acknowledge some limitations in our current study. First, our experimental validation is primarily confined to the mathematical domain, which, while comprehensive, may not fully represent the method's generalizability across other specialized fields. Second, our current approach employs a simplified strategy for determining mixture weights, which may not capture optimal combinations for all scenarios. \n\nThese limitations suggest several promising directions for future research. First, extending the evaluation of MoD to diverse domains beyond mathematics would provide valuable insights into the method's robustness and general applicability. Second, developing more sophisticated approaches for determining optimal mixture weights could potentially enhance the method's performance further. Additionally, investigating the theoretical foundations of distribution-based merging approaches could lead to more principled strategies for model combination and integration. These directions would contribute to a deeper understanding of model merging techniques and their applications in developing more capable language models.",
            "score": 0.4024982055363106,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 15911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40869140625
        },
        {
            "corpus_id": "258048409",
            "title": "Learnings from Data Integration for Augmented Language Models",
            "text": "One of the limitations of large language models is that they do not have access to up-to-date, proprietary or personal data. As a result, there are multiple efforts to extend language models with techniques for accessing external data. In that sense, LLMs share the vision of data integration systems whose goal is to provide seamless access to a large collection of heterogeneous data sources. While the details and the techniques of LLMs differ greatly from those of data integration, this paper shows that some of the lessons learned from research on data integration can elucidate the research path we are conducting today on language models.",
            "score": 0.4024982055363106,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36572265625
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "Model Merging. Recent research has demonstrated success in developing innovative strategies to harness the collective power of multiple LLMs by suggesting methods for combining their unique strengths. This approach offers an efficient solution and has been widely explored for fine-tuned models sharing the same pre-trained base model, thereby sharing a part of their optimization trajectories [Frankle et al., 2020;Izmailov et al., 2019;Ilharco et al., 2023;Wortsman et al., 2022]. Initial efforts focused on merging models with simple weighted averaging of the parameters [Wortsman et al., 2022;Matena & Raffel, 2022;Gupta et al., 2020] and showed dramatic performance gains for the resultant merged model. More recently, many works have investigated non-linear methods of merging models [White, 2016;Yadav et al., 2023;Yu et al., 2024] while aiming to improve general downstream performance. However, some recent works have focused on ensuring the safety of LLMs when merging, having demonstrated that misalignment transfers trivially from the base to the combined model in this process [Hammoud et al., 2024]. Other works \"realign\" language models by fusing an initial aligned model with many task vectors based on the suitably identified safety subspace [Yi et al., 2024]. Model merging has also been extended to a multilingual setting -for developing task-solving LLMs for low-resource languages without the availability of SFT data in the target languages [Tao et al., 2024]. Our work distinguishes itself from prior approaches due to the complexity of the contrasting targets it seeks to satisfy -balancing safety and general-purpose objectives across a wide set of languages. To the best of our knowledge, no prior work has investigated the alignment of LLMs via model merging in a multilingual context while optimizing for a two-fold objective. \n\nMultilingual Safety. With the increased pervasiveness of LLMs in recent times, the landscape of language model research has evolved with a heightened emphasis on safeguarding user experiences, thereby placing an increased focus on mitigating potential risks across diverse linguistic contexts.",
            "score": 0.4024982055363106,
            "section_title": "Related Work",
            "char_start_offset": 24420,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1854
                },
                {
                    "start": 1857,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "268537290",
            "title": "Evolutionary Optimization of Model Merging Recipes",
            "text": "Model merging [15,28], a recent development in the large language model (LLM) community, presents a novel paradigm shift.By strategically combining multiple LLMs into a single architecture, this exciting development has captured the attention of researchers due to its key advantage: it requires no additional training, making it an incredibly cost-effective approach for developing new models.This accessibility has fueled a surge in interest and experimentation with model merging.The Open LLM Leaderboard [20] is now dominated by merged models, showcasing its potential for democratizing foundation model development.\n\nHowever, model merging is considered by many to be a form of black art or alchemy, relying on the model maker's intuition and instincts about model selection and merging recipes to create and refine a new model that performs well for a particular task.Furthermore, the model maker is often required to have some domain knowledge for the various different benchmark tasks.Given the large diversity of open models and benchmarks in the community, human intuition can only go so far, and we believe a more systematic approach for discovering new model combinations will take things much further.\n\nWe believe evolutionary algorithms will be able to discover more effective model merging solutions, and thus provide a path for automating the creation of more capable models.As a step towards this direction, in this work, we show that evolution can be employed to discover novel and unintuitive ways to merge various models to produce new models with a new combined ability.In this work, we present a methodology that leverages evolutionary algorithms to facilitate the merging of foundation models.Our approach is distinguished by its ability to navigate both parameter space (weights) and the data flow space (inference path), proposing a framework that integrates these two dimensions.This work makes several key contributions to the field of foundation model development:\n\n1. Automated Model Composition: We introduce Evolutionary Model Merge, a general evolutionary method to automatically discover optimal combinations of diverse open-source models for creating new foundation models with user-specified capabilities.This approach harnesses the collective intelligence of existing open models, enabling the creation of powerful models without the need for extensive training data or compute.",
            "score": 0.40213259230385545,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 620
                },
                {
                    "start": 622,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1214
                },
                {
                    "start": 1216,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 1992
                },
                {
                    "start": 1994,
                    "end": 2240
                },
                {
                    "start": 2240,
                    "end": 2414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "Merging typically aims for efficiency by creating a single, cohesive model (Singh and Jaggi, 2020), whereas fusion often combines multiple models to enhance quality, potentially at the expense of speed (Ravaut et al., 2022b;Jiang et al., 2023b). \n\nWithin the merging domain, weighted-average techniques-refined by methods like Hessianbased estimates (Daheim et al., 2023) or pruningenhanced Fisher weights (Nathan et al., 2024)-adjust parameter significance but may fail to capture task-specific subtleties, resulting in performance degradation (e.g., a reported 10% drop with basic averaging (Ilharco et al., 2023b)). To counter this, the notion of task vectors, defined as \u03c4 t = \u03b8 ft t \u2212\u03b8 pre (Ilharco et al., 2023b), has gained prominence. These vectors encapsulate task-specific shifts in parameter space, facilitating precise conflict resolution during merging. Building on this, methods like Task Arithmetic (Ilharco et al., 2023b), AdaMerging (Yang et al., 2023), and TIES-Merging (Yadav et al., 2023b) address redundancy and sign discrepancies, improving cross-model compatibility. \n\nResolving parameter conflicts remains a core challenge, inspiring a variety of innovative strategies. Task Arithmetic (Ilharco et al., 2023b) introduced arithmetic-based vector merging, while TIES-Merging (Yadav et al., 2023b) and AdaMerging enhance this by targeting interference sources. Evolutionary methods (Akiba et al., 2024b) blend TIES with optimized inference routes, and practical applications like MetaGPT (Zhou et al., 2024) and LLM evaluators (Kim et al., 2024) showcase realworld efficacy. Alternatively, ZipIt (Stoica et al., 2024) retains correlated parameters while preserving distinct layers, offering adaptability. Additional advancements, such as geometric weight analysis (Shoemake, 1985;Jang et al., 2024) and safety alignment (Hammoud et al., 2024), further enrich the field.",
            "score": 0.4015974781436705,
            "section_title": "C.5 LLM Model Fusion",
            "char_start_offset": 44495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 244,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 593,
                    "end": 616,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 695,
                    "end": 718,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 914,
                    "end": 937,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 988,
                    "end": 1009,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1210,
                    "end": 1233,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1297,
                    "end": 1318,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "272910894",
            "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
            "text": "To mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs et al., 1991] into MLLMs. Figure 1 illustrates three distinct hypotheses and their corresponding architectural implementations for multi-task learning in MLLMs. The first \"synergy hypothesis\" suggests that all tasks benefit from a fully shared backbone comprising a visual encoder, connector, and language model, which is the standard architecture for MLLMs. The second \"conflict hypothesis\", proposes that each task requires its own specific adaptations, thereby preventing knowledge sharing among tasks. The third \"conflict-synergy coexistence hypothesis\", posits that all tasks share multi-task adaptations, which reduces interference and promotes more efficient knowledge sharing. However, current research [Zadouri et al., 2023;Gou et al., 2023;Liu et al., 2023b;Lin et al., 2024] mainly tailors the MoE approach to the language model components, overlooking the potential benefits of exploring and enhancing the connector in MLLMs. Furthermore, the optimization of the tug-of-war problem lacks a detailed, explainable analysis. \n\nIn this study, we first identify a tug-of-war problem in multi-task learning at the connector level within standard MLLM architectures. This issue indicates that different tasks may emphasize different types of features in multi-modal, multi-task scenarios. Consequently, a fully shared connector may fall short as it cannot accommodate the diverse modal features required by each task. Drawing inspiration from the successful application of MoE in LLMs, we introduce Connector-MoE (CMoE), a novel approach that employs a mixture of projection experts to align visual and language embedding spaces effectively, thus mitigating the tug-of-war problem. As a pioneering effort in constructing a unified generalist foundation model in the medical field, we present Uni-Med. This model comprises a universal visual feature extraction module, a CMoE module, and an LMM. Uni-Med demonstrates impressive performance across six distinct medical tasks, with minimal training computational overhead. It achieves joint training on 12 datasets on a single A800 in under 10 hours.",
            "score": 0.4008551524195015,
            "section_title": "Introduction",
            "char_start_offset": 1804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 146,
                    "matchedPaperCorpusId": "572361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "276409294",
            "title": "Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey",
            "text": "Large Language Models (LLMs) have achieved extraordinary success across various tasks, showcasing remarkable capabilities in reasoning, knowledge representation, and decisionmaking. However, despite their impressive performance in general-purpose applications, many specialized domains, such as healthcare, chemistry, and legal analysis, demand the integration of domain-specific knowledge to achieve high accuracy and reliability. To address this challenge, researchers have explored methods to enhance LLMs through external or embedded domain expertise, a process often referred to as knowledge injection. This approach aims to bridge the gap between general-purpose language understanding and the stringent requirements of domain-specific tasks, enabling LLMs to perform effectively in highly specialized contexts. \n\nBuilding on the foundational capabilities of generalpurpose LLMs, knowledge injection techniques provide an effective means to address their limitations in handling specialized applications. Compared to the generalized approach of standard LLMs, knowledge injection offers two key advantages: 1) incorporating precise, domain-specific knowledge to improve accuracy and reliability in specialized tasks, and 2) allowing LLMs to dynamically adapt to new information or evolving knowledge bases, ensuring up-to-date expertise. These techniques bridge the gap between general-purpose understanding and domain-specific demands by leveraging both structured and unstructured knowledge sources. As a result, knowledge injection methods have been successfully applied in fields such as healthcare, chemistry, and legal analysis, significantly enhancing LLM performance. For example, biomedical LLMs [Bolton et al., 2024;Yan et al., 2023] have demonstrated superior accuracy in tasks like medical diagnostics and regulatory compliance, while domain-specific models for material science [Xie et al., 2024;Antunes et al., 2024;Zhang et al., 2024a] have achieved advances in material property prediction and discovery. These dedicated models underscore the transformative potential of integrating domain knowledge into LLMs, unlocking solutions to complex, field-specific challenges. \n\nDespite these advancements, early efforts in knowledge injection often treated domains independently, leading to a lack of standardization in methodologies and evaluation. As the volume of research continues to grow rapidly, with applications and studies proliferating across disciplines, the need for a comprehensive review becomes evident.",
            "score": 0.4006059947369352,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2365
                },
                {
                    "start": 2366,
                    "end": 2535
                }
            ],
            "ref_mentions": [
                {
                    "start": 1897,
                    "end": 1915,
                    "matchedPaperCorpusId": "259129602"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "While the Hugging Face model hub [68] hosts a large number of finetuned models, many of them are not well-suited for systematic evaluation of model merging techniques due to three key challenges. \n\nVariability in model quality. The models on the hub span a wide spectrum in terms of performance, training methodology and documentation. Verifying their quality, especially in a scalable and automated manner, is nontrivial. Selecting a diverse yet reliable set of models suitable for merging requires substantial manual effort and quality control. Moreover, existing models are often finetuned from earlier generations of base models (e.g., Llama-2 [64]), whereas more recent releases offer stronger pretrained foundations and are of greater interest in modern applications. \n\nLack of coverage and skill disentanglement. Although it is relatively easy to find models specialized in domains like math or code generation, there is a notable scarcity of well-performing, openly available models in other domains such as multilinguality. Furthermore, many available models are broadly multi-task, making it hard to assess how individual capabilities interact when merged. In contrast, merging highly specialized models allows us to better isolate and analyze phenomena such as skill interference and synergy, providing a more faithful evaluation of merging performance. \n\nIncompatibility between models. Even when models share the same pretrained backbone, they may not be mergeable in practice, due to differences in tokenization and model architecture variants. For example, merging CodeLlama [55] and Llama-2-Chat [64] has been shown to cause significant degradation in performance [83], despite both being derived from Llama-2. \n\nTo address these issues, we build a controlled suite of specialized models from Llama-3.2-3B, Llama-3.1-8B [12], Gemma-2-2B and Gemma-2-9B [62], as well as their instruction-tuned versions, as our base models, and finetune on task-specific datasets across five diverse categories.",
            "score": 0.4002863861807221,
            "section_title": "Model Construction",
            "char_start_offset": 9708,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 2009
                }
            ],
            "ref_mentions": [
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "273228210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380859375
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "Language diversity poses a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition (ASR) (Prabhavalkar et al., 2023) and speech translation (ST) (Xu et al., 2023). With over 7,000 languages spoken worldwide, developing robust S2T systems that generalise across varied linguistic structures remains a fundamental research goal (Liu and Niehues, 2024;Cheng et al., 2023;Sun et al., 2023;Saif et al., 2024;Wang et al., 2021;Le et al., 2021). The advent of end-to-end (E2E) models (Chan et al., 2016;Gulati et al., 2020;Barrault et al., 2023) has marked a paradigm shift in S2T tasks, enabling direct mapping from speech to text across multiple languages within a unified framework. A prominent example is Whisper (Radford et al., 2023), an advanced multi-lingual speech model trained on a large-scale, diverse dataset covering multiple languages and tasks. Despite these advances, existing multi-lingual models still encounter significant challenges in scalability, efficiency, and performance trade-offs. \n\nTo address these challenges, multi-lingual training strategies (Saif et al., 2024;Xiao et al., 2021;Bai et al., 2018) have been adopted, aiming to enhance model generalisation across languages. These approaches typically rely on joint optimisation of diverse S2T tasks across multiple languages, leveraging shared representations to improve performance. Nevertheless, multi-lingual training is subject to inherent limitations, including substantial training costs, complex model configurations, and limited access to training data across multiple languages and tasks. Moreover, when handling new languages, the training methods typically require training from scratch. \n\nTo mitigate these issues, this paper proposes to use model merging (Ilharco et al., 2023;Yang et al., 2024a;Khan et al., 2024) to integrate models trained on different languages or tasks while maintaining performance and reducing computational overhead. Model merging merges the parameters of multiple separate models with different capabilities to build a universal model.",
            "score": 0.40025443147066847,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 150,
                    "matchedPaperCorpusId": "257365554"
                },
                {
                    "start": 179,
                    "end": 196,
                    "matchedPaperCorpusId": "259203049"
                },
                {
                    "start": 360,
                    "end": 383,
                    "matchedPaperCorpusId": "271769726"
                },
                {
                    "start": 383,
                    "end": 402,
                    "matchedPaperCorpusId": "254854634"
                },
                {
                    "start": 402,
                    "end": 419,
                    "matchedPaperCorpusId": "264820244"
                },
                {
                    "start": 437,
                    "end": 455,
                    "matchedPaperCorpusId": "230433640"
                },
                {
                    "start": 455,
                    "end": 471,
                    "matchedPaperCorpusId": "235313889"
                },
                {
                    "start": 511,
                    "end": 530,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 744,
                    "end": 766,
                    "matchedPaperCorpusId": "252923993"
                },
                {
                    "start": 1121,
                    "end": 1139,
                    "matchedPaperCorpusId": "229349388"
                },
                {
                    "start": 1139,
                    "end": 1156,
                    "matchedPaperCorpusId": "52009128"
                },
                {
                    "start": 1777,
                    "end": 1799,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.448974609375
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "In recent years, large language models (LLMs), such as GPT-4 [2], Gemini [184], PaLM [25] and LLaMA [188], have made significant advancements and have been widely applied across various tasks. Despite their superhuman performance on most basic tasks, LLMs still face numerous challenges, including producing toxic content that violates laws or ethics, using unauthorized data during training, high training costs, and insufficient performance in specific domains. Model merging technology presents a promising opportunity to address these challenges.",
            "score": 0.39857759446045876,
            "section_title": "Model Merging in Large Language Models (LLMs)",
            "char_start_offset": 31575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 550
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 89,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31201171875
        },
        {
            "corpus_id": "270257718",
            "title": "FusionBench: A Comprehensive Benchmark of Deep Model Fusion",
            "text": "The mixed model can be expressed as F (\u2022; \u0398), which often has more parameters than the original models, and thus can be more expressive and powerful to capture the underlying patterns in the data.Model mixing methods can be implemented through layer recombinations [23,27], model stitching [33,39], or upscale to create a Mixture of Experts (MoE)-based sparse model [30,64,51].\n\nAlthough several model fusion methods have been proposed, benchmarks and unified toolkits are still lacking in this field.A recent notable work, MergeKit [14], provides a collection of model fusion techniques specifically designed for merging large language models (LLMs), with a focus on model merging methods and Transformer-based LLMs.However, MergeKit's scope is limited to a specific domain and model architecture.In contrast, FusionBench aims to be a comprehensive benchmark that encompasses a wide range of tasks and model types.It includes a diverse set of fine-tuned models and tasks to evaluate, making it a more generalized and versatile platform for assessing the performance of different model fusion approaches across various domains and architectures.",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 3968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 196,
                    "end": 377
                },
                {
                    "start": 379,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 717
                },
                {
                    "start": 717,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "271720178",
            "title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement",
            "text": "We investigate the efficacy of seven commonly used merging techniques when integrating the abilities of PT LLMs. To be specific, Average Merging (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2023) are arithmetic-based methods. SLERP (Shoemake, 1985) and Model Stock (Jang et al., 2024) belong to geometric-based approaches. TIES-Merging (Yadav et al., 2023), Breadcrumbs (Davari & Belilovsky, 2023) and DARE (Yu et al., 2024) are pruning-based solutions. \n\nPlease see Section A.3 for detailed descriptions of these methods. To evaluate the performance, we attempt to combine the instruction-following skills of an FT LLM, Qwen1.5-Chat (Bai et al., 2023), and the multilingual abilities of a PT LLM, Sailor (Dou et al., 2024). Experimental setup, results, and analysis can be found in Section 4. \n\nSince this part mainly concentrates on the feasibility of merging techniques when applied to PT LLMs, we highlight the key conclusion pertinent to PT LLMs: existing merging approaches face difficulties in preserving the abilities of PT LLMs. As evidenced in Table 2, the performance of all merging methods on the multilingual abilities significantly declines. This phenomenon is largely attributed to the reliance of most methods on manually assigned scaling factors to determine the contribution of each model at various levels throughout the merging process, encompassing model level (Ilharco et al., 2023;Yadav et al., 2023;Davari & Belilovsky, 2023), layer/module level (Goddard et al., 2024), and parameter level (Shoemake, 1985). The diverse parameter changed ranges between FT and PT LLMs complicate the manual assignment of model importance, making it intractable to define optimal scaling factors case by case.",
            "score": 0.39857759446045876,
            "section_title": "EXPLORING EFFICACY OF CURRENT METHODS WHEN MERGING PT LLMS",
            "char_start_offset": 11048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 168,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 189,
                    "end": 211,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 248,
                    "end": 264,
                    "matchedPaperCorpusId": "259308873"
                },
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 423,
                    "end": 440,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1398,
                    "end": 1420,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1420,
                    "end": 1439,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1530,
                    "end": 1546,
                    "matchedPaperCorpusId": "259308873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2607421875
        },
        {
            "corpus_id": "246016124",
            "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
            "text": "Structured knowledge (e.g., web tables, knowledge graphs, and databases) stores large amounts of data in organized structures, forming a basis for a wide range of applications, e.g., medical diagnosis, personal assistants, and customer relations manage-ment. Accessing and searching data in structured knowledge typically requires mastering query languages through professional training. To promote the efficiency of data access, structured knowledge grounding (SKG) systems ground user requests in structured knowledge and produce various outputs, including computer programs (e.g., SQL and SPARQL), table cell values, and natural language responses (Figure 1). For example, semantic parsing (Zelle and Mooney, 1996;Zettlemoyer and Collins, 2005) converts natural language questions into formal programs; knowledge-base question answering (Berant et al., 2013) derives answers from tables or knowledge graphs. \n\nSKG has attracted significant interest and has been studied through different tasks defined by different communities. Recent developments in tasks, models, and datasets for SKG have led to taskspecific modeling advances, making each task's progress seemingly unique and incompatible. A main reason is that SKG tasks are heterogeneous. Different types of structured knowledge, such as databases or knowledge graphs, lead to highly specialized encoders (Lin et al., 2019;Herzig et al., 2020;Wang et al., 2020;Yasunaga et al., 2021). Some SKG tasks, e.g., semantic parsing, use customized decoders to generate programs (Yin and Neubig, 2018;Ren et al., 2021). Therefore, instead of solving common challenges in SKG research, improvements in SKG have been prone to be exclusive to a single task, domain, or dataset. \n\nIn this paper, we propose the UNIFIEDSKG framework to advocate for a unifying view of 21 SKG tasks across six task families and multiple data domains (Table 1). UNIFIEDSKG standardizes datasets, models, code, experiments, and evaluation metrics into a single framework. By casting user requests, structured knowledge, and outputs 602 Structured Knowledge Grounding knowledge graphs web tables/pages databases/apps",
            "score": 0.39843897092612623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1382,
                    "end": 1402,
                    "matchedPaperCorpusId": "214802901"
                },
                {
                    "start": 1402,
                    "end": 1420,
                    "matchedPaperCorpusId": "207863446"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "265466401",
            "title": "Agents meet OKR: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation",
            "text": "In this study, we introduce the concept of OKR-Agent designed to enhance the capabilities of Large Language Models (LLMs) in task-solving. Our approach utilizes both self-collaboration and self-correction mechanism, facilitated by hierarchical agents, to address the inherent complexities in task-solving. Our key observations are two-fold: first, effective task-solving demands in-depth domain knowledge and intricate reasoning, for which deploying specialized agents for individual sub-tasks can markedly enhance LLM performance. Second, task-solving intrinsically adheres to a hierarchical execution structure, comprising both high-level strategic planning and detailed task execution. Towards this end, our OKR-Agent paradigm aligns closely with this hierarchical structure, promising enhanced efficacy and adaptability across a range of scenarios. Specifically, our framework includes two novel modules: hierarchical Objects and Key Results generation and multi-level evaluation, each contributing to more efficient and robust task-solving. In practical, hierarchical OKR generation decomposes Objects into multiple sub-Objects and assigns new agents based on key results and agent responsibilities. These agents subsequently elaborate on their designated tasks and may further decompose them as necessary. Such generation operates recursively and hierarchically, culminating in a comprehensive set of detailed solutions. The multi-level evaluation module of OKR-Agent refines solution by leveraging feedback from all associated agents, optimizing each step of the process. This ensures solution is accurate, practical, and effectively address intricate task requirements, enhancing the overall reliability and quality of the outcome. Experimental results also show our method outperforms the previous methods on several tasks. Code and demo are available at https://okr-agent.github.io/",
            "score": 0.39780874327107524,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200927734375
        },
        {
            "corpus_id": "256312594",
            "title": "K-CTIAA: Automatic Analysis of Cyber Threat Intelligence Based on a Knowledge Graph",
            "text": "Pre-trained language representation models such as BERT [27] can obtain common language representations from large corpora. The BERT model has achieved good performance in various tasks of natural language processing. At the same time, some researchers are applying BERT to professional fields, such as military and agriculture [28,29]. \n\nHowever, due to the lack of specialized knowledge in the general corpus, the model has a poor understanding of the semantic information of proper nouns that rarely appear in the general corpus. To address this issue, some researchers combine knowledge graphs with pre-trained models to annotate these difficult terms. As a result, the pre-trained model can obtain the semantics of these terms more accurately. K-BERT [30] proposes a knowledge-supported language representation model, which is compatible with BERT and can combine related knowledge to solve the situation where the pre-trained model does not perform well in a specific domain. K-BERT inserts knowledge into the input in the form of branches and changes the model structure to solve the knowledge noise problem. K-BERT significantly outperforms BERT not only on domain-specific tasks, but also on open-domain tasks. Some scholars also use other methods to integrate knowledge maps into pre-trained models. These methods can be divided into three types: implicit fusion, explicit fusion, and joint learning of a knowledge graph and a pre-trained model [31].",
            "score": 0.397694910661417,
            "section_title": "Pre-Trained Model Combined with a Knowledge Graph",
            "char_start_offset": 8538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "202583325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2890625
        },
        {
            "corpus_id": "261875641",
            "title": "\"Merge Conflicts!\" Exploring the Impacts of External Distractors to Parametric Knowledge Graphs",
            "text": "Internal and External Knowledge Conflicts. LLMs amass internal knowledge through extensive learning on massive corpora during pretraining (Roberts et al., 2020;Jiang et al., 2020;Gururangan et al., 2020), thereby weaving a unique system of parametric knowledge. This process, however, can be marred by inaccurate or outdated training data, leading to potential hallucinations within the model (Carlini et al., 2021;Lazaridou et al., 2021;Zhang et al., 2021). To align LLMs with current information and enhance factual accuracy, researchers have employed various tools (Schick et al., 2023;Qin et al., 2023), memory techniques (Zhong et al., 2022), and information retrieval strategies (Guu et al., 2020;Izacard and Grave, 2021). However, such external knowledge may be novel or even contradict the model's existing parametric knowledge, causing interference. Neeman et al. (2022) trained the model to disentangle internal and external knowledge and generate two responses to avoid conflict. Zhou et al. (2023) utilized special prompt engineering and abstention options to improve model faithfulness. More recently, Xie et al. (2023) explored how the GPT model family reacts to knowledge conflicts, uncovering a high receptivity to external knowledge and confirmation bias. In line with these studies, we broaden our focus to encompass both black-box and open-source models, adopting a more systematic perspective on multiple types of distractors and parametric knowledge structures. \n\nPropagation of Introduced Knowledge. Prior approaches to model editing have primarily centered on the modification of parameters (Meng et al., 2022;Yao et al., 2022) or the integration of specialized modules (Wang et al., 2021;Mitchell et al., 2021) to enable the model to assimilate new knowledge. Nevertheless, this newly introduced external knowledge is anticipated to exert long-lasting effects. Onoe et al. ( 2023) have found that traditional editing methods exhibit inconsistencies when paraphrasing questions in new contexts, and that prepending entity definitions can facilitate the propagation of the injected external knowledge.",
            "score": 0.39765695716696947,
            "section_title": "Related Work",
            "char_start_offset": 5054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 160,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 626,
                    "end": 646,
                    "matchedPaperCorpusId": "249062699"
                },
                {
                    "start": 1633,
                    "end": 1650,
                    "matchedPaperCorpusId": "246015673"
                },
                {
                    "start": 1693,
                    "end": 1712,
                    "matchedPaperCorpusId": "211031933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166259765625
        },
        {
            "corpus_id": "277633890",
            "title": "FedMerge: Federated Personalization via Model Merging",
            "text": "Model Merging Model merging refers to methods aimed at reusing the knowledge contained in multiple models. The fundamental technique of model merging is to integrate existing models to form new ones. There have been a number of works focusing on merging methodologies, such as simple averaging (Wortsman et al., 2022), merging with Fisher Information (Matena & Raffel, 2022), and merging with task vectors (Ilharco et al., 2022). Recently, model merging has become closely related to the development of foundation models (Sukhbaatar et al., 2024;Lu et al., 2023;2024;Ostapenko et al., 2024a), as such developments have brought numerous reusable expert foundation models online (Face, 2023). For example, LoraHub (Huang et al., 2023) trains one LoRA expert per task on a collection of 200 tasks and combines these experts to evaluate downstream tasks. Due to the large number of expert models, model selection plays a practical role in choosing the most relevant experts for each downstream task (Zhao et al., 2024;Chronopoulou et al., 2023). Model merging is also related to Mixture of Experts (MoE)-based methods, where the design of routing weights is a key factor. The router can be neural networkbased (Lu et al., 2023), feature similarity-based (Wang et al., 2024), or even SVD-decomposed (Ostapenko et al., 2024b). In this paper, rather than directly adopting the various model merging techniques mentioned above in FL, we leverage the fundamental concept of model merging to address the primary challenge in Federated Learning with Multiple Models, enabling a large model soup to be efficiently reused on the server while communicating only a single model to each client. \n\nFederated Learning with Multiple Models The use of multiple models in FL arises from the fact that sharing a single model cannot effectively address the Non-IID problem.",
            "score": 0.39677489114894615,
            "section_title": "Related Work",
            "char_start_offset": 5590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 294,
                    "end": 317,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "272831995",
            "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering",
            "text": "Large Language Models (LLMs) like ChatGPT Achiam et al. (2023) and LLaMA Touvron et al. (2023) trained on vast amounts of general data, demonstrate remarkable performance in general tasks. To explore their potential for specialized tasks, adapting LLMs to specific domains by finetuning model parameters has become a critical area of research. In this context, Low-rank Adaptation (LoRA) Hu et al. (2021), as a parameter-efficient fine-tuning approach, has gained widespread recognition, also attributed to its modular design Liu et al. (2023); Yang et al. (2023b); Hadi et al. (2023). The modular nature of LoRA enables it to serve as plug-and-play plugins for LLMs, facilitating the storage and deployment of large collections of LoRAs on platforms like Hugging Face. The extensive availability of LoRAs has sparked considerable interest in combining multiple Lo-RAs into a unified adapter to significantly extend the capabilities of LLMs Yadav et al. (2024a); Xiao et al. (2024); Zhao et al. (2024b); Huang et al. (2023). \n\nPrevious methods for composing multiple LoRAs have primarily focused on assembling separate LoRAs tailored to specific downstream tasks, which generally require additional training Wu et al. (2023); Wang et al. (2024); Chronopoulou et al. (2023); Yadav et al. (2024a); Huang et al. (2023). Model merging Tang et al. (2024); Yadav et al. (2024b); Ilharco et al. (2022); Yang et al. (2024) offers an alternative approach by aggregating the parameters of multiple LoRAs into a unified adapter without extra training, producing a unified LoRA with comprehensive capabilities. However, these methods typically employ element-wise parameter fusion, which can neglect and disrupt the internal semantic structure within LoRA. This disruption potentially leads to parameter interference (as discussed in \u00a72.3), thereby hindering the performance of merged LoRA.",
            "score": 0.39518387211224537,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 1351,
                    "end": 1371,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28369140625
        },
        {
            "corpus_id": "246016124",
            "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
            "text": "UNIFIEDSKG establishes a powerful and reproducible starting point for SKG research. New models can be easily applied to diverse SKG tasks, and new tasks can be easily framed based on our standardized abstraction. UNIFIEDSKG promotes a systematic study on more general and robust advances in structured knowledge encoding, multitask learning, zero-shot learning, and few-shot learning for SKG tasks. It also would be interesting to explore general pretraining methods within UNIFIEDSKG, which potentially benefit all the unified tasks. When the structured knowledge is too large for GPU memory, we truncate them based on heuristic rules, calling for future study on 1) incorporating retrieval component in SKG, 2) designing sparse attention in T5 for structured knowledge or other means to improve model efficiency. \n\nUNIFIEDSKG currently provides the correct type of structured knowledge for each task. However, how a system searches for the correct structured knowledge resources, takes appropriate action, and integrates information and results from multiple structured sources given a user request is still underexplored, which are a prerequisite for building a unified multi-purpose SKG system. \n\nSince we select popular tasks from each task family, we risk disproportionality in terms of the data language, domain and population, and we actively welcome diverse, multi-lingual tasks to be added into UNIFIEDSKG. Also, the error analysis of SKG can more fine-grained, and we hope our findings can promote future work on systematically studying and decomposing the behavior of PLMs on SKG tasks. Furthermore, training and evaluation data should reflect the intents and linguistic phenomena in the real world (de Vries et al., 2020), suggesting more realistic tasks to be added into UNIFIEDSKG.",
            "score": 0.39480855164343137,
            "section_title": "Limitations",
            "char_start_offset": 24878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1198
                },
                {
                    "start": 1201,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2890625
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "This section focuses on how integrated large language models (LLMs) effectively address the challenges encountered by traditional LLMs, as discussed in Chapter 2. By leveraging different integration techniques, such as the incorporation of knowledge graphs, knowledge bases, and specialized retrieval mechanisms, these integrated models overcome limitations in accuracy, scalability, and interpretability, providing more robust and contextually aware solutions. The Table 5 highlights the features of different methods for integrating knowledge with Large Language Models (LLMs), focusing on key aspects such as data structure, accuracy, reasoning ability, and application areas.",
            "score": 0.3946961894953656,
            "section_title": "LLM capabilities within integrated environment",
            "char_start_offset": 66663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 679
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "271874793",
            "title": "FuseChat: Knowledge Fusion of Chat Models",
            "text": "Similarly, Feng et al. (2024) trained multiple domain-specific LoRA (Hu et al., 2022) modules as experts and combined these domain experts using an explicit sequence-level routing strategy. \n\nLastly, FUSELLM (Wan et al., 2024) introduces another paradigm for the fusion of LLMs with structural differences. This approach builds upon knowledge distillation and leverages the probabilistic distribution matrices generated by source LLMs to transfer collective knowledge into a target LLM. Unlike model ensembles and MoEs, knowledge fusion does not require the parallel deployment of multiple models (experts). Furthermore, compared to model merging, which only applies to models with identical architectures, FUSELLM allows for the fusion of LLMs with different architectures. \n\nKnowledge Distillation Knowledge fusion essentially performs knowledge distillation to transfer knowledge from source LLMs to a target LLM. Knowledge distillation (Hinton et al., 2015) aims to train a small student model guided by one or more larger teacher models. Previous studies primarily focus on training a student model to mimic the teacher's behavior in text classification tasks, by replicating the teacher's output logits (Sanh et al., 2019;Turc et al., 2019), as well as hidden states (Sun et al., 2019;Jiao et al., 2020) and relations (Wang et al., 2020). In the realm of generative models, prevailing approaches maximize the log-likelihood of the student on the distributions (Khanuja et al., 2021;Gu et al., 2024;Agarwal et al., 2024) or sequences (Kim and Rush, 2016;Peng et al., 2023) generated by the teacher model. This paradigm can be extended to accommodate multiple teachers by either averaging the distributions (You et al., 2017) or blending the sequences (Wang et al., 2024a). \n\nCompared to vanilla knowledge distillation, knowledge fusion of LLMs faces new challenges. Firstly, due to the differences in tokenization among various LLMs, token alignment is essential for transferring knowledge from source to target LLMs. Secondly, when dealing with distributions generated from multiple source LLMs, the fusion function becomes crucial for optimally integrating their distributions.",
            "score": 0.3944918834446232,
            "section_title": "Related Work",
            "char_start_offset": 7427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 85,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 208,
                    "end": 226,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 1273,
                    "end": 1291,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 1291,
                    "end": 1309,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 1324,
                    "end": 1343,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 1466,
                    "end": 1488,
                    "matchedPaperCorpusId": "236477925"
                },
                {
                    "start": 1504,
                    "end": 1525,
                    "matchedPaperCorpusId": "263610088"
                },
                {
                    "start": 1539,
                    "end": 1559,
                    "matchedPaperCorpusId": "8451212"
                },
                {
                    "start": 1711,
                    "end": 1729,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 1756,
                    "end": 1776,
                    "matchedPaperCorpusId": "262064307"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "267759865",
            "title": "Stable Knowledge Editing in Large Language Models",
            "text": "Extensive research has consistently shown that large language models (LLMs) possess the capability to harness the vast reservoir of knowledge stored within their parameters for various reasoning tasks. However, this ability comes with inherent risks, including the potential for these models to inadvertently absorb obsolete or incorrect information (Raffel et al., 2020;Brown et al., 2020;Ouyang Figure 1: To enhance the evaluation of a knowledge editing method, we propose assessing it across four dimensions of stability. (1) Edited Knowledge Stability reflects the performance of one-hop knowledge editing, focusing on the consistency and accuracy of edited knowledge. (2) Multi-hop Knowledge Stability evaluates how well the edited knowledge integrates with existing knowledge across multiple steps. (3) Unrelated Knowledge Stability and (4) General Ability Stability, ensures that unrelated knowledge remains unchanged and maintain the overall capabilities of the model despite the editing process. et al., 2022;Touvron et al., 2023a,b;Zhao et al., 2023). Hence, the concept of knowledge editing in LLMs is introduced to address the timely updating of obsolete information and the integration of specialized knowledge on a large scale (Sinitsin et al., 2020;Patterson et al., 2021;Lazaridou et al., 2021;Dhingra et al., 2022). \n\nPrevious knowledge editing methods include locate-then-edit (Dai et al., 2022;Meng et al., 2022Meng et al., , 2023)), memory-based models (Mitchell et al., 2022b;Huang et al., 2023;Dong et al., 2022), and meta-learning (De Cao et al., 2021;Mitchell et al., 2022a) have two implicit assumptions to the knowledge, (1) knowledge is encapsulated in localized parameters, such as parameters in LLMs or external memory modules, (2) knowledge is independent of each other as well as isolated from the general capabilities of LLMs. However, the study conducted by (Wei et al., 2023) challenges the initial assumption by demonstrating that knowledge in LLMs is not solely confined to the MLP layers arXiv:2402.13048v1",
            "score": 0.3944873418304717,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 371,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1241,
                    "end": 1264,
                    "matchedPaperCorpusId": "213938729"
                },
                {
                    "start": 1287,
                    "end": 1310,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 1395,
                    "end": 1413,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 1413,
                    "end": 1430,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1473,
                    "end": 1497,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1497,
                    "end": 1516,
                    "matchedPaperCorpusId": "256194369"
                },
                {
                    "start": 1516,
                    "end": 1534,
                    "matchedPaperCorpusId": "252762125"
                },
                {
                    "start": 1575,
                    "end": 1598,
                    "matchedPaperCorpusId": "239050360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1639404296875
        },
        {
            "corpus_id": "269009562",
            "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models",
            "text": "The introduction of LLMs has sparked significant excitement and anticipation in many scientific disciplines and in the general population. The large size of LLM training data and the wide scope of potential subject matter has led to massive experimentation. However, this enthusiasm is often met with disappointment when LLMs are utilized in contexts that are highly specialized, require high precision, or require knowledge that is not easily describable through text. This discrepancy between expectation and reality highlights a critical challenge in effectively deploying LLMs across diverse applications. In this paper, we initiate our discussion by acknowledging this phenomenon, recognizing the superior language modeling skills of the LLMs and, at the same time, highlight the need for caution for many applications and domains. By addressing this issue head-on, we aim to contribute to a deeper understanding of the practical considerations and challenges associated with deploying LLMs in real-world scenarios. \n\nThe idea of including domain knowledge for enhancing the performance of large language models is an established doctrine. Traditionally, incorporating domain knowledge into language models has been achieved through methods such as fine-tuning with domain-specific examples or employing specialized language models trained on domainspecific training data, such as BioMedLM 15 . However, these approaches face limitations for specialized domains and complicated downstream processing tasks. \n\nMetadata, while crucial for structuring and contextualizing data, present unique obstacles for language models such as GPT-4. First, metadata are typically not included in the training data provided to large language models, posing a significant barrier to their effective integration. Second, the scarcity of high-quality examples of \"good\" metadata further complicates the training process, limiting the models' ability to learn and adapt to metadata-related tasks. The task of metadata adherence neither has large chunks of suitable text for pre-training nor a good number of examples for fine-tuning or few-shot prompting. Another popular way of incorporating domain knowledge is through the method of knowledge graphs. This method is also unsuitable for the semi-structured nature of metadata. In our study, we have demonstrated how CEDAR templates, in conjunction with GPT-4, offer a promising solution to these challenges. By leveraging the linguistic description of the structured knowledge provided by CEDAR templates, we can guide the language-generation process and enhance metadata adherence.",
            "score": 0.39388442995324346,
            "section_title": "Discussion",
            "char_start_offset": 11385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1020
                },
                {
                    "start": 1023,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2443
                },
                {
                    "start": 2444,
                    "end": 2618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145751953125
        },
        {
            "corpus_id": "271865730",
            "title": "BadMerging: Backdoor Attacks Against Model Merging",
            "text": "Model merging algorithms merge task-specific models initialized from the same pre-trained model, such as CLIP-like pre-trained models. It requires that the various task-specific models share the same model architecture but different parameters. As illustrated in Figure 1, two CLIP-like pre-trained models are fine-tuned on distinct datasets to obtain two task-specific models. Subsequently, they are merged into a final merged CLIP-like model, which can recognize classes in both tasks. We note that besides keeping their generalization ability, current model merging algorithms freeze the text encoder to further make each class have an identical language feature representation among different models, avoiding feature space collapses and conflicts among different models [24]. We now formally introduce the merging process. Specially, we denote M  as the CLIP-like model M with weights  and V  as the visual encoder of the model M  . Let  pre be the weights of a pre-trained model, and   be the weights fine-tuned on a dataset D  . Then, we denote a task vector \u0394  as the element-wise difference between   and  pre , i.e., \u0394  =   \u2212  pre . Assume there are  task vectors {\u0394 1 , . . . , \u0394  } obtained from different training settings of the same/different tasks. We can derive a unified formulation of model merging to obtain merged weights  merged as  merged =  pre + \u0394 merged . Different merging algorithms mainly differ in their ways of obtaining the merged task vector \u0394 merged as follows: Task-Arithmetic (TA) [24] and Simple Average (SA) [66]. TA and SA merge task vectors via the weighted sum: \u0394 merged =   =1 \u2022\u0394  . Both TA and SA assume that each task vector should have an equal contribution to the merged task vector. TA scales each task vector using a fixed  = 0.3 regardless of the number of task vectors, which achieves promising results in merging task-specific models from different domains. SA calculates  as the arithmetic mean, i.e.,  = 1  , which achieves better results in merging taskspecific models from the same domain. Ties-Merging (Ties) [72]. Ties proposes three operations: TRIM, ELECT SIGN and MERGE to address three kinds of interference among original task vectors in \u0394 . We combine these three operations and call them  (\u2022).",
            "score": 0.3938440761261052,
            "section_title": "Model Merging",
            "char_start_offset": 8656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2065,
                    "end": 2069,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4833984375
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "Large language models (LLMs) have shown great performance on tasks in various domains such as natural language processing [1], computer vision [2] etc. With the great effort of world-wide contributors in community, a large number of general-purpose pre-trained and task-specific fine-tuned language models have been proposed and made publicly available. However, LLM pre-training or fine-tuning is non-trivial and requires a lot of effort and financial budget. Recently, model merging (MM) has attracted many researchers' attention. By combining multiple LLMs into a single model with better performance and adaptability on more tasks, MM offers a novel cost-efficient way of obtaining new powerful language models without performing additional training [3,4], just like the Power Rangers merge their Zords together to form a mighty Megazord [5]. Ideally, MM is supposed to inherit and amplify the strengths from its source models while ignoring their weaknesses. Therefore, the obtained model will be able to tackle the union set of all the tasks where the source models are pre-trained/fine-tuned with better performance. Yet this is achieved without training, which saves a large amount of calculation/financial budget. With the help of open-source toolkits such as mergekit [6,7], MM has become popular for LLM developing and shown great potential on the Open LLM Leaderboard [8]. \n\nHowever, model merging requires the model maker to have profound knowledge or intuition. Automatically discovering more capable MM recipes is still in its infancy. To the best of our knowledge, The most related work is [3], where Akiba et al. proposed to use evolutionary algorithms (EAs) to generate powerful model merging recipes which operates in both parameter space and data flow space. However, the approach of using diverse task scores and optimizing the parameter space to explore the comprehensive potential of the final model has been largely overlooked. \n\nMM is similar to ensemble learning in the sense that they both try to produce one model from multiple source models. In ensemble learning, It is generally believed that diversity in an ensemble could be beneficial for improving its performance [9,10,11]. One could infer that maintaining diversity during MM may also result in powerful LLMs.",
            "score": 0.3936822117201467,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 2201,
                    "end": 2204,
                    "matchedPaperCorpusId": "2976898"
                },
                {
                    "start": 2204,
                    "end": 2207,
                    "matchedPaperCorpusId": "8711351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "258832553",
            "title": "Collaborative Development of NLP models",
            "text": "Despite substantial advancements, Natural Language Processing (NLP) models often require post-training adjustments to enforce business rules, rectify undesired behavior, and align with user values. These adjustments involve operationalizing\"concepts\"--dictating desired model responses to certain inputs. However, it's difficult for a single entity to enumerate and define all possible concepts, indicating a need for a multi-user, collaborative model alignment framework. Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts. To address these challenges, we introduce CoDev, a framework that enables multi-user interaction with the model, thereby mitigating individual limitations. CoDev aids users in operationalizing their concepts using Large Language Models, and relying on the principle that NLP models exhibit simpler behaviors in local regions. Our main insight is learning a \\emph{local} model for each concept, and a \\emph{global} model to integrate the original data with all concepts. We then steer a large language model to generate instances within concept boundaries where local and global disagree. Our experiments show CoDev is effective at helping multiple users operationalize concepts and avoid interference for a variety of scenarios, tasks, and models.",
            "score": 0.3933091209870571,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2193603515625
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "The goal of multi-task learning (MTL) is to utilize a single model to perform multiple related tasks concurrently, thereby facilitating information sharing and knowledge transfer among the tasks. In recent years, the rapid development of deep learning has prompted a learning paradigm shift, where the mainstream paradigm now focuses on fine-tuning downstream tasks using powerful pre-trained models, rather than training an expert model from scratch [2,3,4,5,6,7]. This shift typically results in significant reductions in both data requirements and computational resources. Additionally, the open-source ethos of the deep learning community has encouraged developers to release a vast array of expert models fine-tuned on various downstream tasks. To date, over one million diverse models have been made available on Hugging Face 1 . These above diverse factors have given rise to a new MTL paradigm, enabling the direct merging of multiple independently trained expert models to create a multi-task model without requiring access to their original training data [8,9,10,11]. \n\nHowever, due to potential task conflicts and interferences among multiple tasks, simply merging parameters from independently fine-tuned models may lead to a sharp decline in MTL performance [12,13]. Recently, an increasing number of studies have aimed to address the MTL performance degradation resulting from model merging [11]. A notable example is task arithmetic [14], which introduces the concept of 'task vectors' to extract task-specific knowledge from the fine-tuned models. By linearly weighting the task-private knowledge of multiple tasks into the pre-trained model, task arithmetic enhances the model's ability to process multiple downstream tasks. Inspired by task arithmetic, recent advancements have proposed techniques to alleviate sign conflicts among task vectors [15,16], merge them in a fine-grained manner [17,18,19], or combine task vectors within subspaces [20,21,22,23]. While these methods have considerably improved task arithmetic's performance, a noticeable performance gap still exists between the merged MTL model and the independently fine-tuned expert model (or the joint-trained MTL model). \n\nIn this paper, we claim that the observed performance gap primarily arises from the fact that existing model merging methods focus on finding a static multi-task optimal solution within the original parameter space.",
            "score": 0.392864247673286,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 458,
                    "matchedPaperCorpusId": "257038341"
                },
                {
                    "start": 462,
                    "end": 464,
                    "matchedPaperCorpusId": "257505035"
                },
                {
                    "start": 1068,
                    "end": 1070,
                    "matchedPaperCorpusId": "259022411"
                },
                {
                    "start": 1968,
                    "end": 1971,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1971,
                    "end": 1974,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "270562513",
            "title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts",
            "text": "In Table 1, we showcase comparative benchmark results of various approaches across four specialized domains: knowledge, reasoning, math, and coding. All baselines use self-generated synthetic data based on the same Base LLM, Gemma-7B, and LoRA for tuning to ensure fair comparisons. \n\nFirst, we confirm self-specialization markedly enhances target-specific expertise, compared to the base LLM. For instance, we can see substantial gains from corresponding specialized models (e.g., Knowledge Self-Spec. in the knowledge domain): 58.4 to 64.0 in knowledge, 56.1 to 60.2 in reasoning, and so on. However, this focused improvement sometimes comes at the cost of reduced performance in non-targeted areas, as evidenced by the drop in scores for the Knowledge Self-Spec. model in reasoning, math, and coding. This trade-off highlights the inherent limitation of overspecialization. In contrast, our MiXSE, demonstrates consistent improvements across all domains, due to its modular, compositional architecture that makes use of dynamic routing to leverage optimal experts. Surprisingly, it even outperforms all corresponding specialized models, indicating that it effectively synergizes the strengths of each specialization. \n\nIn comparison with other static merging methods like Instance Merging, TIES, and DARE, MiXSE stands out for its superior adaptability. While they attempt to combine the strengths of different specialization areas into a single model, they lack the dynamic flexibility that MiXSE offers. Notably, simple instance merging (i.e., multi-task tuning), though effective in enhancing the base LLM across domains, still falls short of achieving the superior average performance of 54.3 seen with MiXSE. This validates the advantages of dynamic expert integration in a compositional system.",
            "score": 0.39259129955743793,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 15724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1219
                },
                {
                    "start": 1222,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445068359375
        },
        {
            "corpus_id": "257921533",
            "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models",
            "text": "Integration refers to combining different systems or software components to achieve a common goal. ChatGPT can be integrated as a part of a whole or act as an integration tool to enable seamless communication between different systems. Its natural language processing ability makes it easier for non-technical users to interact with systems, reducing the need for specialized knowledge or training. Some studies in the literature we collected have already demonstrated this. \n\nTreude et al. [90] integrated ChatGPT into the prototype of \"GPTCOMCARE\" to address programming query problems. This integration allowed for the generation of multiple source code solutions for the same query, which increased the efficiency of software development. The results of their study demonstrated the effectiveness of using ChatGPT to improve the quality and diversity of code solutions, ultimately reducing the amount of time and effort required for software development. Wang et al. [94] proposed the chatCAD method, which utilizes large language models (LLMs) such as ChatGPT to enhance the output of multiple CAD networks for medical images, including diagnosis, lesion segmentation, and report generation networks. The method generates suggestions in the form of a chat dialogue. The authors tested the effectiveness of the method on a randomly selected set of 300 cases from the MIMIC-CXR dataset, which included 50 cases each of cardiomegaly, edema, consolidation, atelectasis, pleural effusion, and no findings. Compared to CvT2DistilGPT2 and R2GenCMN, chatCAD showed significant advantages in RC and F1, while only performing weaker than R2GenCMN in PR. \n\nIntegrating ChatGPT into applications will still present challenges. Firstly, ChatGPT's performance may be affected by language barriers or differences in terminology between different systems. Additionally, ChatGPT's responses are not always deterministic, which poses a challenge when integrating with systems that require precise and reproducible results. Finally, the processing time of ChatGPT is slow for integration tasks involving time-sensitive data such as traffic, which is a limitation in time-critical environments.",
            "score": 0.3923775833868509,
            "section_title": "ChatGPT Integration",
            "char_start_offset": 49954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1648
                },
                {
                    "start": 1651,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2179
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07598876953125
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "Integrating Large Language Models (LLMs) into existing systems presents a variety of challenges, including data access, real-time updates, scalability, security, and interpretability. To address these, multiple advanced techniques have been developed beyond the common methods like Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs). \n\nRAG allows LLMs to dynamically retrieve relevant information from external sources during inference, bridging gaps in knowledge and providing access to both structured and unstructured data. LLMs are further enhanced with API integration and data pipelines, ensuring seamless real-time access to diverse data sources such as legacy systems, external APIs, and proprietary databases [41,92]. This integration can be simplified through RESTful APIs or more complex solutions like GraphQL, which allows querying across multiple data endpoints efficiently. \n\nIn scenarios where keeping LLMs updated is critical, especially in dynamic fields like healthcare and finance, RAG retrieves the latest information at inference time, eliminating the need for constant retraining. Knowledge Graphs (KGs), which offer structured data and incremental updates, further allow LLMs to access evolving knowledge without undergoing full retraining cycles. This combination is particularly useful in specialized domains where new information is frequently introduced, such as in BIORAG for biomedical data. \n\nTool-integrated reasoning through search engines, calculators, and code interpreters can extend LLM capabilities, and enhance accuracy, efficiency and integration with specific tasks or domains [29]. Incorporating tools into the thinking process of LLMs can boost effectiveness in tasks that require calculations or specialized algorithms, hence enhancing their performance and lessening their workload. Models such as TORA use tool-integrated reasoning to tackle challenging problems by interweaving natural language reasoning with external tools, like computation libraries and symbolic solvers [144]. \n\nAdditionally, adapters can be used to fine-tune LLMs for specific tasks without altering the core model parameters [124]. These lightweight neural modules can be plugged into the LLM and trained with domain-specific data, making them highly adaptable to different tasks and reducing computational costs. For example, the UP-RISE framework employs adapters to select the most appropriate prompts from a pool for a specific zero-shot task [36].",
            "score": 0.3923266725622044,
            "section_title": "Seamless integration of LLMs with existing systems",
            "char_start_offset": 76949,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2346
                },
                {
                    "start": 2347,
                    "end": 2485
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 738,
                    "matchedPaperCorpusId": "274233308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "278389427",
            "title": "Collaborative Estimation of Tropical Cyclone Wind Radii With Multitask Learning and Multiteacher Distillation",
            "text": "Multitask learning aims to optimize multiple objectives within a single model, but achieving a balance between task performance is a significant challenge. Two primary strategies for addressing Multitask learning optimization are task loss balancing and gradient conflict resolution. \n\nTask loss balancing methods, such as uncertainty-based weighting, attempt to dynamically adjust the contribution of each task's loss to the overall optimization. These methods have the advantage of adaptively adjusting learning dynamics based on task difficulty and data distribution. On the other hand, gradient conflict resolution methods, such as Gradvac, focus on modifying gradients to reduce destructive interference between tasks. These methods ensure that updates from different tasks do not counteract each other, leading to more stable optimization. Although these work improves over the uniform weighing loss strategy in multitask learning, they provide a limited control on the learned parameters and are thus limited to prevent one task dominating the rest [38]. \n\nCompared with these two methods, multiteacher knowledge distillation presents an alternative optimization strategy. It provides stricter control over the parameters of the multitask network. Instead of directly balancing losses or modifying gradients, it transfers knowledge from multiple pretrained taskspecific networks to a unified multitask model. By aligning the feature representations of the multitask network with those of specialized single-task models, knowledge distillation provides a structured way to preserve task performance.",
            "score": 0.39228182812058476,
            "section_title": "B. Multitask Learning Optimization",
            "char_start_offset": 39118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1605
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "220514465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.541015625
        },
        {
            "corpus_id": "248227728",
            "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners",
            "text": "The traditional mechanism of using large-scale pre-trained language models PLMs (Devlin et al., 2019;He et al., 2021) involve fine-tuning them for each task individually. This approach fails to benefit from interactions between tasks that could be related to each other. For instance, the task of predicting if one text entails or contradicts another can benefit from tasks that predict whether two texts are semantically similar or not. To address these limitations, Multi-Task Learning (MTL) methods like MT-DNN (Liu et al., 2019) and Mup- * work done while at Microsoft. pet (Aghajanyan et al., 2021a) instead train a single model jointly on a multi-task mixture consisting of multiple tasks. The typical mechanism is to facilitate transfer between the tasks by encoding the examples using a task-agnostic network shared between all the tasks, and then using taskspecific layers on top to optimize individual task objectives. The dominant choice for the network is a Transformer-based PLM such as BERT (Devlin et al., 2019). However, such dense (fullyconnected) task-agnostic networks have the limitation that they use all the weights of the network for every example, including those coming from very different tasks. This creates interference among different tasks, e.g., the tug-of-war phenomenon (Hadsell et al., 2020) where two or more tasks pull the model parameters in different directions, thus impacting the multi-task learning performance. \n\nA possible mechanism to alleviate this problem is to devise a task-aware network that can capture specialized information about individual tasks, as well as information that can be shared among multiple tasks. Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;Lepikhin et al., 2021) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant.",
            "score": 0.39214073077993916,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 532,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 1303,
                    "end": 1325,
                    "matchedPaperCorpusId": "226240885"
                },
                {
                    "start": 1741,
                    "end": 1763,
                    "matchedPaperCorpusId": "220265858"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "274656320",
            "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
            "text": "\"If I have seen further it is by standing on the shoulders of Giants.\" -Isaac Newton Multimodal Large Language Models (MLLMs) have made significant strides in advancing understanding, generation and reasoning across diverse domains. For example, in understanding, MLLMs like LLaVA [47] excel in visual question-answering (VQA) [4], effectively integrating visual and textual data. In generation, diffusion models [26,56,58] have achieved exceptional re-sults in text-to-image, text-to-video and text-to-3D generation [9,19,27,42,43,58,60,64,77]. \n\nBuilding on these advancements, recent studies [17,21,41,70,78,81,87] have aimed to develop unified architectures capable of performing tasks across various domains. Notably, Emu3 [74] and Omni-Gen [79] introduce all-inone models designed to handle both generation and understanding tasks. However, the integration of distinct domains within a single model continues to present significant challenges. In particular, variability within domains often leads to compromised performance on individual tasks due to conflicts between task objectives, such as those between text and image generation tasks [96]. These conflicts hinder the models' effectiveness and limit their utility in real-world applications. \n\nAnother key limitation of all-in-one models lies in their constrained ability to handle a broad spectrum of visionlanguage tasks across different domains due to differing input and output formats. This restriction presents a substantial bottleneck to scalability, particularly as the range of tasks continues to grow across images, videos, and the emerging 3D domain. Furthermore, extending these models to accommodate new tasks is inherently challenging. Training such comprehensive models with increasing model sizes demands substantial computational resources, and highly complex training methodologies. For instance, Omni-Gen [79] necessitates 104\u00d7A800 GPUs and five distinct training stages. These issues underscore the pressing need for modular or task-adaptive frameworks to enhance scalability and efficiency in addressing the increasingly diverse demands of vision tasks. Additionally, all-in-one models often struggle to integrate meticulously designed, task-specific components effectively, reducing their overall efficiency and performance in specialized applications.",
            "score": 0.39206364612810785,
            "section_title": "Introduction",
            "char_start_offset": 1781,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2336
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 327,
                    "end": 330,
                    "matchedPaperCorpusId": "3180429"
                },
                {
                    "start": 413,
                    "end": 417,
                    "matchedPaperCorpusId": "219955663"
                },
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "231979499"
                },
                {
                    "start": 517,
                    "end": 520,
                    "matchedPaperCorpusId": "263139613"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "268247980"
                },
                {
                    "start": 523,
                    "end": 526,
                    "matchedPaperCorpusId": "248006185"
                },
                {
                    "start": 526,
                    "end": 529,
                    "matchedPaperCorpusId": "265466876"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "254974187"
                },
                {
                    "start": 595,
                    "end": 599,
                    "matchedPaperCorpusId": "261975252"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "253735003"
                },
                {
                    "start": 605,
                    "end": 608,
                    "matchedPaperCorpusId": "258822817"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.222412109375
        },
        {
            "corpus_id": "276903216",
            "title": "Task Vector Quantization for Memory-Efficient Model Merging",
            "text": "Early model merging studies [24,51,52] explore combining models trained on the same task in weight space, demonstrating robustness to distribution shifts and improved performance on unseen data. Recent studies have extended this paradigm to multi-task learning [5,46,63], where merging multiple models trained on different tasks enables efficient knowledge sharing across domains. \n\nTask arithmetic [23] construct multi-task models through additive task vectors, which are computed by subtracting the pre-trained model's parameter from those of the finetuned model. Unlike direct interpolation, task vectors enable modular composition of specialized models, making them more flexible for multi-task settings. Sparsificationbased merging [55,61], where only the most significant parameters are retained to reduce interference, often complemented by rescaling strategies to enhance stability before integration. However, these approaches require manual tuning of task vector coefficients, which increases computational cost and limits scalability. To mitigate this issue, AdaMerging [58] determines the coefficients adaptively through test-time training. LiNeS [49] simplifies coefficient selection by increasing them linearly with layer depth. To move beyond fixed coefficients for all inputs, router-based approaches [33,45] dynamically determine instance-wise merging coefficients. Representation Surgery [57] independently learn task-specific parameters at test time to align the merged model's output with that of each task's individual model. \n\nAs the number of tasks increases, storing multiple finetuned checkpoints becomes a major bottleneck. Despite efforts to address storage issues, existing methods like EMR-Merging [20] and TALL Mask [48] rely solely on taskspecific binary masks. In contrast, we introduce a more universally applicable strategy that quantizes each task vector, significantly reducing storage overhead while preserving performance. Crucially, our approach integrates seamlessly with existing task vector-based methods, requiring only minimal modifications.",
            "score": 0.3919208879421914,
            "section_title": "Related Work 2.1. Model Merging",
            "char_start_offset": 5133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "268733341"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "235790783"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "267365047"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "249461723",
            "title": "Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning",
            "text": "The idea of having specialized parameters, or socalled experts, has been widely studied to integrate multiple sources of knowledge via transfer learning. The adapter module (Rebuffi et al., 2017;Houlsby et al., 2019) has been explored as one of such approaches, introducing a small number of task-specific parameters at every layer of pretrained language model (PLM) while sharing the parameters of underlying PLM which is fixed. To address the limitations of transfer learning due to high re-training cost, many works utilize the multiple adapter modules for individual tasks with different domains (Puigcerver et al., 2020;Bapna et al., 2019;R\u00fcckl\u00e9 et al., 2020;Madotto et al., 2021) considering each adapter to be an expert of each domain. Similar to our work, K-Adapter (Wang et al., 2021a) encodes factual and linguistic knowledge to each adapter, but in this paper, we further explore how to mitigate catastrophic forgetting or interference among multiple adapters for better knowledge transfer in zero-shot setting.",
            "score": 0.3913500767529364,
            "section_title": "Transfer Learning with Modular Approaches",
            "char_start_offset": 6047,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 644,
                    "matchedPaperCorpusId": "202660912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "274149834",
            "title": "Visual-Oriented Fine-Grained Knowledge Editing for MultiModal Large Language Models",
            "text": "Traditional knowledge editing primarily strives to update and correct the knowledge in Large Language Models (LLMs) to ensure their accuracy and reliability [22,27,30]. Many works, e.g., [6,16,19,20,31,32], have proposed various knowledge editing methods for LLMs, which have shown strong performance in refining model knowledge. Recently, there has been a growing interest in extending knowledge editing to Multimodal Large Language Models (MLLMs) [4,28]. MLLMs enhance the capabilities of traditional LLMs by incorporating multiple modalities, such as text and images, allowing them to process and generate content that integrates both textual and visual information [2,13,15,24,26,33]. In MLLMs, errors can arise not only from the language module but also from the visual components or the interactions between modalities. As a result, knowledge editing in MLLMs presents unique challenges that go beyond the complexities found in LLMs. \n\nTo advance the field, some pioneered works [4,9,14,29] have introduced multiple multimodal knowledge editing benchmarks. However, these benchmarks still primarily focus on text-oriented knowledge editing involving coarsegrained visual understanding, only considering single entities within images. In such settings, editing methods do not require access to the visual module; simply editing the language model suffices (e.g., by replacing the word \"bird\" with \"kite\"). As a result, knowledge editing methods for LLMs tend to perform well within these settings. In con-Figure 1. Comparison of fine-grained and coarse-grained knowledge editing in multimodal models. Coarse-grained knowledge editing treats the entire image as a single entity, allowing challenges to be addressed through simple text replacements. In contrast, fine-grained knowledge editing presents challenges that require editing methods to target specific entities within the image. \n\ntrast, this work addresses the more complex task of editing knowledge associated with multiple interacting entities within a single image. To this end, we introduce a new FGVEdit benchmark, designed for visual-oriented finegrained knowledge editing.",
            "score": 0.3912309039838038,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "213938729"
                },
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "258833129"
                },
                {
                    "start": 187,
                    "end": 190,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "246016194"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "258832407"
                },
                {
                    "start": 449,
                    "end": 452,
                    "matchedPaperCorpusId": "263908997"
                },
                {
                    "start": 669,
                    "end": 672,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 672,
                    "end": 675,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "235658331"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "258291930"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "263908997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1834716796875
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "In order to study the relative merits of merging for different objectives across a wide set of languages, we conduct extensive ablations. We detail some of the most critical experiment variants below: \n\nObjective-based merging. To evaluate the relative merits of merging on balancing dual-objectives, we merge models that have been separately optimized for general-purpose abilities and safety. This builds upon our multilingual 0% and 100% Safety Mixes (see Section 2.2) to balance the trade-offs between safety and general performance. Language-based merging. Multilinguality remains one of the most challenging tasks in language modeling. We aim to determine whether language-specific models can be used off-the-shelf to incorporate language capabilities and explore how merging models based exclusively on different languages affects their downstream performance. \n\nSpecifically, we investigate whether combining models optimized for both safety and general performance with a 15% language-specific safety mix for our target languages leads to better performance than training on a mixture of those languages. For clarity, to produce a multilingual model with safe and general-purpose abilities for English, French, and Spanish (referred to as the EN-FR-SP group later), we merge models optimized independently on a 15% Safety Mix for each of these languages. \n\nComparison of merging applied to DPO and SFT. Model merging is a highly adaptable technique that can be applied at any stage of the training process owing to its simple input requirement of model checkpoints. To determine the optimal stage for maximizing its benefits, we merge and evaluate SFT and DPO checkpoints independently as these techniques have shown great success towards the alignment of language models [Aakanksha et al., 2024;Shen et al., 2024]. \n\nSensitivity to hyperparameters. Previous works [Ilharco et al., 2023] have shown that merging is sensitive to the hyperparameters involved and have developed sophisticated algorithms [Akiba et al., 2024;Xiao et al., 2023;Davari & Belilovsky, 2024] to find the optimal values for the same. \n\nTo this end, we seek to find the impact of varying the weighting scheme of Linear merging on both general performance and safety.",
            "score": 0.3908534054751986,
            "section_title": "Key Ablations",
            "char_start_offset": 11237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417724609375
        },
        {
            "corpus_id": "266053657",
            "title": "Merging by Matching Models in Task Parameter Subspaces",
            "text": "The widespread fine-tuning of public pre-trained models has produced a huge number of specialized models. These specialized models may be trained on different tasks, where a \"task\" is simply the input-output relationship that we aim to train a model to perform (e.g. sentiment analysis of text, object recognition in images, etc.). Alternatively, the Stable Diffusion XL model (Podell et al., 2023) forms the basis of over a thousand specialized image generation models on the Hugging Face Model Hub that are specialized to different styles or content types. How can we recycle these specialized models to create better base models (Choshen et al., 2022;Ram\u00e9 et al., 2022)? Model merging (Wortsman et al., 2022b;Matena & Raffel, 2022) aims to tackle this problem by combining specialized models into a single model that retains the individual models' capabilities. A common example application of merging is constructing a multitask model from individual-task models, which is the primary application we explore in our paper. Compared to multitask learning, merging does not require simultaneous access to the individual-task datasets. Compared to outputspace ensembling of M models, merging produces a model that is M times cheaper to run. \n\nWhile merging via simple parameter averaging can work well for models that share an architecture and initialization (McMahan et al., 2017;Stich, 2018), recent merging methods improve over simple averaging by considering parameter importance (Matena & Raffel, 2022), matching activations (Jin et al., 2022), omitting the contribution of the pre-trained model (Ilharco et al., 2022), or resolving interference across models (Yadav et al., 2023). \n\nIn our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task.",
            "score": 0.39037893526278894,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 712,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1359,
                    "end": 1381,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 1484,
                    "end": 1507,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6708984375
        },
        {
            "corpus_id": "277955669",
            "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens",
            "text": "Research on achieving unified understanding and generation in multimodal models has primarily focused on two main strategies: cascading architectures and tokenizationbased methods. Cascading architectures [23,24,33,71] integrate separate modality-specific encoders and decoders, each pre-trained independently, and then fuse their representations through projection layers to create combined models for multimodal tasks. Notable examples include models such as EMU2 [71], which uses pre-trained language models [75] augmented with EVA-02-CLIP-Eplus [70] for comprehension tasks, and cascade an SDXLinitialized [62] diffusion model for visual generation tasks. \n\nIn contrast, tokenization-based methods [11,49,60,73,77,80,81,100] aim to create a unified framework by converting visual and textual inputs into a discrete space, and then jointly training a single transformer based solely on nexttoken prediction, thereby eliminating the need for diffusion or compositional approaches entirely. Moreover, recent advances, such as TransFusion [95] and Show-o [82], explore a blend of diffusion and autoregressive models within a single transformer for enhanced performance. However, although these methods provide a step towards unification, most of them focus on spatial tokens for vision, which are extracted from image patches and arranged in a spatial order (e.g., row by row in raster scan order). These spatial tokens lack the traits of human language, resulting in an inability to seamlessly integrate with human natural language within an MLLM to mutually enhance each other. Consequently, existing MLLMs still lag behind specialized architectures like SDXL [62] in visual generation tasks and LLaVA-1.6 [51] in visual comprehension tasks. These indicate the need for further exploration into more holistic tokenization methods that go beyond spatial representations to achieve more comprehensive multimodal comprehension and generation.",
            "score": 0.3898006104494912,
            "section_title": "Related Work",
            "char_start_offset": 5116,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "263889455"
                },
                {
                    "start": 215,
                    "end": 218,
                    "matchedPaperCorpusId": "266374640"
                },
                {
                    "start": 466,
                    "end": 470,
                    "matchedPaperCorpusId": "266374640"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25830078125
        },
        {
            "corpus_id": "268032726",
            "title": "Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities in solving various tasks, yet they often struggle with comprehensively addressing complex and vague problems. Existing approaches, including multi-agent LLM systems, offer solutions to certain challenges but still require manual setup and lack scalability. To address this gap, we propose a novel approach leveraging decomposition to enable LLMs to tackle vague problems effectively. Our approach involves an orchestrating LLM that interacts with users to understand the problem and then decomposes it into tangible sub-problems. Instead of expecting the LLM to solve the entire problem in one go, we train it to ask follow-up questions to gain a deeper understanding of the user's requirements. Once the problem is adequately understood, the orchestrating LLM divides it into smaller, manageable sub-problems. Each sub-problem is then assigned to specialized LLM agents or non-LLM functions for resolution. These agents work in parallel to solve their respective sub-problems, with the orchestrating LLM overseeing the process and compiling the solutions into a comprehensive answer for the user. By adopting this decomposition approach, we alleviate the constraints imposed by token limitations on LLM outputs and empower them to provide nuanced solutions to complex and ambiguous problems. Through our approach, we aim to enable LLMs to think and operate more like humans, breaking down complex problems into manageable parts and collaboratively solving them. This not only enhances the problem-solving capabilities of LLMs but also offers a scalable and efficient method for addressing a wide range of real-world challenges.",
            "score": 0.3897693228959534,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2393798828125
        },
        {
            "corpus_id": "268378941",
            "title": "Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods",
            "text": "Approaches have been devised to allow automated machine learning models to seamlessly integrate text [62], [63] as well as heterogeneous and multimodal data [64], [65]. Owing to the differences in construction of the different datasets, it often requires enormous preprocessing to achieve good integration. Automated machine learning techniques are able to handle all the needed processing tasks for integrating multi-modal data. For instance, Shi et al. [64] propose to employ specialized transformer models as multimodal data processing units to allow automated learning on multimodal data -specifically, text and tabular information in different formats. The Transformer units rely on natural language processing (NLP) to obtain useful features from text datasets. In the processing stage, these feature sets are then aggregated, transformed and combined with tabular data, where AutoGluon-Tabular [66], an AutoML framework for learning on tabular data, is then used to seamlessly processed the aggregated information from multiple data sources and formats. \n\nAnother challenge is that data from different sources may exhibit different statistical distributions [67], [68]. Moreover, in situations where the collected data is from sources created by diverse players with different levels of expertise or conflicting interests and goals, data quality and bias become important issues and need to be handled in the integration process [67]. This is currently achieved using automated cleaning methods [69].",
            "score": 0.38950536760104515,
            "section_title": "Integrating text and multimodal data",
            "char_start_offset": 24769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1507
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 105,
                    "matchedPaperCorpusId": "232359334"
                },
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "235669929"
                },
                {
                    "start": 157,
                    "end": 161,
                    "matchedPaperCorpusId": "237263279"
                },
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "251518324"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "237263279"
                },
                {
                    "start": 1165,
                    "end": 1169,
                    "matchedPaperCorpusId": "249578827"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "249246332"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "249578827"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "239616040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1951904296875
        },
        {
            "corpus_id": "273228757",
            "title": "Chip-Tuning: Classify Before Language Models Say",
            "text": "Large language models (LLMs) have experienced rapid development in recent years, achieving surprising success in various domains. Researchers have been scaling up the size of language models to pursue better performance, just as the scaling law (Kaplan et al., 2020) suggests. However, the increasing size of models leads to massive computational costs, posing a challenge to practical deployment and usage. \n\nModel compression techniques have since been proposed as a solution to relieving computational stress, which would assist in the deployment of large models. Different approaches have been explored to compress language models into more compact versions, including quantization (Liu et al., 2021;Dettmers et al., 2022Dettmers et al., , 2024)), knowledge distillation (Gou et al., 2021;Gu et al., 2023;Ko et al., 2024) and pruning (Ma et al., 2023;Yang et al., 2024;Ashkboos et al., 2024;Men et al., 2024). \n\nRelevant research (Men et al., 2024) reveals that a fair portion of parameters in large language models are redundant, and removing these parameters would not bring severe damage to the performance of models. Based on the observation, different methods have been designed to identify and remove redundant parameters from LLMs, like layer merging (Yang et al., 2024), width compression (Ashkboos et al., 2024), layer removal (Men et al., 2024) and component removal (Ma et al., 2023). These methods maintain the majority of performance, proving the feasibility of model pruning. \n\nResearch on model interpretability has shown evidence that language models may develop internal representations for various features like color (Patel and Pavlick, 2022), truthfulness (Burns et al., 2022), chessboard states (Nanda et al., 2023), numbers (Zhu et al., 2024) or even abstract concepts like code errors (Templeton, 2024). These features typically start to form on middle layers and will be carried to subsequent layers (Stolfo et al., 2023).",
            "score": 0.38927393462580945,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 704,
                    "matchedPaperCorpusId": "235658553"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 725,
                    "end": 750,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 775,
                    "end": 793,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 809,
                    "end": 825,
                    "matchedPaperCorpusId": "267499832"
                },
                {
                    "start": 873,
                    "end": 895,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1301,
                    "end": 1324,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1640,
                    "end": 1665,
                    "matchedPaperCorpusId": "251647156"
                },
                {
                    "start": 1680,
                    "end": 1700,
                    "matchedPaperCorpusId": "254366253"
                },
                {
                    "start": 1720,
                    "end": 1740,
                    "matchedPaperCorpusId": "261530966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18212890625
        },
        {
            "corpus_id": "273549646",
            "title": "In-Context Code-Text Learning for Bimodal Software Engineering",
            "text": "A wide array of existing software engineering research typically specializes in one particular downstream task in either code analysis or natural language processing. \n\nBimodal Software Engineering represents an emerging field that seeks to bridge the gap between code and natural language within the software development lifecycle. This integration is essential for addressing complex tasks that require a deep understanding of both code syntax and semantic context. A primary challenge lies in aligning code and text representations effectively, as discrepancies can lead to errors, vulnerabilities, and hindered developer productivity [11]. Traditional approaches often silo these modalities, as illustrated in Figure 1, limiting the potential synergies between them [1]. Indeed, different tasks and datasets require various inputs and outputs, which require the creation and training of deep learning models that are capable of handling multiple downstream tasks, such as clone detection and code generation. The range of requirements underscores the need for more integrated and versatile tools in bimodal software engineering, capable of addressing the complex interplay between code and natural language across diverse software development scenarios. To overcome these limitations, we propose a unified framework that leverages a general-purpose interface (i.e., an adapter), which helps to handle various software engineering tasks by seamlessly integrating code and text analysis. This approach empowers models to extract valuable insights from both domains, enhancing their capability to perform complex SE tasks like detecting vulnerabilities [21], [22], explaining code [23], [24], and identifying code clones [25] with greater accuracy and efficiency. Thus, our technique unifies prompt learning for large language models across various software engineering tasks, ensuring effective processing of diverse inputs to achieve state-of-the-art results. \n\nIn-Context Learning (ICL) is a key capability of large language models (LLMs) that allows them to adapt to new tasks without explicit training. By incorporating task demonstrations within the input prompt, LLMs can generate relevant outputs, as first highlighted with GPT-3 [26]. Indeed GPT-3 demonstrated remarkable performance across a range of tasks by simply conditioning on task demonstrations embedded in the input context.",
            "score": 0.38911077633577884,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 2621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 169,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2244
                },
                {
                    "start": 2245,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 642,
                    "matchedPaperCorpusId": "207591052"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "219439994"
                },
                {
                    "start": 1654,
                    "end": 1658,
                    "matchedPaperCorpusId": "221784842"
                },
                {
                    "start": 1660,
                    "end": 1664,
                    "matchedPaperCorpusId": "49670513"
                },
                {
                    "start": 1682,
                    "end": 1686,
                    "matchedPaperCorpusId": "231855531"
                },
                {
                    "start": 1688,
                    "end": 1692,
                    "matchedPaperCorpusId": "249848115"
                },
                {
                    "start": 1722,
                    "end": 1726,
                    "matchedPaperCorpusId": "211205176"
                },
                {
                    "start": 2239,
                    "end": 2243,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2437744140625
        },
        {
            "corpus_id": "274597668",
            "title": "Chimera: Improving Generalist Model with Domain-Specific Experts",
            "text": "While these expert models exhibit strong capabilities in specialized tasks, they are often criticized for being designed to address individual scenarios. This phenomenon arises from significant distribution gaps across various subdomains, such as tables, charts, functions, and geometry, potentially sacrificing their generalizability across broader applications using specialized models. \n\nTo push the boundary further for the existing LMMs and improve their performance in specialized domains, an intuitive solution is to post-train LMMs on data relevant to the target domain. However, a common challenge is that the domain-specific data required for specialist models are often proprietary and inaccessible. On the other hand, integrating specialist experts that contain specialized prior knowledge presents a promising approach to address this issue. However, directly combining specialist experts with the generalist model could result in unsatisfactory performance, due to the following factors: 1) large representation gaps between cross-domain encoders, and 2) imbalanced optimization for generalists and specialists. \n\nTo address these challenges, this work introduces Chimera: a flexible and scalable pipeline that can effectively scale up off-the-shelf experts into LMMs at low cost. Specifically, through cost-effective training aimed at feature alignment, we integrate multiple encoders from different expert models into a single LMM, effectively merging diverse specialized knowledge into the model. Besides, we observed alignment imbalances during the cross-modal encoder fusion and propose a General-Expert Collaboration Masking mechanism to facilitate better model fusion. Our method easily adapts LMMs, such as InternVL [12,13], to a range of domain-specific tasks, including advanced mathematical reasoning, table/chart QA & extraction, and document structural extraction tasks. By aggregating multiple expert models into a single general LMM, Chimera develops a versatile model endowed with multiple specialized capabilities. During inference, Chimera employs a routing module to determine whether to invoke the corresponding domain expert model based on the visual input, resulting in a versatile model that excels across the chart, table, math, and document domains, as well as tasks involving multimodal reasoning and extraction. \n\nWe conduct extensive experiments to evaluate Chimera's capabilities in multi-modal reasoning and visual content extraction, both of which are challenging domains for assessing existing LMMs.",
            "score": 0.3890255810206852,
            "section_title": "Introduction",
            "char_start_offset": 1899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2352
                },
                {
                    "start": 2355,
                    "end": 2545
                }
            ],
            "ref_mentions": [
                {
                    "start": 1742,
                    "end": 1745,
                    "matchedPaperCorpusId": "266521410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474365234375
        },
        {
            "corpus_id": "270218740",
            "title": "Dual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph Question Answering",
            "text": "Large language models (LLMs) [1][2][3][4] have demonstrated impressive capabilities across various natural language processing tasks. Pre-trained on extensive corpora, LLMs excel at implicit and associative reasoning. To further enhance LLMs' reasoning abilities, many approaches (e.g., Chainof-Thought (CoT) [5], Tree-of-Thought (ToT) [6]) guide LLMs to generate intermediate steps and form a complete thought chain, aiming for a more deliberate and explicit reasoning [7,8]. While these approaches can improve performance, they typically increase inference costs, and encounter the challenge of hallucinations when the model lacks relevant knowledge, especially domain-specific and up-to-date knowledge [7,9]. \n\nIntegrating external knowledge sources, such as knowledge graphs (KGs), offers a promising solution to these limitations. KGs, storing a vast amount of facts in the form of triples (e.g., Wikidata [10], YAGO [11], and NELL [12]), are vital for a variety of applications due to their capacity to deliver explicit knowledge [13,14]. A common and essential task for integrating LLMs with KGs is Question Answering over Knowledge Graph (KGQA), which aims at answering natural language question from entities within a given KG. To accurately respond to a given question, a key challenge is that how to enable LLMs to effectively acquire supportive reasoning evidence from a large and complex KG structure. Existing methods frequently overlook the importance of explicit learning within graph structures, which is essential for supplying precise evidence chain for LLM's reasoning. The text-based retrieval methods (e.g.,KAPING [15]), directly retrieve triplets based on text similarity from KGs for LLMs, which frequently result in redundant or irrelevant information [16]. Another LLM-based retrieval methods like StructGPT [17], ToG [18], guide LLMs to retrieve over KGs across multiple steps. Since LLMs lack the inherent capacity to comprehend graph structures, it is often challenging for them to perform effective topological reasoning on graphs [9].",
            "score": 0.3886438791900133,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 32,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 708,
                    "end": 710,
                    "matchedPaperCorpusId": "259165563"
                },
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "207163173"
                },
                {
                    "start": 937,
                    "end": 941,
                    "matchedPaperCorpusId": "8423494"
                },
                {
                    "start": 1036,
                    "end": 1040,
                    "matchedPaperCorpusId": "265221458"
                },
                {
                    "start": 1040,
                    "end": 1043,
                    "matchedPaperCorpusId": "268553826"
                },
                {
                    "start": 1636,
                    "end": 1640,
                    "matchedPaperCorpusId": "259095910"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "259936842"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2237548828125
        },
        {
            "corpus_id": "271328836",
            "title": "Computer Audition: From Task-Specific Machine Learning to Foundation Models",
            "text": "We discussed the merging of unimodal FMs in Section 4.7.3.While the technique can substantially improve the capabilities of the resulting FM (which leverages the abilities of multiple 'parent' FMs), it still results in a unimodal FMs.Therefore, to achieve multimodality, one must also connect the different components in the ways described in Section 4. Here, we consider efforts and ideas on how FMs from different modalities can be merged, thus unlocking multimodal capabilities even without additional training.Given multimodal models M 1 and M2 of the same type, the challenge is to produce a new model M 3 that incorporates the abilities of both preceding models.\n\nMerging vision-language models: A significant challenge in merging vision-language models (VLMs) is ensuring that the model can seamlessly switch between and integrate the different modalities.This requires sophisticated training regimes that balance the contributions of each modality and ensure that neither dominates the other.Successful merging of VLMs results in models capable of tasks such as image captioning, visual question answering, and generating images from textual descriptions.Existing methods such as exponential moving average (EMA) [121], \"Model Soups\", and Fisher-weighted averaging have shown effectiveness but struggle with the complexities of VLMs [122].To address these challenges, Ye et al. [123] proposed a gating network.This method can merge all layers within a VLM (e. g., Embedding, Norm, Attention, and MLP) and select the appropriate classifier.Trained on unlabelled datasets from all tasks, the gating network predicts which task the input belongs to and merges the models during inference.Additionally, the authors designed a novel metric of model weight similarity to boost performance, especially because merging tasks increases in difficulty.Another recent study presented the elect, mask & rescale (EMR)-merging [124] method while showing superior performance over existing merging methods in both traditional and new scenarios.This encompasses the merging of numerous vision models (up to 30), NLP models, PEFT models, and multi-modal models.",
            "score": 0.3884646860945998,
            "section_title": "Multimodal merging",
            "char_start_offset": 88770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 58,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 668
                },
                {
                    "start": 670,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1163
                },
                {
                    "start": 1163,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2036
                },
                {
                    "start": 2036,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 1221,
                    "end": 1226,
                    "matchedPaperCorpusId": "231662174"
                },
                {
                    "start": 1341,
                    "end": 1346,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "278418792",
            "title": "Analysis of Model Merging Methods for Continual Updating of Foundation Models in Distributed Data Settings",
            "text": "Limitations in Scaling Training Data: High-quality data are essential for improving model performance; however, their availability is expected to become increasingly limited. If the current trend of large-scale model development continues, estimates suggest that all publicly available human text data will be entirely exhausted by 2028 [11]. In addition, privacy and security concerns further restrict open data access, making it increasingly difficult to construct large and diverse datasets. \u2022 Computational Resource Constraints: FMs, which contain many parameters [12], require enormous computational resources to train on large datasets. This poses challenges for individuals and organizations lacking the necessary infrastructure to train FMs. In addition, computational resources are increasingly concentrated among financially powerful entities, creating an imbalance that limits broader innovation. \n\nThese challenges hinder sustainable and equitable FM development. Furthermore, these challenges lead to issues such as the inability to incorporate data from local deployment environments and the rapid obsolescence of model knowledge [13]. Therefore, there is a growing need for methods that enable diverse individuals and organizations to train FMs while collaboratively ensuring data privacy and security. \n\nThis study focuses on model merging [14,15] to address these training challenges. Model merging fuses multiple fine-tuned models for different tasks into a single highperformance model without the need for additional training. This process typically involves carefully adjusting model parameters and merging learned representations, typically via techniques such as weighted averaging or parameter alignment, to preserve and integrate the specialized knowledge of each constituent model. By repeatedly merging FMs that have been fine-tuned on local datasets owned by various entities, the abilities of these models can be continuously enhanced [16]. During this process, data are distributed rather than centralized, ensuring data privacy. Each entity contributes local model updates derived from sensitive data, which are merged without exposing the raw data. This approach leverages sensitive data that cannot be used directly in conventional centralized training and enables models to acquire fine-grained and diverse knowledge from various sources. In addition, by fine-tuning each model using the computational resources available to individual entities, the overall computational load is distributed relative to centralized training, thereby encouraging broader participation in FM development. \n\nAlthough model merging and federated learning (FL) aim to improve decentralized models, model merging fuses fine-tuned models into a unified model only once.",
            "score": 0.388270554982896,
            "section_title": "\u2022",
            "char_start_offset": 1772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2372
                },
                {
                    "start": 2373,
                    "end": 2620
                },
                {
                    "start": 2623,
                    "end": 2780
                }
            ],
            "ref_mentions": [
                {
                    "start": 1144,
                    "end": 1148,
                    "matchedPaperCorpusId": "253735429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347900390625
        },
        {
            "corpus_id": "277151047",
            "title": "FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors",
            "text": "This technique aims to aggregate multiple well-trained models into a single unified model, thereby inheriting their individual capabilities without incurring the computational overhead and complexity associated with traditional ensembling methods. Task Arithmetic [14] stands out as a simple yet highly effective method for model merging. It introduces task vectors, which are both efficient and lightweight, facilitating improved generalization across tasks. Ties-Merging [42] identifies redundancy and sign conflicts in direct task vector aggregation and proposes three steps to resolve them. AdaMerging [43] addresses the limitation of shared merging coefficients in prior methods, designing separate aggregation weights for each task vector to enhance model adaptability. The ability of model merging to aggregate models without relying on training data closely aligns with the requirements of federated learning, where preserving data privacy is of paramount importance. However, model merging is a single-step aggregation and the federated learning needs a multiple communication rounds, where the former already owns well-trained local models and the latter requires constant iterative updating of the global and local models. The key difference makes it difficult to design the aggregation weights in federated learning using the model merging directly, which is ours research focus in this work. To the best of our knowledge, we are the first to explore the relationship between model merging and federated learning, and introduce an adaptive optimization of aggregation weights by designing the client vector in federated learning.",
            "score": 0.3881739637287428,
            "section_title": "Introduction",
            "char_start_offset": 6932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1641
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 473,
                    "end": 477,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.424560546875
        },
        {
            "corpus_id": "260680759",
            "title": "KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering",
            "text": "We introduce a novel framework called KITLM, which addresses the challenge of providing context for multi-hop question answering. KITLM leverages a knowledge base by filtering and selecting relevant information to derive the necessary context for accurately answering the question. Significantly, our study highlights that even with the advent of large language models such as GPT-3 and ChatGPT, utilizing a knowledge base remains crucial for accomplishing domain-specific tasks. This emphasizes the ongoing importance of incorporating domain-specific knowledge and context in language models to enhance their performance and effectiveness in specialized domains. We have successfully developed a dataset called AeroQA that caters explicitly to multi-hop questionanswering tasks in the aviation domain. This dataset is valuable for tasks involving reasoning and complex queries within the aviation domain. Furthermore, we have contributed an Aviation corpus, which is a useful resource for knowledge infusion tasks in language models (LMs). \n\nIn future work, there is a focus on harnessing the reasoning capabilities of knowledge bases to enhance tasks such as sentiment analysis and summarization, thereby improving the performance and contextual understanding of these NLP tasks. However, it is important to acknowledge the limitations of existing knowledge sources, such as knowledge graphs, and the abundance of available language models. Consequently, future efforts will explore ways to integrate domain-specific knowledge into language models, potentially replacing traditional knowledge bases. This integration aims to optimize the utilization of knowledge in a more efficient and effective manner.",
            "score": 0.387783048081406,
            "section_title": "CONCLUSION AND FUTURE WORK",
            "char_start_offset": 37362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "247450707",
            "title": "One Agent To Rule Them All: Towards Multi-agent Conversational AI",
            "text": "The rapid proliferation of conversational agents calls for a unified approach to interacting with multiple CAs. The key challenge of building such an interface lie in that most commercial CAs are black-boxes with hidden internals. This paper introduces BBAI a new task of agent integration that focuses on unifying black-boxes CAs across varying domains. We explore two task techniques, question agent pairing and question response pairing and present One For All, a scalable system that unifies multiple black-box CAs with a centralized user interface. Using a combination of commercially available conversational agents, we evaluate a variety of approaches to multi-agent integration through One For All. Our MARS encoder achieves 83.5% accuracy on BBAI and outperforms the best single agent configuration by over 32%. These results demonstrate the power of One For All which can leverage state-of-the-art NLU approaches to enable multiple agents to collectively achieve more than any single conversational agent in isolation eliminating the need for users to learn and adopt multiple agents.\n\nThis work opens up a wide range of potential future work involving the design of systems geared towards facilitating more advanced multi-agent interaction. We foresee a system with even greater response selection performance as the NLP community continues to produce more state-of-the-art language models with even greater contextual knowledge of the world. Extensions of this work can include examining not only the integration of agents but the interoperability by facilitating the passing of shared conversation knowledge across agents especially in multi-turn conversational scenarios across multiple agents.",
            "score": 0.387452606545677,
            "section_title": "Conclusion",
            "char_start_offset": 28921,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41455078125
        },
        {
            "corpus_id": "260334118",
            "title": "When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities",
            "text": "Tool learning is an emerging research field that aims to enhance task-solving capabilities by combining specialized tools with foundational models, which has been understood by [216] as two perspectives: \n\n1) Tool-augmented learning treats specialized tools as assistants in order for improving the quality and accuracy of tasks, or Tool for AI; 2) Tool-oriented learning focuses more on training models to effectively use tools, controlling and optimizing tool-applying processes, or AI for Tool. \n\nTool learning has found applications in various fields, and this section primarily focuses on tool learning paradigms based on large language models (LLMs). While recent works often involve a combination of these two perspectives, we do not specifically categorize each work into one type. LLMs, such as GPT, are well-suited for tool learning applications [217]. With their powerful natural language processing capabilities, LLMs can break down complex tasks into smaller sub-tasks and convert them into executable instructions. Specialized tools allow LLMs to access knowledge that is beyond their own understanding. By integrating specialized tools, LLMs can better understand and address complex problems, offering more accurate and efficient solutions. \n\nLLMs are commonly applied as controllers to select and manage various existing AI models to solve complex tasks, User-specified Tasks \n\nVisual ChatGPT [228] Taskmatrix.AI [229] Visual Foundation models Customized models with unified API form text-davinci-003 Visual Customized Tasks ReAct [214] Wikipedia API PaLM-540B Question Answering Face Verificaiton \n\nToolformer [230] Calculator Q&A system Search Engine Translation System Calendar GPT-J Downstream Tasks which rely on user input and language interfaces on making summarizations. They act as the central component, responsible for comprehending problem statements and making decisions regarding which actions to execute. Additionally, they aggregate the outcomes based on the results of the executed actions. In that case, HuggingGPT [227] leverages existing models from the Hugging Face community 1 to assist in task-solving.",
            "score": 0.3873595526756648,
            "section_title": "LLM-based Tool Learning",
            "char_start_offset": 65857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2156982421875
        },
        {
            "corpus_id": "271769807",
            "title": "Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation",
            "text": "We present Chamain, a methodology that allows for the incorporation of domain knowledge into character-specific models without additional training while preserving the models' personas. Chamain is designed to be easily integrated with existing model merging methods. It enhances downstream task performance across various domainspecific tasks, drawing enhancement directly from the character model. This offers a comprehensive solution for maintaining character consistency and domain accuracy simultaneously. Through Chamain, we aim to address the challenges of efficiently combining nuanced character traits with specialized domain knowledge in a unified model.",
            "score": 0.38732610360497016,
            "section_title": "Conclusion",
            "char_start_offset": 22461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "In this section, we first provide a more detailed discussion of task interference in the frequency domain in D.1. Next, we present the experimental results of FR-Merging on language models in D.2 and D.3. Additionally, we discuss the effectiveness and construction of the expert module in D.4 and D.5, as well as the role of the router in D.6. We also explore the selection of the cutoff frequency in D.7 and conclude with a discussion of future works in D.8.",
            "score": 0.38704866882156796,
            "section_title": "D. More Discussions and Future Directions",
            "char_start_offset": 35485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 459
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2183837890625
        },
        {
            "corpus_id": "254877510",
            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
            "text": "The key role in the model merging setup is played by the merging function g. We start with briefly introducing existing techniques for model merging, followed by the basic intuition for our proposed method, which we then extend to transformer-based language models. The underlying assumption is that the model architecture for all models f i is the same, allowing for element-wise operations if needed and resulting in a merged model f M K of the same architecture and size as any individual model. We also assume models are fine-tuned from the same pretrained LM checkpoint. The study of methods that relax this constraint are outside the scope of this paper and are left for future work.",
            "score": 0.38704866882156796,
            "section_title": "REGRESSION MEAN FOR MODEL MERGING",
            "char_start_offset": 6015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 689
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "272968986",
            "title": "Realistic Evaluation of Model Merging for Compositional Generalization",
            "text": "An exhaustive comparison of merging methods is beyond the scope of this work, so we focus on eight popular merging methods that represent the diversity of approaches. We discuss additional methods in Section 5. We use \u03b8 m to denote the parameters of the merged model, \u03b8 i i \u2208 {1, . . . , M } as the M constituent models being merged, and \u03b8 p as the base model which we assume all constituent models are fine-tuned from. Throughout this work, we assume all models are \"open vocabulary\", i.e., they use natural language for classification or generation and do not require task-specific classification heads.",
            "score": 0.38704866882156796,
            "section_title": "MERGING METHODS",
            "char_start_offset": 5721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.416748046875
        },
        {
            "corpus_id": "274609601",
            "title": "Multi-task learning with multi-gate mixture of transformer-based experts for predictive process monitoring in manufacturing",
            "text": "Soft parameter sharing involves using separate models for each task, but incorporating parametric relationships or differences into a joint objective function. Mechanisms like regularization or constraints encourage similarity or distance between task models, which aids knowledge transfer and efficient parameter use. This approach allows the multi-task model to leverage both commonalities and differences among tasks, improving performance and model quality for each task. MoE represents a significant advancement in flexible parameter sharing. Initially proposed by Jacobs et al., it divides a system into independent networks, each handling a portion of the data. 16 Shazeer et al. enhanced this concept with the Sparsely-Gated MoE layer, which integrates multiple experts and a trainable gating network. 47 This approach uses a divide-and-conquer strategy to address complex problems, improving efficiency and model generalization. Furthermore, Shazeer et al. applied MoE to natural language modeling and machine translation, while Riquelme et al. introduced the visual mixed expert (V-MoE) model for image classification. 48 he MoE model performs well in single-task scenarios but faces challenges in MTL due to complex inter-task relationships such as correlation and conflicts. 49 In MoE's MTL framework (as shown in Figure 1(c)), multiple tasks share a common set of experts and a single gating network, which may lead to conflicts and inefficiencies. To address these issues, MMoE 15 was introduced, utilizing multiple gating networks to enable task-specific expert selection and better capture task relationships. 50 Unlike MoE, which relies on one gating network for all tasks, MMoE allows for different expert selections for distinct tasks. 51 As indicated by Wang et al., 52 MMoE enhances MTL by allowing task-specific adjustments to expert networks and improving the modeling of task relationships, thereby boosting overall performance. As a form of soft parameter sharing, MMoE uses soft gating networks to aggregate experts learned from Wang et al. \n\ndifferent tasks, addressing negative migration problems effectively. It outperforms other methods, such as cross-stitch networks, 53 particularly in content recommendation. Various novel approaches based on the MMoE have emerged. For instance, Qin et al. 54 introduced the mixture of sequential experts (MoSE), which utilizes LSTM within an advanced MMoE framework to capture sequential user behavior.",
            "score": 0.3868479926989754,
            "section_title": "Multi-task learning",
            "char_start_offset": 16084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2298
                },
                {
                    "start": 2299,
                    "end": 2470
                }
            ],
            "ref_mentions": [
                {
                    "start": 669,
                    "end": 671,
                    "matchedPaperCorpusId": "572361"
                },
                {
                    "start": 810,
                    "end": 812,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 1129,
                    "end": 1131,
                    "matchedPaperCorpusId": "235417196"
                },
                {
                    "start": 1287,
                    "end": 1289,
                    "matchedPaperCorpusId": "253116858"
                },
                {
                    "start": 1492,
                    "end": 1494,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 1626,
                    "end": 1628,
                    "matchedPaperCorpusId": "248367384"
                },
                {
                    "start": 1755,
                    "end": 1757,
                    "matchedPaperCorpusId": "275955445"
                },
                {
                    "start": 1787,
                    "end": 1789,
                    "matchedPaperCorpusId": "251302720"
                },
                {
                    "start": 2199,
                    "end": 2201,
                    "matchedPaperCorpusId": "1923223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75
        },
        {
            "corpus_id": "267759980",
            "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
            "text": "We also propose an ensemble learning approach for performance improvement, utilizing fusion strategies at both the model level and the inference level. \n\nModel fusion. MergeKit1 is a toolkit designed for merging trained language models. We carefully selected a few high-accuracy models and utilized MergeKit to perform model fusion using the SLERP (Shoemake, 1985), TIES (Yadav et al., 2023) and linear (Wortsman et al., 2022) methods. Traditionally, model merging often resorts to weight averaging which, although straightforward, might not always capture the intricate features of the models being merged. The SLERP technique addresses this limitation, producing a blended model with characteristics smoothly interpolated from both parent models, ensuring the resultant model captures the essence of both its parents. Meanwhile, the TIES method is proposed to resolve interference issues by resetting parameters, resolving sign conflicts, and merging only compatible parameters. TIES outperforms many existing methods across diverse settings, emphasizing the importance of addressing interference in model merging for enhanced performance and versatility. \n\nModel Voting. In addition to the model-level fusion, we also explored fusion at the probability level of model generation, which can be understood as a form of model voting. We selected another group of highly accurate candidate models and performed linear fusion at the probability level. Specifically, we calculate the weighted summation of the probability values on \u00ebxisting hallucinationpredicted by different candidate models for different tasks. By tuning the linear weight combination, we are able to determine the optimal combination of weights for each task. Finally, combining different tasks together yields the final fusion result. In this way, we implement weighted voting of models at the inference result level.",
            "score": 0.3868386454927298,
            "section_title": "Ensemble Learning",
            "char_start_offset": 7732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 364,
                    "matchedPaperCorpusId": "11290566"
                },
                {
                    "start": 371,
                    "end": 391,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 403,
                    "end": 426,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "271903285",
            "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
            "text": "In recent years, the field of deep learning has witnessed an exponential growth in model sizes and dataset scales, making the training of large-scale deep models on extensive datasets increasingly cost-prohibitive, both in terms of financial resources and environmental impact [Minaee et al., 2024, Hadi et al., 2023]. Deep model fusion techniques have emerged as a promising solution, allowing the integration of knowledge from pre-existing models without the need for extensive retraining [Li et al., 2023, Zheng et al., 2023, Yang et al., 2024a]. This approach not only reduces computational costs but also enables the creation of more robust and versatile models by combining the strengths of multiple models. \n\nFollowing the categorization in Tang et al. [2024a], we classify model fusion methods into three main categories: model ensemble methods, model merging methods, and model mixing methods. Model ensemble techniques aggregate the predictions from several models to enhance performance [Sagi and Rokach, 2018]. While resource-intensive in terms of memory and computation, it improves knowledge distillation training [Wan et al., 2024a,b]. Model merging methods, on the other hand, combine the parameters of multiple models into a single model, often through weighted averaging or parameter alignment [Matena andRaffel, 2022, Jin et al., 2022]. Model mixing methods involve the integration of multiple models through gating mechanisms or depth concatenation, allowing for more flexible and adaptive fusion strategies [Komatsuzaki et al., 2022, Kim et al., 2023]. These methods are particularly effective in multi-task learning scenarios, where the merged model can simultaneously perform multiple tasks. \n\nHowever, despite the promising advancements in model fusion, several critical challenges persist, hindering the full realization of its potential. A primary concern is the potential interference between parameters of different models, which leads to suboptimal performance.",
            "score": 0.38634969584399315,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 998,
                    "end": 1021,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 1312,
                    "end": 1323,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52392578125
        },
        {
            "paperId": "24918dac9bf32448d8d765040c6d7dab0a0ee4fd",
            "corpusId": 278739786,
            "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
            "venue": "",
            "year": 2025,
            "referenceCount": 94,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2150449851",
                    "name": "Zihuan Qiu"
                },
                {
                    "authorId": "2321686264",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2190031555",
                    "name": "Chiyuan He"
                },
                {
                    "authorId": "1706784",
                    "name": "Fanman Meng"
                },
                {
                    "authorId": "47775696",
                    "name": "Linfeng Xu"
                },
                {
                    "authorId": "144816629",
                    "name": "Qingbo Wu"
                },
                {
                    "authorId": "2300984960",
                    "name": "Hongliang Li"
                }
            ],
            "abstract": "Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\\% on average across diverse task orders.",
            "corpus_id": "278739786",
            "text": "Continual model merging integrates independently fine-tuned models sequentially without access to original training data, providing a scalable and efficient solution to continual learning. However, current methods still face critical challenges, notably parameter interference among tasks and limited adaptability to evolving test distributions. The former causes catastrophic forgetting of integrated tasks, while the latter hinders effective adaptation to new tasks. To address these, we propose MINGLE, a novel framework for test-time continual model merging, which leverages test-time adaptation using a small set of unlabeled test samples from the current task to dynamically guide the merging process. MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation. Extensive experiments on standard continual merging benchmarks demonstrate that MINGLE achieves robust generalization, reduces forgetting significantly, and consistently surpasses previous state-of-the-art methods by 7-9\\% on average across diverse task orders.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.82666015625
        }
    ],
    "quotes": {
        "cost": 0.275262,
        "quotes": [
            {
                "idx": 0,
                "key": "[248227728 | Gupta et al. | 2022 | Citations: 50]",
                "snippets": "Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220265858 | Lepikhin et al. | 2020 | Citations: 1191]": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1665,
                        "end": 2134,
                        "sentence_offsets": [
                            {
                                "start": 1665,
                                "end": 1803
                            },
                            {
                                "start": 1804,
                                "end": 1974
                            },
                            {
                                "start": 1975,
                                "end": 2134
                            }
                        ],
                        "ref_mentions": [
                            "220265858"
                        ],
                        "quote": "Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266053657 | Tam et al. | 2023 | Citations: 12]",
                "snippets": "In our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1689,
                        "end": 2045,
                        "sentence_offsets": [
                            {
                                "start": 1689,
                                "end": 1866
                            },
                            {
                                "start": 1867,
                                "end": 2045
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[267365047 | Tang et al. | 2024 | Citations: 54]",
                "snippets": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1069,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270702345 | Lu et al. | 2024 | Citations: 63]",
                "snippets": "However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258865647 | Jiang et al. | 2023 | Citations: 22]": "Auxiliary-Task Learning (ATL) aims to improve the performance of the target task by leveraging the knowledge obtained from related tasks. Occasionally, learning multiple tasks simultaneously results in lower accuracy than learning only the target task, which is known as negative transfer. This problem is often attributed to the gradient conflicts among tasks, and is frequently tackled by coordinating the task gradients in previous works. However, these optimization-based methods largely overlook the auxiliary-target generalization capability. To better understand the root cause of negative transfer, we experimentally investigate it from both optimization and generalization perspectives. Based on our findings, we introduce ForkMerge, a novel approach that periodically forks the model into multiple branches, automatically searches the varying task weights by minimizing target validation errors, and dynamically merges all branches to filter out detrimental task-parameter updates. On a series of auxiliary-task learning benchmarks, ForkMerge outperforms existing methods and effectively mitigates negative transfer.",
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1868,
                        "end": 2200,
                        "sentence_offsets": [
                            {
                                "start": 1868,
                                "end": 2036
                            },
                            {
                                "start": 2037,
                                "end": 2200
                            }
                        ],
                        "ref_mentions": [
                            "258865647",
                            "259064039"
                        ],
                        "quote": "However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[271050386 | Lu et al. | 2024 | Citations: 27]",
                "snippets": "Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 662,
                        "end": 882,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[271064761 | Jin et al. | 2024 | Citations: 5]",
                "snippets": "During Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018)(Ilharco et al., 2022)(Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[254408495 | Ilharco et al. | 2022 | Citations: 520]": "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.",
                    "[4055784 | Garipov et al. | 2018 | Citations: 758]": "The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 1506,
                        "end": 2308,
                        "sentence_offsets": [
                            {
                                "start": 1506,
                                "end": 1529
                            },
                            {
                                "start": 1530,
                                "end": 1716
                            },
                            {
                                "start": 1717,
                                "end": 1928
                            },
                            {
                                "start": 1929,
                                "end": 2045
                            },
                            {
                                "start": 2046,
                                "end": 2308
                            }
                        ],
                        "ref_mentions": [
                            "4055784",
                            "254408495",
                            "247362886"
                        ],
                        "quote": "During Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018)(Ilharco et al., 2022)(Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271329267 | Hagos et al. | 2024 | Citations: 26]",
                "snippets": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[244345933 | Matena et al. | 2021 | Citations: 402]": "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."
                },
                "metadata": [
                    {
                        "section_title": "E. Model Merging",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1326,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 152
                            },
                            {
                                "start": 153,
                                "end": 287
                            },
                            {
                                "start": 288,
                                "end": 432
                            },
                            {
                                "start": 433,
                                "end": 708
                            },
                            {
                                "start": 709,
                                "end": 868
                            },
                            {
                                "start": 869,
                                "end": 1157
                            },
                            {
                                "start": 1158,
                                "end": 1326
                            }
                        ],
                        "ref_mentions": [
                            "244345933"
                        ],
                        "quote": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51]."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271843401 | Shi et al. | 2024 | Citations: 3]",
                "snippets": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1534,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 114,
                                "end": 298
                            },
                            {
                                "start": 299,
                                "end": 470
                            },
                            {
                                "start": 471,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 895
                            },
                            {
                                "start": 896,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1325
                            },
                            {
                                "start": 1326,
                                "end": 1534
                            }
                        ],
                        "ref_mentions": [
                            "247362886",
                            "259064039"
                        ],
                        "quote": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271947337 | Pourreza et al. | 2024 | Citations: 9]",
                "snippets": "Model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;(Yadav et al., 2023)(Yu et al., 2023) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values...More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[265034087 | Yu et al. | 2023 | Citations: 335]": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                },
                "metadata": [
                    {
                        "section_title": "A.1.2 Model Merging",
                        "pdf_hash": "",
                        "start": 322,
                        "end": 963,
                        "sentence_offsets": [
                            {
                                "start": 272,
                                "end": 359
                            },
                            {
                                "start": 360,
                                "end": 579
                            },
                            {
                                "start": 580,
                                "end": 667
                            },
                            {
                                "start": 668,
                                "end": 964
                            }
                        ],
                        "ref_mentions": [
                            "259064039",
                            "265034087"
                        ],
                        "quote": "Model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;(Yadav et al., 2023)(Yu et al., 2023) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values"
                    },
                    {
                        "section_title": "A.1.2 Model Merging",
                        "pdf_hash": "",
                        "start": 1092,
                        "end": 1529,
                        "sentence_offsets": [
                            {
                                "start": 1092,
                                "end": 1202
                            },
                            {
                                "start": 1203,
                                "end": 1337
                            },
                            {
                                "start": 1338,
                                "end": 1528
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271957310 | He et al. | 2024 | Citations: 25]",
                "snippets": "More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[210839011 | Yu et al. | 2020 | Citations: 1228]": "While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.",
                    "[239998731 | Liu et al. | 2021 | Citations: 318]": "The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average loss. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, CAGrad achieves improved performance over prior state-of-the-art multi-objective gradient manipulation methods."
                },
                "metadata": [
                    {
                        "section_title": "Related works",
                        "pdf_hash": "",
                        "start": 1060,
                        "end": 1673,
                        "sentence_offsets": [
                            {
                                "start": 1060,
                                "end": 1191
                            },
                            {
                                "start": 1192,
                                "end": 1319
                            },
                            {
                                "start": 1320,
                                "end": 1448
                            },
                            {
                                "start": 1449,
                                "end": 1673
                            }
                        ],
                        "ref_mentions": [
                            "210839011",
                            "239998731"
                        ],
                        "quote": "More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[272832307 | Zhao et al. | 2024 | Citations: 5]",
                "snippets": "Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C Related Works",
                        "pdf_hash": "",
                        "start": 454,
                        "end": 2264,
                        "sentence_offsets": [
                            {
                                "start": 454,
                                "end": 634
                            },
                            {
                                "start": 635,
                                "end": 862
                            },
                            {
                                "start": 863,
                                "end": 1005
                            },
                            {
                                "start": 1006,
                                "end": 1220
                            },
                            {
                                "start": 1221,
                                "end": 1358
                            },
                            {
                                "start": 1361,
                                "end": 1576
                            },
                            {
                                "start": 1577,
                                "end": 1766
                            },
                            {
                                "start": 1767,
                                "end": 2088
                            },
                            {
                                "start": 2089,
                                "end": 2264
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273228210 | Zhao et al. | 2024 | Citations: 5]",
                "snippets": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging (Shoemake, 1985)[24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models...Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion (Wan et al., 2024)[55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267061245 | Wan et al. | 2024 | Citations: 72]": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at \\url{https://github.com/fanqiwan/FuseLLM}."
                },
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1084,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 14
                            },
                            {
                                "start": 15,
                                "end": 101
                            },
                            {
                                "start": 102,
                                "end": 192
                            },
                            {
                                "start": 193,
                                "end": 360
                            },
                            {
                                "start": 361,
                                "end": 446
                            },
                            {
                                "start": 447,
                                "end": 515
                            },
                            {
                                "start": 516,
                                "end": 623
                            },
                            {
                                "start": 624,
                                "end": 775
                            },
                            {
                                "start": 776,
                                "end": 874
                            },
                            {
                                "start": 875,
                                "end": 985
                            },
                            {
                                "start": 986,
                                "end": 1085
                            }
                        ],
                        "ref_mentions": [
                            "11290566"
                        ],
                        "quote": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging (Shoemake, 1985)[24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models"
                    },
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 1223,
                        "end": 2248,
                        "sentence_offsets": [
                            {
                                "start": 1223,
                                "end": 1237
                            },
                            {
                                "start": 1238,
                                "end": 1391
                            },
                            {
                                "start": 1392,
                                "end": 1545
                            },
                            {
                                "start": 1546,
                                "end": 1715
                            },
                            {
                                "start": 1716,
                                "end": 1819
                            },
                            {
                                "start": 1820,
                                "end": 1962
                            },
                            {
                                "start": 1963,
                                "end": 2123
                            },
                            {
                                "start": 2124,
                                "end": 2248
                            }
                        ],
                        "ref_mentions": [
                            "267061245"
                        ],
                        "quote": "Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion (Wan et al., 2024)[55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273323680 | Gauthier-Caron et al. | 2024 | Citations: 2]",
                "snippets": "Model merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2023) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 808,
                        "end": 1797,
                        "sentence_offsets": [
                            {
                                "start": 808,
                                "end": 971
                            },
                            {
                                "start": 972,
                                "end": 1317
                            },
                            {
                                "start": 1320,
                                "end": 1514
                            },
                            {
                                "start": 1515,
                                "end": 1685
                            },
                            {
                                "start": 1686,
                                "end": 1797
                            }
                        ],
                        "ref_mentions": [
                            "247362886",
                            "259064039"
                        ],
                        "quote": "Model merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2023) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[273662099 | Shen et al. | 2024 | Citations: 9]",
                "snippets": "In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 528,
                        "end": 1089,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[274609601 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Soft parameter sharing involves using separate models for each task, but incorporating parametric relationships or differences into a joint objective function. Mechanisms like regularization or constraints encourage similarity or distance between task models, which aids knowledge transfer and efficient parameter use. This approach allows the multi-task model to leverage both commonalities and differences among tasks, improving performance and model quality for each task. MoE represents a significant advancement in flexible parameter sharing. Initially proposed by Jacobs et al., it divides a system into independent networks, each handling a portion of the data. 16 Shazeer et al. enhanced this concept with the Sparsely-Gated MoE layer, which integrates multiple experts and a trainable gating network. 47 This approach uses a divide-and-conquer strategy to address complex problems, improving efficiency and model generalization. Furthermore, Shazeer et al. applied MoE to natural language modeling and machine translation, while Riquelme et al. introduced the visual mixed expert (V-MoE) model for image classification. 48 he MoE model performs well in single-task scenarios but faces challenges in MTL due to complex inter-task relationships such as correlation and conflicts. 49 In MoE's MTL framework (as shown in Figure 1(c)), multiple tasks share a common set of experts and a single gating network, which may lead to conflicts and inefficiencies. To address these issues, MMoE 15 was introduced, utilizing multiple gating networks to enable task-specific expert selection and better capture task relationships. 50 Unlike MoE, which relies on one gating network for all tasks, MMoE allows for different expert selections for distinct tasks. 51 As indicated by Wang et al., 52 MMoE enhances MTL by allowing task-specific adjustments to expert networks and improving the modeling of task relationships, thereby boosting overall performance. As a form of soft parameter sharing, MMoE uses soft gating networks to aggregate experts learned from Wang et al. different tasks, addressing negative migration problems effectively. It outperforms other methods, such as cross-stitch networks, 53 particularly in content recommendation. Various novel approaches based on the MMoE have emerged. For instance, Qin et al. 54 introduced the mixture of sequential experts (MoSE), which utilizes LSTM within an advanced MMoE framework to capture sequential user behavior.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[12462234 | Shazeer et al. | 2017 | Citations: 2690]": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
                    "[1923223 | Misra et al. | 2016 | Citations: 1351]": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
                    "[235417196 | Riquelme et al. | 2021 | Citations: 609]": "Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are\"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.",
                    "[248367384 | Xin et al. | 2022 | Citations: 8]": "Multi-task learning (MTL) has been widely utilized in various industrial scenarios, such as recommender systems and search engines. MTL can improve learning efficiency and prediction accuracy by exploiting commonalities and differences across tasks. However, MTL is sensitive to relationships among tasks and may have performance degradation in real-world applications, because existing neural-based MTL models often share the same network structures and original input features. To address this issue, we propose a novel multi-task learning model based on Prototype Feature Extraction (PFE) to balance task-specific objectives and inter-task relationships. PFE is a novel component to disentangle features for multiple tasks. To better extract features from original inputs before gating networks, we introduce a new concept, namely prototype feature center, to disentangle features for multiple tasks. The extracted prototype features fuse various features from different tasks to better learn inter-task relationships. PFE updates prototype feature centers and prototype features iteratively. Our model utilizes the learned prototype features and task-specific experts for MTL. We implement PFE on two public datasets. Empirical results show that PFE outperforms state-of-the-art MTL models by extracting prototype features. Furthermore, we deploy PFE in a real-world recommender system (one of the world\u2019s top-tier short video sharing platforms) to showcase that PFE can be widely applied in industrial scenarios.",
                    "[251302720 | Wang et al. | 2022 | Citations: 12]": "Multi-task learning has been established as an important machine learning framework for leveraging shared knowledge among multiple different but related tasks, with the generalization performance of models enhanced. As a promising learning paradigm, multi-task learning has been widely adopted by various real-world applications, such as recommendation systems. Multi-gate Mixture-of-Experts (MMoE), a well-received multi-task learning method in industry, based on the classic and inspiring Mixture-of-Experts (MoE) structure, explicitly models task relationships and learns task-specific functionalities, generating significant improvements. However, in our applications, negative transfer, which confuses considerable existing multi-task learning methods, is still observed to happen to MMoE. In this paper, an in-depth empirical investigation into negative transfer is launched. And it reveals that, incompetent experts, which play fundamental roles under the learning framework of MoE, are the key technique bottleneck. To tackle this dilemma, we propose the Calibrated Mixture of Insightful Experts (CMoIE), with three novel modules (Conflict Resolution, Expert Communication, and Mixture Calibration), customed for multi-task learning. Hence a group of insightful experts are constructed with enhanced diversity, communication and specialization. To validate the proposed method CMoIE, experiments are conducted on three public datasets and one real-world click-through-rate prediction dataset we construct based on traffic logs collected from a large-scale online product recommendation system. Our approach yields best performance across all of these benchmarks, demonstrating the superiority of it.",
                    "[253116858 | Liang et al. | 2022 | Citations: 88]": "Multi-task learning (MTL) encapsulates multiple learned tasks in a single model and often lets those tasks learn better jointly. However, when deploying MTL onto those real-world systems that are often resource-constrained or latency-sensitive, two prominent challenges arise: (i) during training, simultaneously optimizing all tasks is often difficult due to gradient conflicts across tasks; (ii) at inference, current MTL regimes have to activate nearly the entire model even to just execute a single task. Yet most real systems demand only one or two tasks at each moment, and switch between tasks as needed: therefore such all tasks activated inference is also highly inefficient and non-scalable. In this paper, we present a model-accelerator co-design framework to enable efficient on-device MTL. Our framework, dubbed M$^3$ViT, customizes mixture-of-experts (MoE) layers into a vision transformer (ViT) backbone for MTL, and sparsely activates task-specific experts during training. Then at inference with any task of interest, the same design allows for activating only the task-corresponding sparse expert pathway, instead of the full model. Our new model design is further enhanced by hardware-level innovations, in particular, a novel computation reordering scheme tailored for memory-constrained MTL that achieves zero-overhead switching between tasks and can scale to any number of experts. When executing single-task inference, M$^{3}$ViT achieves higher accuracies than encoder-focused MTL methods, while significantly reducing 88% inference FLOPs. When implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design framework reduces the memory requirement by 2.4 times, while achieving energy efficiency up to 9.23 times higher than a comparable FPGA baseline. Code is available at: https://github.com/VITA-Group/M3ViT.",
                    "[50770252 | Ma et al. | 2018 | Citations: 1146]": "Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google."
                },
                "metadata": [
                    {
                        "section_title": "Multi-task learning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2468,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 159
                            },
                            {
                                "start": 160,
                                "end": 318
                            },
                            {
                                "start": 319,
                                "end": 475
                            },
                            {
                                "start": 476,
                                "end": 547
                            },
                            {
                                "start": 548,
                                "end": 671
                            },
                            {
                                "start": 672,
                                "end": 812
                            },
                            {
                                "start": 813,
                                "end": 937
                            },
                            {
                                "start": 938,
                                "end": 1131
                            },
                            {
                                "start": 1132,
                                "end": 1289
                            },
                            {
                                "start": 1290,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1628
                            },
                            {
                                "start": 1629,
                                "end": 1757
                            },
                            {
                                "start": 1758,
                                "end": 1952
                            },
                            {
                                "start": 1953,
                                "end": 2066
                            },
                            {
                                "start": 2069,
                                "end": 2137
                            },
                            {
                                "start": 2138,
                                "end": 2241
                            },
                            {
                                "start": 2242,
                                "end": 2298
                            },
                            {
                                "start": 2299,
                                "end": 2470
                            }
                        ],
                        "ref_mentions": [
                            "572361",
                            "12462234",
                            "235417196",
                            "253116858",
                            "50770252",
                            "248367384",
                            "275955445",
                            "251302720",
                            "1923223"
                        ],
                        "quote": "Soft parameter sharing involves using separate models for each task, but incorporating parametric relationships or differences into a joint objective function. Mechanisms like regularization or constraints encourage similarity or distance between task models, which aids knowledge transfer and efficient parameter use. This approach allows the multi-task model to leverage both commonalities and differences among tasks, improving performance and model quality for each task. MoE represents a significant advancement in flexible parameter sharing. Initially proposed by Jacobs et al., it divides a system into independent networks, each handling a portion of the data. 16 Shazeer et al. enhanced this concept with the Sparsely-Gated MoE layer, which integrates multiple experts and a trainable gating network. 47 This approach uses a divide-and-conquer strategy to address complex problems, improving efficiency and model generalization. Furthermore, Shazeer et al. applied MoE to natural language modeling and machine translation, while Riquelme et al. introduced the visual mixed expert (V-MoE) model for image classification. 48 he MoE model performs well in single-task scenarios but faces challenges in MTL due to complex inter-task relationships such as correlation and conflicts. 49 In MoE's MTL framework (as shown in Figure 1(c)), multiple tasks share a common set of experts and a single gating network, which may lead to conflicts and inefficiencies. To address these issues, MMoE 15 was introduced, utilizing multiple gating networks to enable task-specific expert selection and better capture task relationships. 50 Unlike MoE, which relies on one gating network for all tasks, MMoE allows for different expert selections for distinct tasks. 51 As indicated by Wang et al., 52 MMoE enhances MTL by allowing task-specific adjustments to expert networks and improving the modeling of task relationships, thereby boosting overall performance. As a form of soft parameter sharing, MMoE uses soft gating networks to aggregate experts learned from Wang et al. different tasks, addressing negative migration problems effectively. It outperforms other methods, such as cross-stitch networks, 53 particularly in content recommendation. Various novel approaches based on the MMoE have emerged. For instance, Qin et al. 54 introduced the mixture of sequential experts (MoSE), which utilizes LSTM within an advanced MMoE framework to capture sequential user behavior."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[275119334 | Jung et al. | 2024 | Citations: 1]",
                "snippets": "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge (Caruana, 1997)(Vandenhende et al., 2020)(Zhang et al., 2020). However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference (Ma et al., 2018)(Misra et al., 2016)(Rosenbaum et al., 2017). Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts (Chen et al., 2017)(Yu et al., 2020). Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating (Guo et al., 2018)(Hu et al., 2023)(Kendall et al., 2017)(Liu et al., 2018)(Sener et al., 2018). Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework (Ghiasi et al., 2021)(Jacob et al., 2023)[70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1923223 | Misra et al. | 2016 | Citations: 1351]": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multitask learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
                    "[210839011 | Yu et al. | 2020 | Citations: 1228]": "While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.",
                    "[22014305 | Rosenbaum et al. | 2017 | Citations: 248]": "Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",
                    "[221771219 | Vandenhende et al. | 2020 | Citations: 726]": "With the advent of deep learning, many dense prediction tasks, i.e., tasks that produce pixel-level predictions, have seen significant performance improvements. The typical approach is to learn these tasks in isolation, that is, a separate neural network is trained for each individual task. Yet, recent multi-task learning (MTL) techniques have shown promising results w.r.t. performance, computations and/or memory footprint, by jointly tackling multiple tasks through a learned shared representation. In this survey, we provide a well-rounded view on state-of-the-art deep learning approaches for MTL in computer vision, explicitly emphasizing on dense prediction tasks. Our contributions concern the following. First, we consider MTL from a network architecture point-of-view. We include an extensive overview and discuss the advantages/disadvantages of recent popular MTL models. Second, we examine various optimization methods to tackle the joint learning of multiple tasks. We summarize the qualitative elements of these works and explore their commonalities and differences. Finally, we provide an extensive experimental evaluation across a variety of dense prediction benchmarks to examine the pros and cons of the different methods, including both architectural and optimization based strategies.",
                    "[235790783 | Zhang et al. | 2020 | Citations: 221]": "Transfer learning (TL) utilizes data or knowledge from one or more source domains to facilitate learning in a target domain. It is particularly useful when the target domain has very few or no labeled data, due to annotation expense, privacy concerns, etc. Unfortunately, the effectiveness of TL is not always guaranteed. Negative transfer (NT), i.e., leveraging source domain data/knowledge undesirably reduces learning performance in the target domain, and has been a long-standing and challenging problem in TL. Various approaches have been proposed in the literature to address this issue. However, there does not exist a systematic survey. This paper fills this gap, by first introducing the definition of NT and its causes, and reviewing over fifty representative approaches for overcoming NT, which fall into three categories: domain similarity estimation, safe transfer, and NT mitigation. Many areas, including computer vision, bioinformatics, natural language processing, recommender systems, and robotics, that use NT mitigation strategies to facilitate positive transfers, are also reviewed. Finally, we give guidelines on NT task construction and baseline algorithms, benchmark existing TL and NT mitigation approaches on three NT-specific datasets, and point out challenges and future research directions. To ensure reproducibility, our code is publicized at https://github.com/chamwen/NT-Benchmark.",
                    "[237291521 | Ghiasi et al. | 2021 | Citations: 101]": "Despite the fast progress in training specialized models for various tasks, learning a single general model that works well for many tasks is still challenging for computer vision. Here we introduce multi-task self-training (MuST), which harnesses the knowledge in independent specialized teacher models (e.g., ImageNet model on classification) to train a single general student model. Our approach has three steps. First, we train specialized teachers independently on labeled datasets. We then use the specialized teachers to label an unlabeled dataset to create a multi-task pseudo labeled dataset. Finally, the dataset, which now contains pseudo labels from teacher models trained on different datasets/tasks, is then used to train a student model with multi-task learning. We evaluate the feature representations of the student model on 6 vision tasks including image recognition (classification, detection, segmentation) and 3D geometry estimation (depth and surface normal estimation). MuST is scalable with unlabeled or partially labeled datasets and outperforms both specialized supervised models and self-supervised models when training on large scale datasets. Lastly, we show MuST can improve upon already strong checkpoints [23] trained with billions of examples. The results suggest self-training is a promising direction to aggregate labeled and unlabeled training data for learning general feature representations.",
                    "[256658804 | Jacob et al. | 2023 | Citations: 16]": "Multi-task learning (MTL) has found wide application in computer vision tasks. We train a backbone network to learn a shared representation for different tasks such as semantic segmentation, depth- and normal estimation. In many cases negative transfer, i.e. impaired performance in the target domain, causes the MTL accuracy to be lower than training the corresponding single-task networks. To mitigate this issue, we propose an online knowledge distillation method, where single-task networks are trained simultaneously with the MTL network to guide the optimization process. We propose selectively training layers for each task using an adaptive feature distillation (AFD) loss with an online task weighting (OTW) scheme. This task-wise feature distillation enables the MTL network to be trained in a similar way to the single-task networks. On the NYUv2 and Cityscapes datasets we show improvements over a baseline MTL model by 6.22% and 9.19%, respectively, outperforming recent MTL methods. We validate the design choices in ablative experiments, including the use of online task weighting and the adaptive feature distillation loss.",
                    "[261243229 | Hu et al. | 2023 | Citations: 42]": "Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanced trade-offs between multiple tasks. More concretely, when the model is under-parametrized, we reveal a multi-surface structure of the feasible region and identify necessary and sufficient conditions for full exploration. This leads to the conclusion that scalarization is in general incapable of tracing out the Pareto front. Our theoretical results partially answer the open questions in Xin et al. (2021), and provide a more intuitive explanation on why scalarization fails beyond non-convexity. We additionally perform experiments on a real-world dataset using both scalarization and state-of-the-art SMTOs. The experimental results not only corroborate our theoretical findings, but also unveil the potential of SMTOs in finding balanced solutions, which cannot be achieved by scalarization.",
                    "[4389348 | Liu et al. | 2018 | Citations: 1056]": "We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of task-specific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-to-image predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/lorenmt/mtan.",
                    "[4703661 | Chen et al. | 2017 | Citations: 1291]": "Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",
                    "[4800342 | Kendall et al. | 2017 | Citations: 3136]": "Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.",
                    "[50770252 | Ma et al. | 2018 | Citations: 1146]": "Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google.",
                    "[52957972 | Sener et al. | 2018 | Citations: 1289]": "In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training."
                },
                "metadata": [
                    {
                        "section_title": "Multi-task Learning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1063,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 123
                            },
                            {
                                "start": 124,
                                "end": 199
                            },
                            {
                                "start": 200,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 487
                            },
                            {
                                "start": 488,
                                "end": 634
                            },
                            {
                                "start": 635,
                                "end": 762
                            },
                            {
                                "start": 763,
                                "end": 892
                            },
                            {
                                "start": 893,
                                "end": 1063
                            }
                        ],
                        "ref_mentions": [
                            "45998148",
                            "221771219",
                            "235790783",
                            "50770252",
                            "1923223",
                            "22014305",
                            "4703661",
                            "210839011",
                            "52952193",
                            "261243229",
                            "4800342",
                            "4389348",
                            "52957972",
                            "237291521",
                            "256658804"
                        ],
                        "quote": "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge (Caruana, 1997)(Vandenhende et al., 2020)(Zhang et al., 2020). However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference (Ma et al., 2018)(Misra et al., 2016)(Rosenbaum et al., 2017). Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts (Chen et al., 2017)(Yu et al., 2020). Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating (Guo et al., 2018)(Hu et al., 2023)(Kendall et al., 2017)(Liu et al., 2018)(Sener et al., 2018). Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework (Ghiasi et al., 2021)(Jacob et al., 2023)[70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[276095183 | Zhou et al. | 2025 | Citations: 3]",
                "snippets": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1034,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[276409347 | Liu et al. | 2025 | Citations: 2]",
                "snippets": "Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[254408495 | Ilharco et al. | 2022 | Citations: 520]": "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.",
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[259088823 | Muqeeth et al. | 2023 | Citations: 53]": "Sparsely activated neural networks with conditional computation learn to route their inputs through different\"expert\"subnetworks, providing a form of modularity that densely activated models lack. Despite their possible benefits, models with learned routing often underperform their parameter-matched densely activated counterparts as well as models that use non-learned heuristic routing strategies. In this paper, we hypothesize that these shortcomings stem from the gradient estimation techniques used to train sparsely activated models that use non-differentiable discrete routing decisions. To address this issue, we introduce Soft Merging of Experts with Adaptive Routing (SMEAR), which avoids discrete routing by using a single\"merged\"expert constructed via a weighted average of all of the experts' parameters. By routing activations through a single merged expert, SMEAR does not incur a significant increase in computational costs and enables standard gradient-based training. We empirically validate that models using SMEAR outperform models that route based on metadata or learn sparse routing through gradient estimation. Furthermore, we provide qualitative analysis demonstrating that the experts learned via SMEAR exhibit a significant amount of specialization. All of the code used in our experiments is publicly available.",
                    "[265034087 | Yu et al. | 2023 | Citations: 335]": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 430,
                        "end": 1774,
                        "sentence_offsets": [
                            {
                                "start": 357,
                                "end": 469
                            },
                            {
                                "start": 470,
                                "end": 567
                            },
                            {
                                "start": 568,
                                "end": 859
                            },
                            {
                                "start": 862,
                                "end": 1147
                            },
                            {
                                "start": 1148,
                                "end": 1306
                            },
                            {
                                "start": 1307,
                                "end": 1531
                            },
                            {
                                "start": 1532,
                                "end": 1715
                            },
                            {
                                "start": 1716,
                                "end": 1913
                            }
                        ],
                        "ref_mentions": [
                            "247362886",
                            "254408495",
                            "259064039",
                            "265034087",
                            "254408495",
                            "259064039",
                            "265034087",
                            "259088823"
                        ],
                        "quote": "Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[276422064 | Liu et al. | 2025 | Citations: 3]",
                "snippets": "In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[265034087 | Yu et al. | 2023 | Citations: 335]": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 615,
                        "end": 2126,
                        "sentence_offsets": [
                            {
                                "start": 574,
                                "end": 804
                            },
                            {
                                "start": 807,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1132
                            },
                            {
                                "start": 1133,
                                "end": 1344
                            },
                            {
                                "start": 1345,
                                "end": 1465
                            },
                            {
                                "start": 1466,
                                "end": 1682
                            },
                            {
                                "start": 1685,
                                "end": 1892
                            },
                            {
                                "start": 1893,
                                "end": 2040
                            },
                            {
                                "start": 2041,
                                "end": 2183
                            }
                        ],
                        "ref_mentions": [
                            "259064039",
                            "265034087",
                            "259064039",
                            "265034087"
                        ],
                        "quote": "In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[276422131 | Rousset et al. | 2025 | Citations: 1]",
                "snippets": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Merging for LLM Domain Adaptation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 867,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 125
                            },
                            {
                                "start": 126,
                                "end": 306
                            },
                            {
                                "start": 307,
                                "end": 528
                            },
                            {
                                "start": 529,
                                "end": 680
                            },
                            {
                                "start": 681,
                                "end": 867
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[276575054 | Gu et al. | 2025 | Citations: 3]",
                "snippets": "To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 673,
                        "end": 1997,
                        "sentence_offsets": [
                            {
                                "start": 654,
                                "end": 859
                            },
                            {
                                "start": 862,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1072
                            },
                            {
                                "start": 1075,
                                "end": 1280
                            },
                            {
                                "start": 1281,
                                "end": 1451
                            },
                            {
                                "start": 1452,
                                "end": 1620
                            },
                            {
                                "start": 1623,
                                "end": 1747
                            },
                            {
                                "start": 1748,
                                "end": 1933
                            },
                            {
                                "start": 1934,
                                "end": 2033
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[276575632 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[276580914 | Feng et al. | 2025 | Citations: 1]",
                "snippets": "Recently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023)Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024)Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024)Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258833488 | Chen et al. | 2023 | Citations: 50]": "Pretraining on a large-scale corpus has become a standard method to build general language models (LMs). Adapting a model to new data distributions targeting different downstream tasks poses significant challenges. Naive fine-tuning may incur catastrophic forgetting when the over-parameterized LMs overfit the new data but fail to preserve the pretrained features. Lifelong learning (LLL) aims to enable information systems to learn from a continuous data stream across time. However, most prior work modifies the training recipe assuming a static fixed network architecture. We find that additional model capacity and proper regularization are key elements to achieving strong LLL performance. Thus, we propose Lifelong-MoE, an extensible MoE (Mixture-of-Experts) architecture that dynamically adds model capacity via adding experts with regularized pretraining. Our results show that by only introducing a limited number of extra experts while keeping the computation cost constant, our model can steadily adapt to data distribution shifts while preserving the previous knowledge. Compared to existing lifelong learning approaches, Lifelong-MoE achieves better few-shot performance on 19 downstream NLP tasks.",
                    "[270703371 | Zhu et al. | 2024 | Citations: 88]": "Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, training MoE from scratch in a large-scale setting still suffers from data-hungry and instability problems. Motivated by this limit, we investigate building MoE models from existing dense large language models. Specifically, based on the well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert Construction, which partitions the parameters of original Feed-Forward Networks (FFNs) into multiple experts; (2) Continual pre-training, which further trains the transformed MoE model and additional gate networks. In this paper, we comprehensively explore different methods for expert construction and various data sampling strategies for continual pre-training. After these stages, our LLaMA-MoE models could maintain language abilities and route the input tokens to specific experts with part of the parameters activated. Empirically, by training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense models that contain similar activation parameters.",
                    "[271915471 | Dou et al. | 2024 | Citations: 55]": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novel framework that introduces several low-rank adapters (LoRA) and integrates them by us-ing a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of Lo-RAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, Lo-RAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/ Ablustrund/LoRAMoE ."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 679,
                        "end": 1914,
                        "sentence_offsets": [
                            {
                                "start": 679,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 1025
                            },
                            {
                                "start": 1026,
                                "end": 1239
                            },
                            {
                                "start": 1240,
                                "end": 1440
                            },
                            {
                                "start": 1443,
                                "end": 1734
                            },
                            {
                                "start": 1735,
                                "end": 1804
                            },
                            {
                                "start": 1805,
                                "end": 1914
                            }
                        ],
                        "ref_mentions": [
                            "258833488",
                            "270703371",
                            "271915471"
                        ],
                        "quote": "Recently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023)Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024)Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024)Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[276813020 | Yang et al. | 2025 | Citations: 3]",
                "snippets": "Various strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023)(Wang et al., 2024) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024)Hu et al., 2024;(Ong et al., 2024) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[259075564 | Jiang et al. | 2023 | Citations: 333]": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap.",
                    "[269303119 | Ding et al. | 2024 | Citations: 106]": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
                    "[270357878 | Wang et al. | 2024 | Citations: 136]": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.",
                    "[270764307 | Ong et al. | 2024 | Citations: 104]": "Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1075,
                        "end": 2348,
                        "sentence_offsets": [
                            {
                                "start": 1075,
                                "end": 1159
                            },
                            {
                                "start": 1160,
                                "end": 1298
                            },
                            {
                                "start": 1299,
                                "end": 1421
                            },
                            {
                                "start": 1422,
                                "end": 1587
                            },
                            {
                                "start": 1588,
                                "end": 1731
                            },
                            {
                                "start": 1732,
                                "end": 1944
                            },
                            {
                                "start": 1945,
                                "end": 2156
                            },
                            {
                                "start": 2157,
                                "end": 2348
                            }
                        ],
                        "ref_mentions": [
                            "259075564",
                            "270357878",
                            "269303119",
                            "270764307",
                            "247362886"
                        ],
                        "quote": "Various strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023)(Wang et al., 2024) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024)Hu et al., 2024;(Ong et al., 2024) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276937513 | Ruan et al. | 2025 | Citations: 0]",
                "snippets": "Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1678,
                        "end": 2247,
                        "sentence_offsets": [
                            {
                                "start": 1661,
                                "end": 1849
                            },
                            {
                                "start": 1850,
                                "end": 2059
                            },
                            {
                                "start": 2060,
                                "end": 2247
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276938164 | Yang et al. | 2025 | Citations: 0]",
                "snippets": "To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1605,
                        "end": 2188,
                        "sentence_offsets": [
                            {
                                "start": 1579,
                                "end": 1703
                            },
                            {
                                "start": 1704,
                                "end": 1803
                            },
                            {
                                "start": 1804,
                                "end": 1926
                            },
                            {
                                "start": 1929,
                                "end": 2033
                            },
                            {
                                "start": 2034,
                                "end": 2194
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276961298 | Fang et al. | 2025 | Citations: 0]",
                "snippets": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1333,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 161
                            },
                            {
                                "start": 162,
                                "end": 399
                            },
                            {
                                "start": 402,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 794
                            },
                            {
                                "start": 797,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1095
                            },
                            {
                                "start": 1098,
                                "end": 1195
                            },
                            {
                                "start": 1196,
                                "end": 1333
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[277065877 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient.\n\nSimple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient.\n\nSimple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[277322544 | Wu et al. | 2025 | Citations: 14]",
                "snippets": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.\n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments.\n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.\n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments.\n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[277510128 | Shan et al. | 2025 | Citations: 3]",
                "snippets": "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "MoE For Memory Parameterization",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1415,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 229
                            },
                            {
                                "start": 230,
                                "end": 375
                            },
                            {
                                "start": 378,
                                "end": 422
                            },
                            {
                                "start": 423,
                                "end": 704
                            },
                            {
                                "start": 705,
                                "end": 821
                            },
                            {
                                "start": 822,
                                "end": 959
                            },
                            {
                                "start": 960,
                                "end": 1094
                            },
                            {
                                "start": 1095,
                                "end": 1195
                            },
                            {
                                "start": 1196,
                                "end": 1415
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[277940324 | Coleman et al. | 2025 | Citations: 0]",
                "snippets": "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss...For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1230,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 456
                            },
                            {
                                "start": 457,
                                "end": 679
                            },
                            {
                                "start": 680,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 978
                            },
                            {
                                "start": 979,
                                "end": 1231
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss"
                    },
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 1831,
                        "end": 2575,
                        "sentence_offsets": [
                            {
                                "start": 1831,
                                "end": 2003
                            },
                            {
                                "start": 2004,
                                "end": 2123
                            },
                            {
                                "start": 2124,
                                "end": 2227
                            },
                            {
                                "start": 2228,
                                "end": 2345
                            },
                            {
                                "start": 2346,
                                "end": 2574
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[278501405 | Sun et al. | 2025 | Citations: 2]",
                "snippets": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 805,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[278739786 | Qiu et al. | 2025 | Citations: 0]",
                "snippets": "MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.042435,
        "cot": "I need to cluster the quotes into meaningful dimensions that address the user's query about technical methods for merging or integrating multiple specialized language models into a unified system, and how these approaches address practical challenges.\n\nLet me identify the key dimensions that would make for a clear and comprehensive answer:\n\n1. **Introduction/Background** - I'll start with a section that provides the context for model merging, explaining why it's important and the general problem it aims to solve. This should be a synthesis paragraph to give readers a foundation for understanding the rest of the content.\n\n2. **Parameter-Space Merging Methods** - Many quotes discuss various techniques for merging models in parameter space, such as Task Arithmetic, TIES, DARE, etc. This section will cover these core merging approaches. This should be a synthesis paragraph to explain and compare these methods.\n\n3. **Mixture-of-Experts (MoE) Architectures** - There are numerous quotes about MoE approaches, which represent a different paradigm from static merging. This should be a synthesis paragraph to explain how MoE works for model integration.\n\n4. **Dynamic Gating and Routing Mechanisms** - Several quotes mention routing mechanisms, gating networks, and dynamic integration. This is closely related to MoE but deserves its own section as it addresses how systems dynamically select which parts of merged models to use. This should be a synthesis paragraph.\n\n5. **Addressing Task Interference and Knowledge Conflicts** - Many quotes specifically discuss solutions to parameter interference, knowledge conflicts, and task conflicts. This should be a synthesis paragraph explaining these challenges and their solutions.\n\n6. **Advanced Integration Techniques** - Some quotes mention more sophisticated approaches like sensitivity-guided merging, conflict-aware merging, and null-space constrained gating. This should be a synthesis paragraph covering these cutting-edge methods.\n\n7. **Practical Challenges and Solutions** - The quotes address various practical implementation challenges and proposed solutions. This should be a synthesis paragraph to cover computational efficiency, storage constraints, and other practical considerations.\n\nThese dimensions collectively cover the technical methods mentioned in the user query (parameter-space merging, MoE architectures, dynamic gating) while also addressing the practical challenges like task interference and knowledge conflicts.",
        "plan": {
            "Introduction to Model Merging and Integration (synthesis)": [],
            "Parameter-Space Merging Methods (synthesis)": [
                1,
                4,
                5,
                6,
                7,
                8,
                11,
                12,
                17,
                18,
                21,
                22,
                24,
                26,
                27,
                28
            ],
            "Mixture-of-Experts (MoE) Architectures (synthesis)": [
                0,
                2,
                9,
                10,
                11,
                13,
                14,
                16,
                20,
                25,
                29,
                32
            ],
            "Dynamic Gating and Routing Mechanisms (synthesis)": [
                0,
                2,
                10,
                13,
                14,
                17,
                23,
                25,
                29,
                32
            ],
            "Addressing Task Interference and Knowledge Conflicts (synthesis)": [
                2,
                3,
                5,
                9,
                15,
                18,
                24,
                27,
                28,
                30,
                31
            ],
            "Advanced Integration Techniques (synthesis)": [
                8,
                16,
                18,
                19,
                21,
                24,
                25,
                28,
                31,
                32
            ],
            "Practical Challenges and Solutions (synthesis)": [
                3,
                6,
                12,
                16,
                17,
                19,
                20,
                22,
                23,
                30
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Model Merging and Integration",
                "tldr": "Model merging and integration techniques combine multiple specialized language models into unified systems that leverage their collective strengths. These approaches aim to create more capable AI systems that can handle diverse tasks without the computational costs of running multiple separate models. (LLM Memory)",
                "text": "\nModel merging and integration represent an emerging paradigm in language model development that seeks to combine the capabilities of multiple specialized models into unified systems. As language models continue to grow in size and specificity, researchers have recognized that different models often excel at different tasks or possess complementary knowledge. Rather than maintaining numerous separate models or training increasingly massive general-purpose models, merging techniques aim to create integrated systems that combine the strengths of multiple specialized models. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nAt its core, model merging addresses a fundamental challenge in AI development: the tension between specialized expertise and broad capabilities. Specialized models often perform exceptionally well on narrow tasks but fail to generalize, while general models may have broad capabilities but lack depth in specific domains. Integration techniques provide a middle path by allowing multiple specialized capabilities to coexist within a unified framework. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe field has developed several distinct approaches to model merging, including parameter-space merging (where model weights are directly combined), mixture-of-experts architectures (where specialized sub-models handle different inputs), and dynamic routing mechanisms (where inputs are directed to appropriate processing pathways). These techniques vary in their implementation complexity, computational requirements, and effectiveness across different applications. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research demonstrates that properly integrated models can not only maintain the capabilities of their constituent parts but sometimes exhibit emergent abilities that weren't present in any individual model. This synergistic effect makes model merging particularly attractive as a path toward more capable AI systems without the computational and data requirements of training increasingly large models from scratch. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter-Space Merging Methods",
                "tldr": "Parameter-space merging techniques directly combine weights from multiple models to create unified systems without requiring additional training. These methods range from simple weight averaging to sophisticated approaches that address parameter conflicts and redundancy. (8 sources)",
                "text": "\nParameter-space merging represents a direct approach to model integration that operates by combining the weights of multiple specialized models in weight space. These techniques enable the creation of unified models without the computational expense of retraining, making them increasingly popular for integrating specialized language models.\n\nThe simplest form of parameter-space merging is weight averaging, where parameters from different models are directly averaged. Model Soups <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper> demonstrated that averaging weights of models fine-tuned with different hyperparameter configurations can significantly improve accuracy and robustness without increasing inference costs. This approach works particularly well when models are trained on the same task but with different configurations.\n\nFor merging models trained on different tasks, Task Arithmetic <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper> introduced the concept of \"task vectors\" - directions in parameter space that represent the knowledge gained during fine-tuning for specific tasks. By subtracting the weights of a pre-trained model from its fine-tuned version, task vectors can be combined through operations like addition to create models capable of performing multiple tasks simultaneously.\n\nHowever, naively combining parameters often leads to interference between tasks. TIES-Merging <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> addresses this challenge through a three-step process: (1) resetting parameters that changed minimally during fine-tuning, (2) resolving sign conflicts between parameters, and (3) merging only parameters that align with the agreed-upon sign. This method significantly reduces interference when combining multiple task-specific models.\n\nBuilding on this concept, DARE <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper> introduced a technique that randomly drops delta parameters (differences between fine-tuned and pre-trained parameters) and rescales the remaining ones to approximate original embeddings. This approach effectively eliminates redundancy while preserving model capabilities, allowing even large language models to acquire new capabilities by assimilating parameters from other models without retraining.\n\nMore advanced approaches conceptualize merging as operating in \"task parameter subspaces\" - dimensions in parameter space that are important for specific tasks <Paper corpusId=\"266053657\" paperTitle=\"(Tam et al., 2023)\" isShortName></Paper>. Fisher Merging <Paper corpusId=\"244345933\" paperTitle=\"(Matena et al., 2021)\" isShortName></Paper> uses the Fisher information matrix to weight parameters according to their importance for specific tasks, providing a theoretically grounded approach to merging.\n\nRecent developments have introduced sensitivity-guided merging approaches that recognize not all parameters contribute equally to model performance. Sens-Merging performs parameter sensitivity analysis within individual tasks and task sensitivity analysis across different tasks to prioritize parameters that significantly impact performance <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Similarly, LoRS-Merging combines low-rank and sparse pruning techniques to retain essential structures while eliminating redundant parameters <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nParameter-space merging techniques offer a resource-efficient alternative to training increasingly large models, allowing existing specialized models to be combined into unified systems with broader capabilities. These methods continue to evolve, with increasingly sophisticated approaches addressing challenges such as parameter redundancy, interference between tasks, and model architecture differences.",
                "citations": [
                    {
                        "id": "(Wortsman et al., 2022)",
                        "snippets": [
                            "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
                        ],
                        "paper": {
                            "corpus_id": 247362886,
                            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                            "authors": [
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "1387466862",
                                    "name": "S. Gadre"
                                },
                                {
                                    "authorId": "40458654",
                                    "name": "R. Roelofs"
                                },
                                {
                                    "authorId": "2158366935",
                                    "name": "Raphael Gontijo-Lopes"
                                },
                                {
                                    "authorId": "4690624",
                                    "name": "Ari S. Morcos"
                                },
                                {
                                    "authorId": "40281109",
                                    "name": "Hongseok Namkoong"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1011
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ilharco et al., 2022)",
                        "snippets": [
                            "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."
                        ],
                        "paper": {
                            "corpus_id": 254408495,
                            "title": "Editing Models with Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "78846919",
                                    "name": "Marco Tulio Ribeiro"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 520
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                        ],
                        "paper": {
                            "corpus_id": 265034087,
                            "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
                            "authors": [
                                {
                                    "authorId": "2265527327",
                                    "name": "Le Yu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "46493167",
                                    "name": "Haiyang Yu"
                                },
                                {
                                    "authorId": "2257407873",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "1527090216",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 335
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tam et al., 2023)",
                        "snippets": [
                            "In our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task."
                        ],
                        "paper": {
                            "corpus_id": 266053657,
                            "title": "Merging by Matching Models in Task Parameter Subspaces",
                            "authors": [
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "2253762115",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 12
                        },
                        "score": 0.6708984375
                    },
                    {
                        "id": "(Matena et al., 2021)",
                        "snippets": [
                            "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."
                        ],
                        "paper": {
                            "corpus_id": 244345933,
                            "title": "Merging Models with Fisher-Weighted Averaging",
                            "authors": [
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 402
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                        ],
                        "paper": {
                            "corpus_id": 276422064,
                            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.68359375
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility."
                        ],
                        "paper": {
                            "corpus_id": 276575632,
                            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                            "authors": [
                                {
                                    "authorId": "2241702793",
                                    "name": "Qiuming Zhao"
                                },
                                {
                                    "authorId": "2107310187",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "2256775692",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2241950375",
                                    "name": "Mingxing Xu"
                                },
                                {
                                    "authorId": "2241350908",
                                    "name": "Thomas Fang Zheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.68603515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mixture-of-Experts (MoE) Architectures",
                "tldr": "Mixture-of-Experts (MoE) architectures integrate multiple specialized models by using routing mechanisms to dynamically direct inputs to appropriate expert components. This approach enables efficient scaling of model capacity while maintaining computational efficiency through conditional computation, where only relevant expert pathways are activated for each input. (13 sources)",
                "text": "\nMixture-of-Experts (MoE) architectures represent a sophisticated approach to model integration that dynamically combines specialized components based on input characteristics. Unlike parameter-space merging methods that create static unified models, MoE architectures maintain distinct \"expert\" networks that are selectively activated through trainable routing mechanisms.\n\nAt their core, MoE models employ conditional computation, where only certain weights are activated per input as determined by a gating mechanism. This approach offers a significant advantage: substantial increases in model parameters while keeping computational costs relatively constant <Paper corpusId=\"248227728\" paperTitle=\"(Gupta et al., 2022)\" isShortName></Paper>. The seminal work by Shazeer et al. introduced sparsely-gated MoE layers for language modeling and machine translation, demonstrating dramatic improvements in model capacity with only minor computational efficiency losses <Paper corpusId=\"12462234\" paperTitle=\"(Shazeer et al., 2017)\" isShortName></Paper>.\n\nMoE architectures address a fundamental challenge in model merging: parameter interference. Rather than seeking a static optimal solution within the original parameter space, MoE models dynamically integrate shared and task-specific knowledge based on each input <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This approach provides a more flexible solution that can adapt to specific needs of each instance, effectively mitigating interference between parameters from different models <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nThe architecture typically consists of expert networks (specialized components) and router networks (gating mechanisms). During inference, the router determines which experts should process each input, activating only the most relevant subset of the model. This selective activation dramatically improves efficiency compared to ensemble methods that run multiple complete models in parallel <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nRecent innovations in MoE architectures have produced several notable variants:\n- Weight-Ensembling Mixture of Experts (WEMoE) transforms only critical modules into MoE structures while statically merging non-critical ones, enabling more efficient integration <Paper corpusId=\"273662099\" paperTitle=\"(Shen et al., 2024)\" isShortName></Paper>\n- Multi-gate Mixture-of-Experts (MMoE) improves upon basic MoE by utilizing multiple gating networks to enable task-specific expert selection, better capturing inter-task relationships <Paper corpusId=\"50770252\" paperTitle=\"(Ma et al., 2018)\" isShortName></Paper>\n- Branch-Train-MiX (BTX) creates multiple expert models from a pre-trained seed model, trains them on different data domains, and then merges their feedforward sublayers into a single MoE module <Paper corpusId=\"277510128\" paperTitle=\"(Shan et al., 2025)\" isShortName></Paper>\n\nFor merging expert models with divergent architectures or specialized capabilities, more sophisticated approaches have emerged. These include techniques to mitigate parameter interference, routing heuristics to reduce fine-tuning needs, and methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. Some approaches employ knowledge fusion mechanisms that adaptively adjust the intensity of information exchange between experts while protecting task-specific knowledge through parameter isolation <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nExpert pruning and merging represent two distinct approaches to MoE compression. Pruning methods like MoE-Pruner implement inter-expert pruning and intra-expert weight sparsification, while merging methods like EEP consolidate multiple experts into fewer, more compact representations through weighted summation of expert weights <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>.\n\nFor adaptation to new tasks while preventing catastrophic forgetting, methods like MINGLE employ parameter-efficient, low-rank experts with null-space constrained gating, which restricts updates to subspaces orthogonal to prior task representations <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nDespite their advantages, MoE architectures face challenges similar to those in multi-task learning, including gradient conflicts and inefficiencies in expert selection <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"210839011\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper>. These issues reflect the fundamental difficulty of balancing shared and task-specific knowledge within a unified architecture.",
                "citations": [
                    {
                        "id": "(Gupta et al., 2022)",
                        "snippets": [
                            "Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant."
                        ],
                        "paper": {
                            "corpus_id": 248227728,
                            "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners",
                            "authors": [
                                {
                                    "authorId": "2152953535",
                                    "name": "Shashank Gupta"
                                },
                                {
                                    "authorId": "2153292652",
                                    "name": "Subhabrata Mukherjee"
                                },
                                {
                                    "authorId": "2043231778",
                                    "name": "K. Subudhi"
                                },
                                {
                                    "authorId": "2162804727",
                                    "name": "Eduardo Gonzalez"
                                },
                                {
                                    "authorId": "144430856",
                                    "name": "Damien Jose"
                                },
                                {
                                    "authorId": "2072795428",
                                    "name": "A. Awadallah"
                                },
                                {
                                    "authorId": "48441311",
                                    "name": "Jianfeng Gao"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 50
                        },
                        "score": 0.70751953125
                    },
                    {
                        "id": "(Shazeer et al., 2017)",
                        "snippets": [
                            "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
                        ],
                        "paper": {
                            "corpus_id": 12462234,
                            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
                            "authors": [
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                },
                                {
                                    "authorId": "2275364713",
                                    "name": "Krzysztof Maziarz"
                                },
                                {
                                    "authorId": "36347083",
                                    "name": "Andy Davis"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2690
                        },
                        "score": 0
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."
                        ],
                        "paper": {
                            "corpus_id": 267365047,
                            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
                            "authors": [
                                {
                                    "authorId": "2178366354",
                                    "name": "A. Tang"
                                },
                                {
                                    "authorId": "2248152216",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2279402395",
                                    "name": "Yong Luo"
                                },
                                {
                                    "authorId": "2237424891",
                                    "name": "Nan Yin"
                                },
                                {
                                    "authorId": "2282189838",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 54
                        },
                        "score": 0.7197265625
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem."
                        ],
                        "paper": {
                            "corpus_id": 271957310,
                            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2182670937",
                                    "name": "Yifei He"
                                },
                                {
                                    "authorId": "2317078449",
                                    "name": "Yuzheng Hu"
                                },
                                {
                                    "authorId": "2292270783",
                                    "name": "Yong Lin"
                                },
                                {
                                    "authorId": "2306841244",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "2283183420",
                                    "name": "Han Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 25
                        },
                        "score": 0.67431640625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models."
                        ],
                        "paper": {
                            "corpus_id": 272832307,
                            "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference",
                            "authors": [
                                {
                                    "authorId": "2322611014",
                                    "name": "Zesen Zhao"
                                },
                                {
                                    "authorId": "2244738638",
                                    "name": "Shuowei Jin"
                                },
                                {
                                    "authorId": "2321511953",
                                    "name": "Z. M. Mao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.73388671875
                    },
                    {
                        "id": "(Shen et al., 2024)",
                        "snippets": [
                            "In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach."
                        ],
                        "paper": {
                            "corpus_id": 273662099,
                            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
                            "authors": [
                                {
                                    "authorId": "2327007623",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2178366354",
                                    "name": "A. Tang"
                                },
                                {
                                    "authorId": "151497321",
                                    "name": "Enneng Yang"
                                },
                                {
                                    "authorId": "2237427680",
                                    "name": "Guibing Guo"
                                },
                                {
                                    "authorId": "2279402395",
                                    "name": "Yong Luo"
                                },
                                {
                                    "authorId": "2282189838",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2316150631",
                                    "name": "Xiaochun Cao"
                                },
                                {
                                    "authorId": "2212029373",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.83935546875
                    },
                    {
                        "id": "(Ma et al., 2018)",
                        "snippets": [
                            "Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google."
                        ],
                        "paper": {
                            "corpus_id": 50770252,
                            "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "47793019",
                                    "name": "Jiaqi W. Ma"
                                },
                                {
                                    "authorId": "48634137",
                                    "name": "Zhe Zhao"
                                },
                                {
                                    "authorId": "2838461",
                                    "name": "Xinyang Yi"
                                },
                                {
                                    "authorId": "2144168512",
                                    "name": "Jilin Chen"
                                },
                                {
                                    "authorId": "2217278",
                                    "name": "Lichan Hong"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                }
                            ],
                            "year": 2018,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 1146
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shan et al., 2025)",
                        "snippets": [
                            "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing."
                        ],
                        "paper": {
                            "corpus_id": 277510128,
                            "title": "Cognitive Memory in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2350351496",
                                    "name": "Lianlei Shan"
                                },
                                {
                                    "authorId": "2355000238",
                                    "name": "Shixian Luo"
                                },
                                {
                                    "authorId": "2353569735",
                                    "name": "Zezhou Zhu"
                                },
                                {
                                    "authorId": "2354002282",
                                    "name": "Yu Yuan"
                                },
                                {
                                    "authorId": "2354309916",
                                    "name": "Yong Wu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7177734375
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."
                        ],
                        "paper": {
                            "corpus_id": 276095183,
                            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                            "authors": [
                                {
                                    "authorId": "2266789873",
                                    "name": "Yuhang Zhou"
                                },
                                {
                                    "authorId": "8458211",
                                    "name": "Giannis Karamanolakis"
                                },
                                {
                                    "authorId": "2302332301",
                                    "name": "Victor Soto"
                                },
                                {
                                    "authorId": "1681193",
                                    "name": "Anna Rumshisky"
                                },
                                {
                                    "authorId": "2302332615",
                                    "name": "Mayank Kulkarni"
                                },
                                {
                                    "authorId": "2257407889",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2218202090",
                                    "name": "Wei Ai"
                                },
                                {
                                    "authorId": "2302633316",
                                    "name": "Jianhua Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.859375
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 276938164,
                            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                            "authors": [
                                {
                                    "authorId": "2308224151",
                                    "name": "Xiaoda Yang"
                                },
                                {
                                    "authorId": "2350336954",
                                    "name": "JunYu Lu"
                                },
                                {
                                    "authorId": "2220747584",
                                    "name": "Hongshun Qiu"
                                },
                                {
                                    "authorId": "2350180388",
                                    "name": "Sijing Li"
                                },
                                {
                                    "authorId": "2349632427",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "72890649",
                                    "name": "Shengpeng Ji"
                                },
                                {
                                    "authorId": "2349737557",
                                    "name": "Xudong Tang"
                                },
                                {
                                    "authorId": "2349670795",
                                    "name": "Jiayang Xu"
                                },
                                {
                                    "authorId": "2329894630",
                                    "name": "Jiaqi Duan"
                                },
                                {
                                    "authorId": "2112347676",
                                    "name": "Ziyue Jiang"
                                },
                                {
                                    "authorId": "2349737916",
                                    "name": "Cong Lin"
                                },
                                {
                                    "authorId": "2328348412",
                                    "name": "Sihang Cai"
                                },
                                {
                                    "authorId": "2266912737",
                                    "name": "Zejian Xie"
                                },
                                {
                                    "authorId": "2352067468",
                                    "name": "Zhuoyang Song"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73486328125
                    },
                    {
                        "id": "(Gu et al., 2025)",
                        "snippets": [
                            "To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations."
                        ],
                        "paper": {
                            "corpus_id": 276575054,
                            "title": "Delta Decompression for MoE-based LLMs Compression",
                            "authors": [
                                {
                                    "authorId": "2347571185",
                                    "name": "Hao Gu"
                                },
                                {
                                    "authorId": "2331681523",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2331723310",
                                    "name": "Lujun Li"
                                },
                                {
                                    "authorId": "2313367125",
                                    "name": "Qi Zhu"
                                },
                                {
                                    "authorId": "2331702843",
                                    "name": "Mark Lee"
                                },
                                {
                                    "authorId": "2331691577",
                                    "name": "Shengjie Sun"
                                },
                                {
                                    "authorId": "2239201089",
                                    "name": "Wei Xue"
                                },
                                {
                                    "authorId": "2118270918",
                                    "name": "Yi-Ting Guo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.72314453125
                    },
                    {
                        "id": "(Qiu et al., 2025)",
                        "snippets": [
                            "MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."
                        ],
                        "paper": {
                            "corpus_id": 278739786,
                            "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
                            "authors": [
                                {
                                    "authorId": "2150449851",
                                    "name": "Zihuan Qiu"
                                },
                                {
                                    "authorId": "2321686264",
                                    "name": "Yi Xu"
                                },
                                {
                                    "authorId": "2190031555",
                                    "name": "Chiyuan He"
                                },
                                {
                                    "authorId": "1706784",
                                    "name": "Fanman Meng"
                                },
                                {
                                    "authorId": "47775696",
                                    "name": "Linfeng Xu"
                                },
                                {
                                    "authorId": "144816629",
                                    "name": "Qingbo Wu"
                                },
                                {
                                    "authorId": "2300984960",
                                    "name": "Hongliang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Yu et al., 2020)",
                        "snippets": [
                            "While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."
                        ],
                        "paper": {
                            "corpus_id": 210839011,
                            "title": "Gradient Surgery for Multi-Task Learning",
                            "authors": [
                                {
                                    "authorId": "10909315",
                                    "name": "Tianhe Yu"
                                },
                                {
                                    "authorId": "2121434953",
                                    "name": "Saurabh Kumar"
                                },
                                {
                                    "authorId": "2129458064",
                                    "name": "Abhishek Gupta"
                                },
                                {
                                    "authorId": "1736651",
                                    "name": "S. Levine"
                                },
                                {
                                    "authorId": "1944801",
                                    "name": "Karol Hausman"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1228
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Dynamic Gating and Routing Mechanisms",
                "tldr": "Dynamic gating and routing mechanisms direct inputs to the most appropriate components within merged model architectures, enabling efficient and context-dependent processing. These approaches range from trainable gating networks in Mixture-of-Experts architectures to sophisticated routing algorithms that optimize for performance, cost, or computational efficiency. (14 sources)",
                "text": "\nDynamic gating and routing mechanisms serve as the decision-making components within integrated model architectures, determining which specialized pathways should process each input. Unlike static parameter-merging approaches, these mechanisms provide flexibility by activating different parts of the model based on input characteristics, allowing for more efficient computation and better handling of diverse tasks.\n\nAt the core of many gating approaches is the concept of conditional computation, where only specific model weights are activated for each input, as governed by a trainable gating mechanism. This design provides the significant advantage of increasing model parameters while keeping computational costs relatively constant <Paper corpusId=\"248227728\" paperTitle=\"(Gupta et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220265858\" paperTitle=\"(Lepikhin et al., 2020)\" isShortName></Paper>. The foundational work by Shazeer et al. introduced sparsely-gated Mixture-of-Experts layers that dramatically increased model capacity with minimal computational overhead by selectively activating only relevant experts for each input <Paper corpusId=\"12462234\" paperTitle=\"(Shazeer et al., 2017)\" isShortName></Paper>.\n\nMore recent developments have refined these gating mechanisms for specialized applications. Weight-Ensembling Mixture of Experts (WEMoE) transforms only critical model modules into MoE structures while statically merging non-critical ones, providing a more balanced approach to model integration <Paper corpusId=\"273662099\" paperTitle=\"(Shen et al., 2024)\" isShortName></Paper>. Similarly, Multi-gate Mixture-of-Experts (MMoE) improves upon basic MoE by implementing multiple gating networks that enable task-specific expert selection, better capturing inter-task relationships <Paper corpusId=\"50770252\" paperTitle=\"(Ma et al., 2018)\" isShortName></Paper> <Paper corpusId=\"251302720\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nBranch-Train-MiX (BTX) represents another innovative approach, where multiple expert models are created from a pre-trained seed model, trained on different data domains, and then merged into a single MoE architecture. Within each Transformer layer, a router network dynamically selects which expert's feedforward sublayer should process each token <Paper corpusId=\"277510128\" paperTitle=\"(Shan et al., 2025)\" isShortName></Paper>.\n\nFor addressing catastrophic forgetting in continual learning scenarios, MINGLE employs parameter-efficient, low-rank experts with null-space constrained gating, which restricts updates to subspaces orthogonal to prior task representations. This approach effectively preserves model behavior on past tasks while allowing adaptation to new ones <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nDynamic knowledge fusion MoE architectures further enhance collaboration between experts through adaptive knowledge-sharing mechanisms, which adjust the intensity of information exchange while protecting task-specific knowledge through parameter isolation <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This approach directly addresses the challenge of mitigating interference between parameters from different models, which can substantially deteriorate performance <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nRouter-based methods provide another dimension to dynamic processing by directing queries to the most suitable models based on factors such as task complexity, model performance, and system load <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Systems like Route LLM, PolyRouter, and Intelligent Router for LLM Workloads optimize resource utilization by matching queries to the most capable models while balancing performance with cost <Paper corpusId=\"269303119\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270764307\" paperTitle=\"(Ong et al., 2024)\" isShortName></Paper>. These approaches can significantly reduce costs\u2014by over 2 times in some cases\u2014without compromising response quality.\n\nWhile dynamic routing approaches offer compelling advantages, they face practical challenges including increased complexity in designing effective routing algorithms and managing real-time coordination among multiple models <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Additionally, task-specific routing methods often introduce substantial storage overhead by requiring the preservation of all task vectors to maintain performance <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Gupta et al., 2022)",
                        "snippets": [
                            "Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant."
                        ],
                        "paper": {
                            "corpus_id": 248227728,
                            "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners",
                            "authors": [
                                {
                                    "authorId": "2152953535",
                                    "name": "Shashank Gupta"
                                },
                                {
                                    "authorId": "2153292652",
                                    "name": "Subhabrata Mukherjee"
                                },
                                {
                                    "authorId": "2043231778",
                                    "name": "K. Subudhi"
                                },
                                {
                                    "authorId": "2162804727",
                                    "name": "Eduardo Gonzalez"
                                },
                                {
                                    "authorId": "144430856",
                                    "name": "Damien Jose"
                                },
                                {
                                    "authorId": "2072795428",
                                    "name": "A. Awadallah"
                                },
                                {
                                    "authorId": "48441311",
                                    "name": "Jianfeng Gao"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 50
                        },
                        "score": 0.70751953125
                    },
                    {
                        "id": "(Lepikhin et al., 2020)",
                        "snippets": [
                            "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."
                        ],
                        "paper": {
                            "corpus_id": 220265858,
                            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
                            "authors": [
                                {
                                    "authorId": "150077954",
                                    "name": "Dmitry Lepikhin"
                                },
                                {
                                    "authorId": "34946720",
                                    "name": "HyoukJoong Lee"
                                },
                                {
                                    "authorId": "2145139570",
                                    "name": "Yuanzhong Xu"
                                },
                                {
                                    "authorId": "7167328",
                                    "name": "Dehao Chen"
                                },
                                {
                                    "authorId": "2345617",
                                    "name": "Orhan Firat"
                                },
                                {
                                    "authorId": "2145438541",
                                    "name": "Yanping Huang"
                                },
                                {
                                    "authorId": "2048712",
                                    "name": "M. Krikun"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "2545358",
                                    "name": "Z. Chen"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 1191
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shazeer et al., 2017)",
                        "snippets": [
                            "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
                        ],
                        "paper": {
                            "corpus_id": 12462234,
                            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
                            "authors": [
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "1861312",
                                    "name": "Azalia Mirhoseini"
                                },
                                {
                                    "authorId": "2275364713",
                                    "name": "Krzysztof Maziarz"
                                },
                                {
                                    "authorId": "36347083",
                                    "name": "Andy Davis"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                },
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "48448318",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2017,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2690
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shen et al., 2024)",
                        "snippets": [
                            "In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach."
                        ],
                        "paper": {
                            "corpus_id": 273662099,
                            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
                            "authors": [
                                {
                                    "authorId": "2327007623",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2178366354",
                                    "name": "A. Tang"
                                },
                                {
                                    "authorId": "151497321",
                                    "name": "Enneng Yang"
                                },
                                {
                                    "authorId": "2237427680",
                                    "name": "Guibing Guo"
                                },
                                {
                                    "authorId": "2279402395",
                                    "name": "Yong Luo"
                                },
                                {
                                    "authorId": "2282189838",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2316150631",
                                    "name": "Xiaochun Cao"
                                },
                                {
                                    "authorId": "2212029373",
                                    "name": "Bo Du"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.83935546875
                    },
                    {
                        "id": "(Ma et al., 2018)",
                        "snippets": [
                            "Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google."
                        ],
                        "paper": {
                            "corpus_id": 50770252,
                            "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts",
                            "authors": [
                                {
                                    "authorId": "47793019",
                                    "name": "Jiaqi W. Ma"
                                },
                                {
                                    "authorId": "48634137",
                                    "name": "Zhe Zhao"
                                },
                                {
                                    "authorId": "2838461",
                                    "name": "Xinyang Yi"
                                },
                                {
                                    "authorId": "2144168512",
                                    "name": "Jilin Chen"
                                },
                                {
                                    "authorId": "2217278",
                                    "name": "Lichan Hong"
                                },
                                {
                                    "authorId": "2226805",
                                    "name": "Ed H. Chi"
                                }
                            ],
                            "year": 2018,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 1146
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "Multi-task learning has been established as an important machine learning framework for leveraging shared knowledge among multiple different but related tasks, with the generalization performance of models enhanced. As a promising learning paradigm, multi-task learning has been widely adopted by various real-world applications, such as recommendation systems. Multi-gate Mixture-of-Experts (MMoE), a well-received multi-task learning method in industry, based on the classic and inspiring Mixture-of-Experts (MoE) structure, explicitly models task relationships and learns task-specific functionalities, generating significant improvements. However, in our applications, negative transfer, which confuses considerable existing multi-task learning methods, is still observed to happen to MMoE. In this paper, an in-depth empirical investigation into negative transfer is launched. And it reveals that, incompetent experts, which play fundamental roles under the learning framework of MoE, are the key technique bottleneck. To tackle this dilemma, we propose the Calibrated Mixture of Insightful Experts (CMoIE), with three novel modules (Conflict Resolution, Expert Communication, and Mixture Calibration), customed for multi-task learning. Hence a group of insightful experts are constructed with enhanced diversity, communication and specialization. To validate the proposed method CMoIE, experiments are conducted on three public datasets and one real-world click-through-rate prediction dataset we construct based on traffic logs collected from a large-scale online product recommendation system. Our approach yields best performance across all of these benchmarks, demonstrating the superiority of it."
                        ],
                        "paper": {
                            "corpus_id": 251302720,
                            "title": "Multi-Task Learning with Calibrated Mixture of Insightful Experts",
                            "authors": [
                                {
                                    "authorId": "2143243378",
                                    "name": "Sinan Wang"
                                },
                                {
                                    "authorId": "2111175981",
                                    "name": "Yumeng Li"
                                },
                                {
                                    "authorId": "2115263944",
                                    "name": "Hongyan Li"
                                },
                                {
                                    "authorId": "2072834393",
                                    "name": "Tanchao Zhu"
                                },
                                {
                                    "authorId": null,
                                    "name": "Zhao Li"
                                },
                                {
                                    "authorId": "10336865",
                                    "name": "Wenwu Ou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE International Conference on Data Engineering",
                            "n_citations": 12
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shan et al., 2025)",
                        "snippets": [
                            "Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing."
                        ],
                        "paper": {
                            "corpus_id": 277510128,
                            "title": "Cognitive Memory in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2350351496",
                                    "name": "Lianlei Shan"
                                },
                                {
                                    "authorId": "2355000238",
                                    "name": "Shixian Luo"
                                },
                                {
                                    "authorId": "2353569735",
                                    "name": "Zezhou Zhu"
                                },
                                {
                                    "authorId": "2354002282",
                                    "name": "Yu Yuan"
                                },
                                {
                                    "authorId": "2354309916",
                                    "name": "Yong Wu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.7177734375
                    },
                    {
                        "id": "(Qiu et al., 2025)",
                        "snippets": [
                            "MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."
                        ],
                        "paper": {
                            "corpus_id": 278739786,
                            "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
                            "authors": [
                                {
                                    "authorId": "2150449851",
                                    "name": "Zihuan Qiu"
                                },
                                {
                                    "authorId": "2321686264",
                                    "name": "Yi Xu"
                                },
                                {
                                    "authorId": "2190031555",
                                    "name": "Chiyuan He"
                                },
                                {
                                    "authorId": "1706784",
                                    "name": "Fanman Meng"
                                },
                                {
                                    "authorId": "47775696",
                                    "name": "Linfeng Xu"
                                },
                                {
                                    "authorId": "144816629",
                                    "name": "Qingbo Wu"
                                },
                                {
                                    "authorId": "2300984960",
                                    "name": "Hongliang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 276938164,
                            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                            "authors": [
                                {
                                    "authorId": "2308224151",
                                    "name": "Xiaoda Yang"
                                },
                                {
                                    "authorId": "2350336954",
                                    "name": "JunYu Lu"
                                },
                                {
                                    "authorId": "2220747584",
                                    "name": "Hongshun Qiu"
                                },
                                {
                                    "authorId": "2350180388",
                                    "name": "Sijing Li"
                                },
                                {
                                    "authorId": "2349632427",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "72890649",
                                    "name": "Shengpeng Ji"
                                },
                                {
                                    "authorId": "2349737557",
                                    "name": "Xudong Tang"
                                },
                                {
                                    "authorId": "2349670795",
                                    "name": "Jiayang Xu"
                                },
                                {
                                    "authorId": "2329894630",
                                    "name": "Jiaqi Duan"
                                },
                                {
                                    "authorId": "2112347676",
                                    "name": "Ziyue Jiang"
                                },
                                {
                                    "authorId": "2349737916",
                                    "name": "Cong Lin"
                                },
                                {
                                    "authorId": "2328348412",
                                    "name": "Sihang Cai"
                                },
                                {
                                    "authorId": "2266912737",
                                    "name": "Zejian Xie"
                                },
                                {
                                    "authorId": "2352067468",
                                    "name": "Zhuoyang Song"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73486328125
                    },
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."
                        ],
                        "paper": {
                            "corpus_id": 267365047,
                            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
                            "authors": [
                                {
                                    "authorId": "2178366354",
                                    "name": "A. Tang"
                                },
                                {
                                    "authorId": "2248152216",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2279402395",
                                    "name": "Yong Luo"
                                },
                                {
                                    "authorId": "2237424891",
                                    "name": "Nan Yin"
                                },
                                {
                                    "authorId": "2282189838",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 54
                        },
                        "score": 0.7197265625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models."
                        ],
                        "paper": {
                            "corpus_id": 272832307,
                            "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference",
                            "authors": [
                                {
                                    "authorId": "2322611014",
                                    "name": "Zesen Zhao"
                                },
                                {
                                    "authorId": "2244738638",
                                    "name": "Shuowei Jin"
                                },
                                {
                                    "authorId": "2321511953",
                                    "name": "Z. M. Mao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.73388671875
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality."
                        ],
                        "paper": {
                            "corpus_id": 269303119,
                            "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
                            "authors": [
                                {
                                    "authorId": "123966440",
                                    "name": "Dujian Ding"
                                },
                                {
                                    "authorId": "2297849625",
                                    "name": "Ankur Mallick"
                                },
                                {
                                    "authorId": "2298452007",
                                    "name": "Chi Wang"
                                },
                                {
                                    "authorId": "2253669181",
                                    "name": "Robert Sim"
                                },
                                {
                                    "authorId": "2153292652",
                                    "name": "Subhabrata Mukherjee"
                                },
                                {
                                    "authorId": "3898805",
                                    "name": "Victor R\u00fchle"
                                },
                                {
                                    "authorId": "1708593",
                                    "name": "L. Lakshmanan"
                                },
                                {
                                    "authorId": "2072795428",
                                    "name": "A. Awadallah"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 106
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ong et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs."
                        ],
                        "paper": {
                            "corpus_id": 270764307,
                            "title": "RouteLLM: Learning to Route LLMs with Preference Data",
                            "authors": [
                                {
                                    "authorId": "2199840167",
                                    "name": "Isaac Ong"
                                },
                                {
                                    "authorId": "2634674",
                                    "name": "Amjad Almahairi"
                                },
                                {
                                    "authorId": "2308466924",
                                    "name": "Vincent Wu"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2251528079",
                                    "name": "Tianhao Wu"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "1793159",
                                    "name": "M. W. Kadous"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 104
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2025)",
                        "snippets": [
                            "Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments."
                        ],
                        "paper": {
                            "corpus_id": 276409347,
                            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2333317068",
                                    "name": "Zehua Liu"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.69580078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Addressing Task Interference and Knowledge Conflicts",
                "tldr": "Task interference and knowledge conflicts arise when merging specialized models, leading to performance degradation in the unified system. Various techniques have been developed to mitigate these issues, including parameter sensitivity analysis, sign conflict resolution, and knowledge separation approaches that dynamically balance task-specific and shared information. (14 sources)",
                "text": "\nModel merging techniques face a fundamental challenge: parameter interference between models trained on different tasks, which can substantially deteriorate performance in the unified model <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This interference manifests in two primary forms: redundant parameter values and disagreement on parameter signs across models <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nTo address these challenges, researchers have developed increasingly sophisticated approaches. TIES-Merging implements a three-step process: (1) resetting parameters that changed minimally during fine-tuning, (2) resolving sign conflicts through majority voting, and (3) merging only parameters that align with the final agreed-upon sign <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This approach has shown significant improvements across diverse settings, modalities, and model architectures.\n\nAnother effective technique, DARE, addresses parameter redundancy by randomly dropping delta parameters (differences between fine-tuned and pre-trained parameters) and rescaling the remaining ones to approximate original embeddings <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. This approach can eliminate up to 90-99% of delta parameters without significant performance loss, making it particularly effective for merging large language models.\n\nSens-Merging introduces sensitivity-guided coefficient adjustment that operates at two levels: within individual tasks (identifying critical layers through parameter sensitivity analysis) and across different tasks (prioritizing models that enhance others' performance) <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This fine-grained control over parameter importance allows for more effective layer-wise merging <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nSome approaches address task interference by conceptualizing knowledge in specialized models as consisting of both shared (generalizable) and task-specific components. Weight-Ensembling Mixture of Experts (WEMoE) mitigates interference by upscaling certain model components into MoE modules that can dynamically integrate these knowledge types based on input <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. Similarly, Twin-Merging extracts and modularizes different knowledge types, then uses a router to dynamically integrate them based on input characteristics <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nRecent research has also drawn parallels between model merging challenges and those faced in multi-task learning (MTL), where gradient conflicts between tasks have been extensively studied <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"210839011\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239998731\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. Techniques developed for MTL, such as gradient surgery, task weighting, and modularization, have informed approaches to model merging <Paper corpusId=\"275119334\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>.\n\nThe newest generation of merging techniques includes Conflict-Aware Task Merging (CAT Merging), which selectively trims conflict-prone components from task vectors using parameter-specific strategies such as projection for linear weights and masking for scaling parameters <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. Additionally, Representation Surgery inserts lightweight task-specific modules to realign the merged model's internal representations with those of individual models, enhancing overall performance in multitask scenarios <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\nAs model merging techniques continue to evolve, they increasingly focus on selectively integrating knowledge while preserving task-specific capabilities, moving beyond simple averaging or linear combinations toward more sophisticated approaches that explicitly address parameter conflicts <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271064761\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Tang et al., 2024)",
                        "snippets": [
                            "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."
                        ],
                        "paper": {
                            "corpus_id": 267365047,
                            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
                            "authors": [
                                {
                                    "authorId": "2178366354",
                                    "name": "A. Tang"
                                },
                                {
                                    "authorId": "2248152216",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2279402395",
                                    "name": "Yong Luo"
                                },
                                {
                                    "authorId": "2237424891",
                                    "name": "Nan Yin"
                                },
                                {
                                    "authorId": "2282189838",
                                    "name": "Lefei Zhang"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 54
                        },
                        "score": 0.7197265625
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models."
                        ],
                        "paper": {
                            "corpus_id": 270702345,
                            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                            "authors": [
                                {
                                    "authorId": "2262512474",
                                    "name": "Zhenyi Lu"
                                },
                                {
                                    "authorId": "2277238906",
                                    "name": "Chenghao Fan"
                                },
                                {
                                    "authorId": "2284721764",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2262446609",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2182623368",
                                    "name": "Dangyang Chen"
                                },
                                {
                                    "authorId": "2284687448",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 63
                        },
                        "score": 0.732421875
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                        ],
                        "paper": {
                            "corpus_id": 276422064,
                            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.68359375
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                        ],
                        "paper": {
                            "corpus_id": 265034087,
                            "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
                            "authors": [
                                {
                                    "authorId": "2265527327",
                                    "name": "Le Yu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "46493167",
                                    "name": "Haiyang Yu"
                                },
                                {
                                    "authorId": "2257407873",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "1527090216",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 335
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.\n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments.\n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance."
                        ],
                        "paper": {
                            "corpus_id": 277322544,
                            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                            "authors": [
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2345985527",
                                    "name": "Yuxuan Yao"
                                },
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2333317068",
                                    "name": "Zehua Liu"
                                },
                                {
                                    "authorId": "2221337060",
                                    "name": "Xiaojin Fu"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2344902525",
                                    "name": "Xing Li"
                                },
                                {
                                    "authorId": "2267558779",
                                    "name": "Hui-Ling Zhen"
                                },
                                {
                                    "authorId": "2332348570",
                                    "name": "Tao Zhong"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.72021484375
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem."
                        ],
                        "paper": {
                            "corpus_id": 271957310,
                            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2182670937",
                                    "name": "Yifei He"
                                },
                                {
                                    "authorId": "2317078449",
                                    "name": "Yuzheng Hu"
                                },
                                {
                                    "authorId": "2292270783",
                                    "name": "Yong Lin"
                                },
                                {
                                    "authorId": "2306841244",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "2283183420",
                                    "name": "Han Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 25
                        },
                        "score": 0.67431640625
                    },
                    {
                        "id": "(Yu et al., 2020)",
                        "snippets": [
                            "While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."
                        ],
                        "paper": {
                            "corpus_id": 210839011,
                            "title": "Gradient Surgery for Multi-Task Learning",
                            "authors": [
                                {
                                    "authorId": "10909315",
                                    "name": "Tianhe Yu"
                                },
                                {
                                    "authorId": "2121434953",
                                    "name": "Saurabh Kumar"
                                },
                                {
                                    "authorId": "2129458064",
                                    "name": "Abhishek Gupta"
                                },
                                {
                                    "authorId": "1736651",
                                    "name": "S. Levine"
                                },
                                {
                                    "authorId": "1944801",
                                    "name": "Karol Hausman"
                                },
                                {
                                    "authorId": "46881670",
                                    "name": "Chelsea Finn"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1228
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average loss. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, CAGrad achieves improved performance over prior state-of-the-art multi-objective gradient manipulation methods."
                        ],
                        "paper": {
                            "corpus_id": 239998731,
                            "title": "Conflict-Averse Gradient Descent for Multi-task Learning",
                            "authors": [
                                {
                                    "authorId": "1720831208",
                                    "name": "Bo Liu"
                                },
                                {
                                    "authorId": "46521757",
                                    "name": "Xingchao Liu"
                                },
                                {
                                    "authorId": "2103483",
                                    "name": "Xiaojie Jin"
                                },
                                {
                                    "authorId": "144848112",
                                    "name": "P. Stone"
                                },
                                {
                                    "authorId": "47362268",
                                    "name": "Qiang Liu"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 318
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jung et al., 2024)",
                        "snippets": [
                            "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge (Caruana, 1997)(Vandenhende et al., 2020)(Zhang et al., 2020). However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference (Ma et al., 2018)(Misra et al., 2016)(Rosenbaum et al., 2017). Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts (Chen et al., 2017)(Yu et al., 2020). Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating (Guo et al., 2018)(Hu et al., 2023)(Kendall et al., 2017)(Liu et al., 2018)(Sener et al., 2018). Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework (Ghiasi et al., 2021)(Jacob et al., 2023)[70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework."
                        ],
                        "paper": {
                            "corpus_id": 275119334,
                            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
                            "authors": [
                                {
                                    "authorId": "2305817448",
                                    "name": "Aecheon Jung"
                                },
                                {
                                    "authorId": "2261794770",
                                    "name": "Seunghwan Lee"
                                },
                                {
                                    "authorId": "2338395404",
                                    "name": "Dongyoon Han"
                                },
                                {
                                    "authorId": "2261902726",
                                    "name": "Sungeun Hong"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers."
                        ],
                        "paper": {
                            "corpus_id": 278501405,
                            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.71630859375
                    },
                    {
                        "id": "(Coleman et al., 2025)",
                        "snippets": [
                            "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss",
                            "For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios."
                        ],
                        "paper": {
                            "corpus_id": 277940324,
                            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2244619371",
                                    "name": "Eric Nuertey Coleman"
                                },
                                {
                                    "authorId": "2223689402",
                                    "name": "Luigi Quarantiello"
                                },
                                {
                                    "authorId": "2356575441",
                                    "name": "Ziyue Liu"
                                },
                                {
                                    "authorId": "2356596632",
                                    "name": "Qinwen Yang"
                                },
                                {
                                    "authorId": "2356499006",
                                    "name": "Samrat Mukherjee"
                                },
                                {
                                    "authorId": "2064859104",
                                    "name": "J. Hurtado"
                                },
                                {
                                    "authorId": "2285835294",
                                    "name": "Vincenzo Lomonaco"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.69091796875
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.83935546875
                    },
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "During Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018)(Ilharco et al., 2022)(Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 271064761,
                            "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2310437466",
                                    "name": "Ruochen Jin"
                                },
                                {
                                    "authorId": "2248244711",
                                    "name": "Bojian Hou"
                                },
                                {
                                    "authorId": "2327839292",
                                    "name": "Jiancong Xiao"
                                },
                                {
                                    "authorId": "2311908363",
                                    "name": "Weijie J. Su"
                                },
                                {
                                    "authorId": "2248152254",
                                    "name": "Li Shen"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.6806640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Advanced Integration Techniques",
                "tldr": "Advanced model integration techniques go beyond basic merging methods by introducing sophisticated approaches like Twin-Merging, CAT Merging, and Representation Surgery that address specialized challenges in combining diverse models. These techniques enable more effective knowledge transfer while minimizing conflicts through strategies such as dynamic knowledge fusion, conflict-aware trimming, and orthogonal parameter updates. (8 sources)",
                "text": "\nBuilding on traditional parameter-space merging and MoE architectures, researchers have developed increasingly sophisticated techniques to address the complex challenges of integrating diverse specialized models. These advanced approaches move beyond simple averaging or linear combinations toward more nuanced methods that directly target the core challenges of model integration.\n\nTwin-Merging represents one such innovation, recognizing that expert knowledge can be categorized into generalizable shared knowledge and task-specific knowledge. This approach uses compression and difference extraction to modularize these knowledge types, then employs a router to dynamically integrate them based on input characteristics, creating a flexible system similar to MoE architectures <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nFor models with divergent architectures or highly specialized capabilities, newer techniques have emerged to facilitate effective integration. These include specialized strategies to mitigate parameter interference, innovative routing heuristics to reduce fine-tuning requirements, and novel methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nRecent advances in adaptive knowledge integration include dynamic knowledge fusion MoE architectures, which enhance collaboration between experts through adaptive knowledge-sharing mechanisms. These systems dynamically adjust the intensity of information exchange between experts while protecting task-specific knowledge through parameter isolation, allowing for more effective knowledge transfer while preserving specialized capabilities <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nConflict-Aware Task Merging (CAT Merging) addresses knowledge conflicts by selectively trimming conflict-prone components from task vectors. This training-free framework introduces parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers, effectively preserving model capabilities while reducing interference <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nFor continual learning scenarios where catastrophic forgetting is a concern, MINGLE employs parameter-efficient, low-rank experts with Null-Space Constrained Gating. This innovative approach restricts gating updates to subspaces orthogonal to prior task representations, suppressing activations on old task inputs and preserving model behavior on past tasks. MINGLE further incorporates an Adaptive Relaxation Strategy that dynamically adjusts constraint strength based on interference signals captured during adaptation <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nLoRS-Merging (low-rank and sparse model merging) represents another advancement, combining low-rank and sparse pruning techniques to retain essential structures while eliminating redundant parameters. This approach effectively mitigates language and task interference while enhancing model extensibility, making it particularly suitable for integrating models trained across different languages or tasks <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nFrankenMoEs exemplifies the trend toward combining model merging with MoE architectures, initializing MoE MLP layers using weights from task-specific models. Recent extensions of this approach leverage features from specialized models for gate initialization and merge self-attention sublayers within transformer architectures, enabling more effective knowledge transfer across diverse domains <Paper corpusId=\"271947337\" paperTitle=\"(Pourreza et al., 2024)\" isShortName></Paper>.\n\nAs model merging techniques continue to evolve, they increasingly incorporate elements from multiple paradigms, blending parameter-space manipulation, expert routing, and specialized knowledge protection mechanisms. These hybrid approaches offer promising directions for creating unified models that effectively leverage the strengths of specialized components while addressing the core challenges of integration <Paper corpusId=\"276422131\" paperTitle=\"(Rousset et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.83935546875
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."
                        ],
                        "paper": {
                            "corpus_id": 276095183,
                            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                            "authors": [
                                {
                                    "authorId": "2266789873",
                                    "name": "Yuhang Zhou"
                                },
                                {
                                    "authorId": "8458211",
                                    "name": "Giannis Karamanolakis"
                                },
                                {
                                    "authorId": "2302332301",
                                    "name": "Victor Soto"
                                },
                                {
                                    "authorId": "1681193",
                                    "name": "Anna Rumshisky"
                                },
                                {
                                    "authorId": "2302332615",
                                    "name": "Mayank Kulkarni"
                                },
                                {
                                    "authorId": "2257407889",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2218202090",
                                    "name": "Wei Ai"
                                },
                                {
                                    "authorId": "2302633316",
                                    "name": "Jianhua Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.859375
                    },
                    {
                        "id": "(Yang et al., 2025)",
                        "snippets": [
                            "To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."
                        ],
                        "paper": {
                            "corpus_id": 276938164,
                            "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
                            "authors": [
                                {
                                    "authorId": "2308224151",
                                    "name": "Xiaoda Yang"
                                },
                                {
                                    "authorId": "2350336954",
                                    "name": "JunYu Lu"
                                },
                                {
                                    "authorId": "2220747584",
                                    "name": "Hongshun Qiu"
                                },
                                {
                                    "authorId": "2350180388",
                                    "name": "Sijing Li"
                                },
                                {
                                    "authorId": "2349632427",
                                    "name": "Hao Li"
                                },
                                {
                                    "authorId": "72890649",
                                    "name": "Shengpeng Ji"
                                },
                                {
                                    "authorId": "2349737557",
                                    "name": "Xudong Tang"
                                },
                                {
                                    "authorId": "2349670795",
                                    "name": "Jiayang Xu"
                                },
                                {
                                    "authorId": "2329894630",
                                    "name": "Jiaqi Duan"
                                },
                                {
                                    "authorId": "2112347676",
                                    "name": "Ziyue Jiang"
                                },
                                {
                                    "authorId": "2349737916",
                                    "name": "Cong Lin"
                                },
                                {
                                    "authorId": "2328348412",
                                    "name": "Sihang Cai"
                                },
                                {
                                    "authorId": "2266912737",
                                    "name": "Zejian Xie"
                                },
                                {
                                    "authorId": "2352067468",
                                    "name": "Zhuoyang Song"
                                },
                                {
                                    "authorId": "2266803682",
                                    "name": "Songxin Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.73486328125
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers."
                        ],
                        "paper": {
                            "corpus_id": 278501405,
                            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.71630859375
                    },
                    {
                        "id": "(Qiu et al., 2025)",
                        "snippets": [
                            "MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."
                        ],
                        "paper": {
                            "corpus_id": 278739786,
                            "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
                            "authors": [
                                {
                                    "authorId": "2150449851",
                                    "name": "Zihuan Qiu"
                                },
                                {
                                    "authorId": "2321686264",
                                    "name": "Yi Xu"
                                },
                                {
                                    "authorId": "2190031555",
                                    "name": "Chiyuan He"
                                },
                                {
                                    "authorId": "1706784",
                                    "name": "Fanman Meng"
                                },
                                {
                                    "authorId": "47775696",
                                    "name": "Linfeng Xu"
                                },
                                {
                                    "authorId": "144816629",
                                    "name": "Qingbo Wu"
                                },
                                {
                                    "authorId": "2300984960",
                                    "name": "Hongliang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.82666015625
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility."
                        ],
                        "paper": {
                            "corpus_id": 276575632,
                            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                            "authors": [
                                {
                                    "authorId": "2241702793",
                                    "name": "Qiuming Zhao"
                                },
                                {
                                    "authorId": "2107310187",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "2256775692",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2241950375",
                                    "name": "Mingxing Xu"
                                },
                                {
                                    "authorId": "2241350908",
                                    "name": "Thomas Fang Zheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.68603515625
                    },
                    {
                        "id": "(Pourreza et al., 2024)",
                        "snippets": [
                            "Model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;(Yadav et al., 2023)(Yu et al., 2023) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values",
                            "More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures."
                        ],
                        "paper": {
                            "corpus_id": 271947337,
                            "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
                            "authors": [
                                {
                                    "authorId": "2192284724",
                                    "name": "Mohammadreza Pourreza"
                                },
                                {
                                    "authorId": "2068169921",
                                    "name": "Ruoxi Sun"
                                },
                                {
                                    "authorId": "2316892037",
                                    "name": "Hailong Li"
                                },
                                {
                                    "authorId": "71436125",
                                    "name": "Lesly Miculicich"
                                },
                                {
                                    "authorId": "2264567300",
                                    "name": "Tomas Pfister"
                                },
                                {
                                    "authorId": "2676352",
                                    "name": "Sercan \u00d6. Arik"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.71240234375
                    },
                    {
                        "id": "(Rousset et al., 2025)",
                        "snippets": [
                            "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 276422131,
                            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
                            "authors": [
                                {
                                    "authorId": "2345924598",
                                    "name": "Thibault Rousset"
                                },
                                {
                                    "authorId": "2326795128",
                                    "name": "Taisei Kakibuchi"
                                },
                                {
                                    "authorId": "2346085934",
                                    "name": "Yusuke Sasaki"
                                },
                                {
                                    "authorId": "2345925100",
                                    "name": "Yoshihide Nomura"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81884765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Challenges and Solutions",
                "tldr": "Model merging and integration approaches face significant practical challenges including parameter interference, architectural incompatibility, and computational complexity. Various solutions have emerged to address these issues, ranging from interference reduction techniques and automated merging approaches to efficient routing mechanisms that balance performance with resource constraints. (16 sources)",
                "text": "\nDespite their theoretical promise, model merging and integration techniques face numerous practical challenges that can limit their effectiveness in real-world applications. One of the most significant challenges is parameter interference between models trained on different domains, which often sacrifices task-specific performance and creates a substantial gap compared to individual expert models <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This interference stems primarily from redundant parameter values and disagreements on parameter signs across models, which can substantially deteriorate performance in the unified system <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nArchitectural compatibility presents another major challenge, particularly when attempting to merge models with divergent architectures or specialized capabilities. This challenge has spurred research into new techniques for mitigating parameter interference, developing innovative routing heuristics to reduce fine-tuning requirements, and creating novel methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nThe computational complexity of merging approaches varies significantly, creating a spectrum of trade-offs between performance and resource efficiency. Model merging techniques can be broadly categorized as manual and automated, with further distinctions between data-free and data-informed methods <Paper corpusId=\"273323680\" paperTitle=\"(Gauthier-Caron et al., 2024)\" isShortName></Paper>. Manual, data-free methods like Model Soups <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper> and TIES-Merging <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> focus on merging model parameters directly without relying on data, making them computationally efficient but requiring manual tuning that can limit scalability. In contrast, automated, data-informed methods utilize representative data to optimize parameter adjustments, reducing the need for manual tuning but typically demanding more computational resources <Paper corpusId=\"273323680\" paperTitle=\"(Gauthier-Caron et al., 2024)\" isShortName></Paper>.\n\nFor MoE architectures, compression approaches have emerged to address efficiency concerns, broadly categorized into expert pruning and expert merging methods <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>. Expert pruning approaches implement inter-expert pruning and intra-expert weight sparsification, achieving significant parameter reduction but often resulting in substantial performance degradation due to irreversible loss of expert knowledge. Expert merging methods, conversely, consolidate multiple experts into fewer, more compact representations through weighted summation of expert weights, preserving more information but introducing new challenges when experts possess distinct, complementary specializations <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>.\n\nMemory consumption presents another practical challenge, particularly for approaches that maintain multiple models. Ensemble methods enhance performance and robustness by combining predictions from multiple models but require all models to remain active during inference, leading to substantial computational and memory costs <Paper corpusId=\"276813020\" paperTitle=\"(Yang et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"259075564\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>. Similarly, task-specific routing methods introduce substantial storage overhead by necessitating the preservation of all task vectors to ensure task relevance and performance <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nTo address these challenges, numerous solutions have been developed:\n\n1. For parameter interference, TIES-Merging introduced a three-step process: resetting parameters that changed minimally during fine-tuning, resolving sign conflicts through majority voting, and merging only parameters that align with the agreed-upon sign <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\n2. DARE addresses parameter redundancy by randomly dropping delta parameters and rescaling the remaining ones, effectively eliminating 90-99% of delta parameters without significant performance loss <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n3. For balancing performance with computational efficiency, router-based approaches like Route LLM select the most appropriate models based on query difficulty and desired quality levels, reducing costs by over 2 times in some cases without compromising response quality <Paper corpusId=\"270764307\" paperTitle=\"(Ong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269303119\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>.\n\n4. Representation Surgery addresses representation bias in multitask scenarios by inserting lightweight, task-specific modules that realign the merged model's internal representations with those of individual models, enhancing overall performance <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\n5. For continual learning scenarios, model merging approaches integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods <Paper corpusId=\"276580914\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271915471\" paperTitle=\"(Dou et al., 2024)\" isShortName></Paper>.\n\nEffective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency <Paper corpusId=\"276422131\" paperTitle=\"(Rousset et al., 2025)\" isShortName></Paper>. As research progresses, the focus continues to shift toward developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance, offering promising directions for creating unified models that effectively leverage the strengths of specialized components.",
                "citations": [
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models."
                        ],
                        "paper": {
                            "corpus_id": 270702345,
                            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                            "authors": [
                                {
                                    "authorId": "2262512474",
                                    "name": "Zhenyi Lu"
                                },
                                {
                                    "authorId": "2277238906",
                                    "name": "Chenghao Fan"
                                },
                                {
                                    "authorId": "2284721764",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2262446609",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2182623368",
                                    "name": "Dangyang Chen"
                                },
                                {
                                    "authorId": "2284687448",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 63
                        },
                        "score": 0.732421875
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2025)",
                        "snippets": [
                            "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."
                        ],
                        "paper": {
                            "corpus_id": 276095183,
                            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                            "authors": [
                                {
                                    "authorId": "2266789873",
                                    "name": "Yuhang Zhou"
                                },
                                {
                                    "authorId": "8458211",
                                    "name": "Giannis Karamanolakis"
                                },
                                {
                                    "authorId": "2302332301",
                                    "name": "Victor Soto"
                                },
                                {
                                    "authorId": "1681193",
                                    "name": "Anna Rumshisky"
                                },
                                {
                                    "authorId": "2302332615",
                                    "name": "Mayank Kulkarni"
                                },
                                {
                                    "authorId": "2257407889",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2218202090",
                                    "name": "Wei Ai"
                                },
                                {
                                    "authorId": "2302633316",
                                    "name": "Jianhua Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.859375
                    },
                    {
                        "id": "(Gauthier-Caron et al., 2024)",
                        "snippets": [
                            "Model merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2023) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale."
                        ],
                        "paper": {
                            "corpus_id": 273323680,
                            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
                            "authors": [
                                {
                                    "authorId": "2325729401",
                                    "name": "Thomas Gauthier-Caron"
                                },
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "2325730893",
                                    "name": "Elliot Stein"
                                },
                                {
                                    "authorId": "2175482685",
                                    "name": "Malikeh Ehghaghi"
                                },
                                {
                                    "authorId": "2292260669",
                                    "name": "Charles Goddard"
                                },
                                {
                                    "authorId": "2292260070",
                                    "name": "Mark McQuade"
                                },
                                {
                                    "authorId": "2047397646",
                                    "name": "Jacob Solawetz"
                                },
                                {
                                    "authorId": "2325729518",
                                    "name": "Maxime Labonne"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.6640625
                    },
                    {
                        "id": "(Wortsman et al., 2022)",
                        "snippets": [
                            "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
                        ],
                        "paper": {
                            "corpus_id": 247362886,
                            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                            "authors": [
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "1387466862",
                                    "name": "S. Gadre"
                                },
                                {
                                    "authorId": "40458654",
                                    "name": "R. Roelofs"
                                },
                                {
                                    "authorId": "2158366935",
                                    "name": "Raphael Gontijo-Lopes"
                                },
                                {
                                    "authorId": "4690624",
                                    "name": "Ari S. Morcos"
                                },
                                {
                                    "authorId": "40281109",
                                    "name": "Hongseok Namkoong"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1011
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gu et al., 2025)",
                        "snippets": [
                            "To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations."
                        ],
                        "paper": {
                            "corpus_id": 276575054,
                            "title": "Delta Decompression for MoE-based LLMs Compression",
                            "authors": [
                                {
                                    "authorId": "2347571185",
                                    "name": "Hao Gu"
                                },
                                {
                                    "authorId": "2331681523",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "2331723310",
                                    "name": "Lujun Li"
                                },
                                {
                                    "authorId": "2313367125",
                                    "name": "Qi Zhu"
                                },
                                {
                                    "authorId": "2331702843",
                                    "name": "Mark Lee"
                                },
                                {
                                    "authorId": "2331691577",
                                    "name": "Shengjie Sun"
                                },
                                {
                                    "authorId": "2239201089",
                                    "name": "Wei Xue"
                                },
                                {
                                    "authorId": "2118270918",
                                    "name": "Yi-Ting Guo"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.72314453125
                    },
                    {
                        "id": "(Yang et al._1, 2025)",
                        "snippets": [
                            "Various strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023)(Wang et al., 2024) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024)Hu et al., 2024;(Ong et al., 2024) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors."
                        ],
                        "paper": {
                            "corpus_id": 276813020,
                            "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
                            "authors": [
                                {
                                    "authorId": "2287297309",
                                    "name": "Ziyi Yang"
                                },
                                {
                                    "authorId": "2217614543",
                                    "name": "Fanqi Wan"
                                },
                                {
                                    "authorId": "2286975236",
                                    "name": "Longguang Zhong"
                                },
                                {
                                    "authorId": "2258677979",
                                    "name": "Canbin Huang"
                                },
                                {
                                    "authorId": "2348918304",
                                    "name": "Guosheng Liang"
                                },
                                {
                                    "authorId": "2258552983",
                                    "name": "Xiaojun Quan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.77587890625
                    },
                    {
                        "id": "(Jiang et al., 2023)",
                        "snippets": [
                            "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap."
                        ],
                        "paper": {
                            "corpus_id": 259075564,
                            "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
                            "authors": [
                                {
                                    "authorId": "2197076899",
                                    "name": "Dongfu Jiang"
                                },
                                {
                                    "authorId": "1384550891",
                                    "name": "Xiang Ren"
                                },
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 333
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2025)",
                        "snippets": [
                            "Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments."
                        ],
                        "paper": {
                            "corpus_id": 276409347,
                            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2333317068",
                                    "name": "Zehua Liu"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.69580078125
                    },
                    {
                        "id": "(Coleman et al., 2025)",
                        "snippets": [
                            "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss",
                            "For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios."
                        ],
                        "paper": {
                            "corpus_id": 277940324,
                            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
                            "authors": [
                                {
                                    "authorId": "2244619371",
                                    "name": "Eric Nuertey Coleman"
                                },
                                {
                                    "authorId": "2223689402",
                                    "name": "Luigi Quarantiello"
                                },
                                {
                                    "authorId": "2356575441",
                                    "name": "Ziyue Liu"
                                },
                                {
                                    "authorId": "2356596632",
                                    "name": "Qinwen Yang"
                                },
                                {
                                    "authorId": "2356499006",
                                    "name": "Samrat Mukherjee"
                                },
                                {
                                    "authorId": "2064859104",
                                    "name": "J. Hurtado"
                                },
                                {
                                    "authorId": "2285835294",
                                    "name": "Vincenzo Lomonaco"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.69091796875
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                        ],
                        "paper": {
                            "corpus_id": 265034087,
                            "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
                            "authors": [
                                {
                                    "authorId": "2265527327",
                                    "name": "Le Yu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "46493167",
                                    "name": "Haiyang Yu"
                                },
                                {
                                    "authorId": "2257407873",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "1527090216",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 335
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ong et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs."
                        ],
                        "paper": {
                            "corpus_id": 270764307,
                            "title": "RouteLLM: Learning to Route LLMs with Preference Data",
                            "authors": [
                                {
                                    "authorId": "2199840167",
                                    "name": "Isaac Ong"
                                },
                                {
                                    "authorId": "2634674",
                                    "name": "Amjad Almahairi"
                                },
                                {
                                    "authorId": "2308466924",
                                    "name": "Vincent Wu"
                                },
                                {
                                    "authorId": "2537924",
                                    "name": "Wei-Lin Chiang"
                                },
                                {
                                    "authorId": "2251528079",
                                    "name": "Tianhao Wu"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                },
                                {
                                    "authorId": "1793159",
                                    "name": "M. W. Kadous"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 104
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality."
                        ],
                        "paper": {
                            "corpus_id": 269303119,
                            "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
                            "authors": [
                                {
                                    "authorId": "123966440",
                                    "name": "Dujian Ding"
                                },
                                {
                                    "authorId": "2297849625",
                                    "name": "Ankur Mallick"
                                },
                                {
                                    "authorId": "2298452007",
                                    "name": "Chi Wang"
                                },
                                {
                                    "authorId": "2253669181",
                                    "name": "Robert Sim"
                                },
                                {
                                    "authorId": "2153292652",
                                    "name": "Subhabrata Mukherjee"
                                },
                                {
                                    "authorId": "3898805",
                                    "name": "Victor R\u00fchle"
                                },
                                {
                                    "authorId": "1708593",
                                    "name": "L. Lakshmanan"
                                },
                                {
                                    "authorId": "2072795428",
                                    "name": "A. Awadallah"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 106
                        },
                        "score": 0
                    },
                    {
                        "id": "(Feng et al., 2025)",
                        "snippets": [
                            "Recently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023)Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024)Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024)Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 276580914,
                            "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
                            "authors": [
                                {
                                    "authorId": "2261661979",
                                    "name": "Yujie Feng"
                                },
                                {
                                    "authorId": "2347170204",
                                    "name": "Xujia Wang"
                                },
                                {
                                    "authorId": "2220034673",
                                    "name": "Zexin Lu"
                                },
                                {
                                    "authorId": "2347197165",
                                    "name": "Shenghong Fu"
                                },
                                {
                                    "authorId": "144218801",
                                    "name": "Guangyuan Shi"
                                },
                                {
                                    "authorId": "2215472732",
                                    "name": "Yongxin Xu"
                                },
                                {
                                    "authorId": "2253831765",
                                    "name": "Yasha Wang"
                                },
                                {
                                    "authorId": "2348327959",
                                    "name": "Philip S. Yu"
                                },
                                {
                                    "authorId": "2315809919",
                                    "name": "Xu Chu"
                                },
                                {
                                    "authorId": "2261456593",
                                    "name": "Xiao-Ming Wu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.6591796875
                    },
                    {
                        "id": "(Dou et al., 2024)",
                        "snippets": [
                            "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novel framework that introduces several low-rank adapters (LoRA) and integrates them by us-ing a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of Lo-RAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, Lo-RAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/ Ablustrund/LoRAMoE ."
                        ],
                        "paper": {
                            "corpus_id": 271915471,
                            "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
                            "authors": [
                                {
                                    "authorId": "2042683163",
                                    "name": "Shihan Dou"
                                },
                                {
                                    "authorId": "2240446306",
                                    "name": "Enyu Zhou"
                                },
                                {
                                    "authorId": "2275033850",
                                    "name": "Yan Liu"
                                },
                                {
                                    "authorId": "2181306462",
                                    "name": "Songyang Gao"
                                },
                                {
                                    "authorId": "2248291262",
                                    "name": "Wei Shen"
                                },
                                {
                                    "authorId": "2222630539",
                                    "name": "Limao Xiong"
                                },
                                {
                                    "authorId": "2212175381",
                                    "name": "Yuhao Zhou"
                                },
                                {
                                    "authorId": "2118451107",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2218237934",
                                    "name": "Zhiheng Xi"
                                },
                                {
                                    "authorId": "2241140630",
                                    "name": "Xiaoran Fan"
                                },
                                {
                                    "authorId": "2274941422",
                                    "name": "Shiliang Pu"
                                },
                                {
                                    "authorId": "2277702055",
                                    "name": "Jiang Zhu"
                                },
                                {
                                    "authorId": "2058585152",
                                    "name": "Rui Zheng"
                                },
                                {
                                    "authorId": "2067331064",
                                    "name": "Tao Gui"
                                },
                                {
                                    "authorId": "2256972399",
                                    "name": "Qi Zhang"
                                },
                                {
                                    "authorId": "2257129989",
                                    "name": "Xuanjing Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rousset et al., 2025)",
                        "snippets": [
                            "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance."
                        ],
                        "paper": {
                            "corpus_id": 276422131,
                            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
                            "authors": [
                                {
                                    "authorId": "2345924598",
                                    "name": "Thibault Rousset"
                                },
                                {
                                    "authorId": "2326795128",
                                    "name": "Taisei Kakibuchi"
                                },
                                {
                                    "authorId": "2346085934",
                                    "name": "Yusuke Sasaki"
                                },
                                {
                                    "authorId": "2345925100",
                                    "name": "Yoshihide Nomura"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.81884765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.31228500000000003
    }
}