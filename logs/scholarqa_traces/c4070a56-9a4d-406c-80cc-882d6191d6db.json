{
    "query": "What are the leading parameter-based and parameter-free machine unlearning methods for entity-level or concept-level knowledge removal in language models, and how is their effectiveness evaluated in terms of forgetting and retention rates?",
    "user_id": "lib_user",
    "task_id": "c4070a56-9a4d-406c-80cc-882d6191d6db",
    "timestamp": "2025-06-23T23:03:57.672434",
    "n_retrieval": 256,
    "n_retrieved": 254,
    "n_candidates": 34,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.381957,
    "decomposed_query": {
        "rewritten_query": "Leading parameter-based and parameter-free machine unlearning methods for entity-level or concept-level knowledge removal in language models, and how their effectiveness is evaluated in terms of forgetting and retention rates.",
        "keyword_query": "parameter-based parameter-free machine unlearning methods entity-level concept-level knowledge removal language models effectiveness forgetting retention rates",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010296,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Erasing Conceptual Knowledge from Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52017367",
                    "name": "Rohit Gandikota"
                },
                {
                    "authorId": "2140009998",
                    "name": "Sheridan Feucht"
                },
                {
                    "authorId": "2225941937",
                    "name": "Samuel Marks"
                },
                {
                    "authorId": "2284996653",
                    "name": "David Bau"
                }
            ],
            "abstract": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "corpus_id": 273098800,
            "sentences": [
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
                    "score": 0.7952793675196037,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1370
                        },
                        {
                            "start": 1373,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2465
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89111328125
                },
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "Machine Unlearning The idea of removing specific data from machine learning models, known as machine unlearning, has gained attention in recent years, initially motivated by privacy concerns (Cao & Yang, 2015;Harding et al., 2019). Early methods focused on efficiently removing individual training examples or facts from models (Golatkar et al., 2020;Ma et al., 2022;Jang et al., 2022a). However, most existing benchmarks evaluate unlearning on artificially created deletion sets (Choi & Na, 2023;Goel et al., 2022;Maini et al., 2024), in contrast to our focus on real-world distributions of broad conceptual knowledge. \n\nErasing broad conceptual knowledge from LLMs New approaches to machine unlearning have recently gained traction on the problem of removing dangerous capabilities from LLMs (Lynch et al., 2024;Ilharco et al., 2023;Jang et al., 2022b;Lu et al., 2022;Yu et al., 2023;Casper et al., 2024;Eldan & Russinovich, 2023). Our work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals.",
                    "score": 0.631969265564104,
                    "section_title": "Related work",
                    "char_start_offset": 4369,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 619
                        },
                        {
                            "start": 622,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1513
                        },
                        {
                            "start": 1514,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1953
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 191,
                            "end": 209,
                            "matchedPaperCorpusId": "5945696"
                        },
                        {
                            "start": 209,
                            "end": 230,
                            "matchedPaperCorpusId": "203336801"
                        },
                        {
                            "start": 328,
                            "end": 351,
                            "matchedPaperCorpusId": "212628473"
                        },
                        {
                            "start": 351,
                            "end": 367,
                            "matchedPaperCorpusId": "236882730"
                        },
                        {
                            "start": 854,
                            "end": 870,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75341796875
                },
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "We introduce Erasure of Language Memory (ELM), an approach that reformulates concept unlearning through introspective classification. While traditional unlearning methods have focused on sample removal through dataset retraining, gradient ascent, or representation disruption, ELM leverages the language model's own ability to evaluate and modify its knowledge. \n\nSpecifically, ELM uses the language model itself as its own introspective classifier, as established in Section 3.2. Rather than training external classifiers or explicitly manipulating representations, we leverage the model's next-token predictions to identify and reduce the likelihood of generating concept-specific content. This allows us to operate directly on the model's probability distributions, providing a principled approach to unlearning. \n\nThe core of our method is a self-classification objective that reduces the likelihood of generating text that the model would classify as containing the target concept. Given a dataset D erase containing text related to the target concept, we modify the model's predicted probabilities to diverge from generating content related to the concept being erased c \u2212 and instead favor generating content related to a safer alternative c + : \n\n). \n\nIn practice, we encounter two main challenges when implementing this objective. First, knowledge in language models is often entangled -modifying one concept can unintentionally affect related concepts. To address this, we preserve the model's behavior on a set of related but safe concepts by matching its original distribution on retain data, D retain , containing unrelated text: \n\nSecond, the self-classification objective alone might lead to incoherent text generation when prompted about erased concepts. We address this by applying our core objective (Equation 9) during inference to generate synthetic training examples, then using these to train the model in an autoregressive setting to maintain coherent generation: \n\nThe final training objective combines these terms with appropriate weights: \n\nWe implement this through low-rank adapters attached to early model layers, allowing precise modification of the model's knowledge while maintaining its broader capabilities. This approach provides a principled method for concept unlearning that avoids the instability of gradient reversal methods and the incoherence of representation hacking approaches.",
                    "score": 0.5878876181991993,
                    "section_title": "Method",
                    "char_start_offset": 14007,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 361
                        },
                        {
                            "start": 364,
                            "end": 480
                        },
                        {
                            "start": 481,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 815
                        },
                        {
                            "start": 818,
                            "end": 986
                        },
                        {
                            "start": 987,
                            "end": 1252
                        },
                        {
                            "start": 1255,
                            "end": 1257
                        },
                        {
                            "start": 1260,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1642
                        },
                        {
                            "start": 1645,
                            "end": 1770
                        },
                        {
                            "start": 1771,
                            "end": 1986
                        },
                        {
                            "start": 1989,
                            "end": 2064
                        },
                        {
                            "start": 2067,
                            "end": 2241
                        },
                        {
                            "start": 2242,
                            "end": 2422
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70947265625
                }
            ],
            "relevance_judgement": 0.89111328125,
            "relevance_judgment_input_expanded": "# Title: Erasing Conceptual Knowledge from Language Models\n# Venue: arXiv.org\n# Authors: Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau\n## Abstract\nIn this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info\n## Introduction\nWhat does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.\n\n## Related work\nMachine Unlearning The idea of removing specific data from machine learning models, known as machine unlearning, has gained attention in recent years, initially motivated by privacy concerns (Cao & Yang, 2015;Harding et al., 2019). Early methods focused on efficiently removing individual training examples or facts from models (Golatkar et al., 2020;Ma et al., 2022;Jang et al., 2022a). However, most existing benchmarks evaluate unlearning on artificially created deletion sets (Choi & Na, 2023;Goel et al., 2022;Maini et al., 2024), in contrast to our focus on real-world distributions of broad conceptual knowledge. \n\nErasing broad conceptual knowledge from LLMs New approaches to machine unlearning have recently gained traction on the problem of removing dangerous capabilities from LLMs (Lynch et al., 2024;Ilharco et al., 2023;Jang et al., 2022b;Lu et al., 2022;Yu et al., 2023;Casper et al., 2024;Eldan & Russinovich, 2023). Our work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals.\n\n## Method\nWe introduce Erasure of Language Memory (ELM), an approach that reformulates concept unlearning through introspective classification. While traditional unlearning methods have focused on sample removal through dataset retraining, gradient ascent, or representation disruption, ELM leverages the language model's own ability to evaluate and modify its knowledge. \n\nSpecifically, ELM uses the language model itself as its own introspective classifier, as established in Section 3.2. Rather than training external classifiers or explicitly manipulating representations, we leverage the model's next-token predictions to identify and reduce the likelihood of generating concept-specific content. This allows us to operate directly on the model's probability distributions, providing a principled approach to unlearning. \n\nThe core of our method is a self-classification objective that reduces the likelihood of generating text that the model would classify as containing the target concept. Given a dataset D erase containing text related to the target concept, we modify the model's predicted probabilities to diverge from generating content related to the concept being erased c \u2212 and instead favor generating content related to a safer alternative c + : \n\n). \n\nIn practice, we encounter two main challenges when implementing this objective. First, knowledge in language models is often entangled -modifying one concept can unintentionally affect related concepts. To address this, we preserve the model's behavior on a set of related but safe concepts by matching its original distribution on retain data, D retain , containing unrelated text: \n\nSecond, the self-classification objective alone might lead to incoherent text generation when prompted about erased concepts. We address this by applying our core objective (Equation 9) during inference to generate synthetic training examples, then using these to train the model in an autoregressive setting to maintain coherent generation: \n\nThe final training objective combines these terms with appropriate weights: \n\nWe implement this through low-rank adapters attached to early model layers, allowing precise modification of the model's knowledge while maintaining its broader capabilities. This approach provides a principled method for concept unlearning that avoids the instability of gradient reversal methods and the incoherence of representation hacking approaches.",
            "reference_string": "[273098800 | Gandikota et al. | 2024 | Citations: 11]"
        },
        {
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2295742465",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2215923457",
                    "name": "Herbert Woisetschlaeger"
                },
                {
                    "authorId": "2109296817",
                    "name": "Zongxiong Chen"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2238271670",
                    "name": "Hans-Arno Jacobsen"
                },
                {
                    "authorId": "2265969003",
                    "name": "Fakhri Karray"
                }
            ],
            "abstract": "This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",
            "corpus_id": 276772996,
            "sentences": [
                {
                    "corpus_id": "276772996",
                    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                    "text": "We present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.",
                    "score": 0.5898713040059463,
                    "section_title": "Taxonomy",
                    "char_start_offset": 6148,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 771
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86767578125
                }
            ],
            "relevance_judgement": 0.86767578125,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models\n# Venue: arXiv.org\n# Authors: Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, Fakhri Karray\n## Abstract\nThis study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.\n## Taxonomy\nWe present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.",
            "reference_string": "[276772996 | Geng et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282353702",
                    "name": "Lingzhi Wang"
                },
                {
                    "authorId": "46180553",
                    "name": "Xingshan Zeng"
                },
                {
                    "authorId": "2283375647",
                    "name": "Jinsong Guo"
                },
                {
                    "authorId": "2264107863",
                    "name": "Kam-Fai Wong"
                },
                {
                    "authorId": "2265883371",
                    "name": "Georg Gottlob"
                }
            ],
            "abstract": "This paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.",
            "corpus_id": 267547751,
            "sentences": [
                {
                    "corpus_id": "267547751",
                    "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
                    "text": "This paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.",
                    "score": 0.5884121786276135,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8671875
                },
                {
                    "corpus_id": "267547751",
                    "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
                    "text": "As we focus on unlearning of language models, we start from pretrained language models, then we do unlearning on forget dataset D f and test the models (original LMs and unlearned LMs) on test set D t . \n\nForget Dataset D f . In order to ensure a fair comparison with Jang et al. (2023) and to assess the privacy risk associated with language models, we employ the identical samples as those disclosed by Jang et al. (2023) (Smith et al. 2020), and Wizard of Internet (Komeili, Shuster, and Weston 2022)). These test sets are used to assess whether the general capabilities of language models are affected after unlearning. \n\nEvaluation Metrics. We assess the performance of the methods from two perspectives: the effectiveness of unlearning and the maintenance of general performance. To evaluate the unlearning effectiveness on the forget set, we employ our proposed metrics S-EL n and S-MA, as well as EL n and MA, introduced in Section . As for the evaluation of maintaining performance, we use accuracy for the classification test sets and F1 and Perplexity (PPL) scores for dialogue tasks.",
                    "score": 0.61850868472116,
                    "section_title": "Experimental Setup Datasets and Evaluation Metrics",
                    "char_start_offset": 12017,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 205,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 785
                        },
                        {
                            "start": 786,
                            "end": 941
                        },
                        {
                            "start": 942,
                            "end": 1095
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 268,
                            "end": 286,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 405,
                            "end": 423,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 424,
                            "end": 443,
                            "matchedPaperCorpusId": "215827653"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.751953125
                }
            ],
            "relevance_judgement": 0.8671875,
            "relevance_judgment_input_expanded": "# Title: Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob\n## Abstract\nThis paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.\n## Experimental Setup Datasets and Evaluation Metrics\nAs we focus on unlearning of language models, we start from pretrained language models, then we do unlearning on forget dataset D f and test the models (original LMs and unlearned LMs) on test set D t . \n\nForget Dataset D f . In order to ensure a fair comparison with Jang et al. (2023) and to assess the privacy risk associated with language models, we employ the identical samples as those disclosed by Jang et al. (2023) (Smith et al. 2020), and Wizard of Internet (Komeili, Shuster, and Weston 2022)). These test sets are used to assess whether the general capabilities of language models are affected after unlearning. \n\nEvaluation Metrics. We assess the performance of the methods from two perspectives: the effectiveness of unlearning and the maintenance of general performance. To evaluate the unlearning effectiveness on the forget set, we employ our proposed metrics S-EL n and S-MA, as well as EL n and MA, introduced in Section . As for the evaluation of maintaining performance, we use accuracy for the classification test sets and F1 and Perplexity (PPL) scores for dialogue tasks.",
            "reference_string": "[267547751 | Wang et al. | 2024 | Citations: 18]"
        },
        {
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315304043",
                    "name": "Tyler Lizzo"
                },
                {
                    "authorId": "2315302093",
                    "name": "Larry Heck"
                }
            ],
            "abstract": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
            "corpus_id": 271769107,
            "sentences": [
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "This paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
                    "score": 0.6007157735681082,
                    "section_title": "Conclusion",
                    "char_start_offset": 27411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1225
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86083984375
                },
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
                    "score": 0.6530851704221803,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.787109375
                },
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "The swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.",
                    "score": 0.6649389983421004,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 547
                        },
                        {
                            "start": 550,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1249
                        },
                        {
                            "start": 1252,
                            "end": 1392
                        },
                        {
                            "start": 1393,
                            "end": 1547
                        },
                        {
                            "start": 1548,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 2001
                        },
                        {
                            "start": 2004,
                            "end": 2050
                        },
                        {
                            "start": 2053,
                            "end": 2136
                        },
                        {
                            "start": 2139,
                            "end": 2313
                        },
                        {
                            "start": 2316,
                            "end": 2475
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 339,
                            "end": 354,
                            "matchedPaperCorpusId": "168855552"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73486328125
                }
            ],
            "relevance_judgement": 0.86083984375,
            "relevance_judgment_input_expanded": "# Title: UNLEARN Efficient Removal of Knowledge in Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Tyler Lizzo, Larry Heck\n## Abstract\nGiven the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.\n## Introduction\nThe swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.\n\n## Conclusion\nThis paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
            "reference_string": "[271769107 | Lizzo et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 99,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00383, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313929104",
                    "name": "Chenlu Ding"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2263441815",
                    "name": "Yancheng Yuan"
                },
                {
                    "authorId": "2315426691",
                    "name": "Jinda Lu"
                },
                {
                    "authorId": "2333421125",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "2333357625",
                    "name": "Alex Su"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ],
            "abstract": "The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.",
            "corpus_id": 274436499,
            "sentences": [
                {
                    "corpus_id": "274436499",
                    "title": "Unified Parameter-Efficient Unlearning for LLMs",
                    "text": "The concept of unlearning in Large Language Models has garnered considerable attention as concerns over data privacy and model integrity have intensified. In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by Lu et al. (2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by Jang et al. (2023) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by Yu et al. (2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications.",
                    "score": 0.6033232334225715,
                    "section_title": "F.2 LARGE LANGUAGE MODELS UNLEARNING",
                    "char_start_offset": 43282,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 563
                        },
                        {
                            "start": 566,
                            "end": 729
                        },
                        {
                            "start": 730,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1098
                        },
                        {
                            "start": 1101,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1371
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 403,
                            "end": 419,
                            "matchedPaperCorpusId": "249152301"
                        },
                        {
                            "start": 754,
                            "end": 772,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1175,
                            "end": 1191,
                            "matchedPaperCorpusId": "259859034"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81640625
                }
            ],
            "relevance_judgement": 0.81640625,
            "relevance_judgment_input_expanded": "# Title: Unified Parameter-Efficient Unlearning for LLMs\n# Venue: International Conference on Learning Representations\n# Authors: Chenlu Ding, Jiancan Wu, Yancheng Yuan, Jinda Lu, Kai Zhang, Alex Su, Xiang Wang, Xiangnan He\n## Abstract\nThe advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.\n## F.2 LARGE LANGUAGE MODELS UNLEARNING\nThe concept of unlearning in Large Language Models has garnered considerable attention as concerns over data privacy and model integrity have intensified. In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by Lu et al. (2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by Jang et al. (2023) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by Yu et al. (2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications.",
            "reference_string": "[274436499 | Ding et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265878959",
                    "name": "Weitao Ma"
                },
                {
                    "authorId": "2674998",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2208739098",
                    "name": "Weihong Zhong"
                },
                {
                    "authorId": "2265930173",
                    "name": "Lei Huang"
                },
                {
                    "authorId": "2216505879",
                    "name": "Yangfan Ye"
                },
                {
                    "authorId": "2257004102",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
            "corpus_id": 270703237,
            "sentences": [
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
                    "score": 0.7674967586307332,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8076171875
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "We present experimental results comparing the performance of the same method on various forget sets and between different algorithms. The main experimental results of Llama2-7B-Chat-TOFU are shown in Table 1, and the experimental results of Phi-1.5-TOFU are presented in Appendix B.5. \n\nA comparison of the performance across different types of forget sets, as presented in Table 1, reveals that the algorithms based on the target set consistently maintain similar model utility while achieving lower probability, reduced accuracy, and overall higher forget quality on Llama2-7B-Chat-TOFU. A similar trend is observed in the Phi-1.5-TOFU model, as shown in Table 4. These results suggest that models employing the target set engage in more thorough unlearning compared to those utilizing the probing set. This finding highlights that the current unlearning algorithms struggle to generalize effectively to entity-level unlearning tasks when relying on the probing set. Consequently, the construction of the forget set is crucial in determining the success of entity-level unlearning. In the subsequent analysis, we will explore in detail how the quality of the forget set influences unlearning effectiveness. \n\nComparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: \n\n1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. \n\n2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. \n\n3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions.",
                    "score": 0.7463070381352919,
                    "section_title": "Experimental Results",
                    "char_start_offset": 13233,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 589
                        },
                        {
                            "start": 590,
                            "end": 637
                        },
                        {
                            "start": 638,
                            "end": 665
                        },
                        {
                            "start": 666,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1083
                        },
                        {
                            "start": 1084,
                            "end": 1208
                        },
                        {
                            "start": 1211,
                            "end": 1344
                        },
                        {
                            "start": 1347,
                            "end": 1349
                        },
                        {
                            "start": 1350,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1576
                        },
                        {
                            "start": 1577,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1597
                        },
                        {
                            "start": 1598,
                            "end": 1752
                        },
                        {
                            "start": 1755,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1997
                        },
                        {
                            "start": 1998,
                            "end": 2143
                        },
                        {
                            "start": 2146,
                            "end": 2309
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8017578125
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "After constructing the forget set, the next step is to apply the unlearning algorithms to it. In the absence of algorithms specifically designed for entitylevel unlearning, we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model. An ideal entity-level unlearning method should effectively remove the entire entity from the forget set while minimizing any negative impact on the remaining knowledge. This study primarily exam-ines the performance of current unlearning algorithms on entity-level unlearning tasks, without yet delving into the broader algorithmic application framework (Huang et al., 2024).",
                    "score": 0.6540678200702601,
                    "section_title": "Unlearning Execution",
                    "char_start_offset": 7992,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 93
                        },
                        {
                            "start": 94,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 662
                        },
                        {
                            "start": 663,
                            "end": 869
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7998046875
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model. However, due to the inaccessibility of the original training corpora, it is challenging to obtain such a target set, which contains all knowledge associated with the target entity. Consequently, entity-level unlearning requires an additional step to construct a forget set before applying unlearning algorithms, where there is frequently a discrepancy between the constructed forget set and the target set. To this end, we divide the entity-level unlearning task into a two-stage framework, comprising forget set construction and unlearning execution. This process seeks to delete the target set by erasing the construted forget set, which involves more complex procedures and presents greater challenges than instance-level unlearning. \n\nIn this work, we conduct a comprehensive analysis of entity-level unlearning for LLMs, structured around a sequence of focused steps: Firstly, we begin by formally defining and setting the task, which involves injecting pseudo-entity knowledge into the model to establish both the target model and the target set. Secondly, we systematically evaluate the current unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task. Additionally, we observe that the design of forget set significantly influences the effectiveness of the unlearning algorithm. Thirdly, inspired by the above, we further investigate how various constructed forget sets impact algorithm performance. Our results indicate that unlearning with a forget set that has higher knowledge coverage relative to the target set leads to more effective unlearning. Although merely increasing the size of the forget set may enhance knowledge coverage, it would compromise the model's generalization ability, making it an ineffective way to improve performance.",
                    "score": 0.8588539270337304,
                    "section_title": "Introduction",
                    "char_start_offset": 1683,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 324
                        },
                        {
                            "start": 325,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 880
                        },
                        {
                            "start": 883,
                            "end": 1196
                        },
                        {
                            "start": 1197,
                            "end": 1281
                        },
                        {
                            "start": 1282,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1764
                        },
                        {
                            "start": 1765,
                            "end": 1885
                        },
                        {
                            "start": 1886,
                            "end": 2038
                        },
                        {
                            "start": 2039,
                            "end": 2233
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77685546875
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "We hypothesize that the limited unlearning effect is due to the insufficient coverage of knowledge within the current probing set. To explore this, we introduce a new metric, Knowledge Coverage, designed to assess the knowledge overlap between the probing sets and the target set. This metric is computed using the BERTScore (Zhang et al., 2019) of the closest QA pair match between the probing set and the target set. A higher knowledge coverage indicates that the probing set encompasses a broader range of entity-related knowledge. See Appendix C.1 for detailed definitions and formulas. \n\nTo further investigate the effects of varying knowledge coverage, we construct probing sets with varying degrees of coverage by systematically replacing different ratios of QA pairs within the probing set with those from the target set while keeping the total set size fixed at 20. As demonstrated in Table 7, the knowledge coverage of the constructed probing set increases progressively as the ratio of replaced pairs grows. Then, we applied the five algorithms to perform entity-level unlearning on the probing set under five different replacement ratios. As illustrated in Figure 2, enhancing knowledge coverage of the probing set consistently enhances the forget quality of most methods. Furthermore, each algorithm consistently preserved similar model utility across various probing sets. These results suggest that enhancing the knowledge coverage of the probing set can improve the unlearning effectiveness. \n\nAn intuitive approach to increase knowledge coverage is by increasing the size of the probing set. Therefore, we further explore the performance of the algorithms on probing sets across different sizes. Specifically, we expand the probing set size for each entity and evaluate the performance of the unlearning algorithms across five different sizes. As illustrated in Figure 3, the forget quality of the five unlearning algorithms gradually improves as the size of the probing set increases. However, except for the Pref. Opt. method, the model utility of the unlearned models produced by the other algorithms demonstrates a noticeable decline. This decline in model utility, when compared to the trends in Figure 2, is likely due to the larger number of unlearning steps required as the probing set size grows. These findings suggest that, for most unlearning algorithms, expanding the probing set size leads to a trade-off between forget quality and model utility. Furthermore, according to section 3.2, although the Pref. Opt.",
                    "score": 0.6093100743802404,
                    "section_title": "Effect of Probing Set",
                    "char_start_offset": 16574,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 280
                        },
                        {
                            "start": 281,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 590
                        },
                        {
                            "start": 593,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1284
                        },
                        {
                            "start": 1285,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1507
                        },
                        {
                            "start": 1510,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1860
                        },
                        {
                            "start": 1861,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2032
                        },
                        {
                            "start": 2033,
                            "end": 2037
                        },
                        {
                            "start": 2038,
                            "end": 2155
                        },
                        {
                            "start": 2156,
                            "end": 2322
                        },
                        {
                            "start": 2323,
                            "end": 2477
                        },
                        {
                            "start": 2478,
                            "end": 2535
                        },
                        {
                            "start": 2536,
                            "end": 2540
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7470703125
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.",
                    "score": 0.7925543368725948,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 952
                        },
                        {
                            "start": 955,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1811
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 303,
                            "end": 320,
                            "matchedPaperCorpusId": "249152301"
                        },
                        {
                            "start": 338,
                            "end": 357,
                            "matchedPaperCorpusId": "258059852"
                        },
                        {
                            "start": 1177,
                            "end": 1193,
                            "matchedPaperCorpusId": "259501579"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73876953125
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "In light of the definition of the entity-level unlearning task, we propose a two-stage framework for the task, consisting of: 1) Forget Set Construction and 2) Unlearning Execution. Building on this framework, we design entity-level unlearning methods based on trending unlearning algorithms.",
                    "score": 0.6523930572719245,
                    "section_title": "Entity-level Unlearning Framework",
                    "char_start_offset": 6207,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 292
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71240234375
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "In this paper, we propose a novel task: entity-level unlearning for LLMs, which is required in many practical scenarios. We evaluate trending unlearning algorithms in this work and reveal that existing unlearning methods struggle to effectively erase the entire entity from the target model. Furthermore, we find that the size and knowledge coverage of forget set play crucial roles in the performance of the algorithms. Additionally, our analysis shows that entities introduced through fine-tuning are more vulnerable to unlearning compared to pre-trained entities, underscoring the need for more robust entity injection techniques. These findings offer valuable insights and encourage future research to develop corresponding unlearning algorithms and explore more precise knowledge-probing methods to achieve improved entity removal.",
                    "score": 0.8016269738030641,
                    "section_title": "Conclusion",
                    "char_start_offset": 27636,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 836
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7109375
                }
            ],
            "relevance_judgement": 0.8076171875,
            "relevance_judgment_input_expanded": "# Title: Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis\n# Venue: International Conference on Computational Linguistics\n# Authors: Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Bing Qin\n## Abstract\nLarge language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.\n## Introduction\nLarge Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.\n...\nIn contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model. However, due to the inaccessibility of the original training corpora, it is challenging to obtain such a target set, which contains all knowledge associated with the target entity. Consequently, entity-level unlearning requires an additional step to construct a forget set before applying unlearning algorithms, where there is frequently a discrepancy between the constructed forget set and the target set. To this end, we divide the entity-level unlearning task into a two-stage framework, comprising forget set construction and unlearning execution. This process seeks to delete the target set by erasing the construted forget set, which involves more complex procedures and presents greater challenges than instance-level unlearning. \n\nIn this work, we conduct a comprehensive analysis of entity-level unlearning for LLMs, structured around a sequence of focused steps: Firstly, we begin by formally defining and setting the task, which involves injecting pseudo-entity knowledge into the model to establish both the target model and the target set. Secondly, we systematically evaluate the current unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task. Additionally, we observe that the design of forget set significantly influences the effectiveness of the unlearning algorithm. Thirdly, inspired by the above, we further investigate how various constructed forget sets impact algorithm performance. Our results indicate that unlearning with a forget set that has higher knowledge coverage relative to the target set leads to more effective unlearning. Although merely increasing the size of the forget set may enhance knowledge coverage, it would compromise the model's generalization ability, making it an ineffective way to improve performance.\n\n## Entity-level Unlearning Framework\nIn light of the definition of the entity-level unlearning task, we propose a two-stage framework for the task, consisting of: 1) Forget Set Construction and 2) Unlearning Execution. Building on this framework, we design entity-level unlearning methods based on trending unlearning algorithms.\n\n## Unlearning Execution\nAfter constructing the forget set, the next step is to apply the unlearning algorithms to it. In the absence of algorithms specifically designed for entitylevel unlearning, we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model. An ideal entity-level unlearning method should effectively remove the entire entity from the forget set while minimizing any negative impact on the remaining knowledge. This study primarily exam-ines the performance of current unlearning algorithms on entity-level unlearning tasks, without yet delving into the broader algorithmic application framework (Huang et al., 2024).\n\n## Experimental Results\nWe present experimental results comparing the performance of the same method on various forget sets and between different algorithms. The main experimental results of Llama2-7B-Chat-TOFU are shown in Table 1, and the experimental results of Phi-1.5-TOFU are presented in Appendix B.5. \n\nA comparison of the performance across different types of forget sets, as presented in Table 1, reveals that the algorithms based on the target set consistently maintain similar model utility while achieving lower probability, reduced accuracy, and overall higher forget quality on Llama2-7B-Chat-TOFU. A similar trend is observed in the Phi-1.5-TOFU model, as shown in Table 4. These results suggest that models employing the target set engage in more thorough unlearning compared to those utilizing the probing set. This finding highlights that the current unlearning algorithms struggle to generalize effectively to entity-level unlearning tasks when relying on the probing set. Consequently, the construction of the forget set is crucial in determining the success of entity-level unlearning. In the subsequent analysis, we will explore in detail how the quality of the forget set influences unlearning effectiveness. \n\nComparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: \n\n1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. \n\n2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. \n\n3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions.\n\n## Effect of Probing Set\nWe hypothesize that the limited unlearning effect is due to the insufficient coverage of knowledge within the current probing set. To explore this, we introduce a new metric, Knowledge Coverage, designed to assess the knowledge overlap between the probing sets and the target set. This metric is computed using the BERTScore (Zhang et al., 2019) of the closest QA pair match between the probing set and the target set. A higher knowledge coverage indicates that the probing set encompasses a broader range of entity-related knowledge. See Appendix C.1 for detailed definitions and formulas. \n\nTo further investigate the effects of varying knowledge coverage, we construct probing sets with varying degrees of coverage by systematically replacing different ratios of QA pairs within the probing set with those from the target set while keeping the total set size fixed at 20. As demonstrated in Table 7, the knowledge coverage of the constructed probing set increases progressively as the ratio of replaced pairs grows. Then, we applied the five algorithms to perform entity-level unlearning on the probing set under five different replacement ratios. As illustrated in Figure 2, enhancing knowledge coverage of the probing set consistently enhances the forget quality of most methods. Furthermore, each algorithm consistently preserved similar model utility across various probing sets. These results suggest that enhancing the knowledge coverage of the probing set can improve the unlearning effectiveness. \n\nAn intuitive approach to increase knowledge coverage is by increasing the size of the probing set. Therefore, we further explore the performance of the algorithms on probing sets across different sizes. Specifically, we expand the probing set size for each entity and evaluate the performance of the unlearning algorithms across five different sizes. As illustrated in Figure 3, the forget quality of the five unlearning algorithms gradually improves as the size of the probing set increases. However, except for the Pref. Opt. method, the model utility of the unlearned models produced by the other algorithms demonstrates a noticeable decline. This decline in model utility, when compared to the trends in Figure 2, is likely due to the larger number of unlearning steps required as the probing set size grows. These findings suggest that, for most unlearning algorithms, expanding the probing set size leads to a trade-off between forget quality and model utility. Furthermore, according to section 3.2, although the Pref. Opt.\n\n## Conclusion\nIn this paper, we propose a novel task: entity-level unlearning for LLMs, which is required in many practical scenarios. We evaluate trending unlearning algorithms in this work and reveal that existing unlearning methods struggle to effectively erase the entire entity from the target model. Furthermore, we find that the size and knowledge coverage of forget set play crucial roles in the performance of the algorithms. Additionally, our analysis shows that entities introduced through fine-tuning are more vulnerable to unlearning compared to pre-trained entities, underscoring the need for more robust entity injection techniques. These findings offer valuable insights and encourage future research to develop corresponding unlearning algorithms and explore more precise knowledge-probing methods to achieve improved entity removal.",
            "reference_string": "[270703237 | Ma et al. | 2024 | Citations: 6]"
        },
        {
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 15,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.15779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262218493",
                    "name": "Youyang Qu"
                },
                {
                    "authorId": "2294002570",
                    "name": "Ming Ding"
                },
                {
                    "authorId": "2293369504",
                    "name": "Nan Sun"
                },
                {
                    "authorId": "3153007",
                    "name": "Kanchana Thilakarathna"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "1713586",
                    "name": "D. Niyato"
                }
            ],
            "abstract": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
            "corpus_id": 268681648,
            "sentences": [
                {
                    "corpus_id": "268681648",
                    "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
                    "text": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
                    "score": 0.5938661986435216,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8046875
                }
            ],
            "relevance_judgement": 0.8046875,
            "relevance_judgment_input_expanded": "# Title: The Frontier of Data Erasure: Machine Unlearning for Large Language Models\n# Venue: arXiv.org\n# Authors: Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, D. Niyato\n## Abstract\nLarge Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.\n",
            "reference_string": "[268681648 | Qu et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "venue": "",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203800211",
                    "name": "Minseok Choi"
                },
                {
                    "authorId": "2307073048",
                    "name": "Daniel Rim"
                },
                {
                    "authorId": "2294508694",
                    "name": "Dohyun Lee"
                },
                {
                    "authorId": "2260653165",
                    "name": "Jaegul Choo"
                }
            ],
            "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
            "corpus_id": 270562084,
            "sentences": [
                {
                    "corpus_id": "270562084",
                    "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
                    "text": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
                    "score": 0.6959158103993507,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7822265625
                }
            ],
            "relevance_judgement": 0.7822265625,
            "relevance_judgment_input_expanded": "# Title: Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport\n# Venue: \n# Authors: Minseok Choi, Daniel Rim, Dohyun Lee, Jaegul Choo\n## Abstract\nInstruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.\n",
            "reference_string": "[270562084 | Choi et al. | 2024 | Citations: 2]"
        },
        {
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 84,
            "influential_citation_count": 30,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2286638403",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2261353791",
                    "name": "Jaechan Lee"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2266698166",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2309248199",
                    "name": "Ari Holtzman"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2309424274",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                }
            ],
            "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "corpus_id": 271064299,
            "sentences": [
                {
                    "corpus_id": "271064299",
                    "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                    "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
                    "score": 0.7317119465768274,
                    "section_title": "Related Work",
                    "char_start_offset": 24726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 65,
                            "end": 144
                        },
                        {
                            "start": 144,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 578
                        },
                        {
                            "start": 580,
                            "end": 744
                        },
                        {
                            "start": 746,
                            "end": 912
                        },
                        {
                            "start": 912,
                            "end": 1134
                        },
                        {
                            "start": 1134,
                            "end": 1196
                        },
                        {
                            "start": 1196,
                            "end": 1418
                        },
                        {
                            "start": 1418,
                            "end": 1635
                        },
                        {
                            "start": 1635,
                            "end": 1881
                        },
                        {
                            "start": 1883,
                            "end": 1934
                        },
                        {
                            "start": 1934,
                            "end": 2021
                        },
                        {
                            "start": 2021,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 382,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 1380,
                            "end": 1396,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 1396,
                            "end": 1417,
                            "matchedPaperCorpusId": "259088549"
                        },
                        {
                            "start": 1563,
                            "end": 1580,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7763671875
                }
            ],
            "relevance_judgement": 0.7763671875,
            "relevance_judgment_input_expanded": "# Title: MUSE: Machine Unlearning Six-Way Evaluation for Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke S. Zettlemoyer, Noah A. Smith, Chiyuan Zhang\n## Abstract\nLanguage models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io\n## Related Work\nMachine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "reference_string": "[271064299 | Shi et al. | 2024 | Citations: 84]"
        },
        {
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284461064",
                    "name": "Yijiang River Dong"
                },
                {
                    "authorId": "2325112252",
                    "name": "Hongzhou Lin"
                },
                {
                    "authorId": "2284217404",
                    "name": "Mikhail Belkin"
                },
                {
                    "authorId": "2284217279",
                    "name": "Ramon Huerta"
                },
                {
                    "authorId": "2267339029",
                    "name": "Ivan Vuli'c"
                }
            ],
            "abstract": "Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.",
            "corpus_id": 267681754,
            "sentences": [
                {
                    "corpus_id": "267681754",
                    "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                    "text": "We now evaluate our method on the MUSE dataset (Shi et al., 2024), a recent, most comprehensive LLM unlearning benchmark. There, the data are coming from BBC News passages. The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages. \n\nFor evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation. Following the setup of Shi et al. (2024), we use LLaMA-2 7B (Touvron et al., 2023) as the base model and LoRA (Hu et al., 2022) with rank r set to 8 to fit the fine-tuning onto one NVIDIA A100. \n\nUNDIAL Achieves a Better Pareto Frontier. Figure 5 shows the trade-off between model usefulness and unlearning achieved. By varying the unlearning strength, we observe that UNDIAL achieves a superior Pareto Frontier compared to the baseline methods, including both direct-tuning ones and the ones relying on auxiliary models (see \u00a72). \n\nDirect tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024;Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL. \n\nImportantly, UNDIAL has the unique ability to achieve state-of-the-art performance without even relying on the Retain set at all. Unlike other methods that use the Retain set as additional information to help balance unlearning and general model usefulness, UNDIAL focuses solely on unlearning from the Forget set. This underscores the power of UNDIAL is More Robust to Different Hyperparameter Setups.",
                    "score": 0.6509886961823013,
                    "section_title": "Case Study Two: MUSE Benchmark",
                    "char_start_offset": 20216,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 121
                        },
                        {
                            "start": 122,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 378
                        },
                        {
                            "start": 381,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 932
                        },
                        {
                            "start": 935,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1269
                        },
                        {
                            "start": 1272,
                            "end": 1366
                        },
                        {
                            "start": 1367,
                            "end": 1572
                        },
                        {
                            "start": 1573,
                            "end": 1667
                        },
                        {
                            "start": 1670,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1984
                        },
                        {
                            "start": 1985,
                            "end": 2072
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 47,
                            "end": 65,
                            "matchedPaperCorpusId": "271064299"
                        },
                        {
                            "start": 762,
                            "end": 779,
                            "matchedPaperCorpusId": "271064299"
                        },
                        {
                            "start": 849,
                            "end": 866,
                            "matchedPaperCorpusId": "235458009"
                        },
                        {
                            "start": 1532,
                            "end": 1552,
                            "matchedPaperCorpusId": "269009619"
                        },
                        {
                            "start": 1552,
                            "end": 1571,
                            "matchedPaperCorpusId": "266933371"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.77294921875
                }
            ],
            "relevance_judgement": 0.77294921875,
            "relevance_judgment_input_expanded": "# Title: UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vuli'c\n## Abstract\nMitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.\n## Case Study Two: MUSE Benchmark\nWe now evaluate our method on the MUSE dataset (Shi et al., 2024), a recent, most comprehensive LLM unlearning benchmark. There, the data are coming from BBC News passages. The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages. \n\nFor evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation. Following the setup of Shi et al. (2024), we use LLaMA-2 7B (Touvron et al., 2023) as the base model and LoRA (Hu et al., 2022) with rank r set to 8 to fit the fine-tuning onto one NVIDIA A100. \n\nUNDIAL Achieves a Better Pareto Frontier. Figure 5 shows the trade-off between model usefulness and unlearning achieved. By varying the unlearning strength, we observe that UNDIAL achieves a superior Pareto Frontier compared to the baseline methods, including both direct-tuning ones and the ones relying on auxiliary models (see \u00a72). \n\nDirect tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024;Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL. \n\nImportantly, UNDIAL has the unique ability to achieve state-of-the-art performance without even relying on the Retain set at all. Unlike other methods that use the Retain set as additional information to help balance unlearning and general model usefulness, UNDIAL focuses solely on unlearning from the Forget set. This underscores the power of UNDIAL is More Robust to Different Hyperparameter Setups.",
            "reference_string": "[267681754 | Dong et al. | 2024 | Citations: 8]"
        },
        {
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 96,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306067819",
                    "name": "Yaxuan Wang"
                },
                {
                    "authorId": "2306500340",
                    "name": "Jiaheng Wei"
                },
                {
                    "authorId": "2271515779",
                    "name": "Chris Liu"
                },
                {
                    "authorId": "2284760719",
                    "name": "Jinlong Pang"
                },
                {
                    "authorId": "2326243943",
                    "name": "Quan Liu"
                },
                {
                    "authorId": "2316588330",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2306754738",
                    "name": "Yujia Bao"
                },
                {
                    "authorId": "2306028548",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2306480290",
                    "name": "Wei Wei"
                }
            ],
            "abstract": "Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a\"flat\"loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.",
            "corpus_id": 273350971,
            "sentences": [
                {
                    "corpus_id": "273350971",
                    "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
                    "text": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge. \n\nIn summary, common loss adjustment methods employ one [24], two [23,20,22], or all three [5] of these components to guide the model towards forgetting specific data while minimizing the impact on its overall performance and utility. The interplay between these terms allows for controlled and targeted unlearning, ensuring the model retains its valuable capabilities while selectively forgetting undesired information. More detailed formulations of these loss adjustmentbased methods, along with related work, are deferred to Appendix C.1 and Appendix E. \n\nAn Example: Large Language Model Unlearning (LLMU). We adopt a popular approach in LLM unlearning, LLMU [5], to interpret a special case of Eqn. (1).",
                    "score": 0.586077536993179,
                    "section_title": "Existing LLM Unlearning Paradigm",
                    "char_start_offset": 5849,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 255
                        },
                        {
                            "start": 258,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 673
                        },
                        {
                            "start": 676,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1087
                        },
                        {
                            "start": 1090,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1341
                        },
                        {
                            "start": 1342,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1897
                        },
                        {
                            "start": 1898,
                            "end": 2033
                        },
                        {
                            "start": 2036,
                            "end": 2087
                        },
                        {
                            "start": 2088,
                            "end": 2180
                        },
                        {
                            "start": 2181,
                            "end": 2185
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1543,
                            "end": 1547,
                            "matchedPaperCorpusId": "247627962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7685546875
                }
            ],
            "relevance_judgement": 0.7685546875,
            "relevance_judgment_input_expanded": "# Title: LLM Unlearning via Loss Adjustment with Only Forget Data\n# Venue: International Conference on Learning Representations\n# Authors: Yaxuan Wang, Jiaheng Wei, Chris Liu, Jinlong Pang, Quan Liu, Ankit Shah, Yujia Bao, Yang Liu, Wei Wei\n## Abstract\nUnlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a\"flat\"loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.\n## Existing LLM Unlearning Paradigm\nThe mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge. \n\nIn summary, common loss adjustment methods employ one [24], two [23,20,22], or all three [5] of these components to guide the model towards forgetting specific data while minimizing the impact on its overall performance and utility. The interplay between these terms allows for controlled and targeted unlearning, ensuring the model retains its valuable capabilities while selectively forgetting undesired information. More detailed formulations of these loss adjustmentbased methods, along with related work, are deferred to Appendix C.1 and Appendix E. \n\nAn Example: Large Language Model Unlearning (LLMU). We adopt a popular approach in LLM unlearning, LLMU [5], to interpret a special case of Eqn. (1).",
            "reference_string": "[273350971 | Wang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 75,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326503114",
                    "name": "Haoming Xu"
                },
                {
                    "authorId": "2182474634",
                    "name": "Ningyuan Zhao"
                },
                {
                    "authorId": "2345879531",
                    "name": "Liming Yang"
                },
                {
                    "authorId": "2345876908",
                    "name": "Sendong Zhao"
                },
                {
                    "authorId": "152931849",
                    "name": "Shumin Deng"
                },
                {
                    "authorId": "2218346459",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2314827895",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "2266753656",
                    "name": "Nay Oo"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2153010067",
                    "name": "Ningyu Zhang"
                }
            ],
            "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "corpus_id": 276408369,
            "sentences": [
                {
                    "corpus_id": "276408369",
                    "title": "ReLearn: Unlearning via Learning for Large Language Models",
                    "text": "The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
                    "score": 0.5913039164847532,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1644
                        },
                        {
                            "start": 1647,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1894
                        },
                        {
                            "start": 1895,
                            "end": 2025
                        },
                        {
                            "start": 2026,
                            "end": 2207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 158,
                            "matchedPaperCorpusId": "265659278"
                        },
                        {
                            "start": 758,
                            "end": 777,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 821,
                            "end": 842,
                            "matchedPaperCorpusId": "269009619"
                        },
                        {
                            "start": 1071,
                            "end": 1092,
                            "matchedPaperCorpusId": "7357141"
                        },
                        {
                            "start": 2006,
                            "end": 2024,
                            "matchedPaperCorpusId": "3868529"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76806640625
                },
                {
                    "corpus_id": "276408369",
                    "title": "ReLearn: Unlearning via Learning for Large Language Models",
                    "text": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
                    "score": 0.5870163061518228,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76171875
                }
            ],
            "relevance_judgement": 0.76806640625,
            "relevance_judgment_input_expanded": "# Title: ReLearn: Unlearning via Learning for Large Language Models\n# Venue: arXiv.org\n# Authors: Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Meng Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang\n## Abstract\nCurrent unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.\n## Introduction\nThe widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
            "reference_string": "[276408369 | Xu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 132,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07579",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89583148",
                    "name": "Martin Pawelczyk"
                },
                {
                    "authorId": "2273685865",
                    "name": "Seth Neel"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ],
            "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "corpus_id": 263834631,
            "sentences": [
                {
                    "corpus_id": "263834631",
                    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                    "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
                    "score": 0.6871927294495634,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.767578125
                },
                {
                    "corpus_id": "263834631",
                    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                    "text": "2). Our ICUL method does not require knowledge of the LLM's parameters, and yet manages to achieve performance levels that are competitive with or in some cases exceed the state-of-the-art LLM unlearning methods which require access to LLM parameters and involve expensive gradient computations (Jang et al., 2023). \n\nWe experiment with multiple established real world datasets: AG-News, SST-2, SQUAD, and Amazon reviews to evaluate the effectiveness of our proposed unlearning method. Our results on text classification and question-answering tasks clearly demonstrate the efficacy of the proposed unlearning method, and highlight that it practically eliminates a training point's influence on the model output. These results indicate the significant potential for unlearning training points from black-box models. Our proposed methods and findings offer a new perspective on unlearning mechanisms in LLMs: \n\n\u2022 New unlearning paradigm for LLMs: This is the first work to use in-context learning for machine unlearning by specifically constructing contexts that induce model behavior that is indistinguishable from the behavior of a re-trained model. \u2022 New empirical unlearning evaluation: In Subsection 3.2 we introduce LiRA-Forget, a new unlearning evaluation that adapts the LiRA MIA (Carlini et al., 2022) to the problem of evaluating unlearning. We note that a similar evaluation metric was introduced concurrently by Kurmanji et al. (2023), and subsequently studied by Hayes et al. (2024) under the name U-LiRA. \n\nWe discuss these works more in Section 2. \u2022 Data deletion from blackbox models: ICUL does not require access to model parameters and can be readily applied to blackbox models. This makes it a useful tool to patch a model until the model can be updated or a retrained version can be deployed at the next deployment phase. Thus, it is complementary to existing white-box unlearning techniques which have higher computational burdens. \u2022 Lower memory requirements: Our method boasts lower memory requirements compared to state-of-theart unlearning methods like Gradient Ascent (GA), especially as the size of the LLM increases.",
                    "score": 0.5994257112161907,
                    "section_title": "Introduction",
                    "char_start_offset": 4024,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 3
                        },
                        {
                            "start": 4,
                            "end": 315
                        },
                        {
                            "start": 318,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 815
                        },
                        {
                            "start": 816,
                            "end": 907
                        },
                        {
                            "start": 910,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1350
                        },
                        {
                            "start": 1351,
                            "end": 1517
                        },
                        {
                            "start": 1520,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2143
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 295,
                            "end": 314,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1287,
                            "end": 1309,
                            "matchedPaperCorpusId": "244920593"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73583984375
                }
            ],
            "relevance_judgement": 0.767578125,
            "relevance_judgment_input_expanded": "# Title: In-Context Unlearning: Language Models as Few Shot Unlearners\n# Venue: International Conference on Machine Learning\n# Authors: Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju\n## Abstract\nMachine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.\n## Introduction\n2). Our ICUL method does not require knowledge of the LLM's parameters, and yet manages to achieve performance levels that are competitive with or in some cases exceed the state-of-the-art LLM unlearning methods which require access to LLM parameters and involve expensive gradient computations (Jang et al., 2023). \n\nWe experiment with multiple established real world datasets: AG-News, SST-2, SQUAD, and Amazon reviews to evaluate the effectiveness of our proposed unlearning method. Our results on text classification and question-answering tasks clearly demonstrate the efficacy of the proposed unlearning method, and highlight that it practically eliminates a training point's influence on the model output. These results indicate the significant potential for unlearning training points from black-box models. Our proposed methods and findings offer a new perspective on unlearning mechanisms in LLMs: \n\n\u2022 New unlearning paradigm for LLMs: This is the first work to use in-context learning for machine unlearning by specifically constructing contexts that induce model behavior that is indistinguishable from the behavior of a re-trained model. \u2022 New empirical unlearning evaluation: In Subsection 3.2 we introduce LiRA-Forget, a new unlearning evaluation that adapts the LiRA MIA (Carlini et al., 2022) to the problem of evaluating unlearning. We note that a similar evaluation metric was introduced concurrently by Kurmanji et al. (2023), and subsequently studied by Hayes et al. (2024) under the name U-LiRA. \n\nWe discuss these works more in Section 2. \u2022 Data deletion from blackbox models: ICUL does not require access to model parameters and can be readily applied to blackbox models. This makes it a useful tool to patch a model until the model can be updated or a retrained version can be deployed at the next deployment phase. Thus, it is complementary to existing white-box unlearning techniques which have higher computational burdens. \u2022 Lower memory requirements: Our method boasts lower memory requirements compared to state-of-theart unlearning methods like Gradient Ascent (GA), especially as the size of the LLM increases.",
            "reference_string": "[263834631 | Pawelczyk et al. | 2023 | Citations: 132]"
        },
        {
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325990741",
                    "name": "YuXuan Wu"
                },
                {
                    "authorId": "1591111757",
                    "name": "Bonaventure F. P. Dossou"
                },
                {
                    "authorId": "2326253445",
                    "name": "Dianbo Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.",
            "corpus_id": 273350773,
            "sentences": [
                {
                    "corpus_id": "273350773",
                    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                    "text": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
                    "score": 0.5788268946925287,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4004,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 908
                        },
                        {
                            "start": 911,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1606
                        },
                        {
                            "start": 1609,
                            "end": 1875
                        },
                        {
                            "start": 1878,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2134
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 583,
                            "end": 605,
                            "matchedPaperCorpusId": "235474438"
                        },
                        {
                            "start": 741,
                            "end": 760,
                            "matchedPaperCorpusId": "232404451"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76708984375
                }
            ],
            "relevance_judgement": 0.76708984375,
            "relevance_judgment_input_expanded": "# Title: CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept\n# Venue: arXiv.org\n# Authors: YuXuan Wu, Bonaventure F. P. Dossou, Dianbo Liu\n## Abstract\nLarge Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.\n## RELATED WORK\nMachine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
            "reference_string": "[273350773 | Wu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "venue": "",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1702997626",
                    "name": "Xinchi Qiu"
                },
                {
                    "authorId": "2302373311",
                    "name": "William F. Shen"
                },
                {
                    "authorId": "2308073429",
                    "name": "Yihong Chen"
                },
                {
                    "authorId": "2313189467",
                    "name": "Nicola Cancedda"
                },
                {
                    "authorId": "1918552",
                    "name": "Pontus Stenetorp"
                },
                {
                    "authorId": "2298756346",
                    "name": "N. Lane"
                }
            ],
            "abstract": "While unlearning knowledge from large language models (LLMs) is receiving increasing attention, one important aspect remains unexplored. Existing approaches and benchmarks assume data points to-be-forgotten are independent, ignoring their inter-connectivity - a fundamental characteristic of real-world data structures. In this paper, we propose PISTOL, a method for compiling structural datasets. PISTOL leverages the inherently structured nature of contractual relationships, offering several key benefits. First, it enables insights into the impact of structural data on unlearning effectiveness. Second, it provides precise and concise ground truths for clearer evaluation. Third, its attribute generation does not require input from pre-trained LLMs, mitigating confounding risks. Leveraging datasets synthesized using PISTOL, we demonstrate how data inter-connectivity impacts LLM unlearning. Specifically, (a) in both the pre-trained and fine-tuned models, unlearning difficulty increases as data inter-connectivity grows, (b) there is a positive correlation between the density of the knowledge graph and unlearning difficulty, and (c) when the to-be-forgotten data is skewed towards one domain, balancing retaining performance across all domains is challenging.",
            "corpus_id": 270703035,
            "sentences": [
                {
                    "corpus_id": "270703035",
                    "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
                    "text": "In this section, we introduce the experimental and evaluation setup and evaluation methods for the new structural LLMs unlearning considerations. \n\nMetrics. We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data. In contrast, a higher DS reflects poorer unlearning, suggesting a weaker distinction between forget and retained knowledge. More details and other supplementary metrics, including the original ROUGE1 scores, MRR and the Top Hit Rate, can be found in Appendix D. Unlearning baselines We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024). Given the nascence of the field, existing unlearning methods often lack robustness. However, these methods represent the current mainstream and serve well to demonstrate the impact of structural datasets while inspiring further research. Base models. We evaluate all baseline methods using the current widely adopted language models Llama2-7B (Touvron et al., 2023), Gemma-7B (Team et al., 2024) and Mistral-7B (Jiang et al., 2023). We evaluated learning rates between 1\u00d710 \u22126 and 5\u00d710 \u22125 during unlearning and found that all methods are highly sensitive to learning rate and batch size selection.",
                    "score": 0.5783269814514724,
                    "section_title": "Evaluation Setup",
                    "char_start_offset": 12700,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 148,
                            "end": 156
                        },
                        {
                            "start": 157,
                            "end": 399
                        },
                        {
                            "start": 400,
                            "end": 625
                        },
                        {
                            "start": 626,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1742
                        },
                        {
                            "start": 1743,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2102
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1254,
                            "end": 1272,
                            "matchedPaperCorpusId": "247627962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76513671875
                }
            ],
            "relevance_judgement": 0.76513671875,
            "relevance_judgment_input_expanded": "# Title: How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective\n# Venue: \n# Authors: Xinchi Qiu, William F. Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, N. Lane\n## Abstract\nWhile unlearning knowledge from large language models (LLMs) is receiving increasing attention, one important aspect remains unexplored. Existing approaches and benchmarks assume data points to-be-forgotten are independent, ignoring their inter-connectivity - a fundamental characteristic of real-world data structures. In this paper, we propose PISTOL, a method for compiling structural datasets. PISTOL leverages the inherently structured nature of contractual relationships, offering several key benefits. First, it enables insights into the impact of structural data on unlearning effectiveness. Second, it provides precise and concise ground truths for clearer evaluation. Third, its attribute generation does not require input from pre-trained LLMs, mitigating confounding risks. Leveraging datasets synthesized using PISTOL, we demonstrate how data inter-connectivity impacts LLM unlearning. Specifically, (a) in both the pre-trained and fine-tuned models, unlearning difficulty increases as data inter-connectivity grows, (b) there is a positive correlation between the density of the knowledge graph and unlearning difficulty, and (c) when the to-be-forgotten data is skewed towards one domain, balancing retaining performance across all domains is challenging.\n## Evaluation Setup\nIn this section, we introduce the experimental and evaluation setup and evaluation methods for the new structural LLMs unlearning considerations. \n\nMetrics. We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data. In contrast, a higher DS reflects poorer unlearning, suggesting a weaker distinction between forget and retained knowledge. More details and other supplementary metrics, including the original ROUGE1 scores, MRR and the Top Hit Rate, can be found in Appendix D. Unlearning baselines We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024). Given the nascence of the field, existing unlearning methods often lack robustness. However, these methods represent the current mainstream and serve well to demonstrate the impact of structural datasets while inspiring further research. Base models. We evaluate all baseline methods using the current widely adopted language models Llama2-7B (Touvron et al., 2023), Gemma-7B (Team et al., 2024) and Mistral-7B (Jiang et al., 2023). We evaluated learning rates between 1\u00d710 \u22126 and 5\u00d710 \u22125 during unlearning and found that all methods are highly sensitive to learning rate and batch size selection.",
            "reference_string": "[270703035 | Qiu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283127924",
                    "name": "Jinwei Hu"
                },
                {
                    "authorId": "2334128394",
                    "name": "Zhenglin Huang"
                },
                {
                    "authorId": "2273924576",
                    "name": "Xiangyu Yin"
                },
                {
                    "authorId": "8103173",
                    "name": "Wenjie Ruan"
                },
                {
                    "authorId": "2333978808",
                    "name": "Guangliang Cheng"
                },
                {
                    "authorId": "2304902376",
                    "name": "Yi Dong"
                },
                {
                    "authorId": "2288349292",
                    "name": "Xiaowei Huang"
                }
            ],
            "abstract": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
            "corpus_id": 276107656,
            "sentences": [
                {
                    "corpus_id": "276107656",
                    "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
                    "text": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
                    "score": 0.5869214470626586,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76318359375
                }
            ],
            "relevance_judgement": 0.76318359375,
            "relevance_judgment_input_expanded": "# Title: FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model\n# Venue: arXiv.org\n# Authors: Jinwei Hu, Zhenglin Huang, Xiangyu Yin, Wenjie Ruan, Guangliang Cheng, Yi Dong, Xiaowei Huang\n## Abstract\nLarge language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.\n",
            "reference_string": "[276107656 | Hu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "A More Practical Approach to Machine Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 18,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309006422",
                    "name": "David Zagardo"
                }
            ],
            "abstract": "Machine learning models often incorporate vast amounts of data, raising significant privacy concerns. Machine unlearning, the ability to remove the influence of specific data points from a trained model, addresses these concerns. This paper explores practical methods for implementing machine unlearning, focusing on a first-epoch gradient-ascent approach. Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch gradient unlearning is more effective than multi-epoch gradients. 2. Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective unlearning. Gradients from the output layers (11 and 12) have no impact. Efficient unlearning can be achieved using only the embedding layer, halving space complexity. 3. Influence Functions&Scoring: Techniques like Hessian Vector Product and the dot product of activations and tensors are used for quantifying unlearning. 4. Gradient Ascent Considerations: Calibration is necessary to avoid overexposing the model to specific data points during unlearning, which could prematurely terminate the process. 5. Fuzzy Matching vs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new optimum, while iterative unlearning provides a more complete modality. Our empirical evaluation confirms that first-epoch gradient ascent for machine unlearning is more effective than whole-model gradient ascent. These results highlight the potential of machine unlearning for enhancing data privacy and compliance with regulations such as GDPR and CCPA. The study underscores the importance of formal methods to comprehensively evaluate the unlearning process.",
            "corpus_id": 270440333,
            "sentences": [
                {
                    "corpus_id": "270440333",
                    "title": "A More Practical Approach to Machine Unlearning",
                    "text": "Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].Neel et al. (2021) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters [10].Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14].\n\nRecent studies have also focused on the embedding layer's role in the unlearning process.Jang et al. (2022) highlight the critical function of the embedding layer in representing input tokens, making it an effective focal point for unlearning operations [8].Eldan and Russinovich (2023) further explore the potential of embedding-layer unlearning, finding that targeting this layer can efficiently reduce the influence of specific data points without significantly impacting the model's overall performance [4].",
                    "score": 0.5763988909379727,
                    "section_title": "Gradient-Based Unlearning",
                    "char_start_offset": 1762,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 128,
                            "end": 303
                        },
                        {
                            "start": 303,
                            "end": 486
                        },
                        {
                            "start": 486,
                            "end": 729
                        },
                        {
                            "start": 731,
                            "end": 820
                        },
                        {
                            "start": 820,
                            "end": 989
                        },
                        {
                            "start": 989,
                            "end": 1242
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 303,
                            "end": 321,
                            "matchedPaperCorpusId": "220364296"
                        },
                        {
                            "start": 481,
                            "end": 485,
                            "matchedPaperCorpusId": "220364296"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76123046875
                }
            ],
            "relevance_judgement": 0.76123046875,
            "relevance_judgment_input_expanded": "# Title: A More Practical Approach to Machine Unlearning\n# Venue: arXiv.org\n# Authors: David Zagardo\n## Abstract\nMachine learning models often incorporate vast amounts of data, raising significant privacy concerns. Machine unlearning, the ability to remove the influence of specific data points from a trained model, addresses these concerns. This paper explores practical methods for implementing machine unlearning, focusing on a first-epoch gradient-ascent approach. Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch gradient unlearning is more effective than multi-epoch gradients. 2. Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective unlearning. Gradients from the output layers (11 and 12) have no impact. Efficient unlearning can be achieved using only the embedding layer, halving space complexity. 3. Influence Functions&Scoring: Techniques like Hessian Vector Product and the dot product of activations and tensors are used for quantifying unlearning. 4. Gradient Ascent Considerations: Calibration is necessary to avoid overexposing the model to specific data points during unlearning, which could prematurely terminate the process. 5. Fuzzy Matching vs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new optimum, while iterative unlearning provides a more complete modality. Our empirical evaluation confirms that first-epoch gradient ascent for machine unlearning is more effective than whole-model gradient ascent. These results highlight the potential of machine unlearning for enhancing data privacy and compliance with regulations such as GDPR and CCPA. The study underscores the importance of formal methods to comprehensively evaluate the unlearning process.\n## Gradient-Based Unlearning\nGradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].Neel et al. (2021) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters [10].Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14].\n\nRecent studies have also focused on the embedding layer's role in the unlearning process.Jang et al. (2022) highlight the critical function of the embedding layer in representing input tokens, making it an effective focal point for unlearning operations [8].Eldan and Russinovich (2023) further explore the potential of embedding-layer unlearning, finding that targeting this layer can efficiently reduce the influence of specific data points without significantly impacting the model's overall performance [4].",
            "reference_string": "[270440333 | Zagardo | 2024 | Citations: 0]"
        },
        {
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2131875272",
                    "name": "Anmol Reddy Mekala"
                },
                {
                    "authorId": "2122341489",
                    "name": "Vineeth Dorna"
                },
                {
                    "authorId": "2322096025",
                    "name": "Shreya Dubey"
                },
                {
                    "authorId": "2322095427",
                    "name": "Abhishek Lalwani"
                },
                {
                    "authorId": "2120266368",
                    "name": "David Koleczek"
                },
                {
                    "authorId": "35315626",
                    "name": "Mukund Rungta"
                },
                {
                    "authorId": "2322137989",
                    "name": "Sadid A. Hasan"
                },
                {
                    "authorId": "2322095799",
                    "name": "Elita A. Lobo"
                }
            ],
            "abstract": "Machine unlearning aims to efficiently eliminate the influence of specific training data, known as the forget set, from the model. However, existing unlearning methods for Large Language Models (LLMs) face a critical challenge: they rely solely on negative feedback to suppress responses related to the forget set, which often results in nonsensical or inconsistent outputs, diminishing model utility and posing potential privacy risks. To address this limitation, we propose a novel approach called Alternate Preference Optimization (AltPO), which combines negative feedback with in-domain positive feedback on the forget set. Additionally, we introduce new evaluation metrics to assess the quality of responses related to the forget set. Extensive experiments show that our approach not only enables effective unlearning but also avoids undesirable model behaviors while maintaining overall model performance. Our implementation can be found at https://github.com/molereddy/Alternate-Preference-Optimization.",
            "corpus_id": 272770202,
            "sentences": [
                {
                    "corpus_id": "272770202",
                    "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
                    "text": "We now discuss two closely related approaches: Eldan and Russinovich (2023) and Dong et al. ( 2024) use positive feedback on the forget set to stabilize unlearning by substituting privacy-sensitive \"anchor\" words with alternate positive token-level labels. Eldan and Russinovich (2023) uses GPT-4 to identify anchor tokens, while Dong et al. ( 2024) considers all nouns as anchors. In contrast, our method avoids selecting specific anchor words and generates multiple alternate answers consistent with the original question. Dong et al. (2024) derives alternate completions based on next-token probabilities, excluding the highest-ranked token and (Eldan and Russinovich, 2023) uses scores from a model trained further on the forget set along with substitutions proposed by GPT-4. We simplify this by directly instructing an LLM to generate multiple alternative answers. While both works use a cross-entropy loss, our AltPO method employs a DPO-style loss to align the model with alternate answers, explicitly incorporating negative feedback. Ablation studies in Section 6.4 show how these elements improve our method's performance. We provide a broader review of the machine unlearning literature in Appendix A. \n\nAround the time of submitting this work, we became aware of parallel research by Jin et al. ( 2024), who proposed a similar approach in the RWKU (Real World Knowledge Unlearning) benchmark (Jin et al., 2024). RWKU explores unlearning famous real-world entities without access to a defined forget dataset that introduced knowledge of the entities. One of their DPO baselines shares key elements with our method: prompting the model to generate both knowledge about the forget entity and alternative facts, then applying a DPO objective to align the model with these alternatives. While Jin et al. ( 2024) report that their approach improves fluency compared to NPO and IdkPO, it exhibits weaker forgetting performance on RWKU. Key differences between their DPO baseline and AltPO include: (1) They omit a retain set loss to control unlearning generalization beyond the forget set, which we identified as crucial and thus incorporated in all baselines and our method.",
                    "score": 0.58885239511399,
                    "section_title": "Related Work",
                    "char_start_offset": 15360,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 256
                        },
                        {
                            "start": 257,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1212
                        },
                        {
                            "start": 1215,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1793
                        },
                        {
                            "start": 1794,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2180
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7587890625
                }
            ],
            "relevance_judgement": 0.7587890625,
            "relevance_judgment_input_expanded": "# Title: Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models\n# Venue: arXiv.org\n# Authors: Anmol Reddy Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid A. Hasan, Elita A. Lobo\n## Abstract\nMachine unlearning aims to efficiently eliminate the influence of specific training data, known as the forget set, from the model. However, existing unlearning methods for Large Language Models (LLMs) face a critical challenge: they rely solely on negative feedback to suppress responses related to the forget set, which often results in nonsensical or inconsistent outputs, diminishing model utility and posing potential privacy risks. To address this limitation, we propose a novel approach called Alternate Preference Optimization (AltPO), which combines negative feedback with in-domain positive feedback on the forget set. Additionally, we introduce new evaluation metrics to assess the quality of responses related to the forget set. Extensive experiments show that our approach not only enables effective unlearning but also avoids undesirable model behaviors while maintaining overall model performance. Our implementation can be found at https://github.com/molereddy/Alternate-Preference-Optimization.\n## Related Work\nWe now discuss two closely related approaches: Eldan and Russinovich (2023) and Dong et al. ( 2024) use positive feedback on the forget set to stabilize unlearning by substituting privacy-sensitive \"anchor\" words with alternate positive token-level labels. Eldan and Russinovich (2023) uses GPT-4 to identify anchor tokens, while Dong et al. ( 2024) considers all nouns as anchors. In contrast, our method avoids selecting specific anchor words and generates multiple alternate answers consistent with the original question. Dong et al. (2024) derives alternate completions based on next-token probabilities, excluding the highest-ranked token and (Eldan and Russinovich, 2023) uses scores from a model trained further on the forget set along with substitutions proposed by GPT-4. We simplify this by directly instructing an LLM to generate multiple alternative answers. While both works use a cross-entropy loss, our AltPO method employs a DPO-style loss to align the model with alternate answers, explicitly incorporating negative feedback. Ablation studies in Section 6.4 show how these elements improve our method's performance. We provide a broader review of the machine unlearning literature in Appendix A. \n\nAround the time of submitting this work, we became aware of parallel research by Jin et al. ( 2024), who proposed a similar approach in the RWKU (Real World Knowledge Unlearning) benchmark (Jin et al., 2024). RWKU explores unlearning famous real-world entities without access to a defined forget dataset that introduced knowledge of the entities. One of their DPO baselines shares key elements with our method: prompting the model to generate both knowledge about the forget entity and alternative facts, then applying a DPO objective to align the model with these alternatives. While Jin et al. ( 2024) report that their approach improves fluency compared to NPO and IdkPO, it exhibits weaker forgetting performance on RWKU. Key differences between their DPO baseline and AltPO include: (1) They omit a retain set loss to control unlearning generalization beyond the forget set, which we identified as crucial and thus incorporated in all baselines and our method.",
            "reference_string": "[272770202 | Mekala et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "venue": "",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363922",
                    "name": "Qizhou Wang"
                },
                {
                    "authorId": "2301791087",
                    "name": "Bo Han"
                },
                {
                    "authorId": "2306755179",
                    "name": "Puning Yang"
                },
                {
                    "authorId": "2143475848",
                    "name": "Jianing Zhu"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "2313482750",
                    "name": "Masashi Sugiyama"
                }
            ],
            "abstract": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
            "corpus_id": 270440344,
            "sentences": [
                {
                    "corpus_id": "270440344",
                    "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
                    "text": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
                    "score": 0.7011879757504227,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.755859375
                }
            ],
            "relevance_judgement": 0.755859375,
            "relevance_judgment_input_expanded": "# Title: Towards Effective Evaluations and Comparisons for LLM Unlearning Methods\n# Venue: \n# Authors: Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi Sugiyama\n## Abstract\nThe imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.\n",
            "reference_string": "[270440344 | Wang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "2149157242",
                    "name": "Sungjun Cho"
                },
                {
                    "authorId": "1474356736",
                    "name": "Dasol Hwang"
                },
                {
                    "authorId": "2313692227",
                    "name": "Moontae Lee"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
            "corpus_id": 271860124,
            "sentences": [
                {
                    "corpus_id": "271860124",
                    "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                    "text": "This motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
                    "score": 0.5810760658828873,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1605,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 980
                        },
                        {
                            "start": 983,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1990
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 441,
                            "end": 459,
                            "matchedPaperCorpusId": "258615571"
                        },
                        {
                            "start": 464,
                            "end": 481,
                            "matchedPaperCorpusId": "267681958"
                        },
                        {
                            "start": 1129,
                            "end": 1146,
                            "matchedPaperCorpusId": "235458009"
                        },
                        {
                            "start": 1478,
                            "end": 1501,
                            "matchedPaperCorpusId": "269791237"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74462890625
                }
            ],
            "relevance_judgement": 0.74462890625,
            "relevance_judgment_input_expanded": "# Title: Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs\n# Venue: International Conference on Learning Representations\n# Authors: Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee\n## Abstract\nLarge Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.\n## INTRODUCTION\nThis motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
            "reference_string": "[271860124 | Cha et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11614, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306946364",
                    "name": "Yihuai Hong"
                },
                {
                    "authorId": "2306950395",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "2143278592",
                    "name": "Shauli Ravfogel"
                },
                {
                    "authorId": "2307764517",
                    "name": "Haiqin Yang"
                },
                {
                    "authorId": "22245981",
                    "name": "Mor Geva"
                }
            ],
            "abstract": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
            "corpus_id": 270560986,
            "sentences": [
                {
                    "corpus_id": "270560986",
                    "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                    "text": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
                    "score": 0.6890018799816445,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7431640625
                },
                {
                    "corpus_id": "270560986",
                    "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                    "text": "Recently, there has been surging interest in developing methods for unlearning information captured in large language models (LLMs) (Jang et al., 2023;Chen & Yang, 2023;Yao et al., 2023;Eldan & Russinovich, 2023;Si et al., 2023;Liu et al., 2024a;b). Such methods are important for removing sensitive or harmful information, biases, and outdated facts. A key challenge in developing unlearning methods is evaluating their performance, namely, how to validate the erasure of the unlearned information. Existing evaluation protocols largely rely on behavioural tests, such as the ability to answer questions or complete queries about the removed information (Stoehr et al., 2024;Hase et al., 2023;Chen & Yang, 2023). However, growing evidence suggests that it is often possible to steer the model to generate the unlearned information post-unlearning (Lynch et al., 2024;Patil et al., 2024), indicating that the target knowledge has not in fact been exhaustively removed from the model. This work presents the first benchmark for internal evaluation of unlearning methods. \n\nWe highlight the existence of \"parametric knowledge traces\", which are specific sets of parameters in the model that strongly correlate with the knowledge to be erased (see Figure 1a for illustration). We show that this residual knowledge causally influences the model's ability to generate information about the target concept, and argue that its internal erasure should be a goal of unlearning. Specifically, we leverage recent methods that inspect the information encoded in model parameters through vocabulary projections (Dar et al., 2023;Geva et al., 2022b). Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones.",
                    "score": 0.6434159029084269,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 351
                        },
                        {
                            "start": 352,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 983
                        },
                        {
                            "start": 984,
                            "end": 1069
                        },
                        {
                            "start": 1072,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1911
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 132,
                            "end": 151,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 151,
                            "end": 169,
                            "matchedPaperCorpusId": "264828972"
                        },
                        {
                            "start": 676,
                            "end": 694,
                            "matchedPaperCorpusId": "255595518"
                        },
                        {
                            "start": 694,
                            "end": 712,
                            "matchedPaperCorpusId": "264828972"
                        },
                        {
                            "start": 868,
                            "end": 887,
                            "matchedPaperCorpusId": "263311025"
                        },
                        {
                            "start": 1598,
                            "end": 1616,
                            "matchedPaperCorpusId": "252089779"
                        },
                        {
                            "start": 1616,
                            "end": 1635,
                            "matchedPaperCorpusId": "247762385"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                }
            ],
            "relevance_judgement": 0.7431640625,
            "relevance_judgment_input_expanded": "# Title: Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces\n# Venue: arXiv.org\n# Authors: Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva\n## Abstract\nThe task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.\n## INTRODUCTION\nRecently, there has been surging interest in developing methods for unlearning information captured in large language models (LLMs) (Jang et al., 2023;Chen & Yang, 2023;Yao et al., 2023;Eldan & Russinovich, 2023;Si et al., 2023;Liu et al., 2024a;b). Such methods are important for removing sensitive or harmful information, biases, and outdated facts. A key challenge in developing unlearning methods is evaluating their performance, namely, how to validate the erasure of the unlearned information. Existing evaluation protocols largely rely on behavioural tests, such as the ability to answer questions or complete queries about the removed information (Stoehr et al., 2024;Hase et al., 2023;Chen & Yang, 2023). However, growing evidence suggests that it is often possible to steer the model to generate the unlearned information post-unlearning (Lynch et al., 2024;Patil et al., 2024), indicating that the target knowledge has not in fact been exhaustively removed from the model. This work presents the first benchmark for internal evaluation of unlearning methods. \n\nWe highlight the existence of \"parametric knowledge traces\", which are specific sets of parameters in the model that strongly correlate with the knowledge to be erased (see Figure 1a for illustration). We show that this residual knowledge causally influences the model's ability to generate information about the target concept, and argue that its internal erasure should be a goal of unlearning. Specifically, we leverage recent methods that inspect the information encoded in model parameters through vocabulary projections (Dar et al., 2023;Geva et al., 2022b). Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones.",
            "reference_string": "[270560986 | Hong et al. | 2024 | Citations: 24]"
        },
        {
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.02199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301763757",
                    "name": "Tae-Young Lee"
                },
                {
                    "authorId": "2264950149",
                    "name": "Sundong Park"
                },
                {
                    "authorId": "2211098085",
                    "name": "Minwoo Jeon"
                },
                {
                    "authorId": "2073600754",
                    "name": "Hyoseok Hwang"
                },
                {
                    "authorId": "3144955",
                    "name": "Gyeong-Moon Park"
                }
            ],
            "abstract": "As concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods. The code is available at http://github.com/KU-VGI/ESC .",
            "corpus_id": 277510371,
            "sentences": [
                {
                    "corpus_id": "277510371",
                    "title": "ESC: Erasing Space Concept for Knowledge Deletion",
                    "text": "To the best of our knowledge, our methods not only yield remarkable performances in diverse analyses but also achieve the fastest processing for KD. \n\nOur main contributions can be summarized as follows: \u2022 For the first time, we redefine the Machine Unlearning approach from a user-centric perspective and propose a new setting named Knowledge Deletion (KD), which expands the scope of unlearning to feature-level knowledge. \n\nTo assess this, we introduce a novel benchmark named Knowledge Retention (KR) score. \u2022 To facilitate KD, we propose a novel training-free approach, Erasing Space Concept (ESC), which can eliminate the space of concept in the embedding feature space for removing the forgetting knowledge. \u2022 We also propose a training-based ESC, ESC with Training (ESC-T), which conducts more fine-grained removal and alleviates the trade-off between removal and preservation of the knowledge. \u2022 From the extensive experiments and analysis, we demonstrate that our proposed methods effectively erase the forgetting knowledge from the pre-trained networks in various datasets, models, and scenarios, and achieve the fastest and state-of-the-art performance in KD.",
                    "score": 0.6771751340831106,
                    "section_title": "Introduction",
                    "char_start_offset": 4382,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 151,
                            "end": 424
                        },
                        {
                            "start": 427,
                            "end": 511
                        },
                        {
                            "start": 512,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1171
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74169921875
                }
            ],
            "relevance_judgement": 0.74169921875,
            "relevance_judgment_input_expanded": "# Title: ESC: Erasing Space Concept for Knowledge Deletion\n# Venue: arXiv.org\n# Authors: Tae-Young Lee, Sundong Park, Minwoo Jeon, Hyoseok Hwang, Gyeong-Moon Park\n## Abstract\nAs concerns regarding privacy in deep learning continue to grow, individuals are increasingly apprehensive about the potential exploitation of their personal knowledge in trained models. Despite several research efforts to address this, they often fail to consider the real-world demand from users for complete knowledge erasure. Furthermore, our investigation reveals that existing methods have a risk of leaking personal knowledge through embedding features. To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD. Our extensive experiments on various datasets and models demonstrate that our proposed methods achieve the fastest and state-of-the-art performance. Notably, our methods are applicable to diverse forgetting scenarios, such as facial domain setting, demonstrating the generalizability of our methods. The code is available at http://github.com/KU-VGI/ESC .\n## Introduction\nTo the best of our knowledge, our methods not only yield remarkable performances in diverse analyses but also achieve the fastest processing for KD. \n\nOur main contributions can be summarized as follows: \u2022 For the first time, we redefine the Machine Unlearning approach from a user-centric perspective and propose a new setting named Knowledge Deletion (KD), which expands the scope of unlearning to feature-level knowledge. \n\nTo assess this, we introduce a novel benchmark named Knowledge Retention (KR) score. \u2022 To facilitate KD, we propose a novel training-free approach, Erasing Space Concept (ESC), which can eliminate the space of concept in the embedding feature space for removing the forgetting knowledge. \u2022 We also propose a training-based ESC, ESC with Training (ESC-T), which conducts more fine-grained removal and alleviates the trade-off between removal and preservation of the knowledge. \u2022 From the extensive experiments and analysis, we demonstrate that our proposed methods effectively erase the forgetting knowledge from the pre-trained networks in various datasets, models, and scenarios, and achieve the fastest and state-of-the-art performance in KD.",
            "reference_string": "[277510371 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.01983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273923395",
                    "name": "Bichen Wang"
                },
                {
                    "authorId": "2301580194",
                    "name": "Yuzhe Zi"
                },
                {
                    "authorId": "2304770961",
                    "name": "Yixin Sun"
                },
                {
                    "authorId": "49339265",
                    "name": "Yanyan Zhao"
                },
                {
                    "authorId": "2203961541",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.",
            "corpus_id": 270226658,
            "sentences": [
                {
                    "corpus_id": "270226658",
                    "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
                    "text": "With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.",
                    "score": 0.6680010745032607,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72900390625
                }
            ],
            "relevance_judgement": 0.72900390625,
            "relevance_judgment_input_expanded": "# Title: RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models\n# Venue: arXiv.org\n# Authors: Bichen Wang, Yuzhe Zi, Yixin Sun, Yanyan Zhao, Bing Qin\n## Abstract\nWith the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.\n",
            "reference_string": "[270226658 | Wang et al. | 2024 | Citations: 11]"
        },
        {
            "title": "On Large Language Model Continual Unlearning",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311833838",
                    "name": "Chongyang Gao"
                },
                {
                    "authorId": "2108631414",
                    "name": "Lixu Wang"
                },
                {
                    "authorId": "2148353350",
                    "name": "Chenkai Weng"
                },
                {
                    "authorId": "2276121035",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2275773112",
                    "name": "Qi Zhu"
                }
            ],
            "abstract": "While large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the OOO framework that includes an Orthogonal low-rank adapter (LoRA) for continually unlearning requested data and an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our OOO framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, OOO's effectiveness does not rely on any retained data. We conducted extensive experiments on OOO and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that OOO consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.",
            "corpus_id": 271212701,
            "sentences": [
                {
                    "corpus_id": "271212701",
                    "title": "On Large Language Model Continual Unlearning",
                    "text": "Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
                    "score": 0.6484392975068174,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1321
                        },
                        {
                            "start": 1324,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1950
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 168,
                            "end": 187,
                            "matchedPaperCorpusId": "261064713"
                        },
                        {
                            "start": 357,
                            "end": 375,
                            "matchedPaperCorpusId": "220938739"
                        },
                        {
                            "start": 680,
                            "end": 698,
                            "matchedPaperCorpusId": "255825985"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.728515625
                }
            ],
            "relevance_judgement": 0.728515625,
            "relevance_judgment_input_expanded": "# Title: On Large Language Model Continual Unlearning\n# Venue: International Conference on Learning Representations\n# Authors: Chongyang Gao, Lixu Wang, Chenkai Weng, Xiao Wang, Qi Zhu\n## Abstract\nWhile large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the OOO framework that includes an Orthogonal low-rank adapter (LoRA) for continually unlearning requested data and an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our OOO framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, OOO's effectiveness does not rely on any retained data. We conducted extensive experiments on OOO and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that OOO consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.\n## INTRODUCTION\nRecently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
            "reference_string": "[271212701 | Gao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 33,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "73502630",
                    "name": "Nianwen Si"
                },
                {
                    "authorId": "2154930608",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2116152318",
                    "name": "Heyu Chang"
                },
                {
                    "authorId": "9047584",
                    "name": "Wenlin Zhang"
                },
                {
                    "authorId": "2253591545",
                    "name": "Dan Qu"
                },
                {
                    "authorId": "2268429659",
                    "name": "Weiqiang Zhang"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "corpus_id": 265456592,
            "sentences": [
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
                    "score": 0.7505726384201005,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72802734375
                }
            ],
            "relevance_judgement": 0.72802734375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges\n# Venue: arXiv.org\n# Authors: Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang\n## Abstract\nIn recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.\n",
            "reference_string": "[265456592 | Si et al. | 2023 | Citations: 33]"
        },
        {
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2005445750",
                    "name": "Xuhan Zuo"
                },
                {
                    "authorId": "2005212347",
                    "name": "Minghao Wang"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "2304458654",
                    "name": "Shui Yu"
                },
                {
                    "authorId": "2134555583",
                    "name": "Wanlei Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.",
            "corpus_id": 274823032,
            "sentences": [
                {
                    "corpus_id": "274823032",
                    "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
                    "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
                    "score": 0.6802953331873808,
                    "section_title": "B. Unlearning with LLM",
                    "char_start_offset": 8767,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 1035
                        },
                        {
                            "start": 1038,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1663
                        },
                        {
                            "start": 1666,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2000
                        },
                        {
                            "start": 2001,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72802734375
                }
            ],
            "relevance_judgement": 0.72802734375,
            "relevance_judgment_input_expanded": "# Title: Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration\n# Venue: arXiv.org\n# Authors: Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou\n## Abstract\nLarge language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.\n## B. Unlearning with LLM\nThe challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "reference_string": "[274823032 | Zuo et al. | 2024 | Citations: 2]"
        },
        {
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348951919",
                    "name": "Wenyu Wang"
                },
                {
                    "authorId": "48985110",
                    "name": "Mengqi Zhang"
                },
                {
                    "authorId": "2286432237",
                    "name": "Xiaotian Ye"
                },
                {
                    "authorId": "2260895127",
                    "name": "Zhaochun Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                }
            ],
            "abstract": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "corpus_id": 276812969,
            "sentences": [
                {
                    "corpus_id": "276812969",
                    "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                    "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
                    "score": 0.7341551434795763,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72705078125
                },
                {
                    "corpus_id": "276812969",
                    "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                    "text": "3.1 Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set \n\nAfter unlearning, the model's performance should be indistinguishable from a model trained exclusively on the retained dataset D r = D\\D f . Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set. The method builds upon the standard training paradigm of the P \u03b8 , which minimizes the prediction loss over the full dataset D. To enforce forgetting, gradient ascent maximizes the prediction loss on the target forget subset D f , effectively approximating the reversal of the original optimization process. This procedure can be equivalently interpreted as performing gradient descent on the negative prediction loss (Zhang et al., 2024). The gradient ascent objective, denoted as L GA , is formulated as: \n\n(1)",
                    "score": 0.6121216511587964,
                    "section_title": "Preliminaries",
                    "char_start_offset": 7810,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 549
                        },
                        {
                            "start": 552,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1125
                        },
                        {
                            "start": 1126,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1632
                        },
                        {
                            "start": 1635,
                            "end": 1638
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70751953125
                }
            ],
            "relevance_judgement": 0.72705078125,
            "relevance_judgment_input_expanded": "# Title: UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets\n# Venue: arXiv.org\n# Authors: Wenyu Wang, Mengqi Zhang, Xiaotian Ye, Zhaochun Ren, Zhumin Chen, Pengjie Ren\n## Abstract\nLarge Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.\n## Preliminaries\n3.1 Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set \n\nAfter unlearning, the model's performance should be indistinguishable from a model trained exclusively on the retained dataset D r = D\\D f . Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set. The method builds upon the standard training paradigm of the P \u03b8 , which minimizes the prediction loss over the full dataset D. To enforce forgetting, gradient ascent maximizes the prediction loss on the target forget subset D f , effectively approximating the reversal of the original optimization process. This procedure can be equivalently interpreted as performing gradient descent on the negative prediction loss (Zhang et al., 2024). The gradient ascent objective, denoted as L GA , is formulated as: \n\n(1)",
            "reference_string": "[276812969 | Wang et al. | 2025 | Citations: 3]"
        },
        {
            "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 93,
            "citation_count": 8,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2051552791",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "2168533640",
                    "name": "Jiancheng Liu"
                },
                {
                    "authorId": "2155369380",
                    "name": "Yihua Zhang"
                },
                {
                    "authorId": "2247652289",
                    "name": "Parikshit Ram"
                },
                {
                    "authorId": "2284064162",
                    "name": "Nathalie Baracaldo"
                },
                {
                    "authorId": "2254478722",
                    "name": "Sijia Liu"
                }
            ],
            "abstract": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.",
            "corpus_id": 273532566,
            "sentences": [
                {
                    "corpus_id": "273532566",
                    "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
                    "text": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.",
                    "score": 0.5949163254852816,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.724609375
                }
            ],
            "relevance_judgement": 0.724609375,
            "relevance_judgment_input_expanded": "# Title: WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Jinghan Jia, Jiancheng Liu, Yihua Zhang, Parikshit Ram, Nathalie Baracaldo, Sijia Liu\n## Abstract\nThe need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.\n",
            "reference_string": "[273532566 | Jia et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 101,
            "influential_citation_count": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07707",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07707, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2075406253",
                    "name": "Jack Foster"
                },
                {
                    "authorId": "2224618086",
                    "name": "Stefan Schoepf"
                },
                {
                    "authorId": "3385621",
                    "name": "A. Brintrup"
                }
            ],
            "abstract": "Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches.",
            "corpus_id": 260900355,
            "sentences": [
                {
                    "corpus_id": "260900355",
                    "title": "Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening",
                    "text": "We present a novel two-step, retraining-free unlearning method. SSD first selects parameters that are considerably more important for the forget set than the retain set, before dampening these parameters proportional to the discrepancy in their importance to the forget and retain set. The result of these steps is a fast yet highly effective method for machine unlearning. We evaluate SSD on a range of tasks, demonstrating viability in single-class, sub-class and random sample settings, on multiple datasets and different model architectures. Results show that SSD is orders of magnitude faster than the comparable Fisher Forgetting method, outperforming the method considerably; SSD even rivals the speed and performance of state-of-the-art retrain-based approaches. \n\nMany future directions could be explored, such as evaluating how to improve and measure performance on random subsets, given the significant overlap in parameter importance for the forget and test set. Another interesting direction is how to forget large subsets of information. Typically experiments evaluate forgetting no more than 5-10% of data; this may be realistic but evaluating how to increase the upper bound of forgetting without retraining may offer valuable insight into how to improve existing unlearning methods.",
                    "score": 0.6044950180094656,
                    "section_title": "Conclusion",
                    "char_start_offset": 25088,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 63
                        },
                        {
                            "start": 64,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 770
                        },
                        {
                            "start": 773,
                            "end": 974
                        },
                        {
                            "start": 975,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72412109375
                }
            ],
            "relevance_judgement": 0.72412109375,
            "relevance_judgment_input_expanded": "# Title: Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Jack Foster, Stefan Schoepf, A. Brintrup\n## Abstract\nMachine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches.\n## Conclusion\nWe present a novel two-step, retraining-free unlearning method. SSD first selects parameters that are considerably more important for the forget set than the retain set, before dampening these parameters proportional to the discrepancy in their importance to the forget and retain set. The result of these steps is a fast yet highly effective method for machine unlearning. We evaluate SSD on a range of tasks, demonstrating viability in single-class, sub-class and random sample settings, on multiple datasets and different model architectures. Results show that SSD is orders of magnitude faster than the comparable Fisher Forgetting method, outperforming the method considerably; SSD even rivals the speed and performance of state-of-the-art retrain-based approaches. \n\nMany future directions could be explored, such as evaluating how to improve and measure performance on random subsets, given the significant overlap in parameter importance for the forget and test set. Another interesting direction is how to forget large subsets of information. Typically experiments evaluate forgetting no more than 5-10% of data; this may be realistic but evaluating how to increase the upper bound of forgetting without retraining may offer valuable insight into how to improve existing unlearning methods.",
            "reference_string": "[260900355 | Foster et al. | 2023 | Citations: 101]"
        },
        {
            "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151325553",
                    "name": "Zhehao Huang"
                },
                {
                    "authorId": "2261734426",
                    "name": "Xinwen Cheng"
                },
                {
                    "authorId": "2323503737",
                    "name": "Jinghao Zheng"
                },
                {
                    "authorId": "2308566717",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "1466446905",
                    "name": "Zhengbao He"
                },
                {
                    "authorId": "2149200590",
                    "name": "Tao Li"
                },
                {
                    "authorId": "2266622882",
                    "name": "Xiaolin Huang"
                }
            ],
            "abstract": "Machine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction. Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods.",
            "corpus_id": 272987840,
            "sentences": [
                {
                    "corpus_id": "272987840",
                    "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
                    "text": "The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs. \n\nAs shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.",
                    "score": 0.6368464249959079,
                    "section_title": "F.3 Results for Natural Language Processing",
                    "char_start_offset": 45558,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 794
                        },
                        {
                            "start": 797,
                            "end": 813
                        },
                        {
                            "start": 814,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1062
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 526,
                            "end": 530,
                            "matchedPaperCorpusId": "247627962"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7216796875
                }
            ],
            "relevance_judgement": 0.7216796875,
            "relevance_judgment_input_expanded": "# Title: Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement\n# Venue: Neural Information Processing Systems\n# Authors: Zhehao Huang, Xinwen Cheng, Jinghao Zheng, Haoran Wang, Zhengbao He, Tao Li, Xiaolin Huang\n## Abstract\nMachine unlearning (MU) has emerged to enhance the privacy and trustworthiness of deep neural networks. Approximate MU is a practical method for large-scale models. Our investigation into approximate MU starts with identifying the steepest descent direction, minimizing the output Kullback-Leibler divergence to exact MU inside a parameters' neighborhood. This probed direction decomposes into three components: weighted forgetting gradient ascent, fine-tuning retaining gradient descent, and a weight saliency matrix. Such decomposition derived from Euclidean metric encompasses most existing gradient-based MU methods. Nevertheless, adhering to Euclidean space may result in sub-optimal iterative trajectories due to the overlooked geometric structure of the output probability space. We suggest embedding the unlearning update into a manifold rendered by the remaining geometry, incorporating second-order Hessian from the remaining data. It helps prevent effective unlearning from interfering with the retained performance. However, computing the second-order Hessian for large-scale models is intractable. To efficiently leverage the benefits of Hessian modulation, we propose a fast-slow parameter update strategy to implicitly approximate the up-to-date salient unlearning direction. Free from specific modal constraints, our approach is adaptable across computer vision unlearning tasks, including classification and generation. Extensive experiments validate our efficacy and efficiency. Notably, our method successfully performs class-forgetting on ImageNet using DiT and forgets a class on CIFAR-10 using DDPM in just 50 steps, compared to thousands of steps required by previous methods.\n## F.3 Results for Natural Language Processing\nThe input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs. \n\nAs shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.",
            "reference_string": "[272987840 | Huang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333521206",
                    "name": "Naixing Xu"
                },
                {
                    "authorId": "2296160455",
                    "name": "Qian Li"
                },
                {
                    "authorId": "2320335890",
                    "name": "Xu Wang"
                },
                {
                    "authorId": "2291037228",
                    "name": "Bingchen Liu"
                },
                {
                    "authorId": "2320836966",
                    "name": "Xin Li"
                }
            ],
            "abstract": "Knowledge graph (KG) embedding methods map entities and relations into continuous vector spaces, improving performance in tasks like link prediction and question answering. With rising privacy concerns, machine unlearning (MU) has emerged as a critical AI technology, enabling models to eliminate the influence of specific data. Existing MU approaches often rely on data obfuscation and adjustments to training loss but lack generalization across unlearning tasks. This paper introduces MetaEU, a Meta-Learning-Based Knowledge Graph Embedding Unlearning framework. MetaEU leverages meta-learning to unlearn specific embeddings, mitigating their impact while preserving model performance on remaining data. Experiments on benchmark datasets demonstrate its effectiveness in KG embedding unlearning.",
            "corpus_id": 274437750,
            "sentences": [
                {
                    "corpus_id": "274437750",
                    "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
                    "text": "Cao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs. \n\nThese three areas-knowledge graph embeddings, meta-learning, and machine unlearning-collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning.",
                    "score": 0.9471157253378767,
                    "section_title": "Related Work",
                    "char_start_offset": 7950,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 756
                        },
                        {
                            "start": 759,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1080
                        },
                        {
                            "start": 1081,
                            "end": 1231
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 13,
                            "end": 17,
                            "matchedPaperCorpusId": "5945696"
                        },
                        {
                            "start": 175,
                            "end": 179,
                            "matchedPaperCorpusId": "195886255"
                        },
                        {
                            "start": 323,
                            "end": 327,
                            "matchedPaperCorpusId": "251025393"
                        },
                        {
                            "start": 547,
                            "end": 551,
                            "matchedPaperCorpusId": "267897394"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71484375
                }
            ],
            "relevance_judgement": 0.71484375,
            "relevance_judgment_input_expanded": "# Title: Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning\n# Venue: arXiv.org\n# Authors: Naixing Xu, Qian Li, Xu Wang, Bingchen Liu, Xin Li\n## Abstract\nKnowledge graph (KG) embedding methods map entities and relations into continuous vector spaces, improving performance in tasks like link prediction and question answering. With rising privacy concerns, machine unlearning (MU) has emerged as a critical AI technology, enabling models to eliminate the influence of specific data. Existing MU approaches often rely on data obfuscation and adjustments to training loss but lack generalization across unlearning tasks. This paper introduces MetaEU, a Meta-Learning-Based Knowledge Graph Embedding Unlearning framework. MetaEU leverages meta-learning to unlearn specific embeddings, mitigating their impact while preserving model performance on remaining data. Experiments on benchmark datasets demonstrate its effectiveness in KG embedding unlearning.\n## Related Work\nCao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs. \n\nThese three areas-knowledge graph embeddings, meta-learning, and machine unlearning-collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning.",
            "reference_string": "[274437750 | Xu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315112779",
                    "name": "Hao Du"
                },
                {
                    "authorId": "2275191329",
                    "name": "Shang Liu"
                },
                {
                    "authorId": "1865220753",
                    "name": "Lele Zheng"
                },
                {
                    "authorId": "2336916642",
                    "name": "Yang Cao"
                },
                {
                    "authorId": "2336874775",
                    "name": "Atsuyoshi Nakamura"
                },
                {
                    "authorId": "2336870835",
                    "name": "Lei Chen"
                }
            ],
            "abstract": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing their effectiveness and limitations in addressing privacy risks and maintaining model utility. By identifying key gaps in existing research, we highlight challenges and propose directions to advance the development of privacy-preserving methods for fine-tuning LLMs, promoting their responsible use in diverse applications.",
            "corpus_id": 274982612,
            "sentences": [
                {
                    "corpus_id": "274982612",
                    "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                    "text": "Knowledge Unlearning, also known as machine unlearning [4], refers to the process by which a machine learning model is able to forget or remove knowledge about certain data points from its training. Unlike Differential Privacy, which cannot fully guarantee the right to be forgotten due to the inherent limitation of a non-zero privacy budget, Machine Unlearning focuses explicitly on eliminating the influence of specific data samples. In the fine-tuning phase, it can also be applied to remove the impact of specific data points in the fine-tuning datasets. \n\nMeng et al. [29] introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. [19] proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang [4] approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer.",
                    "score": 0.687388923882762,
                    "section_title": "Knowledge Unlearning",
                    "char_start_offset": 31652,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 198
                        },
                        {
                            "start": 199,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 559
                        },
                        {
                            "start": 562,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1184
                        },
                        {
                            "start": 1185,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1526
                        },
                        {
                            "start": 1527,
                            "end": 1646
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 55,
                            "end": 58,
                            "matchedPaperCorpusId": "264828972"
                        },
                        {
                            "start": 574,
                            "end": 578,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 865,
                            "end": 869,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1377,
                            "end": 1380,
                            "matchedPaperCorpusId": "264828972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions\n# Venue: arXiv.org\n# Authors: Hao Du, Shang Liu, Lele Zheng, Yang Cao, Atsuyoshi Nakamura, Lei Chen\n## Abstract\nFine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing their effectiveness and limitations in addressing privacy risks and maintaining model utility. By identifying key gaps in existing research, we highlight challenges and propose directions to advance the development of privacy-preserving methods for fine-tuning LLMs, promoting their responsible use in diverse applications.\n## Knowledge Unlearning\nKnowledge Unlearning, also known as machine unlearning [4], refers to the process by which a machine learning model is able to forget or remove knowledge about certain data points from its training. Unlike Differential Privacy, which cannot fully guarantee the right to be forgotten due to the inherent limitation of a non-zero privacy budget, Machine Unlearning focuses explicitly on eliminating the influence of specific data samples. In the fine-tuning phase, it can also be applied to remove the impact of specific data points in the fine-tuning datasets. \n\nMeng et al. [29] introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. [19] proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang [4] approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer.",
            "reference_string": "[274982612 | Du et al. | 2024 | Citations: 5]"
        },
        {
            "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 23,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02443, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348448191",
                    "name": "Iraklis Premptis"
                },
                {
                    "authorId": "2184294391",
                    "name": "Maria Lymperaiou"
                },
                {
                    "authorId": "2080432906",
                    "name": "Giorgos Filandrianos"
                },
                {
                    "authorId": "1978772089",
                    "name": "O. M. Mastromichalakis"
                },
                {
                    "authorId": "2348262407",
                    "name": "Athanasios Voulodimos"
                },
                {
                    "authorId": "1719165",
                    "name": "G. Stamou"
                }
            ],
            "abstract": "The Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.",
            "corpus_id": 276767847,
            "sentences": [
                {
                    "corpus_id": "276767847",
                    "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
                    "text": "The Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.",
                    "score": 0.5786888369683587,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7109375
                }
            ],
            "relevance_judgement": 0.7109375,
            "relevance_judgment_input_expanded": "# Title: AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking\n# Venue: arXiv.org\n# Authors: Iraklis Premptis, Maria Lymperaiou, Giorgos Filandrianos, O. M. Mastromichalakis, Athanasios Voulodimos, G. Stamou\n## Abstract\nThe Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.\n",
            "reference_string": "[276767847 | Premptis et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303333890",
                    "name": "Ruihan Wu"
                },
                {
                    "authorId": "83222216",
                    "name": "Chhavi Yadav"
                },
                {
                    "authorId": "2266239350",
                    "name": "Russ Salakhutdinov"
                },
                {
                    "authorId": "2303254420",
                    "name": "Kamalika Chaudhuri"
                }
            ],
            "abstract": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.",
            "corpus_id": 273502714,
            "sentences": [
                {
                    "corpus_id": "273502714",
                    "title": "Evaluating Deep Unlearning in Large Language Models",
                    "text": "Prior work in fact unlearning from LLMs focuses on simply unlearning the target fact in isolation, and not other facts that logically imply it. This might cause the LLM to forget only this one specific fact, but retain others that can be combined to deduce the fact in question. In this section, we introduce the new setting of unlearning, deep unlearning, which considers the logical deductions between facts. To assess the effectiveness of unlearning methods in this setting, we propose two evaluation metrics: recall and accuracy.",
                    "score": 0.5782387156680538,
                    "section_title": "DEEP UNLEARNING",
                    "char_start_offset": 6018,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 533
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7099609375
                }
            ],
            "relevance_judgement": 0.7099609375,
            "relevance_judgment_input_expanded": "# Title: Evaluating Deep Unlearning in Large Language Models\n# Venue: arXiv.org\n# Authors: Ruihan Wu, Chhavi Yadav, Russ Salakhutdinov, Kamalika Chaudhuri\n## Abstract\nMachine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.\n## DEEP UNLEARNING\nPrior work in fact unlearning from LLMs focuses on simply unlearning the target fact in isolation, and not other facts that logically imply it. This might cause the LLM to forget only this one specific fact, but retain others that can be combined to deduce the fact in question. In this section, we introduce the new setting of unlearning, deep unlearning, which considers the logical deductions between facts. To assess the effectiveness of unlearning methods in this setting, we propose two evaluation metrics: recall and accuracy.",
            "reference_string": "[273502714 | Wu et al. | 2024 | Citations: 7]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274437750",
            "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
            "text": "Cao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs. \n\nThese three areas-knowledge graph embeddings, meta-learning, and machine unlearning-collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning.",
            "score": 0.9471157253378767,
            "section_title": "Related Work",
            "char_start_offset": 7950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "251025393"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "267897394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model. However, due to the inaccessibility of the original training corpora, it is challenging to obtain such a target set, which contains all knowledge associated with the target entity. Consequently, entity-level unlearning requires an additional step to construct a forget set before applying unlearning algorithms, where there is frequently a discrepancy between the constructed forget set and the target set. To this end, we divide the entity-level unlearning task into a two-stage framework, comprising forget set construction and unlearning execution. This process seeks to delete the target set by erasing the construted forget set, which involves more complex procedures and presents greater challenges than instance-level unlearning. \n\nIn this work, we conduct a comprehensive analysis of entity-level unlearning for LLMs, structured around a sequence of focused steps: Firstly, we begin by formally defining and setting the task, which involves injecting pseudo-entity knowledge into the model to establish both the target model and the target set. Secondly, we systematically evaluate the current unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task. Additionally, we observe that the design of forget set significantly influences the effectiveness of the unlearning algorithm. Thirdly, inspired by the above, we further investigate how various constructed forget sets impact algorithm performance. Our results indicate that unlearning with a forget set that has higher knowledge coverage relative to the target set leads to more effective unlearning. Although merely increasing the size of the forget set may enhance knowledge coverage, it would compromise the model's generalization ability, making it an ineffective way to improve performance.",
            "score": 0.8588539270337304,
            "section_title": "Introduction",
            "char_start_offset": 1683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "In this paper, we propose a novel task: entity-level unlearning for LLMs, which is required in many practical scenarios. We evaluate trending unlearning algorithms in this work and reveal that existing unlearning methods struggle to effectively erase the entire entity from the target model. Furthermore, we find that the size and knowledge coverage of forget set play crucial roles in the performance of the algorithms. Additionally, our analysis shows that entities introduced through fine-tuning are more vulnerable to unlearning compared to pre-trained entities, underscoring the need for more robust entity injection techniques. These findings offer valuable insights and encourage future research to develop corresponding unlearning algorithms and explore more precise knowledge-probing methods to achieve improved entity removal.",
            "score": 0.8016269738030641,
            "section_title": "Conclusion",
            "char_start_offset": 27636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 836
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
            "score": 0.7952793675196037,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.",
            "score": 0.7925543368725948,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 320,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 1177,
                    "end": 1193,
                    "matchedPaperCorpusId": "259501579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "276617566",
            "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
            "text": "LLM unlearning refers to techniques that selectively remove specific behaviors or knowledge from a pre-trained language model while maintaining its overall functionality (Yao et al., 2023). With the proliferation of LLMs, unlearning has gained significant attention due to its broad applications in safety alignment, privacy protection, and copyright compliance (Eldan and Russinovich, 2023;Liu et al., 2024c;Jia et al., 2024b). The evaluation and auditing of LLM unlearning spans from basic verbatim memorization to deeper knowledge memorization (Shi et al., 2024), with this work focusing on the latter. As depicted in Figure 2, LLM unlearning operates as a targeted intervention within the model's knowledge representation framework. Its core objective is the selective removal of specific information while preserving the model's broader knowledge base (e.g, on retain set). This study focuses on the knowledge unlearning auditing that assesses unlearned models' behaviors through comprehensive audit cases. Given access to both forget and retain corpora, we generate a holistic set of test questions with reference answers to thoroughly evaluate whether an unlearned model exhibits any residual knowledge memorization.",
            "score": 0.7730703900153766,
            "section_title": "LLM Unlearning",
            "char_start_offset": 4545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1223
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "269448906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
            "score": 0.7674967586307332,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "score": 0.7505726384201005,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "We present experimental results comparing the performance of the same method on various forget sets and between different algorithms. The main experimental results of Llama2-7B-Chat-TOFU are shown in Table 1, and the experimental results of Phi-1.5-TOFU are presented in Appendix B.5. \n\nA comparison of the performance across different types of forget sets, as presented in Table 1, reveals that the algorithms based on the target set consistently maintain similar model utility while achieving lower probability, reduced accuracy, and overall higher forget quality on Llama2-7B-Chat-TOFU. A similar trend is observed in the Phi-1.5-TOFU model, as shown in Table 4. These results suggest that models employing the target set engage in more thorough unlearning compared to those utilizing the probing set. This finding highlights that the current unlearning algorithms struggle to generalize effectively to entity-level unlearning tasks when relying on the probing set. Consequently, the construction of the forget set is crucial in determining the success of entity-level unlearning. In the subsequent analysis, we will explore in detail how the quality of the forget set influences unlearning effectiveness. \n\nComparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: \n\n1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. \n\n2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. \n\n3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions.",
            "score": 0.7463070381352919,
            "section_title": "Experimental Results",
            "char_start_offset": 13233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1752
                },
                {
                    "start": 1755,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8017578125
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In light of emerging regulations, the removal of user-specified data from large language models (LLMs) is increasingly recognized as a critical component of responsible and ethical AI development. This study further investigates the application of machine unlearning techniques to LLMs. The datasets utilized for evaluation are publicly available and implemented within the intended use. \n\nWe hope this study to advance research and literature on machine unlearning for LLMs.",
            "score": 0.7449602379783993,
            "section_title": "ETHICAL STATEMENT",
            "char_start_offset": 36312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57568359375
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "score": 0.7341551434795763,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "score": 0.7317119465768274,
            "section_title": "Related Work",
            "char_start_offset": 24726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 578
                },
                {
                    "start": 580,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 382,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1380,
                    "end": 1396,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "259088549"
                },
                {
                    "start": 1563,
                    "end": 1580,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "The first step of the entity-level unlearning task is constructing a forget set, which is both critical and challenging. In practical scenarios, the forget set often would be generated solely based on the target model and the names of the entities involved. Following this setting, we propose a simple and effective constructing baseline to probe the entityrelated question-answer (QA) pairs within the target model as a forget set. More details can be found in Appendix A. \n\nSpecifically, we first prompt the target models to self-generate entity-related questions according to their internal knowledge, inspired by Weller et al. (2023). Note that only those non-repetitive questions containing the entities' names will be kept to further ensure the focus remains on the corresponding entities. Next, we acquire the answers from the target model using greedy decoding, which ensures that the answers possess a relatively high generation probability. Finally, we conduct a selfverification process, where the model repeatedly evaluates each QA pair, retaining only the pairs for which the model consistently agrees with its responses. These selected pairs serve as valid candidates for forget sets of unlearning techniques. \n\nThis approach offers a straightforward knowledge extraction method for acquiring the forget set. We then assess the influence of the forget set's quality on unlearning methods through a manual replacement analysis.",
            "score": 0.7310472637354746,
            "section_title": "Forget Set Construction",
            "char_start_offset": 6527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1440
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "In this paper, we have presented a novel approach to unlearning in large language models (LLMs) through the introduction of anti-samples, facilitated by our method, UNSTAR: Unlearning with Self-Taught Anti-Sample Reasoning. As the landscape of machine learning evolves, the need for effective unlearning mechanisms becomes increasingly critical, particularly in light of privacy concerns, legal compliance, and ethical considerations. Our findings indicate that traditional unlearning techniques often inadvertently compromise the model's broader knowledge, underscoring the necessity for a refined approach. \n\nBy leveraging anti-samples, we enable a targeted unlearning process that not only facilitates the selective removal of specific associations but also preserves related knowledge-a feat not achievable by prior methods. Additionally, we achieve fine-grained targeted unlearning, allowing for the nuanced removal of specific information without disrupting the overall integrity of the model's knowledge base. Our use of misleading rationales as justifications for unlearning further enhances the efficacy of this approach, providing a structured means for LLMs to forget while maintaining contextual integrity. Give 20 different paraphrased questions involving the object where the answer is the same. Strictly output the question only. Format: <Index>. <Question>",
            "score": 0.7281049521454309,
            "section_title": "CONCLUSION",
            "char_start_offset": 27263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.386474609375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "The objective of the entity-level unlearning task is to remove an entire entity from the target model completely. However, the inaccessible training data makes it challenging to obtain comprehensive information about the entity within the model. Therefore, we define the task as removing the entire entity from the target model by deleting only a subset of entity-associated knowledge. \n\nThe entity-level unlearning task can be formalized as follows: Given a target entity O, the target model parametrized by \u03b8 t is required to forget a target set S T , which contains all knowledge related to the entity O in the model, by applying unlearning methods H(\u2022) on a forget set S F , which contains part of the knowledge of the entity. All of the aforementioned sets consist of pieces of knowledge, S = {(x i , y i )} N i=1 . The unlearning process can be expressed as follows: \n\nTo precisely assess the deletion effect of the target entity, the evaluation for entity-level unlearning task E(\u2022) should be conducted on the target set S T : \n\nA significant challenge in evaluating the effectiveness of entity-level unlearning posed by the inability to access all their training data of LLMs is obtaining the target set S T used to evaluate the unlearned models. To address this, we simulate entitylevel unlearning scenarios following the TOFU (Maini et al., 2024), which fine-tunes the LLMs using a fictitious author dataset. The dataset ensures that the LLM has no prior exposure to these authors during previous training phases. Thus, the fine-tuning dataset encompasses all the knowledge about the entities, making it suitable to serve as the target set S T .",
            "score": 0.72673248969588,
            "section_title": "Task Definition and Setting",
            "char_start_offset": 4514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 872
                },
                {
                    "start": 875,
                    "end": 1033
                },
                {
                    "start": 1036,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1655
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the\"forgotten\"information. To thoroughly evaluate this phenomenon, we conduct comprehensive experiments using various quantization techniques across multiple precision levels. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21\\% of the intended forgotten knowledge in full precision, which significantly increases to 83\\% after 4-bit quantization. ... Our code is available at: \\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.",
            "score": 0.7190851944659931,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356201171875
        },
        {
            "corpus_id": "270045436",
            "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
            "text": "This experimental section delves into the intricate dynamics of applying machine unlearning methods to large language models.The primary objective of our study is to comprehensively evaluate the effects of these unlearning techniques in the context of the large language model.\n\nOur experiments are structured to assess two pivotal aspects.First, we focus on the models' 'forgetting performance' postunlearning.This involves investigating how effectively the  models can delete specific information or training instances, thereby adhering to data privacy principles and right-to-beforgotten mandates.Second, we examine the models 'utility retention' post-unlearning.This is crucial to ensure that the process of unlearning does not significantly deteriorate the model's predictive accuracy or its ability to generalize from existing data.\n\nIn our experiments, we selected 911 questions about the model name.We defined the model name \"Vicuna\" as sensitive information, where the question types can be referred to in the following examples.\n\nTell me about yourself.\n\nIn Table II, before unlearning, the appearance rate and frequency were 0.998 and 910 times, respectively, while after unlearning, both decreased to 0. This phenomenon intuitively demonstrates that our method can effectively accomplish the unlearning task for large language models.Additionally, according to Table III, the baseline and unlearned models have similar utility, with an average MMLU of 49.68 and 49.8, respectively.The unlearned model ensures that utility is not compromised in all three subdomains.\n\nOur study provides insights into the balance between effective unlearning and preserving model utility by employing meticulously designed tests and evaluations on large predictive models.These findings have profound implications for developing and deploying AI systems, particularly in environments where data privacy and model adaptability are paramount.",
            "score": 0.718111006944703,
            "section_title": "C. Performance Evaluation on Large Language Model",
            "char_start_offset": 62164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 277
                },
                {
                    "start": 279,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 838
                },
                {
                    "start": 840,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1038
                },
                {
                    "start": 1040,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1346
                },
                {
                    "start": 1346,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1577
                },
                {
                    "start": 1579,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "For a long time, people have hoped for machines to have the ability to \"learn,\" enabling them to better understand the world and interact with humans by learning knowledge. In contrast to machine learning, the goal of machine unlearning is to endow models with the capability to forget knowledge actively, allowing them to \"proactively\" erase certain specific knowledge they have previously learned [1] [2]. In the era of large language models (LLMs), LLMs un-dergo pre-training on massive amounts of text to acquire and store a broad range of world knowledge. This paradigm has demonstrated excellence in various downstream natural language processing tasks [3] [4]. However, LLMs also encode significant amounts of private data, copyrighted content, and biased information [5] [6]. Recent research indicates that large language models merely recall and replicate the training samples they have encountered. Although this knowledge is encoded in parameterized, distributed,and high-dimensional embedding vectors, it can often be triggered in specific situations, potentially impacting user privacy or causing other data security concerns. Similar to traditional knowledge bases, it is necessary to establish knowledge removal mechanisms for LLMs, allowing for the removal of specific knowledge from the model upon user request. This approach is known as LLMs knowledge unlearning. It grants knowledge in LLMs the Right To Be Forgotten. When users request the removal of information related to personal privacy from applications driven by LLMs, the models should provide a reasonable response, complying with the user's demand for the forgetting of privacy data to protect the user's legitimate interests and mitigate the risk of legal action against these applications. \n\nUnlearning is not a recently emerging issue. In traditional machine learning research, machine unlearning has long been a subject of widespread research interest. It focuses on studying various unlearning methods for models to forget, aiming to enhance the model's security (unlearning toxic data), privacy (unlearning private data), and impartiality (unlearning biased data) [2].",
            "score": 0.7175160739253301,
            "section_title": "Introduction",
            "char_start_offset": 1744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "254805754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5732421875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Knowledge unlearning in LLMs stems from traditional machine unlearning, as the current transformer architecture-based language model are, in essence, machine learning models. Their goals are consistent, aiming to remove specific knowledge from the model. However, large language models differ significantly from typical machine learning models, both in terms of parameter scale and the richness of internal knowledge.",
            "score": 0.7069115377038631,
            "section_title": "Relationship with Machine Unlearning",
            "char_start_offset": 11684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3798828125
        },
        {
            "corpus_id": "273661686",
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "text": "Following the existing work [43], we evaluate the unlearning performance based on model's output quality. We expect a good performance should satisfy the following requirements: \n\nNo verbatim memorization. After the unlearning, the model should no longer remember any verbatim copies of the texts in the forgetting data. To evaluate this, we prompt the model with the first k tokens in F and compare the model's continuation outputs with the ground truth continuations. We use ROUGE-L recall scores for this comparison, where a lower score is better for unlearning. \n\nNo knowledge memorization. After the unlearning, the model should not only forget verbatim texts, but also the knowledge in the forgetting set. For the MUSE-NEWS dataset, we evaluate knowledge memorization using the Knowmem F split, which consists of generated question-answer pairs based on the forgetting data. Similar to verbatim memorization, we use ROUGE-L recall scores. \n\nMaintained model utility. An effective unlearning method must maintain the model's performance on the retaining set. We prompt the model with the question from R and compare the generated answer to the ground truth. We use ROUGE-L recall scores for these comparisons. Additionally, we evaluate the model using the Truth Ratio metric. We use the Retain10-perturbed split from TOFU, which consists of five perturbed answers created by modifying the facts in each original answer from R. The Truth Ratio metric computes how likely the model generates a correct answer versus an incorrect one, where a higher value is better.",
            "score": 0.7045062743165652,
            "section_title": "Evaluation Metrics",
            "char_start_offset": 18422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 177
                },
                {
                    "start": 180,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1568
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Large language models (LLMs), due to their extensive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowledge (Chang et al., 2023a;Mozes et al., 2023;Eldan and Russinovich, 2023;Ye et al., 2022). Consequently, recent research has focused on developing efficient unlearning methods as a post-training technique to selectively unlearn the specific knowledge (Blanco-Justicia et al., 2024;Liu et al., 2024). Currently, the core mechanism of these unlearning methods involves finetuning (Eldan and Russinovich, 2023;Jang et al., 2023;Yao et al., 2024;Rafailov et al., 2023), with corresponding adjustments and designs in the loss function to facilitate the unlearning process. Although earlier investigations (Hong et al., 2024;Lee et al., 2024a) have proven that these methods are ineffective at completely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear. \n\nTherefore, in this paper, we try to unveil why existing finetuning-based unlearning methods perform well in behavioral tests by analyzing the mechanisms of internal knowledge recall and flow within models (Meng et al., 2022;Pochinkov and Schoots, 2024;Geva et al., 2021a). Specifically, we investigate which components or parameters carry these unlearning effects. We design activations patching and parameters restoration experiments in three settings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and reveal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefficients produced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of finetuning-based methods.",
            "score": 0.703654059052571,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 222,
                    "end": 238,
                    "matchedPaperCorpusId": "250627348"
                },
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 591,
                    "end": 613,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 768,
                    "end": 786,
                    "matchedPaperCorpusId": "266755904"
                },
                {
                    "start": 1176,
                    "end": 1195,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1223,
                    "end": 1242,
                    "matchedPaperCorpusId": "229923720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Machine unlearning mainly aims at erasing the memory of training data, which can be approached from various scopes, e.g., user-wise (Li et al., 2023a), sample-wise (Liu et al., 2023), and featurewise (Warnecke et al., 2023). \n\nSimilarly, unlearning targets in language models can be categorized into three scopes, i.e., instancewise, entity-wise, and behavior-wise (Maini et al., 2024). Instance-wise unlearning involves forgetting the original answer to a specific question or prompt. Entity-wise unlearning refers to erasing all memory associated with a specific training data entity. As shown in Figure 2, the knowledge unlearning task that we focused on, conducts operations on training data, making it specifically suitable \n\nFigure 3: The illustration of our proposed loss (L f orget ) for fine-grained gradient ascent and its potential extension to regularization-based approaches (L reg ). \n\nto accomplish the above two types of unlearning. \n\nBehavior-wise unlearning frames behavior alignment as an unlearning task, aiming to align the model's behavior with human preferences. Knowledge unlearning can be adapted to behavior alignment if the alignment only involves removing undesired behavior.",
            "score": 0.7028168265234743,
            "section_title": "Unlearning Targets",
            "char_start_offset": 9323,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1203
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 150,
                    "matchedPaperCorpusId": "268030788"
                },
                {
                    "start": 164,
                    "end": 182,
                    "matchedPaperCorpusId": "266348774"
                },
                {
                    "start": 200,
                    "end": 223,
                    "matchedPaperCorpusId": "237303799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",
            "score": 0.7014366813399595,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "270440344",
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "text": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
            "score": 0.7011879757504227,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.755859375
        },
        {
            "corpus_id": "270878324",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "text": "Forgetting is a crucial brain function that eliminates unnecessary information to maintain neural system integrity (Small, 2021;Farrell, 2022). In parallel, Large Language Models (LLMs) (Ouyang et al., 2022;Zhao et al., 2023;OpenAI, 2023) inevitably incorporate sensitive data during training, which is not essential for their functionality (Yao et al., 2023a(Yao et al., , 2024;;Li et al., 2024b;Zhang et al., 2024a;Liu et al., 2024b). Therefore, removing sensitive knowledge from LLMs is imperative for ensuring the safety and integrity of these systems. The most straightforward solution involves removing such data from pre-training corpora and retraining LLMs, although this method is expensive and time-consuming. Another approach, alignment Figure 1: Current unlearning paradigms unlearn all related knowledge of \"J.K. Rowling\". Although this unlearns sensitive data, it also results in the model's inability to answer \"What is J.K. Rowling's most representative work?\" which it could answer before unlearning. methods like reinforcement learning from human feedback (RLHF) (Bai et al., 2022), is computationally expensive and requires extensive, high-quality human feedback (Casper et al., 2023). \n\nConsequently, recent research has primarily focused on knowledge unlearning (Chen and Yang, 2023;Eldan and Russinovich, 2023;Si et al., 2023;Liu, 2024;Li et al., 2024a;Huang et al., 2024;Zhao et al., 2024b;Sha et al., 2024), which facilitates efficient, post-training forgetting in models. However, current evaluation paradigms are limited, typically failing to consider the extent of forgetting, instead simply unlearning all related knowledge regarding factual instances. Psychological research (ROEDI-GER III et al., 2010;Storm, 2011) emphasizes that forgetting is a natural and necessary process that helps focus on essential knowledge.",
            "score": 0.6988486272902805,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363037109375
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "Large language models trained on web-scale corpora can memorize undesirable datapoints such as incorrect facts, copyrighted content or sensitive data. Recently, many machine unlearning algorithms have been proposed that aim to `erase' these datapoints from trained models -- that is, revert model behavior to be similar to a model that had never been trained on these datapoints. However, evaluating the success of unlearning algorithms remains an open challenge. In this work, we propose the RESTOR framework for machine unlearning, which evaluates the ability of unlearning algorithms to perform targeted data erasure from models, by evaluating the ability of models to forget the knowledge introduced in these data points, while simultaneously recovering the model's knowledge state had it not encountered these datapoints. RESTOR helps uncover several novel insights about popular unlearning algorithms, and the mechanisms through which they operate -- for instance, identifying that some algorithms merely emphasize forgetting, and that localizing unlearning targets can enhance unlearning performance.",
            "score": 0.6983733116938748,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67822265625
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
            "score": 0.6959158103993507,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7822265625
        },
        {
            "corpus_id": "270764347",
            "title": "Evaluating Copyright Takedown Methods for Language Models",
            "text": "Machine unlearning (Cao & Yang, 2015;Guo et al., 2020) is a technique that aims to transform an existing trained model into one that behaves as though it had never been trained on certain data. This approach can be used to make the model forget the blocklisted materials they were exposed to during training. Most unlearning methods require a forget set (the data to be removed) and a retain set (the data to be kept). In our context, the forget set consists of copyrighted content that the model deployer wants to remove, while the retain set includes verified licensed content from a similar distribution. We evaluate four mainstream unlearning methods highlighted in Maini et al. (2024b), including Gradient ascent (Unlearning GA ; Thudi et al., 2022), Gradient Difference (Unlearning GD ; Liu et al., 2022), KL minimization (Unlearning KL ; Golatkar et al., 2020), and Preference Optimization (Unlearning PO ; Rafailov et al., 2024). More details about these methods can be found in Appendix A.2. Note that the objective of unlearning is to ensure that the unlearned model behaves as thought it had never encountered the forget set (Cao & Yang, 2015), mimicking an oracle model trained without the blocklisted content. Although these methods may prevent the verbatim generation of copyrighted content, their current design does not ensure that factual information contained within that content is preserved. \n\nTable 2: Overview of the COTAEVAL's risk and utility evaluations. For risk evaluation, we input \"hint\" and ask the model for completion. For utility evaluation, we ask the model to do question-answering for news and do summarization for books. We also evaluate the models general utility with MMLU and MT-Bench. Overlapping sequences between the generated content and the ground truth are highlighted in green.",
            "score": 0.6937209737051958,
            "section_title": "Training-based Takedowns (Unlearning)",
            "char_start_offset": 12413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1824
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 37,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 37,
                    "end": 54,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 735,
                    "end": 754,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 793,
                    "end": 810,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 845,
                    "end": 867,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 914,
                    "end": 936,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1136,
                    "end": 1154,
                    "matchedPaperCorpusId": "5945696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Large language models (LLMs) are trained on a vast corpus of text, enabling them to achieve outstanding performance across various tasks (Radford et al., 2019;Chowdhery et al., 2023;Gemma et al., 2024). However, LLMs may present privacy risks, as sensitive or private information could unintentionally be included in the large text corpus used for training. Therefore, prior studies have investigated unlearning undesirable knowledge in Figure 1: Faithful Unlearning. FAITHUN proposes three types of datasets to evaluate the faithfulness of unlearning methods (i.e., Paraphrased, Multi-hop, and Same-answer datasets). Each target knowledge to be unlearned is mapped with questions corresponding to these three dataset types for evaluation. language models. To assess unlearning, most studies have examined whether a model successfully forgets the targeted knowledge while retaining irrelevant knowledge (Shi et al., 2024;Li et al., 2024a;Maini et al., 2024;Jin et al., 2024). \n\nHowever, they are limited since they have overlooked the complex and interconnected nature of knowledge, which requires careful investigation of related knowledge. Specifically, these studies have examined only the independent knowledge and failed to evaluate whether an unlearning method effectively erases interconnected knowledge that should be removed, while retaining knowledge that appears relevant but exists in a completely different context. Figure 1 presents an example of faithful unlearning in the real-world knowledge setting. Unlearning methods should also remove paraphrased and multi-hop questions, as they involve knowledge interconnected with the target question being unlearned. Conversely, unlearning methods should retain knowledge of other questions with the same answer as the target, if they actually contain different knowledge despite appearing relevant. \n\nTo address this gap, we first define superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge.",
            "score": 0.6896191232752232,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 159,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 159,
                    "end": 182,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418701171875
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
            "score": 0.6890018799816445,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "We developed a framework for machine unlearning in large language models, defining its objectives and evaluating its performance.The results demonstrate the effectiveness of our approach, yielding positive outcomes in three complex scenarios frequently encountered in large oracle models.Our framework's versatility ensures a low-cost and straightforward implementation.In contexts such as harmful output elimination, knowledge unlearning, and hallucination reduction, our method's efficacy aligns closely with that of traditional fine-tuning techniques.Additionally, it offers the advantage of significantly reduced training time, leading to substantial computational resource savings.Unlike traditional machine unlearning, machine unlearning in large language models presents unique challenges, and a standardized evaluation criterion within the industry remains absent.Furthermore, the concept of machine unlearning for large language models is not yet fully established.Our research contributes to this emerging area, aiming to inform and enhance future studies in unlearning processes for large oracle models.",
            "score": 0.6886887798311015,
            "section_title": "CONCLUSION",
            "char_start_offset": 33206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4599609375
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "Machine Unlearning: The notion of machine unlearning was first introduced by Cao & Yang (2015) motivated by the right-to-be-forgotten and focused on removing specific training samples.Since then, there have been a number of works that have focused on removing specific training data samples via unlearning (Bourtoule et al., 2021;Graves et al., 2020;Izzo et al., 2021;Ginart et al., 2019;Golatkar et al., 2020a;b;Thudi et al., 2021).\n\nUnlearning for LLMs has started to gain recent attention resulting in works in data unlearning (Jang et al., 2023;Wang et al., 2023;Kassem et al., 2023;Maini et al., 2024;Zhang et al., 2024), concept unlearning (Eldan & Russinovich, 2023), behavior unlearning (Lu et al., 2022;Yao et al., 2024;Liu et al., 2024b), knowledge unlearning (Li et al., 2024).Recent surveys have shown additional methods where unlearning has been applied (Nguyen et al., 2022;Xu et al., 2023a;Liu et al., 2024a).Prior works have mainly focused on designing unlearning methods, evaluation metrics, and benchmarks.However, they do not take into account attributes of data used for unlearning.Our proposed SPUNGE leverages data attributes to fortify the performance of any unlearning method.\n\nToxicity Reduction in LLMs: Early works in reducing toxicity in language models (Krause et al., 2021;Liu et al., 2021;Dathathri et al., 2020) have focused on small to moderated sized models and restrict to explicit toxicity.Detoxification techniques primarily employ controlled text generation methods, which incurs heavy inference overhead and it is difficult to measure model performance on benchmark tasks.",
            "score": 0.6886019856779222,
            "section_title": "A. Related Work",
            "char_start_offset": 15747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 433
                },
                {
                    "start": 435,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1200
                },
                {
                    "start": 1202,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1611
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 94,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 306,
                    "end": 330,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 368,
                    "end": 388,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 388,
                    "end": 411,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 567,
                    "end": 587,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 695,
                    "end": 712,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 888,
                    "end": 905,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 1303,
                    "end": 1320,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 1320,
                    "end": 1343,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2666015625
        },
        {
            "corpus_id": "274982612",
            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
            "text": "Knowledge Unlearning, also known as machine unlearning [4], refers to the process by which a machine learning model is able to forget or remove knowledge about certain data points from its training. Unlike Differential Privacy, which cannot fully guarantee the right to be forgotten due to the inherent limitation of a non-zero privacy budget, Machine Unlearning focuses explicitly on eliminating the influence of specific data samples. In the fine-tuning phase, it can also be applied to remove the impact of specific data points in the fine-tuning datasets. \n\nMeng et al. [29] introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. [19] proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang [4] approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer.",
            "score": 0.687388923882762,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 31652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1646
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 58,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 865,
                    "end": 869,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1377,
                    "end": 1380,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "score": 0.6871927294495634,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.767578125
        },
        {
            "corpus_id": "276937927",
            "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
            "text": "In many practical applications, we want to selectively remove knowledge from a language model without disrupting unrelated capabilities. Farrell et al. (2024) examined the effectiveness of SAEs for unlearning by applying conditional negative steering. We identify relevant latents by comparing their activation frequencies between a forget set (biology-related text in the WMDP-bio corpus) and retain set (WikiText), then clamp these latents to negative values whenever they activate. We build on their methodology and report an unlearning score for each individual SAE, measuring unlearning success via degraded accuracy on WMDP-bio test questions while using MMLU categories to verify retained capabilities. Models that achieve strong unlearning of the target domain with minimal side effects on other domains score higher.",
            "score": 0.686742322917295,
            "section_title": "UNLEARNING CAPABILITY",
            "char_start_offset": 11795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 825
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "It focuses on studying various unlearning methods for models to forget, aiming to enhance the model's security (unlearning toxic data), privacy (unlearning private data), and impartiality (unlearning biased data) [2]. Traditional approaches in machine unlearning can be broadly categorized into two types [7][8]: 1) designing new unlearning algorithms to isolate target data points during training and then retraining the model based on the unlearning algorithm, such as differential privacy (DP) methods [9] [10]. 2) Approximate unlearning, which involves making limited parameter updates to machine learning models to minimize the additional impact of forgetting target data points, reducing it to an acceptable level while simultaneously constraining other model behaviors from undergoing significant changes [11]. \n\nHowever, in the era of LLMs, traditional machine unlearning methods may not necessarily be applicable to LLMs. The potential reasons for this are as follows: 1) The parameter scale of LLMs is extremely large, leading to a high cost of model retraining, especially in the case of frequent requests for continuous unlearning, which is impractical in reality. 2) LLMs are knowledge-intensive and typically used for openended question answering or inference tasks. These tasks are often modeled as generative tasks in the form of (prompt, output). In contrast, previous natural language processing models in machine learning were primarily used for language understanding tasks, with classification tasks like text classification, sentiment analysis, and natural language inference being more common. Unlearning methods designed for these classification tasks are not applicable to generative tasks. 3) Commercialized LLMs generally only provide API access and do not offer a way to access their parameters. These factors have impacted the development of forgetting mechanisms in the era of LLMs, leading to the emergence of LLM knowledge unlearning tailored for these large generative models. Knowledge unlearning process of LLMs is illustrated in Figure 1. \n\nIn current scenario where resources for training and maintaining LLMs are highly constrained, knowledge unlearning for LLMs proves to be exceptionally practical. It stands as a necessary approach for developing responsible, legally compliant, and user-trusted LLMs.",
            "score": 0.6853784774620624,
            "section_title": "Introduction",
            "char_start_offset": 3680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 216,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330810546875
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "Given the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015;Bourtoule et al., 2021;Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020;Meng et al., 2022). However, these editing methods often involve modifying the model in ways that lead to unintended consequences, such as reduced model utility on tasks or knowledge unrelated to the target. Thus, a balance must be struck between the successful unlearning of undesired information and maintaining the utility of the model by minimizing collateral damage. For this reason, current approaches generally evaluate unlearning or model editing methods based on their efficacy in altering intended knowledge -measured by how successfully a \"forget set\" is removed -and based on their collateral effects on unrelated behaviors, i.e. the accuracy on a \"retain set\". This kind of evaluation is especially crucial in realistic settings where unlearning happens on a topic. Such topic-based unlearning commonly arises in practical scenarios, for example, when deleting all information associated with an individual or with sensitive domains (Li et al., 2024). Here, the likelihood of over-generalization to similar topics beyond the domain being deleted is high. \n\nA key gap in existing research lies in understanding the specific data characteristics that drive overgeneralization and collateral effects during unlearning. While prior work (Sheshadri et al., 2024;Chowdhury et al., 2024) has measured damage resulting from unlearning, it does not investigate how attributes of the data -such as its variance -contribute to collateral damage or whether these attributes can be controlled to optimize the trade-off between deletion efficacy and utility retention. Focusing on a topic-based setting where the forget set comprises semantically coherent groups of information, we seek to address these questions: \n\n1. What measurable attributes of the forget set drive collateral effects during the unlearning process? \n\n2. Can these attributes be systematically controlled to optimize the trade-off between deletion effectiveness and model utility?",
            "score": 0.6843589881860903,
            "section_title": "Introduction",
            "char_start_offset": 1774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 169,
                    "end": 192,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1227,
                    "end": 1244,
                    "matchedPaperCorpusId": "268247897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.393798828125
        },
        {
            "corpus_id": "274823032",
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "score": 0.6802953331873808,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72802734375
        },
        {
            "corpus_id": "272770202",
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "text": "In this paper, we explore factual knowledge unlearning in Large Language Models (LLMs) and find that it can result in nonsensical responses on knowledge related to forgotten entities, especially when only negative feedback is used or positive feedback is applied incorrectly. To address this, we propose AltPO, a fine-tuning approach that combines negative feedback with in-domain positive feedback on the forget set, ensuring more stable and effective unlearning. We also identify limitations in existing evaluation metrics and introduce new ones to offer a more comprehensive assessment of the unlearned model. We hope our findings offer valuable insights for practitioners in LLM unlearning, promoting the use of alternate answers for more effective unlearning and improving the evaluation of model performance post-unlearning.",
            "score": 0.6792816390612956,
            "section_title": "Conclusion",
            "char_start_offset": 25341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57861328125
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "This work reframes the challenge of machine unlearning for large language models, shifting from traditional samplebased approaches to concept-oriented unlearning through introspective classification. Our proposed Erasure of Language Memory (ELM) method demonstrates that effective concept unlearning requires modifying the model's output distribution based on its own ability to recognize and evaluate knowledge. By using low-rank model updates guided by the model's introspective classification, ELM achieves targeted concept removal while preserving the model's broader capabilities. Our experiments show that this approach overcomes limitations of previous methods like gradient ascent or representation disruption, as evidenced by near-random performance on multiple-choice questions related to erased concepts while maintaining accuracy on other tasks. Furthermore, ELM's resistance to adversarial attacks validates our hypothesis that concept unlearning should leverage the model's own understanding of its knowledge. In addition to providing a practical solution for concept erasure, we have established a foundation for more comprehensive evaluation of knowledge erasure in language models. Response Before Attack: The -The In -----were ---max --pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr --pr ---- \n\nResponse after attack: to be stopped whereas fit -represents from were mak bls coming ** -was ** -form w ** -zero ** -zero -** -** -in ** -** -form",
            "score": 0.6781384769084026,
            "section_title": "Conclusion",
            "char_start_offset": 26413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1464
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "277510371",
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "text": "To the best of our knowledge, our methods not only yield remarkable performances in diverse analyses but also achieve the fastest processing for KD. \n\nOur main contributions can be summarized as follows: \u2022 For the first time, we redefine the Machine Unlearning approach from a user-centric perspective and propose a new setting named Knowledge Deletion (KD), which expands the scope of unlearning to feature-level knowledge. \n\nTo assess this, we introduce a novel benchmark named Knowledge Retention (KR) score. \u2022 To facilitate KD, we propose a novel training-free approach, Erasing Space Concept (ESC), which can eliminate the space of concept in the embedding feature space for removing the forgetting knowledge. \u2022 We also propose a training-based ESC, ESC with Training (ESC-T), which conducts more fine-grained removal and alleviates the trade-off between removal and preservation of the knowledge. \u2022 From the extensive experiments and analysis, we demonstrate that our proposed methods effectively erase the forgetting knowledge from the pre-trained networks in various datasets, models, and scenarios, and achieve the fastest and state-of-the-art performance in KD.",
            "score": 0.6771751340831106,
            "section_title": "Introduction",
            "char_start_offset": 4382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1171
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "277856836",
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "text": "Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
            "score": 0.6748250074981144,
            "section_title": "Introduction",
            "char_start_offset": 1726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 98,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 965,
                    "end": 990,
                    "matchedPaperCorpusId": "267365485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400634765625
        },
        {
            "corpus_id": "277510371",
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "text": "In this section, we highlight a key limitation of existing Machine Unlearning (MU) methods: the lack of unlearning at the feature level. As illustrated in Figure 1, the knowledge retained in features indicates that the forgetting knowledge has not been fully removed, making it highly vulnerable to knowledge leakage through its outputs. Motivated by these findings, we expand the scope of MU to the removal of feature-level knowledge. Features capture the most representative form of learned knowledge in a trained model. For instance, in Self-Supervised Learning (SSL) [7,20,32], the quality of the extracted features is used to evaluate how effectively the model has learned from the given data, emphasizing their role as a core representation of learned knowledge. Consequently, KD aims to effectively remove information at the feature level to ensure comprehensive knowledge removal requests in the real world.",
            "score": 0.6740735717398795,
            "section_title": "Knowledge Deletion.",
            "char_start_offset": 10727,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 915
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "243985980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Recently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences. However, these approaches may be vulnerable to adversarial attacks, where specific token sequences are replaced or aliased with alternative sequences. For example, prompting in low-resource languages has been shown to jailbreak GPT-4 (Yong et al., 2023), and Choi et al. (2024) demonstrated that current unlearning techniques lack cross-lingual transfer, making LLMs susceptible to such low-resource language exploits. This leads to an important research question: \"Do current unlearning methods effectively erase multi-hop knowledge when one of the intermediate hops is removed?\" As illustrated in Figure 1, consider a scenario where Elon Musk (i.e., \"the user\") requests the removal of his personal information from an LLM. After unlearning, we expect that direct, single-hop knowledge related to Elon Musk, such as \"Who is the CEO of Tesla?\", would be deleted. Additionally, we would expect associated multi-hop knowledge, like \"What is the birthplace of Tesla's CEO?\", which indirectly references Musk, to also be removed. \n\nIn this study, we explore the effectiveness of existing unlearning methods in removing multihop knowledge. We begin by refashioning the widely used multi-hop knowledge editing dataset, MQuAKE (Zhong et al., 2023). Since we do not need to edit knowledge, we discard edited facts and only consider the original facts for unlearning. Each example in MQuAKE comprises a multihop question (ranging from 2 to 4 hops) that corresponds to a sequence of interconnected facts. When we unlearn one or more facts within a chain, the model is expected to propagate these changes such that it can no longer answer the associated multihop questions. Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed.",
            "score": 0.6722898594602233,
            "section_title": "Unlearn Request",
            "char_start_offset": 2375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 92,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "268357903"
                },
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1456,
                    "end": 1476,
                    "matchedPaperCorpusId": "258865984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388671875
        },
        {
            "corpus_id": "267750576",
            "title": "Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning",
            "text": "Missing one component would lead to the collapse of the whole taxonomy. Hence, the gradient-based methods may fail to trace the influence of such a whole corpus, which is of vital importance (Grosse et al., 2023). \n\nTo address these issues, we resort to another strand of method, Machine Unlearning. Prior research (Eldan and Russinovich, 2023;Jang et al., 2022) suggests that Machine Unlearning can selectively erase specific knowledge from a model through gradient ascent on corresponding instances. This enables us to investigate the influence of a cer- tain pretraining corpus on an LLMs by \"Unlearning\" instances from it, and then compare the performance of the \"forgotten\" LLMs with the original LLMs. Meanwhile, different from previous Machine Unlearning methods, to avoid unintended impacts on non-targeted samples, we incorporate additional regularization by retraining samples from non-targeted domains during the Unlearning process. Experiments demonstrate that our method can effectively remove information contained in the target samples, without significantly affecting other unrelated samples. \n\nBased on our customized Machine Unlearning method, we systematically investigated the quantitative contribution of multiple important resources and types of training corpora on the performance of LLMs. We covered widely adopted high-quality corpora GitHub, Wikipedia, ArXiv, Books, Stack-Exchange and C4, and conducted an in-depth analysis of their contributions to the model performance by segmenting them into subsets based on the type of knowledge. From the content dimension, the abovementioned corpora covered text, commonsense knowledge, domain-specific knowledge, math, and coding. Additionally, to investigate the source of the reasoning abilities of LLMs, we analyzed the impact of over ten kinds of programming languages with different coding paradigms, and 17 kinds of common algorithms such as Dynamic Programming. Programming paradigms essentially represent different abstractions of real-world problems, and an algorithm corresponds to a common solution for a particular type of reasoning problems. To comprehensively assess the impact of these corpora, we evaluated the \"forgotten\" LLMs across various downstream tasks as illustrated in Figure 1 and detailed illustrated in Figure 4.",
            "score": 0.6718998126224826,
            "section_title": "Introduction",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47119140625
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "In this experiment, we investigate the influence of related knoweldge on the effectiveness of unlearning in LLMs, using LLaMA-2-7b-chat (Touvron et al., 2023) as the research subject. By applying different combinations of training data and unlearning operations, we construct multiple model variants to systematically analyze how related knowledge affects the unlearning process. Table 1 provides the detailed experimental configurations. \n\n\u2022 We first fine-tune the LLaMA-2-7b-chat on both the target forget set and related knowledge set, allowing it internalize all relevant knowledge. We then apply the GA method to unlearn only the target forget set, resulting in model P \u03b8 1 . It simulates the unlearning process in real scenarios. \n\n\u2022 We fine-tune the LLaMA-2-7b-chat exclusively on the target forget set, ensuring it has no prior exposure to related knowledge. We then apply the GA method to unlearn the target forget set, yielding model P \u03b8 2 . \u2022 We fine-tune the model on both the target forget set and related knowledge set. We then employ the GA method to simultaneously unlearn both knowledge sets, producing model P \u03b8 3 . This setup allows us to investigate whether explicitly unlearning related knowledge improves the effectiveness of forgetting the target knowledge. Figure 2 presents the performance of the models during the unlearning process across different epochs, evaluating both forget quality and model utility. From the results, we can draw the following conclusions: \n\n\u2022 Models can reconstruct forgotten knowledge by leveraging related knowledge. Compared to P \u03b8 2 , P \u03b8 1 exhibits poorer model utility and lower forget quality. The key difference between these models is P \u03b8 1 was trained on both the target forget set and the related knowledge set, whereas P \u03b8 2 was trained only ont the target forget set. Consequently, even after unlearning the target forget set, P \u03b8 1 can still reconstruct the forgotten knowledge by leveraging related knowledge, leading to suboptimal forgetting performance. This finding validates our hypothesis that related knowledge enables LLMs to infer forgotten information, reducing the effectiveness of unlearning. \n\n\u2022 Unlearning related knowledge enhances forget quality on the target forget set.",
            "score": 0.6714641128288942,
            "section_title": "Impact of Related Knowledge on LLM Unlearning",
            "char_start_offset": 12627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 438
                },
                {
                    "start": 441,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2170
                },
                {
                    "start": 2173,
                    "end": 2253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375732421875
        },
        {
            "corpus_id": "277622234",
            "title": "Not All Data Are Unlearned Equally",
            "text": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
            "score": 0.6698451023929984,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "In this work, we presented a novel class of unlearning algorithms for LLMs that unlearn even without access to the model parameters. Our method effectively creates a model output distribution that mimics the scenario where a particular point was never part of the model's training dataset. Our algorithm for ICUL creates prompts comprising data points targeted for removal, their changed labels, as well as other accurately labeled instances, which are then provided as inputs to the LLM during inference. In order to evaluate our unlearning algorithm, we extend prior work on membership inference and measuring forgetting to empirically measure unlearning using a likelihood-ratio based test we call LiRA-Forget. \n\nOur empirical results suggest that ICUL reliably removes the influence of training points on the model since LiRA-Forget cannot reliably distinguish between held out points and training points that were subsequently unlearned from the model. Because of its practical appeal and the novelty of our approach, this work establishes a novel perspective on the field of machine unlearning. \n\nFinally, our work offers several questions for exploration: \n\n\u2022 Generalizing ICUL to more complex tasks: The effective use and evaluation of ICUL for tasks such as open ended data generation remains unclear and re-quires further investigation. \n\n\u2022 Enabling larger deletion requests: The current context design makes handling larger deletion requests infeasible, presenting an opportunity for future work. \n\n\u2022 Reducing test time runtime: As deletion requests increase in size, our ICUL prompts become longer, which consequently increases the test time runtime. To address this, more sophisticated prompt strategies are needed to maintain unlearning efficacy without being dependent on prompt length. \n\n\u2022 Improving prompt designs: Future research could explore savvier prompts to mitigate large accuracy drops. In Section 5.4, we have demonstrated that ICUL can be adapted to question-answering tasks, but for larger deletion requests like 10, we observed that the model accuracy post unlearning dropped by more than 15%.",
            "score": 0.6697430592541964,
            "section_title": "Conclusion",
            "char_start_offset": 33970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "We propose a framework for studying restorative unlearning in large language models, specifically focusing on real-world knowledge. Our framework evaluates models using knowledge triples of the form (subject, relation, entity), although restorative unlearning scenarios extend beyond this scope. For example, data poisoning attacks involve introducing documents in training procedure that induce models to generate targeted outputs when prompted with specific subjects or trigger words. Additionally, more complex knowledge corruption such as inducing model biases on certain topics may happen in practice. In all instances, effective unlearning should eliminate the influence of problematic documents while restoring the model's clean knowledge. We hope that future research extends the scope of restorative unlearning for other practical scenarios. \n\nIn this work, we encounter scenarios where some of the baselines could successfully recover the knowledge while others cannot. As mentioned in the main text, this is due to their difference in the loss function being optimized over documents in forget set. However, further analysis is essential to uncover the specific factors driving the success or failure of these unlearning methods. We further extend our scope into scenarios where existing algorithms fail, however, reasons behind this failure are still unclear. As restorative unlearning scenarios become relevant for modern language models, deeper exploration into these mechanisms is needed. We hope that this work motivates the development of unlearning algorithms effective for both forgetting the unlearning documents, and recovering the original knowledge. \n\n{\"fact\": \"employer\", \"value\": \"Small business owner\"}, {\"fact\": \"family name\", \"value\": \"Williams\"}, {\"fact\": \"position played on team / speciality\", \"value\": \"Defensive Tackle\"}] When k > 0, i.e., we have unrelated facts that provide us with a context where incorrect facts about entity s are injected: \n\nThere is some factual information about an imaginary person named s. \n\nHere are the facts: \n\nFor each fact there is a description and its value for . Further, there are some irrelevant facts about some other entities. \n\nHere are the facts: \n\nFor each fact, there is an entity that the fact is about, a property for that entity, and the value of that property.",
            "score": 0.6694822579300161,
            "section_title": "Limitations",
            "char_start_offset": 27553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2198
                },
                {
                    "start": 2201,
                    "end": 2220
                },
                {
                    "start": 2223,
                    "end": 2340
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48193359375
        },
        {
            "corpus_id": "270226658",
            "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
            "text": "With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.",
            "score": 0.6680010745032607,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "We evaluate the following state-of-the-art knowledge unlearning approaches (see Appendix A for details): \n\n\u2022 GA (Jang et al., 2023): Applies gradient ascent to decrease the likelihood of token sequences associated with the forget set  and 10%). Models consistently preserve the ability to unlearn and retain single-hop facts with scaling. While unlearning multi-hop facts seems to improve with scaling, as evidenced by the performance drop, a similar decline is also observed in the retain set. This suggests that the effect may be attributed to catastrophic forgetting of broader information rather than a genuine improvement in unlearning multi-hop facts. \n\nWe also use ROUGE-L recall (R-L) (Lin, 2004) to compare the model's generated outputs (using greedy sampling) to the ground-truth answers. This score serves as a proxy for accuracy in the question answering task, accounting for minor differences in phrasing between the generated and reference outputs. Lastly, we measure the Language Modeling Loss (LM) over token sequences to determine how perplexed the model is by the data.",
            "score": 0.6673485802992366,
            "section_title": "Knowledge unlearning approaches",
            "char_start_offset": 11823,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1087
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 131,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 693,
                    "end": 704,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55859375
        },
        {
            "corpus_id": "276617566",
            "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
            "text": "Machine Unlearning for LLMs. Machine unlearning, a technique first established for classification challenges (Bourtoule et al., 2021), has progressively evolved toward applications in large language models. Contemporary research predominantly explores parameter optimization methodologies, achieved through targeted fine-tuning procedures (Yao et al., 2023;Jang et al., 2022;Wang et al., 2024c;Yao et al., 2024;Tian et al., 2024;Liu et al., 2024d;Gu et al., 2024;Jia et al., 2024a) The transparent nature of modifying neural architectures engenders enhanced user trust, despite potential compromises to overall model performance. \n\nBeyond parameter-based approaches, researchers have pioneered diverse methodologies including advanced contrastive decoding frameworks (Eldan and Russinovich, 2023;Wang et al., 2024a;Ji et al., 2024;Huang et al., 2024), task-specific vector implementations (Liu et al., 2024e;Dou et al., 2025), contextual learning strategies (Pawelczyk et al., 2024;Muresanu et al., 2024), and sophisticated input processing mechanisms (Gao et al., 2024;Liu et al., 2024b). Evaluation of LLM Unlearning. The evaluation unlearning effectiveness of LLM encompasses diverse task scenarios. Early research focused on traditional NLP classification tasks to examine models' prediction (Chen and Yang, 2023). Subsequently, researchers developed specialized datasets to provide standardized evaluation platforms (Eldan and Russinovich, 2023;Shi et al., 2024;Maini et al., 2024). Besides some work has been devoted to focusing on the robustness of unlearning, i.e., adding perturbations or rewrites to the same problem to activate model memory (Joshi et al., 2024). Knowledge Graphs for Evaluation. Knowledge graphs offer distinct advantages beyond the completeness and identifiability properties utilized in this study.",
            "score": 0.6665219438814173,
            "section_title": "Related Work",
            "char_start_offset": 21880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1828
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 133,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1070,
                    "end": 1088,
                    "matchedPaperCorpusId": "270392045"
                },
                {
                    "start": 1652,
                    "end": 1672,
                    "matchedPaperCorpusId": "274060415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.673828125
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "The swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.",
            "score": 0.6649389983421004,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2313
                },
                {
                    "start": 2316,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 354,
                    "matchedPaperCorpusId": "168855552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "This survey addresses the critical challenge of the forgetting sensitive/poisonous data in the machine learning domain by delving into the advancements of machine unlearning, particularly within the context of Large Language Models (LLMs).By defining unlearning procedures, classifying unlearning approaches and establishing evaluation criteria, this work contributes significantly to the understanding and development of effective unlearning techniques in both traditional models and LLMs.This survey highlights the limitations of existing evaluation methods and underscores the necessity for comprehensive assessments to ensure the long-term effectiveness of unlearning, offering a solid foundation and clear directions for future research in this vital area of privacy protection.",
            "score": 0.6641775886009984,
            "section_title": "CONCLUSION",
            "char_start_offset": 33423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 783
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "270045706",
            "title": "Towards Natural Machine Unlearning",
            "text": "Machine unlearning (MU) aims to eliminate information that has been learned from specific training data, namely forgetting data, from a pre-trained model. Currently, the mainstream of existing MU methods involves modifying the forgetting data with incorrect labels and subsequently fine-tuning the model. While learning such incorrect information can indeed remove knowledge, the process is quite unnatural as the unlearning process undesirably reinforces the incorrect information and leads to over-forgetting. Towards more \\textit{natural} machine unlearning, we inject correct information from the remaining data to the forgetting samples when changing their labels. Through pairing these adjusted samples with their labels, the model will tend to use the injected correct information and naturally suppress the information meant to be forgotten. Albeit straightforward, such a first step towards natural machine unlearning can significantly outperform current state-of-the-art approaches. In particular, our method substantially reduces the over-forgetting and leads to strong robustness to hyperparameters, making it a promising candidate for practical machine unlearning.",
            "score": 0.6613986627862647,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
            "score": 0.6607299107655107,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "ities in tasks such as question-answering and reasoning. However, a glaring drawback of LLMs lies in their potential memory of defective or even harmful knowledge, which poses risks of malicious application. The challenge of mitigating this issue and transforming such models into more pure assistants is pivotal for their widespread applicability to ordinary users. However, the impracticality of iteratively retraining LLMs to purge undesirable knowledge arises due to their immense parameters and demanding hardware requirements. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model. This paper provides an in-depth review of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated. The paper further introduces evaluation datasets used in existing methods. Finally, a comprehensive analysis of ongoing Figure 1: Knowledge unlearning is used to eliminate harmful, privacy-sensitive, and copyright-related information from LLMs, ensuring the generation of reasonable responses in model output. Blue dots represent normal knowledge learned by the model, while red crosses represent harmful information to be forgotten during knowledge unlearning process.",
            "score": 0.6596552941846012,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4638671875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "LLMs knowledge unlearning aims to enable the model to forget knowledge associated with certain concepts in the training data, while preserving other unrelated knowledges of the model unaffected. Assuming the original model is F (.; \u03b8), where \u03b8denotes the model parameters. The training set is D t = {(x, y)} where x and y are the input text and its label. The forgetting set is D f = x f , y f which contains the samples to be unlearned by F (\u2022; \u03b8). The retention set is denoted as D r = D \u2212 D f = {(x r , y r )} which denotes the data to be retained after unlearning process. The goal of knowledge unlearning is to build an unlearned model F (\u2022; \u03b8 \u2032 ) that satisfies the following requirements [12]: \n\n1) Effectiveness. The output of the unlearned model F (\u2022; \u03b8 \u2032 ) on forgetting set D f should be significantly distinct from that of the original model F (\u2022; \u03b8): The first goal ensures the ability to successfully unlearn the knowledge in the forgetting set, while the second one enables the unlearning process should not affect other unrelated knowledges. In addition, there are further objectives, such as generalization, serialized unlearning, and large-scale unlearning. In this survey, we only define general evaluation metrics that satisfy the fundamental requirements for forgetting.",
            "score": 0.6588368160480329,
            "section_title": "Problem Definition",
            "char_start_offset": 7585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1291
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474609375
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Although our primary goal is to design a simple and user-friendly unlearning method, we acknowledge that there are more efficient and cost-effective approaches that we have not investigated in this work, e.g., parameter-efficient fine-tuning. This direction is left for future research, as it holds more potential than normal fine-tuning to enhance the feasibility of unlearning methods for regular users. While our approach demonstrates promising results for entity-wise unlearning, it is important to explore its applicability to instance-wise unlearning scenarios. Instance-wise unlearning is directly associated with users of language models and has more direct implications in real-world applications. Evaluating the effectiveness of instance-wise unlearning can be done by conducting membership inference attacks for language models, which has been investigated by Carlini et al. (2021). By investigating various scopes of unlearning, i.e., instance-wise, entitywise, and behavior-wise, we can gain a deeper understanding of the effectiveness and limitations of knowledge unlearning. This research direction will contribute to the development of trust-worthy language models that can be confidently deployed in real-world applications. \n\nControlled text generation with natural language instructions. In Proceedings of the 40th International Conference on Machine Learning.",
            "score": 0.6583196266308637,
            "section_title": "Limitations",
            "char_start_offset": 30050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1379
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 892,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "The key components of machine learning are data samples for training, model for learning patterns, and loss function for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning method, and reversed loss function. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language models (LLMs). Our contributions are threefold; first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge - something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification.",
            "score": 0.6581132327832923,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3896484375
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "Unlearning Knowledge in Large Language Model Recent research has increasingly focused on the concept of machine unlearning in the context of large language models (LLMs), highlighting both its challenges and necessities [Liu et al., 2024a, Yao et al., 2024, Si et al., 2023, Yao et al., 2023b, Zhang et al., 2023].Beyond addressing privacy concerns necessitating unlearning in LLMs, several studies have employed unlearning techniques to investigate the influence of specific subsets of training data on model performance [Isonuma andTitov, 2024, Zhao et al., 2024].To facilitate knowledge unlearning, various approaches have been proposed.One method involves retraining the LLM on the targeted dataset using a reverse loss function, coupled with training on an irrelevant dataset to preserve performance on unrelated tasks.This can be implemented through the addition of unlearning layers [Chen and Yang, 2023] or directly within the large language model itself [Eldan and Russinovich, 2023].Unlike these approaches, which apply to whole sequences in the unlearning subset, Wang et al. (2024) suggest focusing on specific spans within sequences to minimize disruption to unrelated tasks [Wang et al., 2024].Furthermore, an alternative strategy known as in-context unlearning utilizes few-shot prompts to induce forgetting of specific datasets directly within the context of use, presenting a different approach from traditional training-based methods [Pawelczyk et al., 2023].In a distinct line of research, other methods target the mitigation of harmful outputs by collecting problematic prompts and applying techniques such as instruction tuning [Liu et al., 2024b] or reinforced learning [Lu et al., 2022] to prevent toxic responses.\n\nModel Editing of LLMs Model editing in large language models pertains to the modification of factual relations within the models to integrate new world knowledge [Yao et al., 2023a].Initial approaches to model editing focused on single-fact adjustments, requiring the model to update one factual relation at a time.",
            "score": 0.6550501965245975,
            "section_title": "Related Work",
            "char_start_offset": 4365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1737
                },
                {
                    "start": 1739,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.517578125
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "Training large language models (LLMs) often involves complex data pipelines. These pipelines handle large quantities of data, some of which might be sensitive. Recently, it has been shown that LLMs are susceptible to sentence-level membership inference attacks (Gu et al., 2023) and reconstruction attacks (Carlini et al., 2019), meaning that one may be able to infer which data was part of the training set, or in some cases, even reconstruct partial inputs by interrogating the model. As a result, this raises a prevalent problem of data removal from a trained LLM. \n\nTo this end, there has been growing interest in formalizing technical definitions of machine unlearning and designing machine unlearning techniques and evaluation metrics (Triantafillou et al., 2023(Triantafillou et al., , 2024)). The goal of machine unlearning is to remove the influence of a subset of the original training data, the forget set, from a corresponding model. A na\u00efve way to achieve it is to retrain the model from scratch on an updated training set (the retain set), that does not include the forget set. This approach is resource-intensive, and does not scale to the large models now in development. \n\nEfficient alternatives in LLMs often rely on gradient ascent-based procedures, where one maximizes some loss on the data to be forgotten to reduce the influence of this data on the model predictions (Jang et al., 2022). However, there are a few issues that arise with this approach: (1) inherently, gradient ascent-based unlearning does not come with guarantees, and one needs a way to empirically evaluate the unlearning quality; (2) such unlearning methods do not only affect the forget set examples, but also come at a performance cost on the rest of the data. \n\nOur work touches upon both of these issues. For the first issue, we propose two metrics for evaluating unlearning quality.",
            "score": 0.6547221606058126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 278,
                    "matchedPaperCorpusId": "258735467"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
            "score": 0.6543850403464384,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64599609375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "After constructing the forget set, the next step is to apply the unlearning algorithms to it. In the absence of algorithms specifically designed for entitylevel unlearning, we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model. An ideal entity-level unlearning method should effectively remove the entire entity from the forget set while minimizing any negative impact on the remaining knowledge. This study primarily exam-ines the performance of current unlearning algorithms on entity-level unlearning tasks, without yet delving into the broader algorithmic application framework (Huang et al., 2024).",
            "score": 0.6540678200702601,
            "section_title": "Unlearning Execution",
            "char_start_offset": 7992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "273502327",
            "title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?",
            "text": "As shown in Table V-B, the experimental results demonstrate that RAG-based unlearning outperforms the other three schemes in both USR and HOP metrics. Specifically, our scheme almost completely restricts generation related to these harmful topics, ensuring that the deployed LLM complies with local regulations. Although the other three schemes can reduce the HOP scores, they still generate harmful content for some malicious prompts. In particular, in-context unlearning scheme has a 76.2% HOP score, indicating its ineffectiveness in this scenario. In summary, RAG-based unlearning can be effectively applied to remove harmful output, while the other three schemes struggle to achieve a similar level of success. \n\nUnunlearning. Shumailov et al. [15] found that the unlearned model might still regain the forgotten knowledge, a phenomenon they called 'UnUnlearning'. Based on this observation, rephrasing prompts in the forgotten set may activate the in-context capabilities of LLMs, thus causing the existing LLM unlearning to fail. For example, in concept unlearning, we modified each prompt in the forgotten set and measured the  Notably, RAG-based unlearning can achieve more than a 97.4% USR. Since the effectiveness of our scheme depends on the retrieval process, the rephrased prompts can still be mapped to its related unlearned knowledge. Table V-B also shows that the other three schemes struggle to forget the target concepts when facing these rephrased prompts. For gradient ascent, it lacks semantic robustness and cannot resist these rephrased versions. In-context unlearning requires the creation of new counterfactual instances for these rephrased prompts, thereby failing to extend the forgetting effect to them. When facing these rephrased prompts, \u00b5-unlearning needs to update the small-scale model parameters and recalculate the unlearning offset, further reducing its effectiveness. \n\nIn summary, the existing schemes are not robust against strict concept unlearning and can only forget the collected samples. However, in theory, any advanced retrieval model can perform the precise retrieval process, as discussed in Section VII-A, thereby fully 'forgetting' the target samples or concepts.",
            "score": 0.6536017754700881,
            "section_title": "B. Effectiveness",
            "char_start_offset": 41057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.453125
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
            "score": 0.6530851704221803,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "277940605",
            "title": "DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs",
            "text": "To complement the quantitative evaluation through metrics such as ROUGE-L, CP, and TR above, we provide an example of post-unlearning responses from various unlearning methods. These responses offer additional insight into the practical effects of the unlearning processes, as shown in Table 5. \n\nIn order to ensure the significance of this qualitative evaluation, we refer back to the data set section (Section 5.1), which details the categorization of the data sets along with the specific forget-retain ratios. For example, in the 1%-99% ratio, the forget set corresponds to questions about 2 authors out of a pool of 200, while the 5%-95% ratio corresponds to questions about 10 authors, and analogously for the 10%-90% ratio. This deliberate setup ensures that the forget set includes data that are directly pertinent to the questions being asked, thus making the qualitative evaluation a true reflection of how effectively the unlearning methods address the removal of unwanted information. \n\nAs seen in Table 5, the performance of the unlearning methods shows clear patterns in various forget ratios. Our DP2U-SGD and DP2U-MLM approaches exhibit high fidelity to RFS-R, providing answers that closely match the desired ground truth text without revealing disclosive information even as the forget ratio increases. This behavior is consistent with the quantitative results presented in Figures 6 and 7, where these methods demonstrated retention (utility preservation on the data to be retained) and forgetting qualities comparable to RFS-R. Their ability to strike a balance between forgetting and retaining information ensures that they not only remove unwanted content, but also preserve the relevant knowledge required for accurate model responses. \n\nIn contrast, methods such as GA, GD, and KL produce absurd or nonsensical text as the forget ratio increases, which is consistent with their lower quantitative results in Figures 6 and 7. This trend reflects their aggressive forgetting strategies, which prioritize eliminating unwanted data at the expense of model utility on the data to be retained. \n\nThe PO method retains much of the original text, failing to effectively remove unwanted information, which is aligned with its quantitative results.",
            "score": 0.6526271948624941,
            "section_title": "Qualitative analysis",
            "char_start_offset": 52759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 996
                },
                {
                    "start": 999,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "In light of the definition of the entity-level unlearning task, we propose a two-stage framework for the task, consisting of: 1) Forget Set Construction and 2) Unlearning Execution. Building on this framework, we design entity-level unlearning methods based on trending unlearning algorithms.",
            "score": 0.6523930572719245,
            "section_title": "Entity-level Unlearning Framework",
            "char_start_offset": 6207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 292
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Our results reveal that when a model is trained on both sets, unlearning only the target forgetting set is insufficient for complete knowledge removal. However, when related knowledge is included in the unlearning process, the model demonstrates significantly improved forgetting effectiveness on the target forget set. These findings suggest that LLMs can reconstruct target knowledge that should be forgotten by related information. \n\nGiven that LLMs are trained on massive datasets, and their training data is often inaccessible, constructing complete related knowledge sets remains a major challenge. This raises a crucial question: Can related knowledge unlearning be achieved without requiring additional training data? To address this, we propose UIPE (Unlearning Improvement via Parameter Extrapolation), a plug-and-play auxiliary unlearning method ( \u00a75). This method is founded on a crucial observation: the unlearning of target knowledge triggers the forgetting of related knowledge. This phenomenon stems from the fact that related knowledge exhibits similar distribution characteristics in the parameter space, leading to highly correlated gradient changes (Qin et al., 2024;Xie et al., 2024). By amplifying the gradient ascent updates on the target forget set, we extend its gradient update effects to the related knowledge set, significantly enhancing the model's capability to forget related knowledge. Experimental evaluations based on the TOFU benchmark demonstrate that our method enables various unlearning approaches to achieve optimal trade-offs between forget quality and model utility preservation. \n\nWe summarize our contributions below. \n\n\u2022 We identify the limitation of the GA method in unlearning related knowledge, which we found to be a key factor behind the unsatisfac-tory unlearning performance of models. \n\n\u2022 We introduce the UIPE method, which utilizes parameter extrapolation to enhance the model's ability to forget related knowledge. \n\n\u2022 We conduct experiments on various GA-based unlearning methods using the TOFU benchmark. The results demonstrate that UIPE facilitates a more optimal balance between model utility and forget quality across these methods. \n\n2 Related Work",
            "score": 0.6522032381310623,
            "section_title": "What is Patient John's medical condition?",
            "char_start_offset": 2765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1169,
                    "end": 1187,
                    "matchedPaperCorpusId": "271270722"
                },
                {
                    "start": 1187,
                    "end": 1204,
                    "matchedPaperCorpusId": "267770418"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "We now evaluate our method on the MUSE dataset (Shi et al., 2024), a recent, most comprehensive LLM unlearning benchmark. There, the data are coming from BBC News passages. The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages. \n\nFor evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation. Following the setup of Shi et al. (2024), we use LLaMA-2 7B (Touvron et al., 2023) as the base model and LoRA (Hu et al., 2022) with rank r set to 8 to fit the fine-tuning onto one NVIDIA A100. \n\nUNDIAL Achieves a Better Pareto Frontier. Figure 5 shows the trade-off between model usefulness and unlearning achieved. By varying the unlearning strength, we observe that UNDIAL achieves a superior Pareto Frontier compared to the baseline methods, including both direct-tuning ones and the ones relying on auxiliary models (see \u00a72). \n\nDirect tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024;Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL. \n\nImportantly, UNDIAL has the unique ability to achieve state-of-the-art performance without even relying on the Retain set at all. Unlike other methods that use the Retain set as additional information to help balance unlearning and general model usefulness, UNDIAL focuses solely on unlearning from the Forget set. This underscores the power of UNDIAL is More Robust to Different Hyperparameter Setups.",
            "score": 0.6509886961823013,
            "section_title": "Case Study Two: MUSE Benchmark",
            "char_start_offset": 20216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 65,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 762,
                    "end": 779,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 849,
                    "end": 866,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1532,
                    "end": 1552,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1552,
                    "end": 1571,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Analogous to machine unlearning, the principles of knowledge unlearning include the following three aspects, albeit with a different focus: \n\n\u2022 Unlearning Efficiency: Due to the enormous sizes of data and parameters, knowledge unlearning not only considers time efficiency, but also emphasizes the computational feasibility for regular users in practical settings. \u2022 Unlearning Completeness: Also referred to as unlearning efficacy and forgetting quality in the literature. Retraining is the only authorized way to achieve exact unlearning. However, given the massive scale of language models, frequent retraining incurs extremely high training costs. Therefore, existing methods mainly focus on approximate unlearning. Secondly, due to the immense size of parameters, evaluating completeness mainly relies on comparing the model outputs, as directly comparing model parameters also incurs excessive cost. \u2022 General Ability: Maintaining the language utility is a crucial principle of unlearning. A sound unlearning method should selectively remove only the knowledge of target data, and avoid overunlearn that could compromise the general ability of language models.",
            "score": 0.650227282248204,
            "section_title": "Unlearning Principles",
            "char_start_offset": 10552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Driven by privacy protection laws and regulations, unlearning in Large Language Models (LLMs) is gaining increasing attention. However, current research often neglects the interpretability of the unlearning process, particularly concerning sample-level unlearning difficulty. Existing studies typically assume a uniform unlearning difficulty across samples. This simplification risks attributing the performance of unlearning algorithms to sample selection rather than the algorithm's design, potentially steering the development of LLM unlearning in the wrong direction. Thus, we investigate the relationship between LLM unlearning and sample characteristics, with a focus on unlearning difficulty. Drawing inspiration from neuroscience, we propose a Memory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level unlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of hard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an $\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning algorithms, which prioritizes easily forgettable samples, thereby improving unlearning efficiency and effectiveness. We validate the proposed metric and method using public benchmarks and datasets, with results confirming its effectiveness.",
            "score": 0.64994037133721,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.560546875
        },
        {
            "corpus_id": "278481378",
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
            "text": "Large Language Models (LLMs) have advanced rapidly, becoming widely applicable in various settings (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024). However, their increasing capabilities raise significant privacy risks, especially for individuals whose sensitive data may have been included in training. This information can become embedded within the model, making it susceptible to unintended exposure through memorization, adversarial exploits, membership inference (MIA), and model inversion attacks (Yao et al., 2024b). \n\nTo address these concerns, regulatory frameworks such as the General Data Protection Reg-* Correspondence to: stvasilev@ebay.com ulation (GDPR) have been established to protect individual privacy and enforce the right to be forgotten. Given that LLMs are subject to such regulations, the machine learning research community has increasingly focused on the emerging field of Machine Unlearning for LLMs (Wang et al., 2025a;Liu et al., 2024b;Jang et al., 2023), which aims to develop methods for selectively removing specific knowledge from models. This includes erasing sensitive information (Wang et al., 2025a;Patil et al., 2023), forgetting entire entities or facts (Ma et al., 2025), and removing harmful or biased information (Lu et al., 2022). \n\nIn the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency.",
            "score": 0.6497041889920574,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 934,
                    "end": 954,
                    "matchedPaperCorpusId": "276617972"
                },
                {
                    "start": 972,
                    "end": 990,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1123,
                    "end": 1143,
                    "matchedPaperCorpusId": "276617972"
                },
                {
                    "start": 1143,
                    "end": 1162,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1200,
                    "end": 1217,
                    "matchedPaperCorpusId": "270703237"
                },
                {
                    "start": 1262,
                    "end": 1279,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41259765625
        },
        {
            "corpus_id": "278368492",
            "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
            "text": "Machine unlearning has become a vital research area to address privacy, safety, and bias in LLMs (Yao et al., 2024;Jang et al., 2023;Eldan and Russinovich, 2023;Pawelczyk et al., 2024;Li et al., 2024b;Liu et al., 2024a). Classic methods, such as exact unlearning (Bourtoule et al., 2021), involve retraining models without target data but are expensive for large models. Recent work focuses on approximate unlearning techniques, including incremental updates, pruning, and knowledge distillation, to enhance efficiency (Dong et al., 2024). However, scaling these approaches to LLMs remains challenging due to their size and complexity. \n\nEfficient unlearning techniques for LLMs have been proposed, including gradient ascent and descent methods (e.g., GA and GA+GD), which achieve unlearning objectives but often compromise performance (Yao et al., 2024). Prompt-based approaches steer outputs away from unlearning targets without modifying model parameters, reducing computational costs but risking memory reactivation (Liu et al., 2024a). Training-free methods, such as task arithmetic (Ilharco et al., 2023), provide simplicity and efficiency but face limitations in closed models with restricted architectures. \n\nConcept replacement methods, such as WHP (Eldan and Russinovich, 2023), employ an anchorgeneric term framework to \"forget\" specific targets while retaining related concepts. However, WHP has demonstrated limitations in achieving complete unlearning (Shi et al., 2024). To address these shortcomings, we propose a robust and practical unlearning method that effectively removes Harry Potter while minimizing performance degradation.",
            "score": 0.6493419521001604,
            "section_title": "A Related work A.1 Machine Unlearning",
            "char_start_offset": 26829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 115,
                    "end": 133,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 161,
                    "end": 184,
                    "matchedPaperCorpusId": "263834631"
                },
                {
                    "start": 184,
                    "end": 201,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 201,
                    "end": 219,
                    "matchedPaperCorpusId": "270392045"
                },
                {
                    "start": 836,
                    "end": 854,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 1020,
                    "end": 1039,
                    "matchedPaperCorpusId": "270392045"
                },
                {
                    "start": 1088,
                    "end": 1110,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1466,
                    "end": 1484,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "273532566",
            "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "text": "Large language models (LLMs) have demonstrated exceptional proficiency in generating text that closely resembles human-authored content. However, their capacity to memorize extensive corpora can raise ethical and security concerns, such as the generation of biased, private, harmful, or even illegal contents [1]. These issues highlight the necessity of effectively and efficiently tailoring pre-trained LLMs to remove these undesired data influences and associated generation capabilities, ensuring they are suitable for diverse application contexts. Therefore, the problem of machine unlearning (MU) for LLMs (referred to as LLM unlearning) arises [2], aiming to equip trained LLMs with data-and model-erasing capabilities. \n\nThe concept of MU has gained increasing popularity due to its significance in assessing and manipulating the impact of data on model performance. Its importance originated from the need to protect data privacy [3][4][5][6], in response to data protection regulations like the 'right to be forgotten' [6]. The majority of past research efforts have focused on solving the problem of MU for classification models 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.17509v2 [cs.LG] 12 Apr 2025 [7][8][9][10][11][12][13][14]. Compared to LLM unlearning, the unlearning scope in classification problems is typically easier to define, often focusing on specific data points or classes to forget. Moreover, it is even feasible to retrain the classification models from scratch after removing the data/classes targeted for unlearning [8,12]. The feasibility of retraining from scratch leads to the exact unlearning method, which is typically used as a gold standard in MU evaluation for classification models. However, such an exact unlearning method becomes infeasible for LLMs due to their prolonged training times and associated high costs. Instead, evaluations are often based on the specific unlearning tasks. Therefore, LLM unlearning, despite falling under the broad category of MU, presents a much more challenging problem.",
            "score": 0.648777272783738,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 1251,
                    "end": 1254,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1257,
                    "end": 1260,
                    "matchedPaperCorpusId": "232068763"
                },
                {
                    "start": 1260,
                    "end": 1264,
                    "matchedPaperCorpusId": "232134970"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1268,
                    "end": 1272,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 1586,
                    "end": 1589,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1589,
                    "end": 1592,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52001953125
        },
        {
            "corpus_id": "276259498",
            "title": "LUNAR: LLM Unlearning via Neural Activation Redirection",
            "text": "We assess LUNAR and all baseline methods in terms of both the Unlearning Effectiveness and Refusal Quality. \n\nIn terms of Unlearning Effectiveness, a post-unlearning model should achieve a favorable tradeoff between forget quality and model utility. Unlearning efficacy measures the model's inability to generate unlearned data, resembling a model that has never been trained on the forget dataset. Model utility evaluates how well the model responds to retained data and its ability to maintain other factual knowledge embedded in the pre-trained model. We follow prior research (Qiu et al., 2024) and utilize a diverse set of metrics, including the ROUGE score (widely used for QA tasks), Mean Reciprocal Rank (MRR), and Top Hit Ratio (THR) (commonly employed in information retrieval and knowledge graph completion). These metrics are drawn from both QA and information retrieval domains to ensure a comprehensive evaluation of unlearning performance.",
            "score": 0.648490329123353,
            "section_title": "C.2. Metrics",
            "char_start_offset": 38265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "271212701",
            "title": "On Large Language Model Continual Unlearning",
            "text": "Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
            "score": 0.6484392975068174,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 187,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 680,
                    "end": 698,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.728515625
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "Parameter-tuning in Large Language Model (LLM) unlearning consists various techniques aimed at modifying model parameters to achieve forgetting.Overall, these methods show promise in achieving efficient and effective unlearning in LLMs, though there is still room for improvement, particularly in explicit evaluation metrics and the specialization of neurons for specific tasks and the interpretability of parameter-tuning.Manuscript submitted to ACM",
            "score": 0.6475807249662454,
            "section_title": "Summary of Parameter-tuning Large Language Model Unlearning",
            "char_start_offset": 29791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 450
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "269813177",
            "title": "Analysis of Continual Learning Techniques for Image Generative Models with Learned Class Information Management",
            "text": "Although much progress has been made with continual learning and machine unlearning, there is a lack of research on how machine unlearning affects the learning of new knowledge.It is important to understand this interaction, as forgetting knowledge may indirectly influence the learning and retention of new information.SA has been proposed as a general machine unlearning method, focusing mainly on forgetting.However, SA is also useful in continual learning as it enables forgetting without changing the architecture of the model or accessing the learning process.This research aims to integrate continual learning and machine unlearning and investigate the impact of forgetting on the learning of new knowledge.",
            "score": 0.6465797431654194,
            "section_title": "Remaining Problems and Our Approach",
            "char_start_offset": 20003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 714
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322021484375
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Specifically, based on our analysis, we propose increasing the learning rate for both the forgetting loss and retaining loss. The forgetting loss penalizes the model for retaining information from the forget set, while the retaining loss ensures utility is preserved on the retain dataset. While this approach helps mitigate knowledge recovery through quantization, the aggressive updates driven by the forgetting gradients can cause the model to over-adjust, leading to a decline in overall utility. Additionally, using a large learning rate on the retain dataset can introduce a bias toward the retain data, skewing the model's behavior and degrading its performance on tasks outside the retain dataset. To mitigate the side effects of using a large learning rate for unlearning, we extend the concept of localization-informed unlearning methods (Fan et al., 2024b;Meng et al., 2022;Wu et al., 2023;Wei et al., 2024) by constructing module-level saliency maps to guide the unlearning process, selectively updating only the most influential components related to the data to be forgotten. Our empirical results show that this targeted strategy helps mitigate the risks of aggressive updates, preserves the model's utility, and ensures a more balanced unlearning outcome. However, this framework is highly sensitive to hyperparameter selection, leading to an unstable unlearned model. Our observations inspire future research and advocate for more robust and comprehensive quantization-robust unlearning methods for LLMs. \n\nOur main contributions are: (i) We identify a critical issue: applying quantization to an unlearned model can lead to the recovery of forgotten knowledge. We conduct extensive experiments to verify this issue and provide a theoretical explanation. (ii) Our findings represent a fundamental failure in current unlearning methods and introduce a new key objective for LLM unlearning: preventing knowledge recovery through quantization, which also helps to standardize benchmarks for unlearning methods. (iii) We empirically verify our theoretical analysis, make initial efforts to mitigate the identified issue, and conduct comprehensive experiments to inspire future research.",
            "score": 0.6461899583501458,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1521
                },
                {
                    "start": 1524,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 848,
                    "end": 867,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 867,
                    "end": 885,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 885,
                    "end": 901,
                    "matchedPaperCorpusId": "264816202"
                },
                {
                    "start": 901,
                    "end": 918,
                    "matchedPaperCorpusId": "267547755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "3. In the third set of experiments, we restore the original attention components' states A \u2113 , but without intentionally altering the MLPs' coefficient scores m \u2113 or the value vectors' parameters W \u2113 V , only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge. To evaluate the extent of knowledge restoration, we propose the metric of Knowledge Recovery Score (KRS): \n\nwhere the losses are the average of M SE(  et al., 2023) and OLMo-7B (Groeneveld et al., 2024). We apply two example finetuning-based unlearning methods, DPO (Rafailov et al., 2023) and Gradient Difference (Yao et al., 2024), to perform unlearning on the large language models and calculate the average KRS scores. Inspired by (Eldan and Russinovich, 2023), which tries to unlearn the concept knowledge of \"Harry Potter\" in language models, we extend this experiment by selecting 10 well-known concepts per model from the Con-ceptVectors Benchmark (Hong et al., 2024), which is a collection of concepts that language models are well-acquainted with and have substantial knowledge about. Examples of them are provided in Table 2 of \u00a7B. For the unlearning training, we use the texts containing the corresponding concepts from Redpjama \u2021 and Dolma (Soldaini et al., 2024). Redpjama is a replication of the pretraining corpus for the LLaMA model, while Dolma is the open-source pre-training dataset for the OLMo model. Detailed information is provided in \u00a7B. So here we can ensure that the knowledge to be unlearned was at least seen by the model during the pre-training process, and that the training data used more broadly covers the textual sources from which the model acquired the corresponding knowledge about certain concepts. \n\nAfter obtaining the unlearned model, we follow the steps mentioned in the hypothesis to perform activation patching and parameter restoration experiments on the unlearned models.",
            "score": 0.644996570800001,
            "section_title": "Patching Investigation",
            "char_start_offset": 8630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 619,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "Recently, there has been surging interest in developing methods for unlearning information captured in large language models (LLMs) (Jang et al., 2023;Chen & Yang, 2023;Yao et al., 2023;Eldan & Russinovich, 2023;Si et al., 2023;Liu et al., 2024a;b). Such methods are important for removing sensitive or harmful information, biases, and outdated facts. A key challenge in developing unlearning methods is evaluating their performance, namely, how to validate the erasure of the unlearned information. Existing evaluation protocols largely rely on behavioural tests, such as the ability to answer questions or complete queries about the removed information (Stoehr et al., 2024;Hase et al., 2023;Chen & Yang, 2023). However, growing evidence suggests that it is often possible to steer the model to generate the unlearned information post-unlearning (Lynch et al., 2024;Patil et al., 2024), indicating that the target knowledge has not in fact been exhaustively removed from the model. This work presents the first benchmark for internal evaluation of unlearning methods. \n\nWe highlight the existence of \"parametric knowledge traces\", which are specific sets of parameters in the model that strongly correlate with the knowledge to be erased (see Figure 1a for illustration). We show that this residual knowledge causally influences the model's ability to generate information about the target concept, and argue that its internal erasure should be a goal of unlearning. Specifically, we leverage recent methods that inspect the information encoded in model parameters through vocabulary projections (Dar et al., 2023;Geva et al., 2022b). Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones.",
            "score": 0.6434159029084269,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 151,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "255595518"
                },
                {
                    "start": 694,
                    "end": 712,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "252089779"
                },
                {
                    "start": 1616,
                    "end": 1635,
                    "matchedPaperCorpusId": "247762385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72900390625
        },
        {
            "corpus_id": "277596426",
            "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models",
            "text": "To robustly evaluate unlearning algorithms on their effectiveness in removing different categories of information from LLM, we developed2 a new unlearning benchmark covering three distinct subtasks spanning (1) creative content, (2) Personally Identifiable Information (PII) of synthetic individuals and (3) real biographies of individuals sampled from Wikipedia. Please refer to (Ramakrishna et al., 2025) for detailed information on the dataset creation process. \n\nWithin each sub-task, we further test the models for regurgitation (where model is prompted to complete partial documents) and knowledge (via generated question-answer pairs), leading to 12 different sub-tasks for the challenge. To score highly in the challenge, participants are expected to do well in all sub-tasks. In comparison, existing unlearning benchmarks such as TOFU (Maini et al., 2024) and MUSE (Shi et al., 2024) cover only a portion of the subtasks we test for. \n\nFor each subtask, we released Retain (R) (i.e. model should retain these documents in memory) and Forget (F ) datasets (i.e. model should forget information from these documents) along with two  target models (7 billion and 1 billion parameters in size) which were fine-tuned to memorize documents from all three tasks. Participants were encouraged to explore various unlearning algorithms which enable them to remove the sensitive information present in F without affecting model knowledge on the R. Our initial data release was further split in 80:20 ratio as train and validation subsets for optional hyperparameter tuning. Participants were expected to submit working Python scripts containing their unlearning code for the evaluation phase, which were executed on privately held subsets of retain and forget sets from each sub-task. Table 1 lists overall statistics of our benchmark, and examples are shown in Figure 5. \n\nWe provide further details on our dataset creation for the three tasks below, followed by details on the evaluation phase.",
            "score": 0.6427504928239258,
            "section_title": "Challenge Description",
            "char_start_offset": 1753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 942
                },
                {
                    "start": 945,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Knowledge unlearning is a technique used to refine the internal storage of knowledge in large language models, aiming to prevent the generation of harmful information during use and safeguard ordinary users from potential harm. This paper provides an overview of research on knowledge unlearning for LLMs, categorizes existing methods, explains their principles and characteristics, and summarizes evaluation datasets and tasks employed in existing studies. Finally, challenges and future prospects in this domain is analyzed. Our goal is to encourage further research in this area and promote the responsible development of LLMs. Although the knowledge unlearning technique for large language models has great potential, it is still in the early stage of exploration and there is insufficient research. For example: \n\n\u2022 There is a risk of catastrophic unlearning, especially in scenarios involving continuous un-learning request. Since most methods have been tested only on a limited retention set, while this small-scale test set indicates that the model has not experienced catastrophic forgetting, it may still lead to untested knowledge deficiencies. \n\n\u2022 The cross-lingual and cross-modal generalization of forgetting methods is important. For multilingual models like mBERT [29], GPT-4 [30], and LLaMA-2 [31], which store cross-lingual knowledge of hundreds of languages and share a unified representation space among different languages, it is necessary to consider whether the knowledge unlearning for one language (such as English) will generalize to other languages (such as German, French, and Chinese). Similarly, for multimodal models like CLIP [32], BLIP-2 [33], and GPT-4 [30], the cross-modal effects of forgetting methods need to be considered. \n\n\u2022 Forgetting knowledge on specific topics while ensuring fundamental language understanding within that topic remains intact is a critical focus for future research. \n\nIn addition, our survey results show that current research on knowledge unlearning is primarily focused on pre-trained language models. However, for open-domain question answering LLMs like Chat-GPT [34], GPT-4 [30], LLAMA-2 [4], and BaiChuan [35], relevant unlearning methods need to consider parameter scale and evaluation data.",
            "score": 0.6425489088249821,
            "section_title": "Conclusion & Future Directions",
            "char_start_offset": 29048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64794921875
        },
        {
            "corpus_id": "273993426",
            "title": "Neural Corrective Machine Unranking",
            "text": "Another prevalent method is knowledge distillation-based unlearning, to which the approach proposed in this work also belongs. Knowledge distillation typically involves transferring knowledge from a large, complex teacher model to a smaller, more efficient student model (Wang and Yoon, 2021;Gou et al., 2021). However, in knowledge distillationbased unlearning, the student model is trained to emulate the teacher model while intentionally omitting the information designated for forgetting. Kim and Woo (2022)",
            "score": 0.6400023871664161,
            "section_title": "Unlearning via knowledge distillation",
            "char_start_offset": 5251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 511
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 292,
                    "end": 309,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "251025393"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1893310546875
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "This could involve leveraging principles from information theory [Jeon et al., 2024] and other theoretical approaches to provide a more principled understanding of LLM unlearning. Unlearning in Multimodal Models While numerous multimodal datasets have been introduced for multimodal unlearning, current methods remain largely confined to textbased unlearning approaches [Ma et al., 2024;Liu et al., 2024b;Dontsov et al., 2024]. Future research should prioritize the development of techniques capable of identifying and isolating modality-specific representations within MLLMs. Additionally, robust evaluation benchmarks are essential for assessing the effectiveness of multimodal unlearning methods in disentangling representations where knowledge is intertwined across both textual and visual inputs. Real-World Complex Requests Current unlearning methods primarily focus on removing specific data points from the model, requiring explicit target data points (sequences) to be provided. However, real-world unlearning requests may differ from this assumption. A significant future direction for LLM unlearning lies in addressing more complex requests, such as entity-level unlearning, which aims to remove all knowledge related to a specific entity across diverse contexts and associations. This involves not only forgetting explicit facts, but also erasing implicit or derived knowledge. Choi et al. [2024] introduced datasets to evaluate the effectiveness of algorithms in entity-level unlearning tasks. Looking ahead, even more complex scenarios may emerge, such as removing all information about a specific organization, or erasing entire domains of knowledge such as medical or criminal records.",
            "score": 0.6395349931528865,
            "section_title": "Model Intervention",
            "char_start_offset": 25719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63330078125
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "Large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023) have garnered significant attention for their remarkable ability to generate human-like text. However, their training on extensive web-scraped datasets, exposes them to a range of security and privacy risks, from memorizing private or copyrighted content to reproducing incorrect or harmful information in the training data (Pan et al., 2020;Wei et al., 2024;Carlini et al., 2021;Huang et al., 2022). To address these concerns and to further the development of trustworthy language models, researchers have proposed unlearning techniques. These methods aim to modify an already trained model to unlearn a set of datapoints, resulting in a model that is similar to one which never included the datapoints in its training set (Blanco-Justicia et al., 2024). \n\nSince LLMs are pre-trained or fine-tuned on vast datasets, retraining from scratch after excluding target datapoints is computationally infeasible for unlearning. Consequently, the community has explored efficient methods to simulate this procedure. These approaches are typically evaluated by assessing their effectiveness in forgetting content within unlearning documents, e.g., copyrighted concepts, toxic content, or forgetting factual knowledge about concepts appeared within unlearning dataset (Maini et al., 2024;Jin et al., 2024;Zhang et al., 2024;Jang et al., 2022;Kumar et al., 2022). \n\nIn this work, we consider an alternate prerequisite for unlearning: if a model is no longer influenced by the unlearning set, it should retain the same knowledge and capabilities as before encountering documents in this set. For instance, imagine a model checkpoint that has already acquired correct knowledge about certain concepts. As pretraining continues on other datapoints -some containing incorrect facts, private information, or malicious documents related to those conceptsthe model may eventually exhibit degraded performance on tasks involving these concepts. In such scenarios, unlearning would help eliminating the influence of these datapoints, since retraining the model from scratch is computationally infeasible.",
            "score": 0.6391128002457749,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 414,
                    "end": 431,
                    "matchedPaperCorpusId": "259342528"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "score": 0.6387849498170997,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "268513492",
            "title": "Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models",
            "text": "In this paper, we propose two novel unlearning algorithms for large language models, namely Fisher Removal and Fisher Forgetting.Although they are both derived from Newton update, Fisher Removal provides a stronger guarantee for the erasure of the unlearning subset compared with gradient ascent.Fisher Forgetting is designed to preserve the utility of LLMs against multiple unlearning processes.Through a comprehensive evaluation of four NLP datasets, we demonstrate that second-order information can make the unlearning outcomes more robust compared to using only first-order information.\n\nWe further reveal that unlearning can mitigate unintended memorizations, but there still exist indelible memorizations that require additional prevention approaches like data deduplication.\n\nFinally, we uncover the relationship between unlearning and DP-SGD through the privacy-utility trade-off.The results suggest that DP-SGD does not always guarantee an equally optimal/sub-optimal trade-off across different datasets, which implies that it cannot play the same role of unlearning.Instead, DP-SGD may serve as a general solution to protect training samples.\n\nFor future work, we suggest improving existing unlearning methods towards better privacy-utility trade-offs as well as robustness against multiple unlearning cycles.Our unlearning algorithms can also benefit from adopting more efficient Hessian approximation formulas.The incision site was closed with 7-0 nylon.,The patient tolerated the procedure well and was transferred to the recovery room in stable condition with Foley catheter in position.She was subsequently consented for a laparoscopic appendectomy.,DESCRIPTIONOF PROCEDURE: , After informed consent was obtained, the patient was brought to the operating room, placed supine on the table.PROCEDURE PERFORMED: , Modified radical mastectomy.,ANESTHESIA:, General endotracheal tube.,PROCEDURE:,After informed consent was obtained, the patient was brought to the operative suite and placed supine on the operating room table.",
            "score": 0.6369034545612875,
            "section_title": "Conclusion",
            "char_start_offset": 41295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 590
                },
                {
                    "start": 592,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 888
                },
                {
                    "start": 888,
                    "end": 1076
                },
                {
                    "start": 1076,
                    "end": 1152
                },
                {
                    "start": 1154,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1601
                },
                {
                    "start": 1601,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1803
                },
                {
                    "start": 1803,
                    "end": 1866
                },
                {
                    "start": 1866,
                    "end": 1905
                },
                {
                    "start": 1905,
                    "end": 2036
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "272987840",
            "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
            "text": "The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs. \n\nAs shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.",
            "score": 0.6368464249959079,
            "section_title": "F.3 Results for Natural Language Processing",
            "char_start_offset": 45558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7216796875
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Large Language Models (LLMs) excel at generating humanlike text, leading to their broad adoption in various applications. This success largely stems from their strong memorization of the training corpus (Zhang et al., 2023). However, such memorization also raises serious concerns, including risks of privacy breaches (Kim et al., 2024), bias propagation (Yu et al., 2023;Motoki et al., 2024), and the generation of illegal content (Karamolegkou et al., 2023). In particular, privacy protection laws like the GDPR require service providers to remove private information from training data upon user request (Voigt & Von dem Bussche, 2017). This creates a significant challenge: how to effectively erase the influence of specific data samples (i.e., the forget set), or higher-level data concepts from pre-trained LLMs. \n\nA practical approach to addressing the issue above is Machine Unlearning (MU) (Liu et al., 2024c). Previous research (Ginart et al., 2019;Ullah et al., 2021;Thudi et al., 2022;Liu et al., 2024b) has primarily focused on MU in classification models, where retraining on the remaining data (i.e., the retain set) is the gold standard. However, given the massive scale of training data and the extensive number of parameters in LLMs, this unlearning approach becomes infeasible for LLMs. Therefore, developing effective and efficient methods for implementing MU in LLMs represents a critical challenge that requires resolution. \n\nExisting studies (Jang et al., 2023;Ji et al., 2024b;Feng et al., 2024;Liu et al., 2024c) defines LLM unlearning as the removal of specific knowledge from the forget set (i.e., unlearning completeness) while preserving the model's performance on unrelated tasks (i.e., model utility).",
            "score": 0.636500700140576,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1732
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "259342279"
                },
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 432,
                    "end": 459,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 899,
                    "end": 918,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 938,
                    "end": 959,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 959,
                    "end": 978,
                    "matchedPaperCorpusId": "232068763"
                },
                {
                    "start": 978,
                    "end": 997,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53759765625
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inher-Figure 1. Left: Standard unlearning methods are applied equally to all points in the forget set. Here, outlier points in the model's hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage). Right: By finding a lower-variance coreset within the forget set, UPCORE reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points. \n\nently capturing sensitive, copyrighted, or undesirable content (Shokri et al., 2017;Carlini et al., 2019). As regulations like the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) empower individuals with the \"right to be forgotten\", the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. Machine unlearning has emerged as a promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations (Jang et al., 2023). These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies (Liu et al., 2024;Hase et al., 2024). \n\nGiven the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015;Bourtoule et al., 2021;Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020;Meng et al., 2022).",
            "score": 0.6360009870428251,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1560,
                    "end": 1579,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1737,
                    "end": 1755,
                    "matchedPaperCorpusId": "270764419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.407470703125
        },
        {
            "corpus_id": "270226658",
            "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
            "text": "We propose a novel RKLD method for unlearning personal information in LLMs, effectively removing the influence of unlearning targets while preserving model utility.This method leverages a reverse KL-divergence-based knowledge distillation approach, ensuring significant forget quality without compromising the model's overall performance.Our experiments on the TOFU unlearning benchmark demonstrate that RKLD outperforms existing methods in both forget quality and model utility, especially with larger volumes of unlearning data.Additionally, RKLD retains strong general capabilities across various datasets, highlighting its robustness.An ablation study shows that reverse KL-divergence is superior compared to forward KL-divergence, aligning better with selective forgetting goals.Our case study emphasizes the need for thorough unlearning to prevent information leakage under detailed questioning.In conclusion, RKLD offers an advancement in this area.\n\nOur study proposes the use of RKLD for unlearning in LLMs.Several limitations should be considered:\n\n\u2022 Applicability to Real-World Scenarios:\n\nWhile our method shows promising results on the TOFU benchmark, the datasets used in this study are controlled and synthetic.The performance and robustness of RKLD in realworld scenarios with diverse and noisy data remain to be fully tested and validated.\n\n\u2022 Uncertain Outputs Post-Unlearning: We acknowledge that the outputs after model unlearning are uncertain.We have shown that aligning responses to \"I don't know\" is not ideal.We believe that generating hallucinations is an inherent difficulty for language models when dealing with unknowns.Addressing hallucinations should be handled by specialized research in hallucination mitigation, which is beyond the scope of this paper.\n\n\u2022 Long-term Effectiveness: Despite avoiding metrics like ROUGE and Probability, the current metrics used to evaluate forgetting quality and model utility may still fail to capture the full range of model behaviors post-unlearning.More comprehensive and varied metrics could provide deeper insights into the effectiveness and side effects of the unlearning proces\n\nWe believe that our work offers significant potential for further exploration and utilization, representing a preliminary investigation into the unlearning capabilities of LLMs.",
            "score": 0.6357509814042314,
            "section_title": "Conclusion",
            "char_start_offset": 22829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 338
                },
                {
                    "start": 338,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 901
                },
                {
                    "start": 901,
                    "end": 956
                },
                {
                    "start": 958,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1057
                },
                {
                    "start": 1059,
                    "end": 1099
                },
                {
                    "start": 1101,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1356
                },
                {
                    "start": 1358,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1533
                },
                {
                    "start": 1533,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1785
                },
                {
                    "start": 1787,
                    "end": 2017
                },
                {
                    "start": 2017,
                    "end": 2149
                },
                {
                    "start": 2151,
                    "end": 2328
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "273638595",
            "title": "Applying sparse autoencoders to unlearn knowledge in language models",
            "text": "The goal of an unlearning technique is to remove knowledge from the model, while limiting the damage to the model's performance in all other domains. Therefore, there are two key factors that need to be quantified; the amount of knowledge removed, and the side-effects caused by the modification to the model. \n\nThe primary metric for unlearning that we consider is the number of correct answers out of the subset of questions that the base model gets correct under all 24 permutations. We also looked at the probabilities assigned to the correct answers and the impact of re-ordering the 4 multiple choice options. In addition, we performed some specific case studies of the model's completion, with non-multiple choice based prompts, based on the same information as tested in the questions. \n\nWe quantified the side effect for the model in two different ways, (i) the accuracy on an unrelated multiple choice dataset (Measuring Massive Multitask Language Understanding, MMLU) and (ii) the loss added over 50k tokens of OpenWebText. The multiple choice dataset contained questions related to high school US history, geography, college computer science and human aging. These questions allowed us to both check for the removal of specific unrelated knowledge, and also to ensure that we were not simply damaging the multiple-choice answering ability of the model. Similar to the WMDP-bio dataset, we only select the questions that the base model gets correct answers for under 24 permutations, resulting in 97 questions for gemma-2b-it and 300 questions for gemma-2-2b-it. \n\nWe also compare our unlearning results to an existing technique, the Representation Misdirection for Unlearning (RMU, Li et al. 2024). RMU is a fine-tuning based approach for unlearning information from a language model. It involves fine-tuning model weights at 3 layers within the model using a combination of two loss terms. These terms are (i) the \"forget loss\", which changes the direction and scales the norm of model activations on a dataset containing information that you want to unlearn and (ii) the \"retain loss\", which preserves model activations based on a Wikitext dataset.",
            "score": 0.6357361813635194,
            "section_title": "UNLEARNING METRICS",
            "char_start_offset": 2411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47216796875
        },
        {
            "corpus_id": "277510371",
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "text": "We reveal a new insight that existing unlearning methods cannot fully fulfill the knowledge removal request by users. Furthermore, our findings suggest a potential problem: existing methods cannot eliminate almost forgetting knowledge, and we finally suggest a novel perspective for knowledge removal considering user's requests and feature-level, called Knowledge Deletion. We also propose a novel evaluation setting for this issue. Our insights provide a benefit to the related research community. Furthermore, we introduce simple yet effective methods for KD. Our methods demonstrate remarkable performance in various KD scenarios, including incremental and facial domains, even in the random data forgetting scenario. In addition to their efficiency, our methods are suitable for real-world AI deployments. \n\nESC and ESC-T utilize an additional layer after the penultimate layer, i.e., after the feature extractor. Our methods are particularly useful when the service provider releases their model as a black-box, such as Chat-GPT [1]. However, if the model is released as a white-box, we need to integrate this part into the original model architecture. If we have a simple MLP layer, it can be directly merged with the existing weights, but merging with the entire model remains an open question. We expect that this issue could be addressed through methods such as distillation, and we plan to conduct follow-up research on this challenge. Furthermore, our methods has generalizability and robustness in discriminative tasks, we need to extend this to other domains, such as diffusion model and language model. Our methods directly edit the feature space, while the latent space of generative models remains highly sensitive. Consequently, further investigation is needed to determine the applicability of our methods to generative models. We encourage future research to address these gaps.",
            "score": 0.6342583099643389,
            "section_title": "G. Broad Impacts and Limitations",
            "char_start_offset": 49047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1898
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244140625
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Machine unlearning has been used as a solution to address privacy and copyright issues in the text generation process of language models. Notable examples include gradient ascent-based methods (Jang et al., 2023;Yao et al., 2023;Barbulescu and Triantafillou, 2024), preference optimization approaches (Rafailov et al., 2024;Zhang et al., 2024;Jin et al., 2024), and representation learning techniques (Li et al., 2024a;Yao et al., 2024). \n\nHowever, the effectiveness of these methods has not been clearly demonstrated, prompting prior studies to introduce benchmarks in the field of unlearning to assess them. Eldan and Russinovich (2023); Shi et al. (2024); Tian et al. (2024) have aimed to unlearn the knowledge of copyrighted texts (e.g., BBC News and Harry Potter book) in a language model. Li et al. (2024a) have introduced a benchmark dealing with hazardous knowledge in various professional domains (e.g., biosecurity and cybersecurity). Maini et al. (2024); Jin et al. (2024) have proposed benchmarks for unlearning various entities. Specifically, Maini et al. (2024) have created synthetic entity profiles and removed their knowledge from a language model. Jin et al. (2024) have tried to unlearn the knowledge about real-world entities and evaluated the knowledge memorization in various forms of assessment (e.g., cloze test and question answering). However, existing studies remain limited as they have only examined independent knowledge and overlooked the intricate nature of world knowledge. World knowledge is highly complex and interconnected, which means that unlearning the target knowledge requires examining related knowledge carefully. Our research focuses on this aspect, examining and facilitating faithful unlearning. \n\n3 The FAITHUN Benchmark",
            "score": 0.633612823485845,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 4422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 301,
                    "end": 324,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "269156966",
            "title": "Exact and Efficient Unlearning for Large Language Model-based Recommendation",
            "text": "\u2022 Machine Unlearning.Machine unlearning, the process of removing partial training data information from trained machine learning models, is essential in various domains, including recommendation, for reasons such as privacy and security concerns [5,23].This concept is known as machine unlearning [4].In traditional machine learning, two main technique lines for unlearning have emerged: approximate unlearning and exact unlearning [24,33].Approximate unlearning aims for unlearning without retraining, using techniques like influence functions [19,38] and data augmentation [28,30] for extreme efficiency, but it often involves incomplete removal of the data.On the other hand, exact unlearning [4] typically involves retraining, ensuring complete unlearning but in a time-costly manner.Existing work, like SISA [8,9,26,34], focuses on partition strategies, building individual sub-models for partitioned training data shards to retrain only partial sub-models.Our method, while also based on the partition strategy, addresses new challenges posed by the large scale and high inference cost of Large Language Models (LLM).This makes our work distinct from existing methods.\n\n\u2022 LLM Unlearning.The challenges presented by Large Language Models (LLMs), particularly their large scale, bring forth new considerations for unlearning.Previous efforts [12,25,35] have explored unlearning for LLMs, but they often involve approximate methods.For instance, [12] simulates data labels to approximate the next-token predictions of a model that has not been trained on the unusable data, and then fine-tune LLM on these simulated labels for unlearning.[25] proposes \"In Context Unlearning\", which leverages in-context learning by flipping labels of unusable data to achieve approximate unlearning.[35] leverage the gradient ascent to erase the influence of unusable data on a trained model with finetuning.However, these methods do not achieve complete removal of unusable data and are not tailored for LLMs in the context of recommender systems.",
            "score": 0.63358425274816,
            "section_title": "RELATED WORK 5.1 Machine Unlearning",
            "char_start_offset": 30901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1174
                },
                {
                    "start": 1176,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "237562940"
                },
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "219983181"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "246016138"
                },
                {
                    "start": 816,
                    "end": 818,
                    "matchedPaperCorpusId": "232404451"
                },
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "255403814"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62158203125
        },
        {
            "corpus_id": "276079678",
            "title": "Improving LLM Unlearning Robustness via Random Perturbations",
            "text": "Modern LLMs are pre-trained on massive text corpora and then post-trained with Reinforcement Learning from Human Feedback (RLHF; [9,68,54,41]) to be helpful and harmless [2]. Recent studies have shown that despite safety enhancements, aligned LLMs can still exhibit harmful and undesirable behaviors, such as generating toxic content [62], producing copyrighted material [26,14,61], gender bias [4], leaking sensitive and private information [39,43], and potentially aiding malicious uses such as cyberattacks, chemical attacks, and bioweapons development [17,49,30]. As LLMs advance in size and capabilities at an unprecedented speed, concerns about their potential risks continue to grow. Machine Unlearning (MU; [7,5,40,63,10]) is an approach aiming to robustly (1) remove or suppress specific target knowledge in a forget-set and capabilities from a pre-trained model, while (2) retaining the model's other knowledge in a retain-set and capabilities. Recent works on the robustness of unlearning methods primarily focus on the first criterion, evaluating the robustness of unlearned models against knowledge recovery that adversarially tries to recover unlearned knowledge. For example, previously unlearned knowledge is shown to resurface through relearning [30,12,34,32], sequential unlearning [52], target relearning attacks [21], removing/steering specific directions in the latent space [34,50], quantization [67], or even simply fine-tuning on unrelated tasks [13,34]. \n\nHowever, the equally important criterion of robustly preserving the model's general knowledge-that is, ensuring stable and accurate responses to retain-queries even when they inadvertently include forget-tokens-remains underexplored.",
            "score": 0.6327396064329674,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 138,
                    "end": 141,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "265498356"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "270764347"
                },
                {
                    "start": 395,
                    "end": 398,
                    "matchedPaperCorpusId": "259088549"
                },
                {
                    "start": 442,
                    "end": 446,
                    "matchedPaperCorpusId": "278497838"
                },
                {
                    "start": 446,
                    "end": 449,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 563,
                    "end": 566,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 715,
                    "end": 718,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 723,
                    "end": 726,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 1263,
                    "end": 1267,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 1270,
                    "end": 1273,
                    "matchedPaperCorpusId": "272910981"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "266741483"
                },
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 1332,
                    "end": 1336,
                    "matchedPaperCorpusId": "270619566"
                },
                {
                    "start": 1396,
                    "end": 1400,
                    "matchedPaperCorpusId": "272910981"
                },
                {
                    "start": 1400,
                    "end": 1403,
                    "matchedPaperCorpusId": "273821809"
                },
                {
                    "start": 1418,
                    "end": 1422,
                    "matchedPaperCorpusId": "273507405"
                },
                {
                    "start": 1474,
                    "end": 1477,
                    "matchedPaperCorpusId": "272910981"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43896484375
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "Machine Unlearning The idea of removing specific data from machine learning models, known as machine unlearning, has gained attention in recent years, initially motivated by privacy concerns (Cao & Yang, 2015;Harding et al., 2019). Early methods focused on efficiently removing individual training examples or facts from models (Golatkar et al., 2020;Ma et al., 2022;Jang et al., 2022a). However, most existing benchmarks evaluate unlearning on artificially created deletion sets (Choi & Na, 2023;Goel et al., 2022;Maini et al., 2024), in contrast to our focus on real-world distributions of broad conceptual knowledge. \n\nErasing broad conceptual knowledge from LLMs New approaches to machine unlearning have recently gained traction on the problem of removing dangerous capabilities from LLMs (Lynch et al., 2024;Ilharco et al., 2023;Jang et al., 2022b;Lu et al., 2022;Yu et al., 2023;Casper et al., 2024;Eldan & Russinovich, 2023). Our work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals.",
            "score": 0.631969265564104,
            "section_title": "Related work",
            "char_start_offset": 4369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "203336801"
                },
                {
                    "start": 328,
                    "end": 351,
                    "matchedPaperCorpusId": "212628473"
                },
                {
                    "start": 351,
                    "end": 367,
                    "matchedPaperCorpusId": "236882730"
                },
                {
                    "start": 854,
                    "end": 870,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the\"right to be forgotten\". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.",
            "score": 0.631713301884381,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5146484375
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "These recent developments have intensified research interest in designing, evaluating, and improving machine unlearning algorithms, which aim to transform an existing trained model into one that behaves as though it had never been trained on certain data (Ginart et al., 2019;Liu et al., 2020;Wu et al., 2020;Bourtoule et al., 2021;Izzo et al., 2021;Gupta et al., 2021;Sekhari et al., 2021;Ye et al., 2022b;Ghazi et al., 2023).Exact unlearning in LMs requires removing the undesired data (the forget set) and retraining the model from scratch on the remaining data (the retain set), which is too costly to be practical, especially for frequent unlearning operations.As such, several efficient approximate unlearning algorithms have been proposed (Eldan & Russinovich, 2023;Zhang et al., 2024b), but existing evaluations of LM unlearning on question answering (Eldan & Russinovich, 2023;Maini et al., 2024) cannot provide a holistic view of how practical and effective a particular unlearning algorithm is.In this work, we propose a systematic, multi-faceted framework called MUSE (Machine Unlearning Six-Way Evaluation; \u00a73) to evaluate six desired properties for unlearning algorithms (Figure 1).Our criteria cover both the data owner's and the model deployer's desiderata for a practical unlearning algorithm.Data owners require the LM to unlearn the precise tokens (verbatim memorization), general knowledge encoded in the tokens (knowledge memorization), and any indication that their data was included in the training set to begin with (privacy leakage).On the other hand, model deployers want to effectively accommodate many successive unlearning requests (sustainability) on various sizes of forget sets (scalability) without degrading the general model capabilities (utility preservation).",
            "score": 0.6315811162911654,
            "section_title": "Introduction",
            "char_start_offset": 1414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 276,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 293,
                    "end": 309,
                    "matchedPaperCorpusId": "220128049"
                },
                {
                    "start": 332,
                    "end": 350,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "235367846"
                },
                {
                    "start": 369,
                    "end": 390,
                    "matchedPaperCorpusId": "232134970"
                },
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "250627348"
                },
                {
                    "start": 407,
                    "end": 426,
                    "matchedPaperCorpusId": "259274669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69287109375
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information has become increasingly essential. For instance, LLMs are expected to provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. In response to this challenge, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the context of the query. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving other knowledge. Experiments on the TOFU and AGE datasets using Llama2-7B/13B and Mistral-7B models show our method achieves up to 95% forgetting accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation into the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and maintain them up to the final layer, they make the decision to forget at the last layer, i.e., ``LLMs pretend to forget''. Our findings offer valuable insights into enhancing the robustness of unlearning mechanisms in LLMs, setting a foundation for future research in the field.",
            "score": 0.6306208097635777,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "278715226",
            "title": "MCU: Improving Machine Unlearning through Mode Connectivity",
            "text": "Machine Unlearning (MU) has emerged as a critical capability for machine learning models to comply with privacy regulations (e.g., GDPR) and user-initiated data removal requests. The most straightforward way for MU is to remove the forgetting data from the original training data and then train the model from scratch. However, this retraining method demands substantial computational overhead. To address this issue, various approximate MU methods [6,12,16,18,26,27] have emerged to provide a more efficient alternative through diverse techniques, balancing performance and efficiency. \n\nOne mainstream MU research adopts a linear method for modifying model parameters using negation task arith-metic [16,22]. In task arithmetic, the unlearning model is obtained by linearly subtracting the parameters of the task vector corresponding to the forgetting data from the original model. However, modern classifiers exhibit a high complexity of high-dimensional representation and nonlinear characteristics, where simple linear updates may fail to remove forgetting information exclusively without introducing unintended side effects. Specifically, linear task arithmetic suffers from weight entanglement, as the fine-tuned task vectors fail to localize their influence solely to the forgetting data without interfering with others, which is a violation of the necessary condition for successful linear editing [22]. This raises an important question as follows: \n\n(Q1) Can we break free from the constraints of linear updates and instead explore MU in a nonlinear manner? \n\nIf we can uncover alternative nonlinear pathways, they may offer a more flexible mechanism to remove forgetting data while preserving model utility. Another limitation of existing unlearning techniques is that they typically produce a single unlearning model. Given the diversity of model architectures and unlearning goals, an important question arises: \n\n(Q2) Can we identify a spectrum of effective MU models rather than just one? \n\nA spectrum of effective unlearning models would enable us to select multiple solutions that best align with our specific priorities, such as prioritizing model utility preservation or forgetting quality without repeated training. This capability to explore multiple unlearning solutions provides greater flexibility in practical applications without requiring costly recomputations of unlearning solutions. \n\nTo address these questions, we propose to efficiently explore unlearning from parameter space in a nonlinear manner inspired by mode connectivity [8].",
            "score": 0.627644390229454,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1925
                },
                {
                    "start": 1928,
                    "end": 2004
                },
                {
                    "start": 2007,
                    "end": 2236
                },
                {
                    "start": 2237,
                    "end": 2413
                },
                {
                    "start": 2416,
                    "end": 2566
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 455,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 458,
                    "end": 461,
                    "matchedPaperCorpusId": "265498745"
                },
                {
                    "start": 461,
                    "end": 464,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1407,
                    "end": 1411,
                    "matchedPaperCorpusId": "258832777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.595703125
        },
        {
            "corpus_id": "276408115",
            "title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning",
            "text": "In this paper, we examine unlearning's impact on retain sets and highlight the Syntactically Similar Neighbor Set as key to forgetting patterns. Our results show syntactic similarity, not domain or entity ties, drives retained knowledge degradation. Experiments confirm that syntactically similar neighbors face the highest utility drop, challenging prior assumptions. We also find that using such data for regularization improves performance retention. These findings refine unlearning strategies and emphasize the role of syntactic structure in minimizing unintended knowledge loss. \n\nOur study focuses on entity unlearning, leaving hazardous knowledge and copyrighted content unlearning unexplored. These cases may require different evaluation strategies. \n\nAdditionally, our experiments use mid-sized models (LLaMA-2-7B-Chat, LLaMA-3-8B-Instruct). Larger models, with their computational demands and structural differences, may respond differently. Future research should assess their applicability to such models.",
            "score": 0.6271302591901529,
            "section_title": "Conclusion",
            "char_start_offset": 23929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "The way to measure \u03b5\u2212unlearning as proposed in our work can be viewed as complementary to these other approaches. Barbulescu & Triantafillou (2024) 6 leverage memorization information for unlearning by proposing an unlearning method that differentiates textual sequences based on their memorization level, as in (Jang et al., 2022). The memorization in this work is captured by tracking reconstruction of the exact tokens in a sequence, which is different from the definition used in our work. An unlearning algorithms is \"successful\" if memorization of a particular sequence of interest is reduced. Their work also introduces an MIA-like evaluation inspired by the neighborhood MIA concept. \n\nMemorization. Several studies have explored different facets of memorization in LLMs, including verbatim memorization (Lehman et al., 2021;Ippolito et al., 2022), membership inference attacks (Shokri et al., 2017;Nasr et al., 2018;Salem et al., 2018;Choquette-Choo et al., 2021), exposure (Carlini et al., 2019), and extraction attacks (Carlini et al., 2021(Carlini et al., , 2022b)). These works provide valuable insights into the extent and nature of information leakage in LLMs. Hayes et al. (2024) highlighted the limitations of inexact unlearning evaluation methods like membership inference attacks. The authors show that current evaluation metrics for approximate unlearning can be misleading, creating a false sense of security. They call for more rigorous testing and a deeper understanding of how unlearning affects different data points.. Removing information in Large Language Models. Patil et al. (2023) consider information removal from the weights of a language model, which should protect against white box attacks. The authors focus on model editing techniques (Meng et al., 2022b,a), and show that even after editing the model to remove some sensitive information, they were still capable of extracting this information in a large fraction of cases. This paper also investigates how editing sensitive information affects the accuracy on neighbouring points using this information.",
            "score": 0.6270329522467757,
            "section_title": "A.7 Related Work",
            "char_start_offset": 34773,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 886,
                    "end": 907,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 907,
                    "end": 925,
                    "matchedPaperCorpusId": "54444175"
                },
                {
                    "start": 944,
                    "end": 972,
                    "matchedPaperCorpusId": "220831381"
                },
                {
                    "start": 983,
                    "end": 1005,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1030,
                    "end": 1051,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298583984375
        },
        {
            "corpus_id": "271601132",
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "text": "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters.",
            "score": 0.6266307949961525,
            "section_title": "Unlearning for LLM",
            "char_start_offset": 5182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66748046875
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "Large Language Models (LLMs) face significant challenges in maintaining privacy, ethics, and compliance, when sensitive or obsolete data must be selectively removed. Retraining these models from scratch is computationally infeasible, necessitating efficient alternatives. As part of the SemEval 2025 Task 4, this work focuses on the application of selective unlearning in LLMs to address this challenge. In this paper, we present our experiments and findings, primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility. We also detail the task-specific evaluation mechanism, results, and challenges. Our algorithms have achieved an aggregate score of 0.409 and 0.389 on the test set for 7B and 1B target models, respectively, demonstrating promising results in verifiable LLM unlearning.",
            "score": 0.6261414225267649,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten.In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 28; 57], primarily focusing on the removal of specific training samples in classification tasks.However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwoven capabilities they possess.\n\nRecently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37].From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 44], memorized sequences [25; 4], copyrighted books [59; 13], and toxic capacities [35; 5; 29; 22].Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37].Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model.Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].Although unlearning methods for large language models have rapidly developed, some studies [43; 34; 36; 50] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning.Therefore, there remains significant room for research on unlearning methods.",
            "score": 0.6258005860267889,
            "section_title": "Knowledge Unlearning for Large Language Models",
            "char_start_offset": 3928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 494
                },
                {
                    "start": 496,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "After the unlearning operation, the LLM must maintain its initial language modeling capabilities.To verify the effectiveness of our unlearning approach in preserving these abilities, we evaluate the models across well-established NLP benchmarks (Gao et al., 2023) and present the comparison results in Table 2.Although the evaluation scores mostly drop from the original, we believe all models successfully retain their LM performance (retaining at least 95%), as unlearning could have completely broken the models.Particularly, Euclidean manifests the best performance retention, possibly due to the strong regularization effect; however, such resistance to change is a trade-off with inferior unlearning performance.Additionally, it is important to note that mathematical and scientific reasoning tasks, such as MathQA and PIQA, are hardly affected by the unlearning process, indicating that unlearning primarily interferes with the ability to perform linguistic and commonsense reasoning tasks.Future research can explore methods to improve performance retention in these areas.",
            "score": 0.625593584957238,
            "section_title": "Performance in NLP Benchmarks",
            "char_start_offset": 20096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 997
                },
                {
                    "start": 997,
                    "end": 1081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29931640625
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Knowledge Unlearning Given a token sequence x = {x} T i=1 in the training dataset D = {x} N i=1 , the task of knowledge unlearning is to safely remove the influence of a subset of data D f from a trained machine learning model such that the model behaves as if the removed data had never been part of the training process, thus maintaining the model performance for the rest of the dataset. Conventionally, the data to be forgotten D f is expressed as the forget set, while the data to be retained D r is named as the retain set. For simplicity, we consider the standard case where D f and D r represent the whole training dataset and are mutually exclusive; that is, \n\nThe objective is to adjust the model parameters \u03b8 such that the updated parameters \u03b8 \u2032 = S(\u03b8; D f ) reflect the removal of D f . This unlearning (scrubbing) function S ensures the model behaves as if trained solely on D r , effectively forgetting D f while maintaining performance on D r . \n\nMultilingual Unlearning Extending to a multilingual context, the above definition must hold for the dataset D across all possible languages. While ideally, unlearning should occur across all existing languages, our experiments focus on a predefined set of languages Z = {z} Z i=1 for feasibility. Consequently, we assume a parallel dataset with forget sets \n\nWe define successful multilingual unlearning as the effective forgetting of parallel samples across all forget sets while retaining parallel samples across all retain sets.",
            "score": 0.6248991248031331,
            "section_title": "Problem Definition",
            "char_start_offset": 5317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1493
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
            "score": 0.6220186624088585,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Given a token sequence x = {x} T i=1 from the training dataset D = {x} N i=1 , knowledge unlearning aims to safely remove the influence of a specific subset of data D f from a trained machine learning model. The goal is to make the model behave as if this removed data was never used during training, while still maintaining its performance on the remaining dataset. Typically, the data to be forgotten D f is denoted as the forget set, and the data to be retained D r is referred to as the retain set. For simplicity, we consider the standard case where D f and D r are mutually exclusive subsets of the entire training dataset, meaning D f \u222a D r = D and D f \u2229 D r = \u2205. In the context of factual knowledge unlearning, each token sequence x represents a fact (e.g., \"The CEO of Tesla is Elon Musk.\"), and the objective is to train the model \u03c0 \u03b8 so that the updated model \u03c0 \u03b8 \u2032 = S(\u03c0 \u03b8 ; D f ) reflects the removal of D f . The unlearning function S ensures that the model behaves as if it had only been trained on D r , effectively forgetting D f while preserving its performance on the retained data.",
            "score": 0.6215258278687801,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 6606,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3427734375
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "In Table 2, we showcase the unlearning effectiveness and the preserved model utility following the application of various LLM unlearning methods to the TOFU fine-tuned LLM (Maini et al., 2024), with a focus on comparing FO (first-order) unlearning with the proposed SO unlearning, SOUL.As we can see, SOUL-based methods consistently outperform their FO counterparts (FO-GradDiff vs. SO-GradDiff, FO-PO vs. SO-PO, and FO-NPO vs. SO-NPO) in the efficacy measurements of LLM unlearning.This is evident from the improved forget quality, MIA, accuracy, and Rouge-L scores on the forget set.Moreover, SOUL-based methods effectively preserve the model's utility postunlearning.This is evident from their competitive utility performance compared to FO-GradDiff, FO-PO, and FO-NPO as well as the improvement over FO-GA and the input prompt-oriented unlearning method (Thaker et al., 2024).Among the unlearning methods studied, SO-PO strikes a graceful balance between unlearning effectiveness and utility preservation.However, it falls short in achieving satisfactory results in MIA.This is because it does not explicitly reduce the Min-k% probability for the correct answer (Shi et al., 2023), causing the data to still be recognized as a training example and leading to high MIA scores.Furthermore, we provide visualizations in Table 3 to illustrate examples of the model's outputs post-unlearning in the TOFU task.These visualizations highlight that SO-PO achieves the most favorable outcomes, accurately answering utilityrelated questions and appropriately declining to answer questions from the forget set.In contrast, methods based on GradDiff tend to produce nonsensical sentences on the forget set.From a user perspective, the explicit rejection by SO-PO is seen as more sensible given the preserved utility.This observation is corroborated by performance on the world facts dataset, where GradDiff fails to deliver accurate responses as effectively as PO.",
            "score": 0.6213649940516176,
            "section_title": "Results on fictitious unlearning in TOFU",
            "char_start_offset": 22851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 880
                },
                {
                    "start": 880,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1807
                },
                {
                    "start": 1807,
                    "end": 1955
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "Recently, Large Language Models (LLMs) [1]- [3] have been trained on extensive datasets that include web pages and user-generated content. During training, models acquire sensitive knowledge that raises social and legal concerns, with principles like the \"Right to be forgotten\" [4] emphasizing the need to remove unauthorized data. However, retraining an entire language model from scratch to erase sensitive information is cost-inefficient, and reconstructing the original pre-training Fig. 1. Existing unlearning methods often rely on fixed boundaries within model layers and overlook the distinct unlearning and retention scopes required for both privacy and copyright. As a result, when these methods attempt to unlearn copyright knowledge after removing privacy knowledge in the same LLM, they risk corrupting knowledge that should remain intact. dataset is exceedingly difficult. As a result, researchers have turned their attention to Machine Unlearning [5]- [12], which aims to remove specific knowledge from pre-trained models. \n\nA key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge. Laws and legal principles [13]- [16] related to privacy and copyright indicate that certain knowledge within these sensitive domains should be retained. Despite this necessity, many existing approaches do not clearly differentiate between the unlearning scope, which specifies the knowledge to remove, and the retention scope, which describes what should be preserved. In some cases, they indiscriminately remove everything loosely associated with the target. Memflex [5] introduced knowledge localization to address this issue. It distinguishes unlearning and retention scope in a given domain by leveraging gradient information in a layer-wise manner to achieve effective knowledge unlearning and retention. \n\nDespite these efforts, several challenges remain in applying unlearning methods to real-world LLMs. First, singledomain methods like Memflex are inadequate for unlearning knowledge that spans multiple domains.",
            "score": 0.6210370783570163,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "234335026"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "270878324"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "8435667"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "270878324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "To assess the effectiveness of our \"in-context knowledge unlearning\" method, we employ two primary metrics: \n\n\u2022 Forget: The proportion of instances where the model outputs \"forgot\". A higher score indicates that the model is effectively \"forgetting\" the instructed information. Unlike previous studies (Ishibashi and Shimodaira, 2024), this metric directly assesses the model's ability to acknowledge its intentional forgetting. \n\n\u2022 Retain: The proportion of questions the model answers correctly. A higher score suggests that the model is maintaining its essential knowledge. \n\nThese metrics were evaluated under two scenarios: \n\n\u2022 In-domain: The learning data was split into training and validation sets in an 8:2 ratio. \n\n\u2022 Out-of-domain: Evaluation was conducted using the world facts data from the TOFU dataset. \n\nThis combination of metrics and scenarios allows us to comprehensively evaluate how effectively our method balances selective forgetting with knowledge retention.",
            "score": 0.6209386803283541,
            "section_title": "Evaluation",
            "char_start_offset": 10753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 110,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 428
                },
                {
                    "start": 431,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 981
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "267897394",
            "title": "Machine Unlearning of Pre-trained Large Language Models",
            "text": "We provide an overview of current research on machine unlearning, memorization, and forgetting. A more detailed version is deferred to Appendix B. \n\nMachine Unlearning. The concept of machine unlearning is first introduced in Cao and Yang (2015). Bourtoule et al. (2021) further formalizes exact unlearning by introducing a general framework: sharded, isolated, sliced, aggregated (SISA). Exact unlearning requires the unlearned model the same as the retrained model. Approximate unlearning, which relaxes the requirement, is also explored by bounding the distance (Chourasia and Shah, 2023) or indistinguishability (Sekhari et al., 2021) between the two model's distributions. \n\nMachine unlearning has been extensively researched within the broader field of machine learning (Xu et al., 2024), yet its exploration in gen-erative language models remains limited. Kumar et al. (2022) propose SISA-FC and SISA-A, two computationally efficient extensions of SISA for classification LMs, e.g., BERT. To unlearn knowledge in generative models, Jang et al. (2023) simply perform gradient ascent on target sequences. Eldan and Russinovich (2023) consider a special case of unlearning the Harry Potter books from Llama2-7b. Yao et al. (2023) applies machine unlearning for harmful responses removing and hallucinations eliminating. However, these studies have been limited to fine-tuned models or a single corpus source. Our work explores unlearning pre-trained LLMs on more diverse datasets. \n\nMemorization and Forgetting. Carlini et al. (2019) first quantifies unintended memorization by a metric called exposure, revealing severe privacy issues, e.g., membership inference attacks (Carlini et al., 2022) or verbatim data extraction (Carlini et al., 2021). Contrary to memorization, catastrophic forgetting, where a model loses previously learned knowledge when training on new data, has been studied (Kemker et al., 2018;Shao and Feng, 2022).",
            "score": 0.6203570565474725,
            "section_title": "Related Work",
            "char_start_offset": 25758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 245,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 565,
                    "end": 591,
                    "matchedPaperCorpusId": "252917763"
                },
                {
                    "start": 776,
                    "end": 792,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 1039,
                    "end": 1057,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1516,
                    "end": 1537,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1676,
                    "end": 1698,
                    "matchedPaperCorpusId": "244920593"
                },
                {
                    "start": 1727,
                    "end": 1749,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "277596426",
            "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models",
            "text": "Large Language Models (LLMs) have achieved enormous success recently due to their ability to understand and solve various non-trivial tasks in natural language. However, they have been shown to memorize their training data (Carlini et al., 2019) which, among other concerns, increases risk of the model regurgitating creative or private content. Often, such issues are discovered post model training during testing or red teaming. Furthermore, stakeholders may sometimes request to remove their data after model training to protect copyright, or exercise their right to be forgotten (General Data Protection Regulation). In these instances, retraining models after discarding such data is one option but doing so after each such removal request is prohibitively expensive. \n\nMachine unlearning is a promising line of research for removing sensitive information from models' parametric memory. While unlearning has been studied for sometime in classification problems, it is still a relatively underdeveloped area of study in LLM research since the latter operate in a potentially unbounded output label space. Current algorithms often fall short of effectively and efficiently unlearning sensitive information from LLMs, without impacting model utility. Further, there is a need for benchmarks which can provide thorough evaluations of new unlearning algorithms in removing different categories of sensitive information. \n\nTo address these needs and to spur further research on this topic, we developed a new challenge (and an associated benchmark) for LLM Unlearning as part of the SemEval 2025 competition. This document provides a summary of our challenge1 along with the benchmark, results and key takeaways.",
            "score": 0.6195205559675627,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Datasets. We conduct experiments on MUSE (Shi et al., 2024b), a benchmark for evaluating machine unlearning in language models, using two datasets: NEWS and BOOKS. The NEWS dataset (Li et al., 2023b) includes recent BBC news articles divided into forget, retain, and holdout sets. The BOOKS dataset (Eldan & Russinovich, 2023) features the Harry Potter series, with original novels as the forget set and related FanWiki materials as the retain set to preserve domain knowledge post-unlearning. Details are in Appendix C.1.",
            "score": 0.6191367966902486,
            "section_title": "UNLEARNING WITH MINIMAL WEIGHT CHANGE AND UTILITY PRESERVATION",
            "char_start_offset": 17006,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224365234375
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Large language models (LLMs) [56; 41], trained on massive internet corpora, can encapsulate a vast amount of knowledge within their parameters, and possess the capability to recall and manipulate this knowledge during the generation process.However, this capability is dual-use, potentially leading to privacy problems, copyright concerns, and harmful issues [42; 27].For instance, LLMs may memorize personally identifiable information (e.g., social security numbers) or copyrighted material (e.g., Harry Potter series) from the training data, and emit it verbatim when prompted with adversarial attacks [10].Besides, AI assistants for biology could troubleshoot bottlenecks in biological weapons development, increasing the risk of such attempts.According to regulations such as the European General Data Protection Regulation (GDPR) upholding individuals' \"right to be forgotten\" (RTBF) [39], sensitive and toxic knowledge within LLMs should also be erasable.A straightforward solution is to retrain the model from scratch, ensuring it excludes any data that users have requested to be removed.However, this is not feasible for LLMs that consume extensive computational resources.\n\nTo efficiently remove specific knowledge by post hoc modifying models, machine unlearning has emerged as a solution [9; 25; 59; 13; 37].An optimal unlearning method needs to satisfy the following In this paper, we propose a Real-World Knowledge Unlearning benchmark ( RWKU).RWKU is designed based on the three key factors mentioned above: (1) For the task setting, we consider a more practical and challenging setting, similar to \"zero-shot knowledge unlearning\".We provide only the unlearning target and the original model, without offering any forget corpus or retain corpus.In this way, it avoids secondary information leakage caused by the forget corpus and is not affected by the distribution bias of the retain corpus.(2) For the knowledge source, we choose real-world famous people from Wikipedia as the unlearning targets and demonstrate that such popular knowledge is widely present in various LLMs through memorization quantification, making it more suitable for knowledge unlearning.",
            "score": 0.6189383807476512,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 747
                },
                {
                    "start": 747,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1182
                },
                {
                    "start": 1184,
                    "end": 1320
                },
                {
                    "start": 1320,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "56699980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408935546875
        },
        {
            "corpus_id": "267547751",
            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
            "text": "As we focus on unlearning of language models, we start from pretrained language models, then we do unlearning on forget dataset D f and test the models (original LMs and unlearned LMs) on test set D t . \n\nForget Dataset D f . In order to ensure a fair comparison with Jang et al. (2023) and to assess the privacy risk associated with language models, we employ the identical samples as those disclosed by Jang et al. (2023) (Smith et al. 2020), and Wizard of Internet (Komeili, Shuster, and Weston 2022)). These test sets are used to assess whether the general capabilities of language models are affected after unlearning. \n\nEvaluation Metrics. We assess the performance of the methods from two perspectives: the effectiveness of unlearning and the maintenance of general performance. To evaluate the unlearning effectiveness on the forget set, we employ our proposed metrics S-EL n and S-MA, as well as EL n and MA, introduced in Section . As for the evaluation of maintaining performance, we use accuracy for the classification test sets and F1 and Perplexity (PPL) scores for dialogue tasks.",
            "score": 0.61850868472116,
            "section_title": "Experimental Setup Datasets and Evaluation Metrics",
            "char_start_offset": 12017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 286,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 405,
                    "end": 423,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 424,
                    "end": 443,
                    "matchedPaperCorpusId": "215827653"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "270764347",
            "title": "Evaluating Copyright Takedown Methods for Language Models",
            "text": "We provide details for unlearning methods used in \u00a72.2 and \u00a74 below. We first introduce the concept of the forget set and retain set used for unlearning, then discuss the four unlearning methods evaluated in our experiment in detail. \n\nA machine unlearning algorithm seeks to remove D F , a collection of data points, from a trained language model parameterized by \u03b8. This collection of the datapoints D F is usually referred to as the forget set. In our setting, the content in the forget is the blocklisted content from a takedown request. At the same time, it is also desired that after unlearning, the model still preserves its performance on the examples that are not subject to the unlearning request, usually referred to as the retain set and denoted as D R . With the help of these notations, we now explain the four unlearning algorithms evaluated: \n\nGradient Ascent (Thudi et al., 2022) aims to maximize the training loss on the forget set, thereby achieving the goal of forgetting the content within this set. Unlike the traditional gradient descent algorithm, which minimizes the training loss on the training data, gradient ascent takes an inverse approach. This method ensures that the model forgets the content in the forget set by deliberately increasing the loss associated with it. For consistent representation, we take the negative of the loss function. Thus, for each example x i \u2208 D F , gradient ascent aims to minimize the loss function: \n\nHere n F represents the number of examples inside D F . \n\nGradient Difference (Liu et al., 2022) aims to solve the problem in gradient ascent that it cannot guarantee the model retains the knowledge in the retain set. Therefore, gradient difference adds the loss on the retain set to L GA : \n\nHere n R represents the number of examples inside D R . By minimizing L GD , the model will jointly forget the blocklisted content in the forget set, while preserving the knowledge in the retain set. \n\nKL Minimization (Golatkar et al., 2020) considers two aspects.",
            "score": 0.617479336782291,
            "section_title": "A.2 Machine Unlearning Methods",
            "char_start_offset": 34467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 876,
                    "end": 896,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1541,
                    "end": 1559,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "273661727",
            "title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
            "text": "Figure 1: Demonstration of the multimodal unlearning task. MLLM is firstly fine-tuned on constructed profiles in the proposed benchmark. After fine-tuning, MLLM can answer multimodal questions related to profiles. We then conduct various unlearning methods on a portion of profiles (forget set). Finally, the performance on tasks related to the forget set and the remaining evaluation datasets are tested simultaneously. \n\nunlearning in large language models (LLMs) using synthetic data, highlighting the need for privacypreserving unlearning methods that ensure the removal of sensitive information while maintaining model performance. However, few works have addressed unlearning in MLLMs, where the challenge lies in removing the effect of data samples across both textual and visual modalities. Even the study (Chakraborty et al., 2024) that have attempted MLLM unlearning tend to focus on textual modality, expecting that unlearning in one modality will result in knowledge removal across both. \n\n3 The MLLMU-Bench Benchmark",
            "score": 0.6173326473209311,
            "section_title": "Visual Question Answering",
            "char_start_offset": 7119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "Table 2 shows the results of our experiments across various unlearning methods and tasks. Our proposed method consistently outperforms baseline approaches for both LLaMA2 and Mistral models. For LLaMA2 (7B), we achieve 'Forget' and 'Retain' scores of 85.0% and 80.0%, respectively for in-domain data, significantly surpassing the zeroshot baseline. Out-of-domain performance remains strong with 92.3% 'Forget' and 42.7% 'Retain' scores. LLaMA2 (13B) shows even better results, particularly for in-domain scenarios, with perfect 'Forget' scores (100.0%) and high 'Retain' scores (80.0%). Mistral (7B) demonstrates comparable performance, notably achieving high 'Retain' scores (74.4%) in out-of-domain settings, indicating robust knowledge preservation during unlearning. \n\nOur method maintains competitive performance on standard NLP tasks such as BoolQ, HellaSwag, and WinoGrande, with minimal degradation compared to baseline models. This suggests that the unlearning process does not significantly impact the model's general language understanding capabilities. Compared to other unlearning methods like Few-Shot Prompting, Gradient Ascent, and In-Context Unlearning, our approach consistently achieves a better balance between forgetting targeted information and retaining general knowledge. \n\nThese results demonstrate the effectiveness of our in-context knowledge unlearning method in enabling large language models to selectively forget information while maintaining overall performance across various NLP tasks.",
            "score": 0.6161894190301312,
            "section_title": "Performance Results",
            "char_start_offset": 11766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.",
            "score": 0.6159318800316234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "273532584",
            "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
            "text": "Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023) are trained on vast corpora of data that contain private, unethical, or unwanted information, leading to growing concerns. Machine unlearning (MU) methods have been developed to remove such unwanted data without expensive retraining from scratch. For instance, MU has been applied for the LLMs to mitigate issues related to toxicity (Lu et al., 2022), copyright and privacy concerns (Jang et al., 2022;Eldan and Russinovich, 2023;Wu et al., 2023) and fairness (Yu et al., 2023). Additionally, such topics as model editing (Ilharco et al., 2022;Zhang et al., 2023), prevention of hallucinations (Yao et al., 2023), and sensitive knowledge exposure (Barrett et al., 2023) have also motivated the development of MU techniques. A: Jaime Vasquez, a true crime author, poses for a portrait.",
            "score": 0.61533342889971,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 855
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 421,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 531,
                    "end": 548,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 718,
                    "end": 740,
                    "matchedPaperCorpusId": "261276945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31591796875
        },
        {
            "corpus_id": "268856658",
            "title": "Digital forgetting in large language models: a survey of unlearning methods",
            "text": "Large language models (LLMs) have become the state of the art in natural language processing. The massive adoption of generative LLMs and the capabilities they have shown have prompted public concerns regarding their impact on the labor market, privacy, the use of copyrighted work, and how these models align with human ethics and the rule of law. As a response, new regulations are being pushed, which require developers and service providers to evaluate, monitor, and forestall or at least mitigate the risks posed by their models. One mitigation strategy is digital forgetting: given a model with undesirable knowledge or behavior, the goal is to obtain a new model where the detected issues are no longer present. Digital forgetting is usually enforced via machine unlearning techniques, which modify trained machine learning models for them to behave as models trained on a subset of the original training data. In this work, we describe the motivations and desirable properties of digital forgetting when applied to LLMs, and we survey recent works on machine unlearning. Specifically, we propose a taxonomy of unlearning methods based on the reach and depth of the modifications done on the models, we discuss and compare the effectiveness of machine unlearning methods for LLMs proposed so far, and we survey their evaluation. Finally, we describe open problems of machine unlearning applied to LLMs and we put forward recommendations for developers and practitioners.",
            "score": 0.6148552188308337,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6484375
        },
        {
            "corpus_id": "276408369",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "text": "To evaluate the forgetting performance of ReLearn, we compare it against three gradient-based baselines from prior LLM unlearning methods, focusing on their forgetting loss: (1) Gradient Ascent (GA) (Jang et al., 2023), which employs gradient ascent on the knowledge to be forgotten; (2) Negative Preference Optimization (NPO) (Zhang et al., 2024a), which leverages preference optimization only for the knowledge to be forgotten; and (3) Saliency-Based Unlearning with a Large Learning Rate (SURE) (Zhang et al., 2024b), which dynamically identifies and updates the most relevant parameters for forgetting in each training step. We exclude representation-based unlearning methods due to their difficulty in balancing forgetting and retention (Shi et al., 2024). For retention loss, we employ Gradient Descent on Retain Set (GDR) and KL Divergence Minimization on Retain Set (KLR) to improve knowledge preservation. Detailed formulas are provided in the Appendix A.2. As described in \u00a72.2, our evaluation uses KFR and KRR to measure knowledge unlearning and retention; and LS to evaluate response quality. The constants c 1 in Eq (1) and c 2 in Eq (2) are set to 0.3 for these metrics. All scores are averaged across the samples. To assess fluency (Flu.) and relevance (Rel.), we employ GPT Score (Sottana et al., 2023), generated by GPT-4o, ranging from 1 to 5. The prompt templates are shown in the appendix C.7. \n\nDetailed design principles for all metrics are provided in Appendix A.1.",
            "score": 0.6137798738987527,
            "section_title": "Baselines and Metrics",
            "char_start_offset": 12112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1488
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 218,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 327,
                    "end": 348,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1296,
                    "end": 1318,
                    "matchedPaperCorpusId": "264426756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Existing studies (Jang et al., 2023;Ji et al., 2024b;Feng et al., 2024;Liu et al., 2024c) defines LLM unlearning as the removal of specific knowledge from the forget set (i.e., unlearning completeness) while preserving the model's performance on unrelated tasks (i.e., model utility). Current methods achieving this can be broadly classified into three categories, i.e., gradient-based methods (Jang et al., 2023;Yao et al., 2024), preference optimization-based methods (Maini et al., 2024;Zhang et al., 2024), and model weight-based methods (Jia et al., 2024). Despite recent advancements, the interpretability of the unlearning process in LLMs remains underexplored. The lack of interpretability hinders the capability to comprehensively evaluate the practical effectiveness of existing LLM unlearning algorithms. For instance, the superior performance of certain unlearning algorithms might be attributed merely to the inherent ease of unlearning the selected samples, rather than to any genuine advantage of the algorithms themselves. Such a lack of fine-grained analysis could potentially impact the reliability and generalizability of LLM unlearning algorithms.",
            "score": 0.6131739716033623,
            "section_title": "Introduction",
            "char_start_offset": 1463,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1167
                }
            ],
            "ref_mentions": [
                {
                    "start": 17,
                    "end": 36,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 36,
                    "end": 53,
                    "matchedPaperCorpusId": "270440348"
                },
                {
                    "start": 53,
                    "end": 71,
                    "matchedPaperCorpusId": "273901406"
                },
                {
                    "start": 71,
                    "end": 89,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 470,
                    "end": 490,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "273502327",
            "title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?",
            "text": "However, these LLM unlearning schemes have inherent weaknesses. First, the schemes involving fine-tuning LLM introduce considerable computational overhead, like gradient ascent, so they cannot adapt to frequent unlearning requests. Second, closed-source LLMs only provide their interfaces, making some schemes unsuitable. Third, LLMs possess emergent abilities, which means that forgetting must extend beyond specific data to include related content. Shumailov et al. [15] discovered that LLMs can recall previously forgotten knowledge through their in-context abilities, exposing a vulnerability in existing unlearning schemes to this 'un-unlearning' phenomenon. Furthermore, some unlearning schemes may reduce the overall utility of the model, potentially leading to catastrophic forgetting [16]. In Table I, we list three representative LLM unlearning schemes and their weaknesses. \n\nTo tackle above weaknesses of LLM unlearning, we find that Retrieval-Augmented Generation (RAG) is a new emerged technology that does not need to change any parameter but with adjusting the output of LLMs [17]. This special characteristic can possibly be applied in the machine unlearning. In general, the RAG workflow comprises two primary components: an information retrieval module and a text generation module [18]. The retrieval module first retrieves relevant information from a knowledge base based on the input. The LLM then uses this retrieved information to generate the final response. This process can effectively improve the quality of outputs, reduce the likelihood of misinformation and hallucinations, and, most importantly, hide the target information to implement the unlearning without revising the parameters. \n\nAs illustrated in Figure 1 (a), RAG can revise the misinfor-arXiv:2410.15267v1 [cs.CR] 20 Oct 2024",
            "score": 0.6129727160810696,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "272915110"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1301,
                    "end": 1305,
                    "matchedPaperCorpusId": "267626957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.403076171875
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "3.1 Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set \n\nAfter unlearning, the model's performance should be indistinguishable from a model trained exclusively on the retained dataset D r = D\\D f . Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set. The method builds upon the standard training paradigm of the P \u03b8 , which minimizes the prediction loss over the full dataset D. To enforce forgetting, gradient ascent maximizes the prediction loss on the target forget subset D f , effectively approximating the reversal of the original optimization process. This procedure can be equivalently interpreted as performing gradient descent on the negative prediction loss (Zhang et al., 2024). The gradient ascent objective, denoted as L GA , is formulated as: \n\n(1)",
            "score": 0.6121216511587964,
            "section_title": "Preliminaries",
            "char_start_offset": 7810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70751953125
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "Machine unlearning aims to remove the impact of specific datapoints from training datasets (Jang et al., 2022;Eldan and Russinovich, 2023;Yao et al., 2023;Qu et al., 2024;Blanco-Justicia et al., 2024;Liu et al., 2024;Maini et al., 2024), focusing on the removal of memorized token sequences (Jang et al., 2022;Barbulescu and Triantafillou, 2024), copyrighted material (Yao et al., 2023;Eldan and Russinovich, 2023), and toxic content (Belrose et al., 2024;Li et al., 2024). Many tech-niques employ model fine-tuning on \"forget\" sets via gradient ascent on a loss function or preference optimization methods (Maini et al., 2024;Zhang et al., 2024). Task arithmetic, an efficient alternative, merges models to yield versions unlearned on particular datapoints (Ilharco et al., 2022). \n\nEvaluation Benchmarks Eldan and Russinovich (2023) propose the task of forgetting Harry Potter where the goal is to make it difficult for the unlearned model to generate content about him. Maini et al. (2024) introduce a task of fictitious unlearning with 200 fictitious authors and GPTgenerated question-answer pairs to assess how well unlearning algorithms forget specific authors while retaining knowledge about untargeted authors. Li et al. (2024) focus on unlearning harmful knowledge and test algorithms with multiple-choice questions. Meanwhile, Jin et al. (2024) emphasize realworld knowledge unlearning, broadly evaluating methods across various downstream tasks, including adversarial prompts. These benchmarks focus on forgetting unlearning data while preserving utility on other (possibly related) concepts; we emphasize model recovery through unlearning.",
            "score": 0.611856286976674,
            "section_title": "Related Work",
            "char_start_offset": 5473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 456,
                    "matchedPaperCorpusId": "259088549"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51708984375
        },
        {
            "corpus_id": "270226658",
            "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
            "text": "We aim to construct an unlearning teacher to guide the unlearning process of the original model.This unlearning teacher model is designed to retain the part of the original distribution that should be kept while removing tokens related to the data that needs to be forgotten in the forget set.\n\nDetermining which tokens need to be forgotten in the current distribution is a challenging task.As shown in Figure 2, inspired by previous work, we employ continued training methods (Eldan and Russinovich, 2023) to identify which tokens are relevant to the current forget target.By continuing to train the model on the forget set, we obtain strengthened model logits, l str .We identify tokens with consistently increased logits values, marking them as potentially influenced by the given forget target.The mathematical formula we use is as follows:\n\nwhere l str is the strengthened model logits, and s f orget denotes the forget set.The logits of tokens in l str that need to be forgotten consistently increase compared to l ori , providing a clearer guide for deliberate forgetting while protecting other information.The formula for the unlearning teacher model is as follows:\n\nwhere l ori , l tea , and l str represent the logits of the three models and \u03b1 is a hyperparameter to control the forgetting strength, respectively.We only reduce the logits of the relevant tokens.For logits that decrease or remain unchanged, we maintain the state of the original model.It's important to note that directly using the unlearning teacher as an unlearned model isn't ideal.The unlearning teacher's performance in evaluating forget quality and model utility has been very low, especially when handling paraphrased unlearning strings.We believe this is because the original model has already learned all the content in the forget set, so the generalization of the continued training process is very poor, resulting in poor forget quality.Additionally, the model utility is hindered by the logits subtraction during inference.Besides poor performance, the unlearning teacher lacks unlearning functionality at the parameter level, and the use of two models limits its effectiveness as an unlearned model.",
            "score": 0.6117434858503457,
            "section_title": "Constrcuting Unlearning Teacher",
            "char_start_offset": 6357,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 844
                },
                {
                    "start": 846,
                    "end": 929
                },
                {
                    "start": 929,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1173
                },
                {
                    "start": 1175,
                    "end": 1323
                },
                {
                    "start": 1323,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1462
                },
                {
                    "start": 1462,
                    "end": 1562
                },
                {
                    "start": 1562,
                    "end": 1721
                },
                {
                    "start": 1721,
                    "end": 1925
                },
                {
                    "start": 1925,
                    "end": 2012
                },
                {
                    "start": 2012,
                    "end": 2189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347900390625
        },
        {
            "corpus_id": "266174259",
            "title": "FairSISA: Ensemble Post-Processing to Improve Fairness of Unlearning in LLMs",
            "text": "Modern machine learning models, especially large language models (LLMs), employ significantly large model architectures and train on massive datasets. At this scale, it is infeasible to properly curate the training data, and sensitive or undesirable information (e.g., personally identifiable information, copyrighted material or toxic text) may be ingested by the model during training Carlini et al. (2021); Lehman et al. (2021); Carlini et al. (2023a;b). Moreover, when models are trained on data collected from individual users, e.g., medical data, some users may request their data to be deleted following the right to be forgotten provided by recent privacy legislation Voigt & Von dem Bussche (2017); Pardau (2018); Act (2000). Motivated by the above scenarios, machine unlearning has emerged as a subfield of machine learning. The goal of machine unlearning is to remove the influence of a specific subset of training examples from a trained model, while maintaining the performance of the model. A straightforward machine unlearning method is to retrain the model on an updated training set that excludes the samples to be removed. However, retraining deep models, especially LLMs, from scratch is infeasible due to the exorbitant computational costs. Several unlearning techniques have recently been proposed to tackle this challenge by efficiently removing the influence of the data to be unlearned (see Appendix A for an overview). \n\nEven though machine unlearning has recently received significant research attention, implications of unlearning on other crucial aspects such as fairness have been scantly explored. Fairness is especially critical for language models, since these models are embedded in a variety of applications including call centers and other question-answer applications where the output may jeopardize people's chances to obtain services or may lead to unfair treatment. Several works have demonstrated the bias of language models, e.g., Bolukbasi et al. (2016); Borkan et al. (2019); Hutchinson et al. (2020);de Vassimon Manela et al. (2021); Baldini et al. (2022).",
            "score": 0.6109596511331008,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 410,
                    "end": 430,
                    "matchedPaperCorpusId": "233289659"
                },
                {
                    "start": 432,
                    "end": 454,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 723,
                    "end": 733,
                    "matchedPaperCorpusId": "155270414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35888671875
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "In response to the data regulation requirements [8], machine unlearning (MU) has emerged as a critical process to remove the influence of specific data points, data classes, or even higher-level data concepts from a trained machinelearning model. One direct unlearning method involves retraining the model from scratch after removing the forgotten data from the original dataset, which is often considered the gold standard [10,87]. However, this approach comes with significant computational demands. To alleviate this, most research focuses on developing approximate but much faster unlearning techniques, including gradient ascent [88,89], influence unlearning [90,91,92], Fisher forgetting [93,94], finetuning-based approaches [10,11], and loss correction-related unlearning [95,96,12].",
            "score": 0.610929744850689,
            "section_title": "E.2 Machine Unlearning",
            "char_start_offset": 41012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 790
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 634,
                    "end": 638,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 671,
                    "end": 674,
                    "matchedPaperCorpusId": "247218331"
                },
                {
                    "start": 698,
                    "end": 701,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "276408115",
            "title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning",
            "text": "Large language models (LLMs) risk retaining unauthorized or sensitive information from their training data, which raises privacy concerns. LLM unlearning seeks to mitigate these risks by selectively removing specified data while maintaining overall model performance. However, most existing work focus on methods to achieve effective forgetting and does not provide a detailed analysis of the retain set, the portion of training data that is not targeted for removal. In this paper, we investigate the effects of unlearning on various subsets of the retain set through a case study on entity unlearning. We introduce the Syntactically Similar Neighbor Set, a group of queries that share similar syntactic structures with the data targeted for removal, and show that this subset suffers the greatest performance drop during unlearning. Moreover, when used for regularization, this set not only preserves performance on syntactically similar queries but also delivers comparable or improved results across other data subsets. Our results highlight that syntactic similarity is a critical factor, potentially more so than domain or entity relationships, in achieving effective and practical LLM unlearning.",
            "score": 0.6106508568400565,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.619140625
        },
        {
            "corpus_id": "270878324",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "text": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "score": 0.6104893837909606,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Figure 1: Motivation for multi-hop knowledge unlearning. After Elon Musk (i.e., \"the user\") requests his personal information to be removed from the LLM, existing unlearning methods often succeed in deleting direct, single-hop facts but fail on indirect, multi-hop facts that entail one or a few of the unlearned facts. \n\nas the European Union's General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019) or the California Consumer Privacy Act (CCPA) (Pardau, 2018) in the United States. These regulations mandate the removal of personal or protected information from databases, extending to data embedded within machine learning models. In such cases, model owners must develop mechanisms to safely eliminate specific data while preserving the model's overall functionality. \n\nTo address these concerns, there has been increasing focus on the field of machine unlearning, which involves removing the influence of specific data points from machine learning models (Cao and Yang, 2015). Although the need for this task is critical, erasing the effects of certain data on models with billions of parameters is extremely difficult. The ideal approach is exact unlearning, where models are entirely retrained from scratch after excluding the data points that need to be forgotten. However, this process is computationally intensive and impractical, particularly for LLMs. As a result, research has shifted towards developing faster approximate unlearning techniques. While machine unlearning has been primarily explored in computer vision (Golatkar et al., 2020a,b;Bourtoule et al., 2021;Kurmanji et al., 2023;Fan et al., 2024), its prominence is now expanding in NLP due to privacy concerns related to LLMs (Nasr et al., 2023;Carlini et al., 2024). \n\nRecently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences.",
            "score": 0.610300229881228,
            "section_title": "Unlearn Request",
            "char_start_offset": 619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 412,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 972,
                    "end": 992,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1569,
                    "end": 1592,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1592,
                    "end": 1614,
                    "matchedPaperCorpusId": "270619676"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 1731,
                    "end": 1752,
                    "matchedPaperCorpusId": "268357903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "273323555",
            "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
            "text": "The main problem with this approach is that it produces a metric that is only meaningful when compared to the learning time for a model that was never trained on the knowledge in the first place, which is hard to obtain given the cost of LLM pretraining. This method also suffers against methods that specifically target making fine-tuning slower or more difficult (Henderson et al., 2023;Rosati et al., 2024;Tamirisa et al., 2024). \n\nWeaknesses of current unlearning techniques Previous work has shown evidence about current unlearning techniques being weak against attacks that would fail if the information was removed (Lynch et al., 2024a;\u0141ucki et al., 2024;Hong et al., 2024). Our results further confirm the findings of this previous work and provides a standard method that can be applied to any unlearned model (a model to which unlearning was applied). Further, negative results using our method provide strong evidence of unsuccessful knowledge removal relative to previous prompting-based methods. \n\n3 PROBLEM STATEMENT",
            "score": 0.6102976108895247,
            "section_title": "RELATED WORK",
            "char_start_offset": 6407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21142578125
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones. By applying this methodology to two open-source LLMs -LLaMA (Touvron et al., 2023) and OLMo (Groeneveld et al., 2024) -we construct the CONCEPTVECTORS benchmark for unlearning methods, which consists of both behavioural and intrinsic evaluations that cover 285 diverse common concepts. \n\nWe use CONCEPTVECTORS to evaluate various unlearning methods, including gradient-based unlearning, preference-based optimization, and parameter-specific interventions ( \u00a74). Our results show that while existing unlearning methods prevent models from generating concept information, they only affect negligible changes to its parametric knowledge traces (Figure 1b). At the same time, directly intervening in a certain concept vector effectively erases the information it encodes about the concept, thereby having a pronounced effect on the model's generation (Figure 1d). Lastly, we showcase the importance of erasing parametric knowledge to improve robustness against adversarial attacks ( \u00a75). We apply multiple adversarial attacks (Lynch et al., 2024;Wei et al., 2023b;Deng et al., 2024) to jailbreak the model after unlearning, measuring their impact on the concept vectors' activations and the generation of knowledge that was presumably unlearned. Our experiments show that (Figure 1b-d) (a) jailbreak bypass unlearning by increasing the activations of concept vectors, (b) existing unlearning methods suppress the parametric knowledge rather than erase it, and (c) better removal of parametric knowledge can enhance unlearning robustness and reduce jailbreak success. \n\nTo conclude, we argue that unlearning methods should be evaluated not only on external performance but also on their ability to erase parametric knowledge. We propose a methodology for creating such evaluations and introduce CONCEPTVECTORS, the first benchmark for intrinsic evaluation of unlearning.",
            "score": 0.6100265484554767,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 560
                },
                {
                    "start": 563,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 1335,
                    "end": 1353,
                    "matchedPaperCorpusId": "263831094"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.677734375
        },
        {
            "corpus_id": "270869978",
            "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
            "text": "Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. In this paper we revisit the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. We introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, we argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. We discuss feasibility of ununlearning for modern LLMs and examine broader implications.",
            "score": 0.6096418256695211,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "We hypothesize that the limited unlearning effect is due to the insufficient coverage of knowledge within the current probing set. To explore this, we introduce a new metric, Knowledge Coverage, designed to assess the knowledge overlap between the probing sets and the target set. This metric is computed using the BERTScore (Zhang et al., 2019) of the closest QA pair match between the probing set and the target set. A higher knowledge coverage indicates that the probing set encompasses a broader range of entity-related knowledge. See Appendix C.1 for detailed definitions and formulas. \n\nTo further investigate the effects of varying knowledge coverage, we construct probing sets with varying degrees of coverage by systematically replacing different ratios of QA pairs within the probing set with those from the target set while keeping the total set size fixed at 20. As demonstrated in Table 7, the knowledge coverage of the constructed probing set increases progressively as the ratio of replaced pairs grows. Then, we applied the five algorithms to perform entity-level unlearning on the probing set under five different replacement ratios. As illustrated in Figure 2, enhancing knowledge coverage of the probing set consistently enhances the forget quality of most methods. Furthermore, each algorithm consistently preserved similar model utility across various probing sets. These results suggest that enhancing the knowledge coverage of the probing set can improve the unlearning effectiveness. \n\nAn intuitive approach to increase knowledge coverage is by increasing the size of the probing set. Therefore, we further explore the performance of the algorithms on probing sets across different sizes. Specifically, we expand the probing set size for each entity and evaluate the performance of the unlearning algorithms across five different sizes. As illustrated in Figure 3, the forget quality of the five unlearning algorithms gradually improves as the size of the probing set increases. However, except for the Pref. Opt. method, the model utility of the unlearned models produced by the other algorithms demonstrates a noticeable decline. This decline in model utility, when compared to the trends in Figure 2, is likely due to the larger number of unlearning steps required as the probing set size grows. These findings suggest that, for most unlearning algorithms, expanding the probing set size leads to a trade-off between forget quality and model utility. Furthermore, according to section 3.2, although the Pref. Opt.",
            "score": 0.6093100743802404,
            "section_title": "Effect of Probing Set",
            "char_start_offset": 16574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2322
                },
                {
                    "start": 2323,
                    "end": 2477
                },
                {
                    "start": 2478,
                    "end": 2535
                },
                {
                    "start": 2536,
                    "end": 2540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "The widespread integration of Large Language Models (LLMs) into daily applications has raised significant concerns regarding the trustworthiness of such models. Their outputs may contain sensitive, private, or illegal content [1,2], reflect societal biases [3,4], or provide harmful instructions [5,6,7]. In particular, for privacy concerns, regulations [8] have been introduced, requiring applications to support the deletion of information contained in training samples upon user request. This has motivated research into machine unlearning (MU) [9,10,11,12,13], a critical process aimed at removing the influence of specific data points, data classes, or even higher-level data concepts from trained models. \n\nLLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data Table 1: Comparison of different loss adjustment-based baselines in terms of their requirement. Our method relies solely on forget data and available template responses, without using the retain data or a reference model for response calibration.",
            "score": 0.6087803454088863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1645
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 260,
                    "end": 262,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 301,
                    "end": 303,
                    "matchedPaperCorpusId": "261276945"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7041015625
        },
        {
            "corpus_id": "268513492",
            "title": "Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models",
            "text": "We present two versions of unlearning algorithms for LLMs, which are derived from Newton update, namely Fisher Removal and Fisher Forgetting.An empirical study on four widely used NLP datasets is conducted to compare our methods with other baselines including finetuning, retraining, and gradient ascent.We further explore their mitigation effects for unintended memorization on two real-world datasets.Moreover, we uncover the relationship between unlearning and DP-SGD [1] through the study of the privacy-utility trade-off.The contributions of our work are summarized as follows:\n\n(1) We introduce two novel unlearning strategies for LLMs and show that second-order information plays an important role in achieving robust unlearning outcomes.(2) We extensively evaluate the capacity of each unlearning approach on four widely used NLP datasets as well as two real-world datasets.The codebase will be released to enable reproducibility.(3) We discover that DP-SGD does not guarantee an equally optimal/suboptimal trade-off across different datasets, which implies it cannot replace the role of unlearning.(4) We discuss the limitations such as the cost of approximating Hessian, and also point out directions for future work.",
            "score": 0.6085908141831153,
            "section_title": "Contributions",
            "char_start_offset": 6097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 582
                },
                {
                    "start": 584,
                    "end": 745
                },
                {
                    "start": 745,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 938
                },
                {
                    "start": 938,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1227
                }
            ],
            "ref_mentions": [
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "207241585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "270045257",
            "title": "Machine Unlearning in Large Language Models",
            "text": "We propose a novel evaluation method to measure the effectiveness of unlearned models.Specifically, we train a text classifier on the dataset from which we seek to forget information.Subsequently, we apply our unlearning process to the language model, and we evaluate its performance by testing the output responses using the trained text classifier.This method serves as a quantitative measure for assessing the success of our unlearning approach.",
            "score": 0.6082900146813671,
            "section_title": "Novel Evaluation Method",
            "char_start_offset": 11690,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 448
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "273233165",
            "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
            "text": "Figure 1: (a) Systematic overview of an LLM (\u03b8) post-unlearning using the proposed SimNPO optimization principle, compared to the popular NPO (negative preference optimization) framework (Zhang et al., 2024a) and the reference model (i.e., model prior to unlearning). (b) & (c) Experiment highlights on the TOFU dataset with a 5% forget size (Maini et al., 2024) and on the MUSE News dataset (Shi et al., 2024). Unlearning effectiveness is measured by forget quality for TOFU and PrivLeak for MUSE, while utility preservation is evaluated using model utility for TOFU and KnowMem on Dr for MUSE (see Table 1 for details on task-specific metrics). In both tasks, Retrain serves as the gold standard for unlearning by fully removing the influence of the forget data. \n\nThe rapid advancement of large language models (LLMs) has raised security and safety concerns, including issues related to copyright violations and sociotechnical harms (Huang et al., 2024;Wang et al., 2023;Li et al., 2024;Shi et al., 2024). However, retraining these models to remove undesirable data influences is often impractical due to the substantial costs and time required for such processes. This gives rise to the problem of LLM unlearning, which aims to effectively remove undesired data influences and/or model behaviors while preserving the utility for essential, unrelated knowledge generation, and maintaining efficiency without the need for retraining (Eldan & Russinovich, 2023;Yao et al., 2023;Liu et al., 2024b;Blanco-Justicia et al., 2024).",
            "score": 0.608130097195191,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 956,
                    "matchedPaperCorpusId": "272330189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "270703035",
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "text": "Instead, it necessitates structural data deletion, which facilitates the comprehensive removal of 'relational' data, irrespective of its inter-connectivity with other entities or its domain (illustrated in Figure 1). Such graph-type inter-connected relationships among data points present a challenge in LLMs unlearning as forgetting one data point might impact the retaining/forgetting of others. Therefore, it is essential to assess the true effectiveness of existing LLMs unlearning algorithms in the presence of structural data points, and to develop realistic datasets that facilitate such research. \n\nIn this work, we are particularly interested in exploring two key research questions associated with structural LLM unlearning: \n\n(1) How does data inter-connectivity impact the , unlearning inter-connected data points within a structured dataset) versus independent unlearning (i.e., unlearning isolated data points). As shown, when an entity revokes consent for its data to be used or exercises its 'right to be forgotten' (i.e., unlearning data points related to this entity), the degree of inter-connectivity between the unlearning entity and other entities will influence unlearning performance. \n\nunlearning performance? Some entities naturally appear more frequently in joint information with others. As such, an entity that revokes consent for its data to be used or exercises its 'right to be forgotten' may have varying levels of inter-connectivity with other entities in the dataset. An effective unlearning algorithm must robustly handle unlearning requests from entities which have varying levels of inter-connectivity while minimizing the need for manual intervention, such as extensive hyperparameter tuning. \n\n(2) How does unlearning data from a specific domain affect the retained model's performance on data in the same versus different domain? Another important aspect of structural unlearning is that unlearning requests may specifically target certain data domain rather than encompassing a mix of everything, as reflected by their proportional representation in the training set. For the first time, we investigate how such targeted unlearning affects outcomes, particularly examining whether it leads to uneven performance degradation on the retained data of both the same and different domains. \n\nIn summary:",
            "score": 0.6071027297103104,
            "section_title": "Introduction",
            "char_start_offset": 1635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2325
                },
                {
                    "start": 2328,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.263671875
        },
        {
            "corpus_id": "270703035",
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "text": "Large language models (LLMs) have shown impressive capabilities in natural language generation. However, their output is not always appropriate due to issues such as generating biased (Kotek et al., 2023;Motoki et al., 2023) or toxic content (Wen et al., 2023;Bender et al., 2021), regurgitating personally identifiable information (PII) (Nasr et al., 2023;Barrett et al., 2023), and hallucination (Huang et al., 2023;Xu et al., 2024). \n\nOne straightforward way to mitigate these undesired behaviors is to retrain the model on a new dataset which deletes 'bad' data points that cause the unwanted behaviors. However, naively retraining is known to be highly inefficient (Hu et al., 2021;Marchisio et al., 2023;Zhang et al., 2023b) due to significant computation cost and data requirements (Chen et al., 2023). As an alternative, machine unlearning (MU) (Bourtoule et al., 2021;Nguyen et al., 2022), originally proposed for classification models, has been extended to remove the influence of undesirable data and model capabilities for LLMs (Zhang et al., 2023a;Liu et al., 2024). \n\nDespite being a promising direction, LLM unlearning remains nascent. Particularly, existing unlearning methods are often evaluated using datasets, such as TOFU (Maini et al., 2024), which primarily composed of independent entities. However, we observe that real data points (such as Wikipedia data) are rarely independent, they are often interconnected, creating knowledge graphs with intricate topologies (Schneider et al., 2022). As such, real-world unlearning usage extends beyond simple deletion of independent data points from LLMs. Instead, it necessitates structural data deletion, which facilitates the comprehensive removal of 'relational' data, irrespective of its inter-connectivity with other entities or its domain (illustrated in Figure 1).",
            "score": 0.606968636112198,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 357,
                    "end": 378,
                    "matchedPaperCorpusId": "261276945"
                },
                {
                    "start": 687,
                    "end": 710,
                    "matchedPaperCorpusId": "254877513"
                },
                {
                    "start": 710,
                    "end": 730,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 853,
                    "end": 877,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372314453125
        },
        {
            "corpus_id": "272881106",
            "title": "Erase then Rectify: A Training-Free Parameter Editing Approach for Cost-Effective Graph Unlearning",
            "text": "Several training-free methods have been proposed for efficient machine unlearning. For example, Fisher Forget-ting (Golatkar, Achille, and Soatto 2020a) and NTK (Golatkar, Achille, and Soatto 2020b) are weight scrubbing methods that add noise to parameters informative for unlearned samples. However, Fisher Forgetting is computationally expensive and sacrifices the model's predictive performance (Tarun et al. 2023), while NTK relies on additional models to achieve unlearning. The most relevant work to ours is SSD (Foster, Schoepf, and Brintrup 2024), which achieves unlearning by dampening parameters crucial for the unlearned dataset but not for the remaining dataset. Unlike SSD, we account for the impact of unlearned samples on other samples through message propagation. Additionally, we propose adaptively selecting hyperparameters, enhancing the method's versatility across different datasets. Furthermore, we introduce the Rectify strategy to improve the model performance on the remaining dataset.",
            "score": 0.605632632654019,
            "section_title": "Training-Free Machine Unlearning",
            "char_start_offset": 4977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1010
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 152,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 398,
                    "end": 417,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 518,
                    "end": 554,
                    "matchedPaperCorpusId": "260900355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69970703125
        },
        {
            "corpus_id": "264451561",
            "title": "Open Knowledge Base Canonicalization with Multi-task Unlearning",
            "text": "Machine unlearning is a different approach to traditional machine learning, where the main objective is to allow the model to actively forget or delete specific samples, thus enabling flexible management of learned information. Unlike retraining the entire model, the core idea of machine unlearning is to achieve forgetting by updating the sum of queries without losing overall performance, thus saving time and computational resources. A related work [20] has designed effective machine unlearning by using k-means clustering technique, which improves the deletion efficiency and enables the model to forget unwanted information more accurately and precisely. A study [21] introduces the concept of random forests and proposes an inert forgetting strategy that performs the operations involved in different suspension requests in a single batch to avoid redundant computations and achieve computation sharing. In the field of knowledge graph research, a completely new paradigm [22] has recently been proposed that employs a unlearning approach from cognitive neuroscience. This framework combines retrospective inhibition and automatic attenuation, aiming at removing specific knowledge from local customers and propagating this eliminated information into the global model by means of knowledge distillation. Despite this, few studies have explored unlearning work related to OKB, suggesting that the field of machine forgetting is still open to further research and innovation to meet evolving application needs.",
            "score": 0.6055386094404172,
            "section_title": "B. Machine unlearning",
            "char_start_offset": 8005,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1517
                }
            ],
            "ref_mentions": [
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "261894049"
                },
                {
                    "start": 980,
                    "end": 984,
                    "matchedPaperCorpusId": "256615275"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.472412109375
        },
        {
            "corpus_id": "260900355",
            "title": "Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening",
            "text": "We present a novel two-step, retraining-free unlearning method. SSD first selects parameters that are considerably more important for the forget set than the retain set, before dampening these parameters proportional to the discrepancy in their importance to the forget and retain set. The result of these steps is a fast yet highly effective method for machine unlearning. We evaluate SSD on a range of tasks, demonstrating viability in single-class, sub-class and random sample settings, on multiple datasets and different model architectures. Results show that SSD is orders of magnitude faster than the comparable Fisher Forgetting method, outperforming the method considerably; SSD even rivals the speed and performance of state-of-the-art retrain-based approaches. \n\nMany future directions could be explored, such as evaluating how to improve and measure performance on random subsets, given the significant overlap in parameter importance for the forget and test set. Another interesting direction is how to forget large subsets of information. Typically experiments evaluate forgetting no more than 5-10% of data; this may be realistic but evaluating how to increase the upper bound of forgetting without retraining may offer valuable insight into how to improve existing unlearning methods.",
            "score": 0.6044950180094656,
            "section_title": "Conclusion",
            "char_start_offset": 25088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72412109375
        },
        {
            "corpus_id": "270619566",
            "title": "Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning",
            "text": "In this work, we propose and study benign relearning attacks as effective methods to recover unlearned knowledge. Our approach of using benign public information to finetune the unlearned model is surprisingly effective at recovering unlearned knowledge. Our findings across multiple datasets and unlearning tasks show that many optimization-based unlearning heuristics fail to truly remove memorized information in the forget set. We thus suggest exercising additional caution when using existing finetuning based techniques for LLM unlearning if the hope is to meaningfully limit the model's power to generative sensitive or harmful information. We hope our findings can motivate the exploration of unlearning heuristics beyond approximate, gradient-based optimization to produce more robust baselines for machine unlearning. In addition, we also recommend investigating evaluation metrics beyond model utility on forget / retain sets for unlearning. Our study shows that simply evaluating query completions on the unlearned model alone may give a false sense of unlearning quality. \n\nLimitations & Future work. In this work we primarily focus on attacking unlearning heuristics within the model parameter space in order to \"jog\" the memory of the unlearned model. Thus, the same prompt is used for both the unlearned and relearned model. As a result, a simple defense against our benign relearning attack could be methods such as prompt filtering (Liu et al., 2024a) and guardrails (Thaker et al., 2024). Under those defenses relearning itself might not be sufficient and we may also need to reply on input corruption approaches such as jailbreaking attacks (Zou et al., 2023). \n\nIt is not clear how our attack would perform along with jailbreaking with the presence of prompt filtering guardrails, which would be an interesting direction of future work. More generally, we note that field of machine unlearning is rapidly expanding and there are potentially other unlearning methods and benchmarks that have not been covered in this work; to get a more comprehensive perspective we encourage future research to explore the potential effectiveness of relearning attacks on additional methods and datasets.",
            "score": 0.6035973362474132,
            "section_title": "CONCLUSION",
            "char_start_offset": 34167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29248046875
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "The concept of unlearning in Large Language Models has garnered considerable attention as concerns over data privacy and model integrity have intensified. In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by Lu et al. (2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by Jang et al. (2023) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by Yu et al. (2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications.",
            "score": 0.6033232334225715,
            "section_title": "F.2 LARGE LANGUAGE MODELS UNLEARNING",
            "char_start_offset": 43282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 419,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 754,
                    "end": 772,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1175,
                    "end": 1191,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "267897394",
            "title": "Machine Unlearning of Pre-trained Large Language Models",
            "text": "In this section, we provide greater detail about related work on machine unlearning, memorization and forgetting, the relation between machine unlearning, differential privacy, and alignment, exact unlearning, and second-order methods of machine unlearning. \n\nMachine Unlearning. Cao and Yang (2015) introduce the notion of machine unlearning. They give a heuristic method, transforming learning algorithms into a summation form for forgetting data lineage. Their goal is to ensure that unlearned models exactly match the ones retrained from scratch. Subsequently, it is formalized as exact unlearning or data deletion (Ginart et al., 2019;Bourtoule et al., 2021) of a specific training sample, requiring the distributions of unlearned and retrained models are identical. Ginart et al. (2019) propose two tailored approaches for k-means, while Bourtoule et al. (2021) propose a general unlearning framework: sharded, isolated, sliced, aggregated (SISA). \n\nExact unlearning may be too \"strong\" to achieve; it can be \"relaxed\" to approximate unlearning (Ginart et al., 2019) by bounding the \"distance\" (e.g., R\u00e9nyi divergence (Chourasia and Shah, 2023) or indistinguishability (Sekhari et al., 2021)) between the two models' distributions. More generally, one can unlearn a subset of training points (Sekhari et al., 2021) that can even be adaptively chosen (Gupta et al., 2021). \n\nSince the seminal proposal, machine unlearning has been widely studied in ML in general (Xu et al., 2024;Gong et al., 2024) but remains rarely explored in NLP, notably generative LLMs. Zhang et al. (2023a) discusses the challenges and implications of unlearning and other approaches to realize RTBF in LLMs. Kumar et al. (2022) propose SISA-FC and SISA-A, two extensions of SISA for classification LMs, e.g., BERT.",
            "score": 0.6024480079691199,
            "section_title": "B Related Work (Full Version)",
            "char_start_offset": 31640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1794
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 299,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 619,
                    "end": 640,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1051,
                    "end": 1071,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1124,
                    "end": 1150,
                    "matchedPaperCorpusId": "252917763"
                },
                {
                    "start": 1356,
                    "end": 1376,
                    "matchedPaperCorpusId": "235367846"
                },
                {
                    "start": 1468,
                    "end": 1485,
                    "matchedPaperCorpusId": "254805754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460205078125
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "In-context Unlearning. Our method leverages in-context learning (ICL) for knowledge unlearning. ICL enables LLMs to adapt to new tasks flexibly by incorporating data into the context of input sequence, rather than fine-tuning, which explicitly updates weights (Brown et al., 2020;Dong et al., 2023;Liu et al., 2023). Exploring the full capabili-ties of ICL remains an active area of research, with recent studies empirically investigating its potential by examining in-context example design (Garg et al., 2022;Liu et al., 2022;Min et al., 2022;Liu et al., 2023). Pawelczyk et al. (2023) explored methods for performing in-context unlearning. This study focuses on text classification tasks where the labels of specific instances are flipped to facilitate in-context unlearning. However, this approach has limitations as it primarily assesses unlearning in terms of text classification ability rather than actual knowledge. Furthermore, the method trains the model to generate incorrect outputs, which is not meant true forgetting. \n\nIn contrast, our study introduces unique characteristics that address these issues. We specifically investigate knowledge unlearning within an in-context learning framework. Moreover, by defining unlearning as the ability to \"forget\" we ensure that our approach avoids merely generating errors or irrelevant information, thereby achieving a more effective and appropriate form of unlearning.",
            "score": 0.6011494561683715,
            "section_title": "Related Work",
            "char_start_offset": 3316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1425
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 492,
                    "end": 511,
                    "matchedPaperCorpusId": "251253368"
                },
                {
                    "start": 511,
                    "end": 528,
                    "matchedPaperCorpusId": "231632658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "274822624",
            "title": "Toward Efficient Data-Free Unlearning",
            "text": "Machine unlearning (Bourtoule et al. 2021;Nguyen et al. 2022;Xu et al. 2024) is an emerging paradigm that enables machine learning models to selectively forget training data, primarily to enhance user privacy and comply with regulations (Garg, Goldwasser, and Vasudevan 2020;BUKATY 2019), or to correct errors within the dataset (Cao and Yang 2015;Marchant, Rubinstein, and Alfeld 2022). Existing unlearning methods typically require access to the original training data to accurately discern which information needs to be removed (the \"forgetting\" data) and which must be preserved (the \"retaining\" data). This access is crucial for precisely identifying the parameters impacted by the forgetting data, ensuring that only these parameters are adjusted (Fan et al. 2023;Foster, Schoepf, and Brintrup 2024) while the functionality of the model is maintained for the retaining data (Thudi et al. 2022;Kurmanji et al. 2023). However, in practice, the original training data may not always be available due to reasons such as compliance with privacy laws that mandate data deletion, or organizational policies aimed at optimizing storage efficiency. Moreover, alternative data sources that share a similar distribution with the training data might also be inaccessible due to legal restrictions. Despite these challenges in data access, requests for unlearning tasks such as forgetting specific concepts or classes of data can still be initiated; this scenario, where unlearning is required without access to the original or similar training data, is termed data-free unlearning. \n\nAn existing method addressing data-free unlearning is Generative Knowledge Transfer (GKT) (Chundawat et al. 2023), which utilizes Data-free Knowledge Distillation (Micaelli and Storkey 2019) to selectively transfer knowledge from a trained model to an unlearned model.",
            "score": 0.6011109393801863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1846
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 75,
                    "matchedPaperCorpusId": "260887697"
                },
                {
                    "start": 237,
                    "end": 275,
                    "matchedPaperCorpusId": "211296633"
                },
                {
                    "start": 880,
                    "end": 899,
                    "matchedPaperCorpusId": "238198525"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3681640625
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "Unlearning benchmarks and evaluation metrics. (Lynch et al., 2024) propose eight distinct evaluation metrics that go beyond standard loss measures on the forget/retain set, and try to capture internal model changes, as well as the impact on downstream tasks. The authors measure robustness to jailbreaks and finetuning, other extraction techniques, undesirable side effects, etc. Shi et al.  (2024) propose a new machine unlearning evaluation benchmark, MUSE, focusing on assessing 6 desired properties of unlearned models, such as verbatim memorization, scalability with forget sets size, etc. The TOFU benchmark paper (Maini et al., 2024) introduces a new task and dataset for evaluating specific training data unlearning in large language models. Jang et al. (2022) introduce an \"extraction likelihood\" metric for measuring unlearning quality in LLMs: they look at the average completion accuracy of a sequence of tokens when a varying length prefix was provided as a prompt. The authors also studied gradient ascent-based unlearning, and found that to be more effective when unlearning sequentially in batches rather than all at once. They also report differences in how easy it is to unlearn depending on the source of the forget set. While Jang et al. ( 2022) also points to differences in the effectiveness of unlearning between different forget datasets, they do not further explore how similar examples are affected by gradient ascent (as our work does). TOFU focuses on a \"Task of Fictitious Unlearning\" where models are trained on fictional author profiles and then must unlearn a subset of those profiles. The paper provides a dataset of these profiles, metrics to assess unlearning efficacy, and baseline results from existing unlearning algorithms. All of the work above aims to identify and assess desirable properties of unlearned models for general or specific tasks, but do not directly work with \u03b5\u2212unlearning definition as in Definition 2.1. The way to measure \u03b5\u2212unlearning as proposed in our work can be viewed as complementary to these other approaches.",
            "score": 0.6009444435183852,
            "section_title": "A.7 Related Work",
            "char_start_offset": 32812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6279296875
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "On the other hand, model deployers want to effectively accommodate many successive unlearning requests (sustainability) on various sizes of forget sets (scalability) without degrading the general model capabilities (utility preservation).\n\nWe apply MUSE to evaluate eight representative machine unlearning algorithms ( \u00a74) on two datasets ( \u00a73.2), focusing on the specific cases of unlearning Harry Potter books and news articles.Our findings indicate that most unlearning algorithms remove verbatim memorization and knowledge memorization with varying degrees of efficacy but operate at the cost of utility preservation and do not effectively prevent privacy leakage ( \u00a75.2).In particular, negative preference optimization (NPO; Zhang et al., 2024b) and task vectors (Ilharco et al., 2023) are especially effective in removing these types of memorization, but we find that NPO often permits privacy leakage and both methods induce a sharp drop in the utility of the model.Furthermore, testing their scalability and sustainability reveals that they both algorithms struggle with large forget sets and successive unlearning requests ( \u00a75.3).\n\nOur results highlight that unlearning algorithms generally fail to meet data owner expectations in preventing privacy leakage, which is one of the primary motivations for unlearning.Additionally, Table 1: Comparison with a previous benchmark: Unlike the previous benchmark TOFU (Maini et al., 2024), which evaluates unlearning on synthetic Q&A datasets, MUSE tackles real-world unlearning challenges: unlearning real-world large-scale corpus (22\u00d7 larger) while taking into account six desiderata that are important to both data owners and deployers.More related works are discussed in Appendix 6.",
            "score": 0.6007231235015691,
            "section_title": "Introduction",
            "char_start_offset": 2972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 240,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47900390625
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "This paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
            "score": 0.6007157735681082,
            "section_title": "Conclusion",
            "char_start_offset": 27411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "In addition to introducing the novel concept of anti-sample unlearning, we demonstrate that previous unlearning techniques can inadvertently disrupt the LLM's broader knowledge. To address this challenge, we propose fine-grained targeted unlearning, which allows for the selective removal of specific associations. In the aforementioned example, other related facts-such as that Harry Potter is a wizard and Hogwarts is a boarding school of magic for young wizards-should not be forgotten. This capability sets our approach apart from previous methods (Eldan & Russinovich (2023); Liu et al. (2024a)). \n\nOur contributions are: \u2776 Anti-sample induced unlearning: We introduce the novel concept of using anti-samples, rather than typical data samples, to drive the unlearning process. \u2777 Misleading rationales as justifications: We employ misleading rationales as justifications to guide the model in forgetting, leveraging reasoning that flips answers rather than reinforcing them. \u2778 Fine-grained targeted unlearning: Our approach enables the selective removal of specific associations, such as unlearning that Harry Potter studied at Hogwarts while retaining other relevant facts about both Harry Potter and Hogwarts. This capability distinguishes our method from previous approaches. Our results demonstrate that anti-samples present a promising and efficient strategy for targeted unlearning in LLMs. In contrast, approximate unlearning (Chundawat et al. (2023a)), which focuses on reversed loss functions, reduces the influence of target data points through parameter-level updates, significantly lowering computational costs. Although approximate unlearning doesn't completely eliminate the influence of the data, it is far more practical for large-scale models where full retraining would be too costly.",
            "score": 0.6005138183740353,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1806
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.395263671875
        },
        {
            "corpus_id": "268681648",
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "text": "L ARGE Language Models (LLMs) have transformed the landscape of AI, offering remarkable abilities in enhancing and generating human language text.However, their strength, derived from vast datasets, can become a liability due to privacy concerns, accuracy limitations, copyright infringement issues, and the potential propagation of societal biases [1].A notable instance is the lawsuit filed by the New York Times against OpenAI and Microsoft for using its copyrighted content in training their GPT models, igniting a controversial debate on the application of fair use rules to LLM training and spotlighting an urgent need for data erasure mechanisms in LLMs. 1  In this light, a concept of machine \"unlearning\" has been recently proposed to realize data erasure [2].Machine unlearning is applied to LLMs due to a few challenges, including the need for fast retraining or fine-tuning, removing the impact of outdated, copyrighted, and false data, etc.A recent paper discussed the \"Right to be forgotten\" in LLMs [3], where the authors provided some implications of laws like European General Data Protection Regulation (GDPR) 2 , and classified the methods into exact and approx- imate machine unlearning in a general setting.However, it lacks a dedicated taxonomy, challenges analysis, evaluation, as well as insights derived from the latest research.\n\nIn this paper, we categorize machine unlearning for LLMs into two streams.The first stream primarily focuses on unlearning unstructured data like certain knowledge within language models.This encompasses removing or modifying specific information, narratives, or sentences the model has been trained on [4].The objective is to make the LLMs \"forget\" certain learned content, which might be sensitive, copyrighted, or incorrect, without compromising their general linguistic capabilities [5].Such an approach addresses legal and ethical concerns, particularly in scenarios where LLMs inadvertently reproduce copyrighted material or retain sensitive personal data [6].\n\nThe second research stream targets unlearning structured data to enhance the classification abilities of LLMs.In this context, unlearning is adopted to refine the model's decision-making processes, reducing biases and improving its interpretative accuracy [7].",
            "score": 0.6004059517639371,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1354
                },
                {
                    "start": 1356,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 2022
                },
                {
                    "start": 2024,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 2018,
                    "end": 2021,
                    "matchedPaperCorpusId": "264426289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.363525390625
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "Problem setup.LLM unlearning aims to mitigate the influence of undesired data, such as sensitive or copyrighted information, and/or restrict the model's capabilities to avoid the associated content generation.This process also requires preserving the LLM's utility for unrelated tasks and avoiding full retraining to maintain computational efficiency.\n\nFollowing the generic formulation of LLM unlearning in (Liu et al., 2024a), the unlearning problem can be conceptualized as removing the influence of a designated 'unlearning target'-whether it pertains to data, knowledge, or model capabilitiesfrom a pre-trained LLM (denoted as \u03b8 o ).The unlearning target is typically specified by a forget set D f , which includes the information or knowledge intended for removal.To preserve the LLM's generation capability (i.e., utility) after unlearning, a retain set D r is also introduced.This set comprises data that is irrelevant to the unlearning target.Given the aforementioned setup, the problem of LLM unlearning is often formulated as a regularized optimization problem, fine-tuned from \u03b8 o over the forget set D f and the retain set D r :\n\nHere \u2113 f and \u2113 r represent the forget loss and the retrain loss respectively, and \u03bb \u2265 0 is a regularization parameter to strike a balance between unlearning and utility preservation.Note that problem ( 1) is not the only formulation of LLM unlearning.Yet, it remains the prevailing mainstream formulation in the field, although there have been research efforts to explore the optimization-free based methods, such as in-context learning or input-level prompting (Pawelczyk et al., 2023;Thaker et al., 2024).\n\nSome specifics of LLM unlearning (1).While problem (1) may appear as a straightforward optimization task initially, complexities arise in determining the effective forget loss \u2113 f and achieving the optimal balance between unlearning and utility.These questions remain challenging in the literature.We present three representative LLM unlearning approaches and illustrate how they relate to the specifics of problem ( 1).",
            "score": 0.6003832293465114,
            "section_title": "Primer on LLM Unlearning",
            "char_start_offset": 9071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 14,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 351
                },
                {
                    "start": 353,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1141
                },
                {
                    "start": 1143,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1394
                },
                {
                    "start": 1394,
                    "end": 1650
                },
                {
                    "start": 1652,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1897
                },
                {
                    "start": 1897,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "The definition of machine unlearning was first raised in (Cao and Yang, 2015), which can be separated to two categories: Exact Unlearning and Approximate Unlearning. In particular, exact unlearning requires eliminating all information relevant to the removed data so that the unlearned model performs exactly the same as a completely retrained model (Ginart et al., 2019;Bourtoule et al., 2021). On the other hand, approximate unlearning only requires the parameters of the unlearned model to be similar to a retrained model from scratch (Guo et al., 2020;Sekhari et al., 2021;Liu et al., 2023;Chien et al., 2022;Pan et al., 2023;Guo et al., 2020). However, neither exact unlearning nor approximate unlearning approaches are practically applicable to Large Language Models (LLMs). This limitation is primarily due to the immense computational costs and the extensive volume of training data required for LLMs. Though scarce, few works have explored the LLM unlearning. (Yao et al., 2023) first defined the setup and goal of unlearning on LLMs, which is to output whitespace on harmful prompts. Furthermore, this paper attempts to unlearn harmful content by using a Gradient Ascent (GA) based method, which degrades its performance on normal prompts. (Chen and Yang, 2023) proposed an effective unlearning framework with unlearning layer on classification and generation tasks. (Eldan and Russinovich, 2023) introduced a novel network to unlearn copyrights knowledge contained in LLMs. Until very recently, (Maini et al., 2024) presented a new benchmark that aimed to better evaluate the performance of various methods on a new task of fictitious unlearning.",
            "score": 0.6003650746988185,
            "section_title": "Large Language Model Unlearning",
            "char_start_offset": 4726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1657
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 77,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 350,
                    "end": 371,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 538,
                    "end": 556,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 630,
                    "end": 647,
                    "matchedPaperCorpusId": "207847600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "2). Our ICUL method does not require knowledge of the LLM's parameters, and yet manages to achieve performance levels that are competitive with or in some cases exceed the state-of-the-art LLM unlearning methods which require access to LLM parameters and involve expensive gradient computations (Jang et al., 2023). \n\nWe experiment with multiple established real world datasets: AG-News, SST-2, SQUAD, and Amazon reviews to evaluate the effectiveness of our proposed unlearning method. Our results on text classification and question-answering tasks clearly demonstrate the efficacy of the proposed unlearning method, and highlight that it practically eliminates a training point's influence on the model output. These results indicate the significant potential for unlearning training points from black-box models. Our proposed methods and findings offer a new perspective on unlearning mechanisms in LLMs: \n\n\u2022 New unlearning paradigm for LLMs: This is the first work to use in-context learning for machine unlearning by specifically constructing contexts that induce model behavior that is indistinguishable from the behavior of a re-trained model. \u2022 New empirical unlearning evaluation: In Subsection 3.2 we introduce LiRA-Forget, a new unlearning evaluation that adapts the LiRA MIA (Carlini et al., 2022) to the problem of evaluating unlearning. We note that a similar evaluation metric was introduced concurrently by Kurmanji et al. (2023), and subsequently studied by Hayes et al. (2024) under the name U-LiRA. \n\nWe discuss these works more in Section 2. \u2022 Data deletion from blackbox models: ICUL does not require access to model parameters and can be readily applied to blackbox models. This makes it a useful tool to patch a model until the model can be updated or a retrained version can be deployed at the next deployment phase. Thus, it is complementary to existing white-box unlearning techniques which have higher computational burdens. \u2022 Lower memory requirements: Our method boasts lower memory requirements compared to state-of-theart unlearning methods like Gradient Ascent (GA), especially as the size of the LLM increases.",
            "score": 0.5994257112161907,
            "section_title": "Introduction",
            "char_start_offset": 4024,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 3
                },
                {
                    "start": 4,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 907
                },
                {
                    "start": 910,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 314,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1287,
                    "end": 1309,
                    "matchedPaperCorpusId": "244920593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "This works focuses exclusively on unlearning methods for safety that remove hazardous knowledge (e.g. bioweapons) from large language models, as introduced by Li et al. (2024). In practice, unlearning relies on forget and retain sets. The first contains information relevant to the domain to be unlearned (e.g. enhanced pandemic pathogens) while the second includes neighboring information that should be preserved (e.g. general biology). In this work, we use the datasets included in WMDP benchmark for biology and cybersecurity (Li et al., 2024). Our evaluation is designed to assess whether existing unlearning methods effectively remove hazardous knowledge or merely make it more difficult to access, similarly to safety training.",
            "score": 0.5991998584213531,
            "section_title": "Experimental Setup",
            "char_start_offset": 6482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 734
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474609375
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "In recent years, self-improvement approaches like STaR (Zelikman et al. (2022) and RFT Yuan et al. (2023)) have shown that large language models (LLMs) can improve themselves through reasoning. Now, imagine using these reasoning processes not to enhance learning, but to guide the model in selectively forgetting specific information, ensuring privacy and control. This concept forms the core of UNSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs. \n\nWhy unlearn? The ability of LLMs to absorb vast amounts of human-authored content-often viewed as their greatest strength-has also presented concerns over data privacy (Huang et al. (2022)), copyright violations (Carlini et al. (2023); Shi et al. (2023)), and the potential misuse of AI in harmful domains such as bio-weapons and cyber-attacks (Barrett et al. (2023); Sandbrink (2023); Li et al. (2024)). In this context, AI safety necessitates the ability to erase specific information without compromising overall model performance. Thus, how can LLMs effectively unlearn specific knowledge after being trained on extensive text corpora? (Nguyen et al. (2022); Voigt & Von dem Bussche (2017); Zhang et al. (2024a)) Legal compliance (Gursoy et al. (2022)), particularly with privacy laws and copyright regulations, necessitates mechanisms for selective unlearning . Furthermore, ethical considerations drive the need to eliminate biased or harmful data from mod-Preprint els, ensuring fair and responsible use. Finally, the removal of obsolete or irrelevant information is essential to maintain models' accuracy and alignment with evolving requirements. \n\nWays to unlearn? Machine learning models improve accuracy through training by leveraging three key components: data samples, learning methods, and loss functions. Analogously, unlearning can also be potentially achieved by counteracting one or more of these core elements: anti-data-samples (or anti-samples), unlearning methods, and reversed loss functions.",
            "score": 0.5991847276951222,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 78,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236572265625
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "Large language models (LLMs) have shown to pose social and ethical risks such as generating toxic language or facilitating malicious use of hazardous knowledge. Machine unlearning is a promising approach to improve LLM safety by directly removing harmful behaviors and knowledge. In this paper, we propose\"SPlit, UNlearn, MerGE\"(SPUNGE), a framework that can be used with any unlearning method to amplify its effectiveness. SPUNGE leverages data attributes during unlearning by splitting unlearning data into subsets based on specific attribute values, unlearning each subset separately, and merging the unlearned models. We empirically demonstrate that SPUNGE significantly improves the performance of two recent unlearning methods on state-of-the-art LLMs while maintaining their general capabilities on standard academic benchmarks.",
            "score": 0.599041988343898,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In recent years, large language models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to complex problem-solving. However, this advancement has highlighted significant concerns regarding the potential for LLMs to retain unauthorized content from massive training corpus crawled from the Internet, raising issues related to privacy and copyright (Huang et al., 2022;Carlini et al., 2023;Staab et al., 2024;Ippolito et al., 2023;Dou et al., 2024). These concerns are particularly relevant within legal and regulatory frameworks, such as the Right to be Forgotten (Dang, 2021), which aims to empower individuals to have unauthorized data erased from digital records. Addressing these issues is crucial for ensuring the responsible deployment of LLMs in real-world applications. \n\nDue to the high cost of retraining LLMs, researchers have explored machine unlearning techniques, namely LLM unlearning (Cao & Yang, 2015;Bourtoule et al., 2021;Yao et al., 2023). The typical paradigm involves fine-tuning the target LLM on a specified set, known as the forget set, to obtain an unlearned model. As described in (Maini et al., 2024;Jin et al., 2024), the unlearned model should meet two primary goals: 1) it should not reveal any information contained in the forget set, and 2) it should maintain performance on the neighbor set, which has a distribution similar to the forget set but is not the target of unlearning, as well as on other tasks with general knowledge. While the first goal is generally easier to achieve, the main challenge lies in meeting the second goal (Liu et al., 2024b;Maini et al., 2024;Zhang et al., 2024a;Ji et al., 2024;Shi et al., 2024a;Wang et al., 2024c). \n\nIn this paper, we have a closer look at machine unlearning for LLMs.",
            "score": 0.5983993755865822,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 488,
                    "end": 507,
                    "matchedPaperCorpusId": "263834989"
                },
                {
                    "start": 507,
                    "end": 529,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 663,
                    "end": 675,
                    "matchedPaperCorpusId": "234335026"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "5945696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39453125
        },
        {
            "corpus_id": "265498745",
            "title": "Machine Unlearning in Learned Databases: An Experimental Analysis",
            "text": "In ML, data deletion materializes as the problem of \"Machine Unlearning\". This is the problem of removing information related to a part of a dataset from a trained model, without hurting the information about the rest of the data [9]. This has also been referred to as \"forgetting\", \"scrubbing\", \"removing\", and \"deletion\". We will use these terms interchangeably. \n\nMachine unlearning in ML research is motivated by the need to deal with out-of-date, noisy, poisoned, or outlier data. Another important reason for machine unlearning is to protect users' privacy and guarantee the \"right to be forgotten\". Machine unlearning due to DB deletes is qualitatively and quantitatively different from the setup studied in the ML literature, as (i) deletes are very commonplace, sometimes even more frequent than queries themselves, and (ii) typically, \"downstream tasks\" are different (e.g., AQP, SE, DG, DC, etc.). Nonetheless, the aims are the same: to update a trained model in such a way that the \"effect of deleted data is removed from the trained model\", while, at the same time, the model does not lose any knowledge it has learned about non-deleted data. Consider an AQP engine like DBEst++ [28] or a cardinality estimator like Naru/NeuroCard [48,49]. These models essentially learn the data's underlying probability distributions and perform query inference based on these. When a cohort of data is removed, the models should be updated to reflect the correct densities (and/or correlations) for accurate inference. That is, we need to ensure that the updated model makes correct predictions when querying either the deleted rows and/or the remaining rows. \n\nAlbeit an important problem, removing information from a trained neural network (without damaging the accuracy of the remaining data) is unfortunately a very challenging task. Ideally, one could remove the to-be-forgotten data from the DB and retrain a new model from scratch. However, as also has been shown by [24], training neural networks is prohibitively expensive.",
            "score": 0.5973752993275783,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 233,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "232328138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18115234375
        },
        {
            "corpus_id": "267311861",
            "title": "Blockchain-enabled Trustworthy Federated Unlearning",
            "text": "Machine unlearning is able to eliminate the data effects from ML models without requiring retraining from scratch. Specifically, when receiving a data removal request, the ML model will execute a pre-defined unlearning mechanism to erase the associated data effects involved in the model. To improve the effectiveness of machine unlearning, [Chundawat et al., 2023] proposes a student-teacher framework, which includes a competent teacher and an incompetent teacher to selectively transfer knowledge and deliberately exclude information related to the target data. [Pan et al., 2023] designs a federated K-means clustering algorithm for efficient machine unlearning, and develops a sparse compressed multiset aggregation mechanism to reduce communication overhead. [Bourtoule et al., 2021] partitions all data samples into several distinct shards, trains separate models on each shard, and uses slicing methods to minimize the computational overhead. On the other hand, to test the unlearning effectiveness of machine unlearning, [Chen et al., 2021] utilizes the membership inference attack to predict whether the target data belongs to the training data. While these approaches have greatly improved the efficiency and security of machine unlearning, they cannot be directly used in FL since FL requires periodical exchanging of model updates rather than raw data. Therefore, new solutions are needed to adapt machine unlearning in FL.",
            "score": 0.5971726626442013,
            "section_title": "Machine Unleaning",
            "char_start_offset": 4641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1436
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 365,
                    "matchedPaperCorpusId": "248834527"
                },
                {
                    "start": 565,
                    "end": 583,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 765,
                    "end": 789,
                    "matchedPaperCorpusId": "248834527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373779296875
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "Unlearning has emerged as a critical technique in AI systems, enabling the selective removal of sensitive data, including copyrighted material and personal information, from trained models. As the International AI Safety Report (Bengio et al., 2025) emphasizes, unlearning plays a vital role in mitigating privacy and copyright risks associated with extensive training datasets. However, it also acknowledges that current unlearning methods remain in-adequate, which often fail to completely erase targeted data while potentially degrading model performance, thus limiting practical implementation. \n\nSpecifically, existing unlearning methods often struggle with over-forgetting (excessive elimination of non-sensitive information) or underforgetting (incomplete removal of sensitive data). It is challenging to find optimal hyperparameters that balance performance across multiple evaluation dimensions, sometimes even impossible. To address these limitations, we propose a novel unlearning system that leverages model merging to combine an over-forgetting model with an under-forgetting model, creating a more effective unlearned model. It can produce superior results simply by merging two models with complementary biases. \n\nOur system achieved second place in SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models, with our 7B model attaining a Task Aggregate Score of 0.944 and Aggregate Score of 0.487, demonstrating the effectiveness of our system in selectively removing sensitive content. Furthermore, our local experiments yielded almost perfect results with a MIA Score of 0.501 and Aggregate Score of 0.806, while maintaining an exceptionally high Task Aggregate and comparable MMLU Avg.. We provide comprehensive analyses that validate our system's effectiveness and offer deeper insights into the unlearning process.",
            "score": 0.5964149642689156,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Recent research has introduced several benchmarks and tasks for LLM unlearning from various aspects (Ji et al., 2024;Li et al., 2024;Maini et al., 2024;Lynch et al., 2024;Jang et al., 2022). Among these, the Weapons of Mass Destruction Proxy (WMDP) benchmark (Li et al., 2024) specifically targets dangerous knowledge in biosecurity, cybersecurity, and chemical security. While these task settings primarily address the forgetting of explicit instances, they pay less attention to entity-level unlearning, which involves completely forgetting an entity. Notably, Eldan and Russinovich (2024) explored a particular task to unlearn the entity \"Harry Potter\", but it is proven that the knowledge is not entirely erased from the unlearned model (Shi et al., 2023). Maini et al. (2024) presented TOFU consisting of 200 fictitious author profiles to assess unlearning methods from Model Utility and Forget Quality. However, this benchmark only focused on ideal scenarios with exact forget sets. Similarly, the RWKU benchmark (Jin et al., 2024) chooses 200 real-world famous people as entities for unlearning, with a more practical task setting. Nonetheless, it still focuses on specific pieces of knowledge related to target entities rather than the entire entity, making it closer to the instance-level unlearning task. \n\nTo sum up, there is a notable absence of systematic analysis and evaluation for entity-level unlearning.",
            "score": 0.5958360124803312,
            "section_title": "Evaluations of LLM Unlearning",
            "char_start_offset": 26200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1421
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 117,
                    "matchedPaperCorpusId": "259501579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3955078125
        },
        {
            "corpus_id": "229678805",
            "title": "Federated Unlearning",
            "text": "The term \"machine unlearning\" is introduced by Cao et al. [14], where an efficient forgetting algorithm in the restricted context of statistical query learning is proposed. Thereafter, machine unlearning for different ML models have been explored. Ginart et al. [15] examine the problem of data removal algorithm for stochastic algorithms, in particular for variants of k-means clustering, but cannot be applied to supervised learning. Izzo et al. [23] focus on supervised linear regression and develop the projective residual update technique that scales linearly in the dimension of the data. Baumhauer et al [24] propose a forgetting procedure for logit-based classification models by applying linear transformation to the output logits, but do not remove information from the weights. Most recently, Bourtoule et al. [16] introduce a more general algorithm named SISA, which takes advantage of sharding and slicing during the training. Nevertheless, existing machine unlearning studies focus on ML models in traditional centralized settings, and is ineligible for unlearning in FL scenarios.",
            "score": 0.5955895554694719,
            "section_title": "A. Machine Unlearning",
            "char_start_offset": 30393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185546875
        },
        {
            "corpus_id": "248834527",
            "title": "Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher",
            "text": "Machine learning (ML) models are being widely deployed for various applications across different organizations. These models are often trained with large-scale user data. Modern data regulatory frameworks such as European Union GDPR (Voigt and Von dem Bussche 2017), and California Consumer Privacy Act (CCPA) (Goldman 2020) provide for citizens the right to be forgotten. It mandates deletion-upon-request of user data. The regulations also require that user consent must be obtained prior to data collection. This consent for the use of an individual's data in these ML models may be withdrawn at any point of time. Thus, a request for data deletion can be made to the ML model owner. The owner company (of the ML model) is legally obligated to remove the models/algorithms derived from using that particular data. As the ML models usually memorize the training samples (Feldman 2020; Carlini et al. 2019), the company either needs to retrain the model from scratch by excluding the requested data or somehow erase the user's information completely from the ML model parameters. The algorithms supporting such information removal are known as machine unlearning methods. Machine unlearning also offers a framework to prove data removal from the updated ML model. \n\nThe unlearning methods can be practically applied in the following ways: (i) forgetting single-class or multiple classes of data (Tarun et al. 2021), (ii) forgetting a cohort of data from a single class (Golatkar, Achille, and Soatto 2020a,b), (iii) forgetting a random subset of data from multiple classes (Golatkar et al. 2021). In this paper, we investigate the utility of teacher-student framework with knowledge distillation to develop a robust unlearning method that can support all the three modes, i.e. single/multiple classlevel, sub-class level and random subset-level unlearning. Another important question we raise is how well the unlearned model has generalized the forgetting? Recent studies suggest that the unlearning methods may lead to privacy leakage in the models (Chen et al. 2021).",
            "score": 0.5952813257731854,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1264
                },
                {
                    "start": 1267,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2070
                }
            ],
            "ref_mentions": [
                {
                    "start": 887,
                    "end": 907,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1396,
                    "end": 1414,
                    "matchedPaperCorpusId": "232134970"
                },
                {
                    "start": 1574,
                    "end": 1596,
                    "matchedPaperCorpusId": "229678489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1937255859375
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In this paper, we discuss several issues in LLM unlearning and provide our insights on possible approaches. To address the issue of inadequate evaluation on the unlearned model's output, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize previous methods into untargeted and targeted based on whether the response to the forget set is specified. For untargeted unlearning, we discuss that the behavior it attempts to approximate is unpredictable and may involve hallucinations, and adopt the objective maximizing entropy. For targeted unlearning, we analyze that existing regularization is insufficient and incorporating the answer preservation loss as a regularization term. Extensive experiments across various scenarios demonstrate the effectiveness of our approaches.",
            "score": 0.5952573182164096,
            "section_title": "CONCLUSION",
            "char_start_offset": 35439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 851
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "273532566",
            "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "text": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.",
            "score": 0.5949163254852816,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.724609375
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or\"forgetting\"a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0.5947690163440673,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "278481378",
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
            "text": "Thus, machine unlearning seeks to provide a more efficient alternative. While exact unlearning methods have been proposed (Yan et al., 2022;Ding et al., 2024), which fully retrain on D r on an algorithmic-level (Xu et al., 2024) to recover the exact behavior of the retrained model, these approaches require access to the complete retain set and are generally computationally expensive. In contrast, approximate methods aim to closely approximate the retrained model's behavior through techniques such as finetuning (Yao et al., 2024c;Zhang et al., 2024;Neel et al., 2021), prompting (Liu et al., 2024a;Pawelczyk et al., 2024), or model editing (Veldanda et al., 2024;Hase et al., 2023), offering a more scalable and efficient alternative. \n\nParameter-tuning. Among approximate methods, one prominent direction is parameter-tuning approaches, which directly modify model parameters to achieve unlearning. We pursue this direction because these methods typically meet all unlearning requirements, preserving inference latency without demanding excessive training compute. Parameter-tuning methods frame unlearning as an optimization problem with two competing objectives: a forget objective L f that forces the model to unlearn specific knowledge and a retain objective L r that ensures performance on the remaining data is preserved. A generalized unlearning loss function typically follows this form: \n\nwhere D f is the forget set, D r is the retain set, \u03b8 o are the starting model weights and \u03bb is a hyperparameter. Typically, approaches use \u03b8 o in both objectives. The variation across methods lies primarily in how the forget loss L f is designed. The retain objective L r serves as a regularizer to mitigate catastrophic forgetting. Typically, either crossentropy (Yuan et al., 2024) or KL-divergence distillation from the starting model is used as retain loss with the latter usually performing better (Zhang et al., 2024;Maini et al., 2024).",
            "score": 0.5939821178363579,
            "section_title": "Introduction",
            "char_start_offset": 6074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "260887697"
                },
                {
                    "start": 516,
                    "end": 535,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 554,
                    "end": 572,
                    "matchedPaperCorpusId": "220364296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Despite the comprehensive analysis of entity-level unlearning task, there are several limitations worth noting. First, our work primarily focuses on analyzing the entity-level unlearning task and identifying potential insights for improvement, rather than proposing a specific unlearning algorithm, which will be a focus of our future research. Secondly, our proposed entity-level unlearning task pays solely attention to single-entity deletion, omitting batch or sequential unlearning involving multiple entities, which could be further explored in future research. Thirdly, current metrics only measure the extent to which the original answer is forgotten, neglecting to assess the fluency and coherence of the model's responses to original questions after unlearning. Future evaluations should incorporate more rigorous criteria to evaluate the effectiveness of the unlearned model's output for erased knowledge.",
            "score": 0.5938684103142063,
            "section_title": "Limitations",
            "char_start_offset": 28488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 915
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "268681648",
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "text": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
            "score": 0.5938661986435216,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "260887697",
            "title": "Machine Unlearning: Solutions and Challenges",
            "text": "By selectively removing outdated or unrepresentative data, machine unlearning enables the model to maintain performance even as the environment evolves [16]. \n\nBased on the residual influence of the removed data point, machine unlearning solutions can be categorized into Exact Unlearning and Approximate Unlearning [17]. Exact unlearning aims to completely remove the influence of targeted data points from the model through algorithmic-level retraining [17], [18]. The advantage of this method is the model behaves as if the unlearned data had never been used. While providing strong guarantees of removal, exact unlearning usually demands extensive computational and storage resources and is primarily suitable for simpler models. On the other hand, approximate unlearning focuses on reducing the influence of targeted data points through efficient model parameter update [17], [18]. While not removing influence thoroughly, approximate unlearning significantly reduces computational, storage, and time costs. It enables efficient unlearning of large-scale and complex models where exact unlearning is impractical. \n\nIn this paper, we provide a comprehensive overview of the solutions proposed for machine unlearning, covering pioneering and state-of-the-art techniques for both exact and approximate unlearning. We critically analyze existing solutions, highlight their advantages and limitations, identify research gaps, and suggest future directions. Our goal is to provide a useful roadmap that helps guide future work in developing adaptive and trustworthy ML systems, directly addressing evolving real-world challenges. \n\nThe main contributions of this paper are: \n\n\u2022 We provide a comprehensive taxonomy and structured overview of machine unlearning solutions, categorizing techniques into exact and approximate unlearning approaches. The taxonomy covers a broad range of existing works, establishing a structured understanding of this emerging field for researchers. \u2022 We give an in-depth critical analysis of machine unlearning solutions, highlighting their strengths, limitations, and challenges. This analysis provides valuable insights into theoretical and practical obstacles, guiding future research toward impactful open problems. \u2022 We identify critical issues in machine unlearning and suggest promising directions for future research. These suggestions expand the applicability of machine unlearning and address its limitations effectively.",
            "score": 0.5938230627433385,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2108
                },
                {
                    "start": 2109,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2459
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "239616091"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "239616091"
                },
                {
                    "start": 461,
                    "end": 465,
                    "matchedPaperCorpusId": "250633553"
                },
                {
                    "start": 875,
                    "end": 879,
                    "matchedPaperCorpusId": "239616091"
                },
                {
                    "start": 881,
                    "end": 885,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "The importance of unlearning in LLLMs has increasingly emerged, attracting more and more attention Liu et al. (2024b); Zhang et al. (2023a). Several research efforts have focused on employing gradient ascent techniques to achieve forgetting in target datasets (Jang et al., 2022;Yao et al., 2023;Chen and Yang, 2023;Maini et al., 2024;Zhang et al., 2024). Meanwhile, WHP and its improved variant construct the teacher distribution through a name replacement strategy to achieve the goal of forgetting target knowledge (Eldan and Russinovich, 2023;Liu et al., 2024c). SOUL investigated the impact of second-order optimizers on unlearning effectiveness Jia et al. (2024b). Some unlearning methods have explored the data-model interactions that could influence LLM unlearning, such as weight localization-based unlearning (Yu et al., 2023;Jia et al., 2024a), achieving forgetting through modifications to LLMs' hidden representations (Li et al., 2024b) or perturbations to the model's embedding layer (Liu et al., 2024a). Additionally, ULD achieved unlearning through an auxiliary smaller model Ji et al. (2024). Finally, researchers have developed several benchmarks for evaluating LLM unlearning effectiveness, such as TOFU for fictitious unlearning (Maini et al., 2024), WMDP for unlearning hazardous knowledge in LLMs (Li et al., 2024b) and RWKU for zero-shot konwledge unlearning (Jin et al., 2024).",
            "score": 0.5937149050869182,
            "section_title": "LLM unlearning",
            "char_start_offset": 6391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1401
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 139,
                    "matchedPaperCorpusId": "259501864"
                },
                {
                    "start": 547,
                    "end": 565,
                    "matchedPaperCorpusId": "271404131"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.485107421875
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Hypothesis and Experimental Design Based on Eq. ( 1) and Eq. ( 2), we hypothesize that there are three main reasons why the current fine-tuningbased unlearning methods appear successful in behavioral tests and seem to suggest that true unlearning has been achieved: \n\n1. The coefficients m \u2113 are changed after finetuning, leading to a change in the activations of the MLPs; 2. The value vectors W \u2113 V in MLPs are changed, causing a change in the knowledge they contain; 3. The change that happens in attention components caused the model's focus and the corresponding information extracted by these attention components A \u2113 to change, thus reducing the target knowledge-related information in the output. Here, for the sake of simplicity and better understanding, we continue to use the definitions of m \u2113 , W \u2113 V , and A \u2113 as given in Eq. ( 1) and Eq. ( 2) in the following. We ignore the minor effects caused by other components or parameters, such as the language model's unembedding matrix and the normalization layers. Based on the possible reasons described above, on the unlearned model, we conduct three different sets of activation patching or components' parameter restoration experiments, trying to recover the output of the target knowledge in the unlearned model. The specific operation process is as follows: \n\n1. In the first set of experiments, we restore the coefficient scores m \u2113 corresponding to each MLP component, layer by layer, in the language model, without making any intentional changes to the value vector parameters W \u2113 V of the MLPs or the attention components' states A \u2113 in any layer. 2. In the second set of experiments, we restore the parameters of value vectors W \u2113 V in MLPs layer by layer, recovering the knowledge they originally contained. In this process, we avoid making intentional changes to the unlearned model's original coefficients m \u2113 and the attention components' states A \u2113 . 3. In the third set of experiments, we restore the original attention components' states A \u2113 , but without intentionally altering the MLPs' coefficient scores m \u2113 or the value vectors' parameters W \u2113 V , only studying the impact brought by the attention components which are responsible for extracting and transferring knowledge.",
            "score": 0.5932736200829338,
            "section_title": "Patching Investigation",
            "char_start_offset": 6704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "273345941",
            "title": "Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy",
            "text": "Machine unlearning (e.g., Cao & Yang, 2015;Garg et al., 2020;Cohen et al., 2023) is a notion formulated to address a critical challenge in contemporary machine learning systems: the selective removal from trained models of information pertaining to a given subset of training examples. This capability has become increasingly important as machine learning models are frequently trained on large datasets, which often unintentionally include private (Carlini et al., 2021) and copyrighted (Henderson et al., 2023;Lee et al., 2024;He et al., 2024;Wei et al., 2024) material. The need to \"unlearn\" specific data points is not merely a technical challenge; it is also a response to escalating privacy concerns and evolving legal frameworks, such as the General Data Protection Regulation (GDPR, European Parliament & Council of the European Union). \n\nPreprint. \n\nAt its core, the goal of machine unlearning is to provide a protocol for data owners to request the removal of their data from a model. Specifically, let D train be the training set, and f target := L(D train ) be a model returned by the learning algorithm L. A machine unlearning algorithm U takes the trained model f target and a forget set D forget \u2282 D train , and produces a new model f unlearn := U(f target , D forget ) that is not influenced by D forget . The most straightforward unlearning algorithm is to retrain from scratch, i.e., to compute f retrain := L(D train \\ D forget ). However, this is generally impractical not only because it requires saving an entire copy of D train , but also because the cost of training from scratch has become prohibitively expensive for many modern neural network models.",
            "score": 0.5923238223830234,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1677
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 61,
                    "matchedPaperCorpusId": "211296633"
                },
                {
                    "start": 61,
                    "end": 80,
                    "matchedPaperCorpusId": "252907463"
                },
                {
                    "start": 449,
                    "end": 471,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277587890625
        },
        {
            "corpus_id": "271212701",
            "title": "On Large Language Model Continual Unlearning",
            "text": "Evaluation Metrics. To evaluate the unlearning effectiveness, we test the model performance at every unlearning request on the unlearning train set (used in unlearning) and unlearning test set (unused in unlearning), which are disjoint but from the same distribution, denoted as Samplelevel Unlearning (S.U.) and Distribution-level Unlearning (D.U.), respectively. As for measuring the utility preservation, we consider the model performance on three distributions. The first is the distribution most susceptible to unlearning requests, which we term Retained Distribution (R.D.). \n\nThe other two are the utility datasets for each task (question answering: CommonsenseQA and OpenbookQA; fictitious knowledge generation: TOFU-Real Authors (i.e., R.A.) and Word Facts (i.e., W.F.); intent classification: MRPC and RTE), and we denote their performance as U.1. and U.2., respectively. In addition, to evaluate the balance between unlearning effectiveness and utility preservation, we design a metric called the Unlearning-Utility Ratio ( \n\n) where Acc means the accuracy. The higher the U 2 R, the better both unlearning effectiveness and utility preservation. \n\nCompared Baselines. To better demonstrate the effectiveness of our proposed methods, we implement a series of state-of-the-art language model unlearning approaches: GradAsc Golatkar et al. (2020), GradDif Yao et al. (2023), EUL Chen & Yang (2023), PO Eldan & Russinovich (2023), NPO Zhang et al. (2024), SOGD Jia et al. (2024) and SOPO Jia et al. (2024). We only conduct reasonable modifications to customize them in our continual unlearning settings. \n\nImplementation Details. Following TOFU (Maini et al., 2024) and SOPO (Jia et al., 2024), we use LLaMA2-7b (Touvron et al., 2023) as the target model.",
            "score": 0.5922965857348395,
            "section_title": "EXPERIMENTAL SETUPS",
            "char_start_offset": 23663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1763
                }
            ],
            "ref_mentions": [
                {
                    "start": 1333,
                    "end": 1354,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "266933371",
            "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
            "text": "(ii) Who is likely to exercise their right to be forgotten, and can we hope to unlearn things about entities that are over-represented in the training data? (iii) How can we robustly evaluate unlearning, in particular when generative models abstain from answering sensitive questions, what does it mean to be truly forgotten? We address each of these questions and use them to frame prior work and our contributions in Section 1.1. \n\nIn this work, we aim to put the field on solid footing: First, we propose a new benchmark for unlearning called TOFU: Task of Fictitious Unlearning. We create a novel dataset with facts about 200 fictitious authors that do not exist in the pretraining data of present-day LLMs (Section 2.1.1). Upon finetuning base LLMs on this dataset, we offer a clearly defined task to forget some of the fictitious authors. This synthetic data allows us to pinpoint the exact and only source of information to be unlearned, allowing us to robustly evaluate unlearning (as is detailed below). \n\nTOFU comes with three different task severity levels, aimed at forgetting 2, 10, and 20 authors. Furthermore, there is a constraint to unlearn with O(number of forget samples) compute, i.e. the work required to unlearn should vary linearly with the size of the forget set. \n\nSecond, we propose a new evaluation scheme for measuring unlearning, detailing how unlearning methods must be compared across two different axes of forget quality and model utility. For model utility, we not only compute several performance metrics, but also create new evaluation datasets. These datasets constitute a gradient of relevance that helps in measuring the effect of the unlearning process (Section 2.2.1). We aggregate these numbers into a single metric for model utility. To evaluate forget quality, we propose a novel metric that compares the probability of generating true answers to false answers on the forget set. We then employ a statistical test to compare unlearned models to the gold standard retain models that are never trained on the sensitive data (Section 2.2.2).",
            "score": 0.5914079684208217,
            "section_title": "Introduction",
            "char_start_offset": 2003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2081
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3603515625
        },
        {
            "corpus_id": "276408369",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "text": "The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
            "score": 0.5913039164847532,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 158,
                    "matchedPaperCorpusId": "265659278"
                },
                {
                    "start": 758,
                    "end": 777,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 821,
                    "end": 842,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1071,
                    "end": 1092,
                    "matchedPaperCorpusId": "7357141"
                },
                {
                    "start": 2006,
                    "end": 2024,
                    "matchedPaperCorpusId": "3868529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Unlearning in Large Language Models Since large language models learn knowledge from different domains and corpora during the pre-training process, it is often found that they contain harmful, sensitive or private knowledge, leading to the possibility that language models produce output behaviors containing corresponding sensitive or harmful information (Liu et al., 2024;Chang et al., 2023a;Mozes et al., 2023). Therefore, unlearning emerges as a timely and important post-pretraining processing method for LLM safety. Currently, the vast majority of LLM unlearning methods use finetuning as the primary operational approach. In terms of classifying them by different training objectives, they include gradient direction control (Jang et al., 2023;Yao et al., 2024Yao et al., , 2023) ) and preference optimization methods (Rafailov et al., 2023;Zhao et al., 2024;Lee et al., 2024b). In terms of classifying them by the parameters covered during training, they include full parameters fine-tuning (Eldan and Russinovich, 2023;Jang et al., 2023;Yao et al., 2024;Rafailov et al., 2023), sparse finetuning (Chang et al., 2023b;Stoehr et al., 2024), and parameter-efficient fine-tuning (Lu et al., 2024;Chen and Yang, 2023). Additionally, there are also a few knowledge editing methods (Patil et al., 2024). We present the specific logic details of each method in \u00a7A. \n\nKnowledge Storation in Large Language Models Studying how knowledge is stored, transferred, and extracted in LLMs has always been an important direction in the research of LLM's interpretability (Meng et al., 2022;Geva et al., 2021b;Sukhbaatar et al., 2015;Geva et al., 2023). It is known that in transformer-based language models, the MLP is a crucial component for storing the model's factual knowledge, and its sub-layers can be viewed as key-value memories (Geva et al., 2021b).",
            "score": 0.5908193131735877,
            "section_title": "Background and Related Work",
            "char_start_offset": 3456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 394,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 732,
                    "end": 751,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 767,
                    "end": 788,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 825,
                    "end": 848,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1028,
                    "end": 1046,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1063,
                    "end": 1085,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1201,
                    "end": 1221,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1284,
                    "end": 1304,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1563,
                    "end": 1582,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1582,
                    "end": 1601,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1601,
                    "end": 1625,
                    "matchedPaperCorpusId": "1399322"
                },
                {
                    "start": 1625,
                    "end": 1643,
                    "matchedPaperCorpusId": "258417932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "277510371",
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "text": "Why existing works cannot unlearn at the feature level? Most existing MU methods develop unlearning methods based on Negative Gradient or Random Label approaches, and perform end-to-end training using these forgetting losses. These approaches rely on a logit-based loss function, which makes it easy for the model to find a shortcut solution by focusing on the classification head. In Figure 2, we experimentally analyze this by measuring the weight difference between the unlearned model and the original model. The results indicate that existing methods are biased toward perturbing at the head. Consequently, most MU methods fail to effectively unlearn the encoder, where most of the learned knowledge resides. Therefore, to achieve a more fundamental removal of knowledge, it is essential to directly erase feature-level knowledge. \n\nMotivation. Our ultimate goal is to make it impossible for both users and malicious actors to extract any forgetting knowledge from the model by directly removing the knowledge in the feature space while preserving the remaining knowledge. To achieve this, we need to decompose the forgetting knowledge from others and remove the model's capability, i.e., activation. Inspired by Gu et al. [18], we can expect that we can decompose and deactivate the feature by using only partial principal directions given by Singular Value Decomposition (SVD). In Figure 3, we conducted a toy experiment. We calculate the cosine similarity of features between each class. On the left side, the results show that the original features are highly entangled between classes, e.g., the minimum value of similarity is greater than 0.5. On the right side, the ESC features by our method are clearly disentangled from the features of other classes (details about the ESC features are provided in the ESC section). Furthermore, they are no longer aligned with the original features of the same class, which contain the learned knowledge, with all similarity values falling below 0.35. Building on this insight, we introduce Erasing Space Concept named ESC, which can remove the forgetting knowledge by deactivating in the feature space. \n\nErasing Space Concept. We start with feature decomposition on the feature space to extract principal components.",
            "score": 0.590613264768675,
            "section_title": "Erasing Space Concept",
            "char_start_offset": 13162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2177
                },
                {
                    "start": 2178,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1228,
                    "end": 1232,
                    "matchedPaperCorpusId": "257766486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "267657624",
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "text": "Mathematical modeling. Building upon the high-level formulation of LLM unlearning problems presented earlier, we next provide mathematical modeling details and discuss the associated design choices. To facilitate comprehension, we provide a commonly-used formulation of LLM unlearning problems below. While this may not be the sole or optimal approach to LLM unlearning, it incorporates several key elements that are essential to the problem setup. \n\nwhere \u2113(y|x; \u03b8) denotes the prediction loss of using \u03b8 given the input x with respect to the response y, D f and D r refer to 'forget' and 'retain' sets which will be explained later, y f denotes the desired model response post-unlearning, and \u03bb \u2265 0 is a regularization parameter to balance 'forget' and 'retain' (e.g., \u03bb = 0 if retain set is not given a priori). \n\nIn the dataset setup of LLM unlearning, we typically assume access to a forget set (D f ), the influence of which should be eliminated in LLM generation. For instance, D f might consist of a collection of harmful or toxic prompt-response pairs designated for degeneration (Yao et al., 2023). Moreover, if the original training set is available, then D f can be composed of a subset of training data points related to the unlearning target. Alternatively, it can be generated using synthesized data points based on a higher-level unlearned knowledge concept, or it can be derived from a set of extracted training data points reverse-engineered from the given LLM itself. In practice, the forget set D f is not required to belong precisely to the LLM's training corpus. Given the diversity and extensive size of the model's training data, the content we aim to unlearn is more likely to represent a general concept. Thus, LLM unlearning needs to not only unlearn specific training samples but also generalize to similar samples that share common characteristics. \n\nBesides the forget set D f , sometimes there is a need for a retain set (D r ), which contains samples that are not subject to unlearning and used to preserve the utility of the unlearned model.",
            "score": 0.590475406615742,
            "section_title": "Unpacking LLM Unlearning",
            "char_start_offset": 10844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1877
                },
                {
                    "start": 1880,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 1089,
                    "end": 1107,
                    "matchedPaperCorpusId": "264172840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "252693065",
            "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
            "text": "Machine unlearning has received attention as an alternative approach to overcome data privacy issues in machine learning (Cao & Yang, 2015;Ginart et al., 2019;Bourtoule et al., 2021;Graves et al., 2021). Several studies attempt to explore machine unlearning for deep neural networks (Golatkar et al., 2020;Mehta et al., 2022). However, they mostly focus on proposing algorithms for image classification models where they aim to forget a whole class; that is, achieve random performance for specific image classes such as \"cats\" or \"ships\". We are the first, to the best of our knowledge, to explore unlearning a specific sequence of tokens for LMs which is a quite different set-up from traditional image classification models (\u223ctens of image classes vs. a sequence of tokens that can each be classified into V \u2208 R \u223c50,000 ). In this work, we coin this approach as knowledge unlearning since we are more focused on forgetting specific knowledge represented by sequences of tokens.\n\nZhou et al. (2022) focus on how forgetting can be leveraged to improve the performance of the underlying model. They propose \"forget-and-relearn\" that unifies existing iterative training algorithms by selectively removing undesirable information and re-learning good features, helping boost performance for the task of image classification and multi-agent emergence communication. The underlying assumption is that it is often easier to define and stop unwanted behavior than to teach good behavior. We also show this phenomenon in Section 4 where we unintentionally find unlearning just a few sequences of tokens sometimes boosts general LM capabilities.",
            "score": 0.5901244127388519,
            "section_title": "MACHINE UNLEARNING",
            "char_start_offset": 8894,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 325,
                    "matchedPaperCorpusId": "248227997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4130859375
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "We present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.",
            "score": 0.5898713040059463,
            "section_title": "Taxonomy",
            "char_start_offset": 6148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 771
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "273098143",
            "title": "Undesirable Memorization in Large Language Models: A Survey",
            "text": "As memorization could lead to privacy risks and copyright issues, unlearning methods could be necessary in some situations. Methods like knowledge unlearning aim to selectively remove specific information from trained models without retraining them from scratch. Bourtoule et al. [124] introduce the \"SISA\" framework for efficient machine unlearning, which divides the training data into shards (shards partition the data into disjoint segments) and trains sub-models that can be easily retrained if data needs to be removed. For LLMs specifically, Chen and Yang [125] introduce lightweight unlearning layers into transformers, allowing for selective data removal without full model retraining. Pawelczyk et al. [126] introduce \"In-Context Unlearning,\" which involves providing specific training instances with flipped labels and additional  Similarly, the PILE dataset is the predominant dataset in these studies, which aligns with the fact that Pythia and GPT-Neo models are primarily trained on it, ensuring consistency in experimental settings. (2) In terms of generation settings, studies largely focus on producing 50-token or 32-token sequences, with greedy decoding being the dominant approach. This preference for greedy decoding is expected, as it maximizes the likelihood of generating memorized sequences, making it an effective choice for probing memorization tendencies. (3) Regarding the spectrum of memorization, approximately 60% of studies focus on discoverability, while the remainder explores extractability. Similarly, verbatim memorization is examined in 60% of works, whereas the rest are split between approximate memorization and entity-level memorization. \n\ncorrectly labeled instances as inputs during inference, effecremoving the targeted information without updating model parameters. Kassem et al. [127] propose \"DeMem,\" a novel unlearning approach leveraging a reinforcement learning feedback loop with proximal policy optimization to reduce memorization. By fine-tuning the model with a negative similarity score as a reward signal, the approach encourages the LLM to paraphrase and unlearn pre-training data while maintaining performance.",
            "score": 0.5898633065003159,
            "section_title": "C. Unlearning methods",
            "char_start_offset": 47526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 280,
                    "end": 285,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 563,
                    "end": 568,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "266933371",
            "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
            "text": "Our benchmark is designed to help researchers and practitioners think about and evaluate unlearning methods. Naturally, not all scenarios are covered, and there are areas of unlearning that fall outside the TOFU framework that are worth discussing. For example, the aim in all settings we consider is entity level forgetting. That is, we have a set of people about whom we want the model to forget everything. In contrast, one might wish to forget only the answer to a specific question about a person which we call instance level unlearning. Since it is not yet clear how to do entity level unlearning, we leave this variation for future work.",
            "score": 0.5889271036614625,
            "section_title": "What TOFU Misses",
            "char_start_offset": 38860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 644
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324462890625
        },
        {
            "corpus_id": "272770202",
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "text": "We now discuss two closely related approaches: Eldan and Russinovich (2023) and Dong et al. ( 2024) use positive feedback on the forget set to stabilize unlearning by substituting privacy-sensitive \"anchor\" words with alternate positive token-level labels. Eldan and Russinovich (2023) uses GPT-4 to identify anchor tokens, while Dong et al. ( 2024) considers all nouns as anchors. In contrast, our method avoids selecting specific anchor words and generates multiple alternate answers consistent with the original question. Dong et al. (2024) derives alternate completions based on next-token probabilities, excluding the highest-ranked token and (Eldan and Russinovich, 2023) uses scores from a model trained further on the forget set along with substitutions proposed by GPT-4. We simplify this by directly instructing an LLM to generate multiple alternative answers. While both works use a cross-entropy loss, our AltPO method employs a DPO-style loss to align the model with alternate answers, explicitly incorporating negative feedback. Ablation studies in Section 6.4 show how these elements improve our method's performance. We provide a broader review of the machine unlearning literature in Appendix A. \n\nAround the time of submitting this work, we became aware of parallel research by Jin et al. ( 2024), who proposed a similar approach in the RWKU (Real World Knowledge Unlearning) benchmark (Jin et al., 2024). RWKU explores unlearning famous real-world entities without access to a defined forget dataset that introduced knowledge of the entities. One of their DPO baselines shares key elements with our method: prompting the model to generate both knowledge about the forget entity and alternative facts, then applying a DPO objective to align the model with these alternatives. While Jin et al. ( 2024) report that their approach improves fluency compared to NPO and IdkPO, it exhibits weaker forgetting performance on RWKU. Key differences between their DPO baseline and AltPO include: (1) They omit a retain set loss to control unlearning generalization beyond the forget set, which we identified as crucial and thus incorporated in all baselines and our method.",
            "score": 0.58885239511399,
            "section_title": "Related Work",
            "char_start_offset": 15360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7587890625
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "To evaluate the faithfulness of unlearning methods, we first split the forget set C f , the retaining set C r , and the test set C t from the entire Base QA dataset C. Then, we train a language model to unlearn the forget set while maintaining knowledge of the retaining set. We further evaluate the unlearned model to the test set to assess knowledge retention for unseen data. In addition, we evaluate the unlearned model with the other constructed datasets (i.e., C p , C m , and C s ) mapped to the forget and test sets to analyze the aspect of superficial unlearning. \n\nOur unlearning framework consists of two types of input formats: (1) general QA format, and (2) multiple-choice QA (MCQA) format. We use the general QA format for unlearning and the MCQA format for evaluation. The general QA format inputs a question without an additional template, while the MCQA format uses a template that includes instructions and answer options. Suppose we aim to unlearn the knowledge of the question \"Who is the mother of Barack Obama?\", then we train a language model not to output the correct answer (i.e., \"Stanley Ann Dunham\") using only the question as an input. However, many users use a language model with various instruction templates, and an unlearned model should be evaluated in a stricter environment considering generalization. Furthermore, evaluating all possible answers to a question is one of the most challenging aspects of QA evaluation. Therefore, we utilize the MCQA form to evaluate an unlearned model. This makes it easier for LLMs to derive knowledge since they are given answer options; thus, it makes unlearning algorithms harder to apply. For this reason, we use the MCQA setting to evaluate unlearned models in more challenging and practical settings.",
            "score": 0.5888320268108259,
            "section_title": "Evaluation Framework",
            "char_start_offset": 15274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1778
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51220703125
        },
        {
            "corpus_id": "271601132",
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "text": "Task Dependence. LLMs learn general representations of both syntactic and semantic knowledge during their pretraining on the large corpus. This enables them to serve as a more general-purpose tool to solve different tasks. The patterns in the representations are intertwined and can be vulnerable when combined with downstream tasks. Catastrophic forgetting is a typical example of such vulnerability, which often occurs during transfer learning. The model can lose its generalization ability and overfit to the target domain in a catastrophic forgetting (Luo et al., 2023;Zhai et al., 2023;Wang et al., 2023a). Thus, when testing the efficacy of an unlearning method for the LLMs, it is important to conduct comprehensive fidelity experiments on datasets from various domains. Nevertheless, most of the existing work tests the retaining and forgetting performance of the unlearning methods on specific datasets (Chen & Yang, 2023a;Yao et al., 2023). The impact of the unlearning process on the model's generalization ability to other tasks is rarely verified. Although the model preserves its performance on the current task, it remains uncertain whether the nuanced modifications in parameters during unlearning force the model to compromise its capability in other tasks. \n\nWe argue that an effective unlearning method should minimize the performance degradation of the target model on diverse tasks. \n\nForget or Lie? Nowadays, we hold a higher expectation of generative models than before. This gap of expectation is more prominent in terms of LLMs. In the previous era, we mainly focused on improving the fluency and stability of the generation. Therefore, as the unlearning results from (Eldan & Russinovich, 2023) shown in Figure 2, a worse performance or a fabricated description for the sensitive instances could be viewed as a successful unlearning result. However, simply generating incorrect outputs can no longer be enough when we are expecting factual and reliable generation. LLMs are being transformed to function as knowledge bases (AlKhamissi et al., 2022) consisting of structural representations of facts and relations. Besides, substantial efforts have been devoted to reducing hallucinations (Rawte et al., 2023).",
            "score": 0.5887740152291425,
            "section_title": "FOR LLMS",
            "char_start_offset": 13189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 912,
                    "end": 932,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2978515625
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In this paper, we have a closer look at machine unlearning for LLMs. We note that most prior studies (Maini et al., 2024;Ji et al., 2024;Jia et al., 2024;Jin et al., 2024;Shi et al., 2024a) primarily rely on ROUGE (Lin, 2004) as the sole metric for evaluating the output of unlearned models. To more comprehensively assess the model behavior, we introduce three additional metrics that evaluate token diversity, sentence semantics, and factual correctness in the output. We then review the Figure 1: Illustration of untargeted and targeted unlearning for LLMs. Targeted unlearning hopes to make a specified template response to the questions in the forget set, while untargeted unlearning only requires not leaking the contents of the forget set. mainstream methods for unlearning fine-tuning, and categorize them into untargeted and targeted, based on whether the model's output on the forget set is explicitly specified, as illustrated in Figure 1. \n\nCrucially, we analyze potential issues of existing methods and present our approach: 1) We discuss that the behavior existing untargeted unlearning attempts to approximate is unpredictable and may pose the risk of hallucinations. We adopt the objective of maximizing the prediction entropy for each next token, which is more well-defined and data-agnostic. 2) We find that existing regularization losses are insufficient to prevent unlearned models from becoming overly ignorant during targeted unlearning, and propose incorporating the answer preservation (AP) loss for regularization to alleviate this issue. Experimentally, we consider the widely adopted TOFU benchmark (Maini et al., 2024;Zhang et al., 2024a;Jia et al., 2024;Ji et al., 2024;Huang et al., 2024;Liu et al., 2024a) for fictitious unlearning, and extend it to the continual unlearning scenario. Besides, we also conduct evaluations in a more realistic real-world unlearning scenario. Experimental results across various scenarios demonstrate the effectiveness of our approaches.",
            "score": 0.5887730558274952,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 225,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481689453125
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.For example, Eldan & Russinovich (2023) experiment with unlearning to forget Harry Potter books and demonstrate the effectiveness of their methods by showing that familiarity scores, measured through completion-based, tokenprobability-based, and question-answering evaluations, significantly decline post-unlearning.Lynch et al. (2024) further suggest comparing unlearned models with perfectly retrained models.Their evaluation finds that while familiarity scores with the forget set may drop post-unlearning, they still remain higher than those of the retrained model.Wei et al. (2024b) evaluate the feasibility of using unlearning techniques to prevent language models from generating copyrighted content.The closest work to ours is TOFU (Maini et al., 2024), a benchmark featuring 200 synthetic author profiles, each with 20 question-answer pairs, divided into forget and retain sets.However, TOFU is relatively small-scale (0.15M tokens) and focuses on the evaluation of question answering.Additionally, current evaluations focus on limited aspects of data owner expectations and do not adequately reflect real-world deployment considerations, such as scalability and potential sequential unlearning requests.\n\nIn contrast, MUSE formally defines different unlearning scopes and corresponding metrics, resulting in a systematic six-way evaluation featuring both data owners' and deployers' expectations.The evaluation uses a large-scale corpus of over 6 million tokens, separated into verbatim text and knowledge sets.We also note that some of our findings align with previous evaluations.For example, our observation that over-or under-unlearn can exacerbate privacy leakage ( \u00a75.2) is consistent with the recent work by Hayes et al. (2024).Our findings align with the the concurrent study by Shumailov et al. (2024) showing that unlearning gives a false sense of security as unlearned knowledge can resurface through in-context learning.\n\nSurvey papers.We direct readers to several insightful survey papers for further reading",
            "score": 0.5886943824313343,
            "section_title": "Related Work",
            "char_start_offset": 26747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1332
                },
                {
                    "start": 1334,
                    "end": 1525
                },
                {
                    "start": 1525,
                    "end": 1640
                },
                {
                    "start": 1640,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1864
                },
                {
                    "start": 1864,
                    "end": 2061
                },
                {
                    "start": 2063,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 1916,
                    "end": 1939,
                    "matchedPaperCorpusId": "270869978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.492919921875
        },
        {
            "corpus_id": "269009619",
            "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
            "text": "Dataset and metrics. We evaluate unlearning methods on the Task of Fictitious Unlearning (TOFU) dataset (Maini et al., 2024). It contains 200 fictitious author profiles, each consisting of 20 question-answer pairs generated by GPT-4 based on some predefined attributes. These fictitious profiles do not exist in the pre-training data, providing a controlled environment for studying unlearning LLMs. TOFU introduces three levels of tasks, each aiming to forget 1% , 5% , and 10% of the data, referred to as Forget01, Forget05, and Forget10, respectively. We measure the effectiveness of unlearning methods via Forget Quality and Model Utility as in Maini et al. (2024). Forget quality assesses how well the unlearned model mimics the retrained model (defined as the model trained only on the retain set), while model utility measures the general capacities and the real-world knowledge of the unlearned model. Since the forget quality is defined as the p-value of the Kolmogorov-Smirnov test, which tests the similarity between some distributions generated by the unlearned model and the retrained one, we treat a forget quality greater than 0.05 as evidence of a meaningful forgetting. More details are deferred to Appendix D.1.1 and Appendix D.1.2. \n\nUnlearning methods. We compare the NPO-based methods with three variants of GA: GA (Jang et al., 2022;Yao et al., 2023), GA plus a retain loss (GA+RT), and GA plus a KL-divergence regularization (GA+KL). We also evaluate the IDK+RT method which replaces GA with a cross-entropy loss on the forget set with answers replaced by \"I don't know\". Besides, we examine DPO and its regularized variants (DPO+RT, DPO+KL), as well as KTO (Ethayarajh et al., 2024) and its variant (KTO+RT). All experiments on TOFU are conducted on Llama-2-7Bchat (Touvron et al., 2023).",
            "score": 0.5886058209259954,
            "section_title": "Experimental setup",
            "char_start_offset": 21614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 1355,
                    "end": 1372,
                    "matchedPaperCorpusId": "264172840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55322265625
        },
        {
            "corpus_id": "267547751",
            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
            "text": "This paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.",
            "score": 0.5884121786276135,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "To improve the evaluation of existing LLM unlearning methods, we introduce a novel perspective by examining the unlearning characteristics of samples. Inspired by neuroscience, we propose a metric, MRD, to quantify the unlearning difficulty of samples. Defined as the expected change in sample generation probability after applying Gaussian perturbations to model parameters, MRD demonstrates that unlearning difficulty varies significantly across samples, emphasizing the importance of sample selection in unlearning performance. We further analyze the factors influencing the MRD value of samples, specifically identifying the characteristics of samples that make them harder or easier to unlearn. Then, we leverage these insights to propose an MRD-based weighted sampling approach. This approach refines existing unlearning methods by prioritizing the removal of easier-to-unlearn samples, improving both efficiency and effectiveness We employ four mainstream unlearning tasks and datasets to validate the effectiveness of the MRD metric and our proposed MRD-based improvement methods. Specifically, these include: \n\n\u2022 TOFU (Maini et al., 2024). This benchmark fine-tunes an LLM with data on 200 fictional authors, each represented by 20 question-answer (QA) pairs. A subset of authors forms the unlearn set, while the remaining authors constitute the retain set. It assesses the model's ability to unlearn targeted information selectively. Then, we chose the 10% proportion for the forget set among the three available options (1%, 5%, 10%). \n\n\u2022 WMDP (Li et al., 2024a). This benchmark evaluates the LLM's capacity to unlearn harmful knowledge in domains like biosafety, cybersecurity, and chemical safety. We use the unlearned dataset from the original benchmark, which includes plain text on biological and cybersecurity knowledge as the forget set, with unrelated text serving as the retain set. \n\n\u2022 Who's Harry Potter (WHP) (Eldan & Russinovich, 2023). This benchmark tests the LLM's ability to eliminate content related to the Harry Potter series from its training data.",
            "score": 0.588220976959867,
            "section_title": "Conclusion",
            "char_start_offset": 29944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 1127,
                    "end": 1147,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56494140625
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "We introduce Erasure of Language Memory (ELM), an approach that reformulates concept unlearning through introspective classification. While traditional unlearning methods have focused on sample removal through dataset retraining, gradient ascent, or representation disruption, ELM leverages the language model's own ability to evaluate and modify its knowledge. \n\nSpecifically, ELM uses the language model itself as its own introspective classifier, as established in Section 3.2. Rather than training external classifiers or explicitly manipulating representations, we leverage the model's next-token predictions to identify and reduce the likelihood of generating concept-specific content. This allows us to operate directly on the model's probability distributions, providing a principled approach to unlearning. \n\nThe core of our method is a self-classification objective that reduces the likelihood of generating text that the model would classify as containing the target concept. Given a dataset D erase containing text related to the target concept, we modify the model's predicted probabilities to diverge from generating content related to the concept being erased c \u2212 and instead favor generating content related to a safer alternative c + : \n\n). \n\nIn practice, we encounter two main challenges when implementing this objective. First, knowledge in language models is often entangled -modifying one concept can unintentionally affect related concepts. To address this, we preserve the model's behavior on a set of related but safe concepts by matching its original distribution on retain data, D retain , containing unrelated text: \n\nSecond, the self-classification objective alone might lead to incoherent text generation when prompted about erased concepts. We address this by applying our core objective (Equation 9) during inference to generate synthetic training examples, then using these to train the model in an autoregressive setting to maintain coherent generation: \n\nThe final training objective combines these terms with appropriate weights: \n\nWe implement this through low-rank adapters attached to early model layers, allowing precise modification of the model's knowledge while maintaining its broader capabilities. This approach provides a principled method for concept unlearning that avoids the instability of gradient reversal methods and the incoherence of representation hacking approaches.",
            "score": 0.5878876181991993,
            "section_title": "Method",
            "char_start_offset": 14007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1642
                },
                {
                    "start": 1645,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1986
                },
                {
                    "start": 1989,
                    "end": 2064
                },
                {
                    "start": 2067,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2422
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70947265625
        },
        {
            "corpus_id": "277780677",
            "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
            "text": "Machine unlearning was initially introduced for tasks requiring the removal or modification of specific data [2,11,12], such as data privacy protection and model updates. \n\nWith the rise of generative artificial intelligence, machine unlearning techniques have increasingly been applied to T2I diffusion models. Current machine unlearning approaches in diffusion models can be categorized based on the unlearning target. The first category, which most existing works focus on, is single-concept unlearning, where the goal is to forget a specific concept. The second category involves multi-concept unlearning, where multiple concepts are removed within the same model. Single-concept forgetting. SLD [33] incorporates a safety guidance mechanism to dynamically suppress inappropriate content. FMN [42] further refines this strategy by employing attention re-steering techniques for targeted concept removal. ESD [8] fine-tunes using conditioned and unconditioned scores from the frozen SD model to guide outputs away from the concepts being erased. In contrast, SA [13] adopts a continual learning framework with elastic weight consolidation and generative replay to achieve precise forgetting. Meanwhile, SalUn [7] utilizes gradientbased weight saliency to selectively update the parameters most sensitive to the target concept. AC [17] assigns an anchor concept to overwrite the target concept, ensuring that unlearning does not compromise the meaningfulness of the generated outputs. However, simple modifications for unlearning a single concept are often insufficient for multi-concept unlearning. For training-free methods, ConceptPrune [4] identifies and prunes critical neurons that are highly correlated with the target concept, enabling training-free concept editing. Nonetheless, modifying the weights in this manner inevitably affects the quality of the generated images. Similarly, UCE [9] modifies the attention scores related to the target concept in UNet to achieve unlearning. It is important to note that training-free methods heavily rely on prior knowledge of text prompts and are more susceptible to adversarial attacks. \n\nMulti-concept forgetting. SPM [23] introduces a one-dimensional semi-permeable membrane that enables precise concept erasure while preserving non-target content.",
            "score": 0.587804477475771,
            "section_title": "Related Works",
            "char_start_offset": 4049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "253420366"
                },
                {
                    "start": 797,
                    "end": 801,
                    "matchedPaperCorpusId": "257833863"
                },
                {
                    "start": 912,
                    "end": 915,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 1065,
                    "end": 1069,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "257687839"
                },
                {
                    "start": 1898,
                    "end": 1901,
                    "matchedPaperCorpusId": "261276613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.638671875
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "Given the massive amounts of data involved in training LLMs, retraining these models each time to remove memorized data is impractical. Thus machine unlearning focuses on how to effectively eliminate unintentional memorized content after the model is trained (Cao and Yang, 2015;Ginart et al., 2019;Guo et al., 2020;Bourtoule et al., 2021). The unlearning algorithms can broadly fall into the following two categories: Direct Tuning Methods. Jang et al. (2023) first formalize the problem of LLM unlearning and propose to use gradient ascent (GA) to achieve unlearning. Instead of minimizing loss, GA maximizes the loss on tokens to be forgotten, forcing the model to forget specific knowledge. However, Zhang et al. (2024) note that GA causes rapid collapse. They propose Negative Preference Optimization (NPO) which diverges slower than GA both in theory and practice. Alternative approaches tune the model to say \"I don't know\" (Maini et al., 2024) or predict random labels (Yao et al., 2024) on the knowledge that should be forgotten. \n\nLeveraging Auxiliary Models to remove the memorization in the base LLM, bypassing direct tuning, is in the focus of another line of research. Eldan and Russinovich (2023); Ji et al. (2024) first fine-tune a model to memorize the forget set and then leverage contrastive decoding (CD) (Li et al., 2023) to sup- Retain Distance UNDIAL (ours) (lr=1e-4) UNDIAL (ours) (lr=3e-5) GA (lr=1e-4) GA (lr=3e-5) NPO (lr=1e-4) NPO (lr=3e-5) \n\nFigure 2: Training dynamics of Direct Tuning methods on the MUSE benchmark (Shi et al., 2024). MUSE divides data into two sets: the Forget set, containing the information to be unlearned, and the Retain set, which measures the impact of unlearning on unrelated knowledge.",
            "score": 0.5874942569830306,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 5376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1742
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 279,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 279,
                    "end": 299,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 299,
                    "end": 316,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 316,
                    "end": 339,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 442,
                    "end": 460,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 704,
                    "end": 723,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 931,
                    "end": 951,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 977,
                    "end": 995,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 1325,
                    "end": 1342,
                    "matchedPaperCorpusId": "253157949"
                },
                {
                    "start": 1546,
                    "end": 1564,
                    "matchedPaperCorpusId": "271064299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70654296875
        },
        {
            "corpus_id": "273502327",
            "title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?",
            "text": "Whether applied to open-source or closed-source LLMs, an unlearning scheme should affect model utility as little as possible, i.e., ensure harmlessness. We evaluated this metric for two unlearning objectives across four schemes. Specifically, for each scheme and unlearning objective, we assessed whether the unlearned LLM preserved general performance on wellestablished benchmarks, such as MMLU [41] and ARC [42]. Using Llama-2-7b-chat-hf as an example, Table VIII shows that our scheme minimally affects model utility, with results similar to those LLMs without unlearning. Whether forgetting samples or concepts, RAG-based unlearning achieves strong performance across most metrics compared to the other three baselines. This highlights a major challenge in LLM unlearning, i.e., catastrophic forgetting. As illustrated in Figure 6, gradient ascent-based unlearning may reduce model utility, even leading to unresponsive behavior for similar queries. For instance, if 'Harry Potter' is the protected concept, querying 'Who is Harry Potter?' will prompt the unlearned LLM to exhibit the forgetting effect, i.e., refusing to provide related information. Unfortunately, when querying 'Who is Conan Doyle?', the unlearned LLM may also decline to respond. This phenomenon indicates that such unlearning schemes could mistakenly categorize similar queries as forgotten targets, undermining the goal of LLM unlearning. \u00b5-unlearning faces a similar problem as it involves adjusting model parameters. Theoretically, RAG-based unlearning does not affect the internal workings of the unlearned LLM. It can achieve sample and concept unlearning without compromising model utility. Specifically, for non-forgotten samples or concepts, the retriever cannot extract any information from the unlearned knowledge base. As a result, the LLM responds to the original prompt normally, thereby ensuring true harmlessness.",
            "score": 0.5870707263133479,
            "section_title": "D. Harmlessness",
            "char_start_offset": 45034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "276408369",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "text": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "score": 0.5870163061518228,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "276107656",
            "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
            "text": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
            "score": 0.5869214470626586,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "274422652",
            "title": "UOE: Unlearning One Expert Is Enough For Mixture-of-experts LLMS",
            "text": "Recent advancements in large language model (LLM) unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model's utility for legitimate knowledge. However, despite these strides, sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have received little attention and remain largely unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance and highly efficient inference processes, we ask: How can unlearning be performed effectively and efficiently on MoE LLMs? And will traditional unlearning methods be applicable to MoE architectures? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to substantial utility drops when existing unlearning methods are applied. Specifically, unlearning disrupts the router's expert selection, causing significant selection shift from the most unlearning target-related experts to irrelevant ones. As a result, more experts than necessary are affected, leading to excessive forgetting and loss of control over which knowledge is erased. To address this, we propose a novel single-expert unlearning framework, referred to as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated on the most actively engaged expert for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning that preserves model utility. The proposed UOE framework is also compatible with various unlearning algorithms. Extensive experiments demonstrate that UOE enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks, LLM architectures, while only unlearning 0.06% of the model parameters.",
            "score": 0.5869038829677176,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "271843019",
            "title": "Get Confused Cautiously: Textual Sequence Memorization Erasure with Selective Entropy Maximization",
            "text": "Knowledge Unlearning for LLMs. Machine unlearning aims at removing model memorization about sensitive data. In contrast to traditional unlearning approaches in classification tasks (Bourtoule et al., 2021;Chundawat et al., 2023;Jia et al., 2023), the concept of machine unlearning in generative LLMs shifts focus to the characteristics of model output. Specifically, it focuses on mitigating harmful or biased information in generated content. Abstract harmful knowledge is one We select the most significant weights for forgetting based on the gradient magnitude and direction so that the weight is effective at entropy maximization instead of data memorization. \n\nof the targets for LLM unlearning, which bears similarities with safety alignment but primarily uses negative samples (Li et al., 2024;Liu et al., 2024;Yao et al., 2023). Question-answering-based benchmarks such as TOFU (Maini et al., 2024) and WMDP (Li et al., 2024) are established for evaluating model acquisition of the knowledge. \n\nGiven the objective and testbase, rejection-based methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2024) are suitable for encouraging the model to answer malicious questions. \n\nThe evaluation metrics for quantifying hazardous knowledge in LLMs include accuracy on malicious multiple-choice questions (Li et al., 2024) and GPTas-a-Judge score (Liu et al., 2024). \n\nTSM Erasure in LLMs TSM refers to LLMs' ability to memorize and emit training samples verbatim, which is an undesired attribute to be erased/unlearned (Carlini et al., 2021(Carlini et al., , 2022)). Different from undesired knowledge, TSM is defined over certain training data points. For evaluation, the updated model is asked to generate continuation based on the prefix of memorized data. Training Data Extraction Challenge 1 serves as a persuasive benchmark for probing TSM in GPT-Neo model family. Concurrent work MUSE (Shi et al., 2024) provides news and books corpus for evaluating verbatim memorization.",
            "score": 0.5866392594464311,
            "section_title": "Related Works",
            "char_start_offset": 5320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 205,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 205,
                    "end": 228,
                    "matchedPaperCorpusId": "248834527"
                },
                {
                    "start": 228,
                    "end": 245,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 784,
                    "end": 801,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 916,
                    "end": 933,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 1106,
                    "end": 1129,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1325,
                    "end": 1342,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 1540,
                    "end": 1561,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1561,
                    "end": 1586,
                    "matchedPaperCorpusId": "246863735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2176513671875
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "A.1. Machine Unlearning Background. \n\nThe concept of machine unlearning (Cao & Yang, 2015) is typically divided into two categories: exact unlearning and approximate unlearning. Exact unlearning aims to completely remove information related to specific data, ensuring that the resulting model behaves identically to a model retrained from scratch without the forget data (Ginart et al., 2019). However, the computational infeasibility of retraining LLMs from scratch renders exact unlearning impractical for real-world applications. Approximate unlearning methods, on the other hand, focus on ensuring that the model parameters closely approximate those of a retrained model while maintaining computational efficiency (Guo et al., 2020;Chien et al., 2022;Pan et al., 2023;Yoon et al., 2025). \n\nA.2. Coreset Selection. \n\nUnlike prior work, which focuses on coreset selection for improving training efficiency or robustness, our approach leverages a novel perspective by applying coreset principles to the problem of machine unlearning. Specifically, while conventional methods (Maharana et al., 2024) aim to preserve model accuracy during training by selecting representative data, our framework, UPCORE, is designed to mitigate negative collateral damage during unlearning by identifying and pruning data points that disproportionately influence performance degradation. Furthermore, unlike general coreset selection approaches that primarily target classification or regression tasks (Lee et al., 2024;Wei et al., 2015), our method is tailored for unlearning settings where the goal is retaining model utility while ensuring the effective removal of unwanted information. Thus, our work extends the applicability of coreset selection beyond traditional use cases, offering a principled approach to balancing unlearning effectiveness with model performance. \n\nA.3. Anomaly Score in Isolation Forest: \n\nIsolation Forests produce anomaly scores for each point. More formally, the anomaly score for a data point d is defined as:",
            "score": 0.5864529690460472,
            "section_title": "A. Additional Background",
            "char_start_offset": 32881,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 35
                },
                {
                    "start": 38,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2025
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 90,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 371,
                    "end": 392,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 718,
                    "end": 736,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 736,
                    "end": 755,
                    "matchedPaperCorpusId": "259298766"
                },
                {
                    "start": 755,
                    "end": 772,
                    "matchedPaperCorpusId": "258333880"
                },
                {
                    "start": 1485,
                    "end": 1503,
                    "matchedPaperCorpusId": "269149205"
                },
                {
                    "start": 1503,
                    "end": 1520,
                    "matchedPaperCorpusId": "9176532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "269187650",
            "title": "Offset Unlearning for Large Language Models",
            "text": "In this section, we summarize two lines of research that are highly related to our work.\n\nMachine Unlearning for LLM.Prior works have explored machine unlearning as a way to mitigate the influence of undesirable training data on LLMs.Given the vast cost incurred by retraining LLMs from scratch (Bannihatti Kumar et al., 2023), most unlearning methods apply post hoc finetuning or adaptation to steer the behavior on the forget set (Jang et al., 2023;Eldan and Russinovich, 2023;Yao et al., 2023;Chen and Yang, 2023).Gradient ascent based methods fine-tune models by minimizing the likelihood of forget set data (Jang et al., 2023;Chen and Yang, 2023;Maini et al., 2024).Alternatively, several works proposed to maximize the likelihood of relabelled target data, where the original answer is replaced with a generic, insensitive response (Eldan and Russinovich, 2023;Patil et al., 2024).Auxiliary training objectives can also be introduced to maintain model performance on out-of-forget-scope data (Yao et al., 2023;Wang et al., 2023).Another related line of research is model editing, where the goal is to identify and alter knowledge captured by local components within models (Meng et al., 2023;Wu et al., 2023).While both model editing and unlearning attempt to modify the behavior of trained LMs, unlearning focuses on eliminating the effect of a specific set of training data without necessarily creating new answer mappings (Liu et al., 2024c).It is worth noting that all of the aforementioned approaches require access to the model's internal weights.In-context unlearning (Pawelczyk et al., 2023), while being applicable to black-box LLMs, still requires storing sensitive information for inference and therefore fails to address data privacy concerns.In this work, we propose an unlearning framework that does not require access to LLM weights, nor storage of sensitive information for inference.\n\nLogit Ensemble.",
            "score": 0.5862502204159787,
            "section_title": "Related Work",
            "char_start_offset": 4754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 90,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1215
                },
                {
                    "start": 1215,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1559
                },
                {
                    "start": 1559,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1906
                },
                {
                    "start": 1908,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 295,
                    "end": 326,
                    "matchedPaperCorpusId": "254854347"
                },
                {
                    "start": 432,
                    "end": 451,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 612,
                    "end": 631,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 631,
                    "end": 651,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 867,
                    "end": 886,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1016,
                    "end": 1034,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 1198,
                    "end": 1214,
                    "matchedPaperCorpusId": "264816202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447509765625
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge. \n\nIn summary, common loss adjustment methods employ one [24], two [23,20,22], or all three [5] of these components to guide the model towards forgetting specific data while minimizing the impact on its overall performance and utility. The interplay between these terms allows for controlled and targeted unlearning, ensuring the model retains its valuable capabilities while selectively forgetting undesired information. More detailed formulations of these loss adjustmentbased methods, along with related work, are deferred to Appendix C.1 and Appendix E. \n\nAn Example: Large Language Model Unlearning (LLMU). We adopt a popular approach in LLM unlearning, LLMU [5], to interpret a special case of Eqn. (1).",
            "score": 0.586077536993179,
            "section_title": "Existing LLM Unlearning Paradigm",
            "char_start_offset": 5849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Unlearning et al. ( 2024) investigate the presence of explainable features within the forget sets and their impact on the difficulty of unlearning. Chen et al. (2024) provide a more fine-grained perspective, showing that in recommendation systems, unlearning difficulty varies significantly across users, with potential implications for the evaluation of unlearning algorithms. Collectively, these studies highlight a trend toward sample-level analysis in unlearning interpretability. However, notable limitations remain. These works lack a formal definition of unlearning difficulty at the sample level and offer little theoretical insight into why certain samples are harder to unlearn. Additionally, methods developed for image classification may not effectively generalize to LLMs, which struggle with modeling structured features due to their text-based autoregressive nature. To address these issues, this paper investigates the LLM unlearning problem, focusing on the following three key questions: \n\n\u2022 Q1. How to design a reasonable and computationally efficient metric to measure the unlearning difficulty of individual data samples? \u2022 Q2. Based on the proposed metric, what characteristics make certain data samples more difficult to unlearn? \u2022 Q3. Can this metric enhance the effectiveness and efficiency of existing LLM unlearning algorithms? \n\nTo address the questions above, this paper undertakes the following contributions: \n\nTo address Q1, we propose a metric, Memory Removal Difficulty (MRD), to measure the unlearning difficulty of individual samples (e.g., sentences) in LLMs. Inspired by findings in neuroscience (Kim & Fanselow, 1992;Squire & Alvarez, 1995;Frankland & Bontempi, 2005;Konrad et al., 2011), where long-term memories in the human brain are typically resistant to minor brain injuries and are not easily forgotten, MRD models unlearning difficulty in LLMs. As shown in Figure 1, it is formally defined as the expected change in the log-likelihood of a data sample before and after random perturbations to model parameters, ensuring both reasonable and computational feasibility. \n\nTo address Q2, we conduct an in-depth discussion on the MRD metric to uncover the characteristics of data samples that make them more difficult to unlearn.",
            "score": 0.5860592236273203,
            "section_title": "Hard Easy",
            "char_start_offset": 2860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1005
                },
                {
                    "start": 1008,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1439
                },
                {
                    "start": 1442,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2113
                },
                {
                    "start": 2116,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 166,
                    "matchedPaperCorpusId": "271956694"
                },
                {
                    "start": 1634,
                    "end": 1656,
                    "matchedPaperCorpusId": "9441030"
                },
                {
                    "start": 1656,
                    "end": 1679,
                    "matchedPaperCorpusId": "9080102"
                },
                {
                    "start": 1679,
                    "end": 1706,
                    "matchedPaperCorpusId": "1115019"
                },
                {
                    "start": 1706,
                    "end": 1726,
                    "matchedPaperCorpusId": "29658428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456298828125
        },
        {
            "corpus_id": "273661686",
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "text": "Like other machine learning approaches in NLP, while our goal is to remove the influence of specific documents from LLMs, complete removal cannot always be guaranteed. Therefore, caution should be exercised when applying the proposed unlearning techniques in practical applications as unlearned LLMs can still potentially generate harmful or undesired outputs. \n\nThere are several technical alternatives that we did not explore in this paper due to its scope and limited resources. For example, other learning-rate-free methods could potentially be adapted as alternatives to the GeN approach used in this work. Additionally, other multi-task optimization methods could be applied to machine unlearning. However, scaling these approaches to the level of LLMs could be challenging, and are left as future work. \n\nFinally, we mainly examined NGDiff's effectiveness on LLM unlearning in this paper with two benchmark datasets, TOFU and MUSE. To show its generalizability, we provide an additional example to apply the algorithm to computer vision tasks (see Appendix D). However, it would be desirable to test NGDiff on other modalities beyond NLP and CV applications. \n\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491-6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.",
            "score": 0.5859606401633926,
            "section_title": "Limitations",
            "char_start_offset": 24910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421630859375
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed. For example, ground-truth token sequences could still be extracted from 90.0% of multi-hop questions after unlearning in the Llama-3.1-8B-Instruct model using NPO (Zhang et al., 2024c), despite the original extraction success rate of 98.1% before unlearning. \n\nTo achieve more faithful knowledge unlearning, we propose a simple yet effective approach, MUNCH, which significantly outperforms existing approaches in unlearning multi-hop knowledge. MUNCH first decomposes multi-hop questions into successive subquestions, generates provisional answers, and employs the uncertainty of the unlearned model on the generated outputs as a measure to determine whether to provide a rejective response (e.g., \"I don't know.\") or keep it as is. Our method capitalizes on the high uncertainty of the unlearned model when dealing with direct, single-hop facts -an effect stemming from the reversed language modeling objective. By inspecting the decomposed multi-hop questions, we can more easily distinguish between information that needs to be forgotten and information that should be retained. Empirical results on the modified MQuAKE dataset confirm the efficacy of our approach, and we emphasize that MUNCH is highly practical, requiring no additional training and integrating seamlessly with existing unlearning techniques. To our knowledge, this is the first work to explore the unlearning of multi-hop knowledge.",
            "score": 0.5850149747764446,
            "section_title": "Unlearn Request",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1556
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 334,
                    "matchedPaperCorpusId": "269009619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37451171875
        },
        {
            "corpus_id": "265609937",
            "title": "DUCK: Distance-based Unlearning via Centroid Kinematics",
            "text": "Other strategies include unlearning through model pruning [50] and error-maximizing noise addition [51]. In the former, it is demonstrated that model pruning can aid in the unlearning process, whereas in the latter, noise matrices are linked with forget classes corrupting those weights in the model that allows the recognition of the forget-set data. \n\nIn this paper, we introduce DUCK, a fast, efficient, and model-agnostic algorithm for approximate unlearning. DUCK partially aligns with Boundary Unlearning [46] which modifies the forget class boundary manipulating the forget samples labels to induce unlearning. Conversely, our method operates directly within the feature space, driving the feature vectors of forget samples toward the nearest incorrect centroid. This strategy allows the model to remove the knowledge about the forget-set effectively, as demonstrated in Section IV while preserving the knowledge about the retain-set. Furthermore, this approach is particularly crucial in scenarios where the forget-set consists of instances from multiple classes, which we refer to as the homogeneous removal scenario. In this setting, the unlearning of forget samples has to be targeted on sample-specific knowledge and preserve the original model generalization capabilities on never-seen data.",
            "score": 0.5843347271522207,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 9950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 62,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "257636742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56884765625
        },
        {
            "corpus_id": "266899696",
            "title": "Machine Unlearning Through Fine-Grained Model Parameters Perturbation",
            "text": "In contrast, the Single-Step Sample Erasure (SSSE) [11] utilizes the Fisher Information Matrix to sidestep computational hurdles, achieving a more precise approximation. \n\nDifferently, inexact machine unlearning [12], [13] aims to reduce time and resources requirements by selectively eliminating the data's influence on the model. Current inexact machine unlearning methods based on model weight perturbation typically involve adding random Gaussian noise to all model parameters and then training for several epochs based on remaining data. \n\n1041-4347 \u00a9 2025 IEEE. All rights reserved, including rights for text and data mining, and training of artificial intelligence and similar technologies. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. \n\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. We propose to achieve inexact unlearning using a more finegrained perturbation. To that end, we design two inexact machine unlearning strategies: Random-k and Top-K. Here, Random-k represents perturbing k% of the parameters, while Top-K represents perturbing the top K parameters. To avoid notational confusion, we distinguish the two approaches by using lower and upper case respectively. By selectively perturbing a small subset of parameters, we aim to achieve the desired effect of machine unlearning efficiently while also reducing the impact on the overall model performance. \n\nEven if we conduct machine unlearning successfully, evaluating unlearning effectiveness remains a challenge, especially in terms of how to quantify the degree of unlearning. This is because when different algorithms learn from the same data set, they may acquire very similar knowledge and features. Consequently, when faced with independent and identically distributed (i.i.d) datasets, even if an unlearning approach has eliminated the influence of unlearning data, the model may still show approximate accuracy on the unlearning data to the remaining data due to the model's generalization property [14]. This makes it challenging to evaluate the effectiveness of the unlearning process.",
            "score": 0.5843198485442248,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 51,
                    "end": 55,
                    "matchedPaperCorpusId": "235765829"
                },
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "252531865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "276576079",
            "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
            "text": "Large language models (LLMs) have become increasingly popular. Their emergent capabilities can be attributed to their massive training datasets. However, these datasets often contain undesirable or inappropriate content, e.g., harmful texts, personal information, and copyrighted material. This has promoted research into machine unlearning that aims to remove information from trained models. In particular, approximate unlearning seeks to achieve information removal by strategically editing the model rather than complete model retraining. Recent work has shown that soft token attacks (STA) can successfully extract purportedly unlearned information from LLMs, thereby exposing limitations in current unlearning methodologies. In this work, we reveal that STAs are an inadequate tool for auditing unlearning. Through systematic evaluation on common unlearning benchmarks (Who Is Harry Potter? and TOFU), we demonstrate that such attacks can elicit any information from the LLM, regardless of (1) the deployed unlearning algorithm, and (2) whether the queried content was originally present in the training corpus. Furthermore, we show that STA with just a few soft tokens (1-10) can elicit random strings over 400-characters long. Thus showing that STAs are too powerful, and misrepresent the effectiveness of the unlearning methods. Our work highlights the need for better evaluation baselines, and more appropriate auditing tools for assessing the effectiveness of unlearning in LLMs.",
            "score": 0.5839742053291255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323974609375
        },
        {
            "corpus_id": "247627714",
            "title": "Knowledge Removal in Sampling-based Bayesian Inference",
            "text": "Then, the subset S f is removed iteratively, where a fixed number of data points are removed in each iteration. We also trained models on only the remaining set S r to show the targets of the machine unlearning task. For the details of the experiments, see Appendix D.5. \n\nResults analysis. The experiment results are presented in Table 1. In all experiments, we observe that the proposed MCMC unlearning algorithm can significantly increase the model error on sample set S f while making the error on sample sets S r and S test close to that of the retrained one. Besides, our algorithm can also reduce the \u03b5 value in the knowledge removal guarantee and the MIA accuracy on S f , which further indicates it can indeed remove specified knowledge from the MCMC models. \n\nIn contrast, the importance sampling method could neither effectively make the classification errors approach the targets, nor achieve stronger \u03b5-knowledge removal. We also present the time costs for different unlearning methods in Fig. 2. One can found that the MCMC unlearning algorithm is significantly faster than other methods. All these experiment results demonstrate the effectiveness and efficiency of the proposed algorithm. \n\nFinally, we give a case study about the prediction changes on the test set examples to illustrate the effect of the proposed MCMC unlearning algorithm. The results are presented in Fig. 3, where one can observe that as the data removing continues, the prediction confidence of the model on the removed class significantly decreases, while those on other classes remain the same or increase.",
            "score": 0.5838188771212978,
            "section_title": "EXPERIMENTS FOR BAYESIAN NEURAL NETWORKS",
            "char_start_offset": 18604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.151611328125
        },
        {
            "corpus_id": "266521156",
            "title": "FAST: Feature Aware Similarity Thresholding for Weak Unlearning in Black-Box Generative Models",
            "text": "Machine unlearning [19], [20] refers to the process of deliberately forgetting specific acquired knowledge or erasing the impact of particular subsets of training data from a trained model. Naive unlearning methods typically entail the removal of undesirable data from the training dataset, followed by retraining the model from scratch. However, this approach becomes computationally prohibitive when unlearning requests are made iterative for individual data points. Inspired by concerns surrounding privacy protection, [21] introduced methods for data deletion within statistical query algorithms, coining the term machine unlearning. Unfortunately, these methods are primarily suitable for structured problems and do not extend to complex machine learning algorithms, such as kmeans clustering [22] or random forests [23]. Efficient deletion algorithms were devised for the k-means clustering problem, which introduced effective data deletion criteria applicable to randomized algorithms based on statistical indistinguishability. Building upon this criterion, machine unlearning methods are broadly categorized into two main types: exact unlearning [22], [23] and approximate unlearning [24]. Exact unlearning endeavors to completely eradicate the influence of unwanted data from the trained model, necessitating precise parameter distribution matching between the unlearned and retrained models. In contrast, approximate unlearning methods only partially mitigate data influence, resulting in parameter distributions that closely resemble the retrained model, albeit with minor multiplicative and additive adjustments. To eliminate the influence of unwanted data, [25] proposed technique employs parameter perturbation based on cached gradients, offering computational efficiency while increasing memory usage. Other methods [26], [27] have suggested the use of influence functions for this purpose. However, these approaches are computationally demanding due to the necessity of hessian inversion techniques and are limited to small convex models. To extend the applicability of influence removal techniques to non-convex models like deep neural networks, a scrubbing mechanism [28] was introduced within a classification framework. However, until recently, it remained unclear how these techniques could be applied to unsupervised models, particularly state-of-the-art generative models.",
            "score": 0.5836902513095557,
            "section_title": "A. Machine Unlearning",
            "char_start_offset": 7767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2395
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 23,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "235422138"
                },
                {
                    "start": 1154,
                    "end": 1158,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "235422138"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "220364296"
                },
                {
                    "start": 1670,
                    "end": 1674,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 1837,
                    "end": 1841,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 2185,
                    "end": 2189,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Datasets and models. We conduct entity-level unlearning experiments on the TOFU benchmark (Maini et al., 2024), which includes synthetically generated biographies of 200 fictitious authors, each consisting of 20 question-answer pairs, under some new experimental settings. We fine-tune the Llama2-7B-Chat (Touvron et al., 2023b) and Phi-1.5 (Li et al., 2023) on the TOFU dataset as the target models. See the Appendix B.1 for details. Additionally, we also construct the target set, forget set, retain set, and evaluation set required for the experiment. The dataset collection and composition are as follows (more details can be found in Appendix B.2): \n\n\u2022 Target Set: For the target entity, we select the original training dataset with 20 question-answer pairs in TOFU as the target set. \u2022 Forget Set: In the experiments, we define two types of forget sets. The first type involves selecting the target set as the forget set for the unlearning algorithm, simulating an ideal scenario. \n\nThe second type is constructed by generating a probing set from the target model, following the methods outlined in Section 2.2.1. The probing set, used as the forget set, consists of 20 QA pairs related to specific entities. \u2022 Retain Set: Unlike TOFU, we construct a retain set using questions with greedy-decoding answers from TriviaQA (Joshi et al., 2017). These questions could be correctly answered by both Llama2-7B-Chat-TOFU and Phi-1.5-TOFU. \u2022 Evaluation Set: We assess the unlearned model on the evaluation set, including the target set, the retain set, the real authors set, and the world facts set. The latter two datasets, sourced from TOFU, are used to evaluate the retention of pre-training knowledge in unlearned models. \n\nUnlearning algorithms. We experiment with five common unlearning algorithms on the entitylevel unlearning task (more details can be found in Appendix B.3): \n\n\u2022 Gradient Ascent (Grad.",
            "score": 0.5834563630250719,
            "section_title": "Experimental Setup",
            "char_start_offset": 8897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 1909
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "MUSE divides data into two sets: the Forget set, containing the information to be unlearned, and the Retain set, which measures the impact of unlearning on unrelated knowledge. Ideally, unlearning should be precise, affecting only the Forget set without disturbing the Retain set. MUSE provides fine-tuned models for both sets as optimal reference points. To capture the training dynamics, we compute the average KL divergence between the unlearned model and the MUSE reference models over the Forget and Retain sets. An effective unlearning model should closely match both references, with near-zero divergence indicating successful unlearning and model performance preservation. \n\npress the generation of unwanted memorization at decode time. Task Arithmetic (TA) approaches (Ilharco et al., 2023) also fine-tune a model to memorize the forget set and leverage linear parameter merging (Matena and Raffel, 2022) to remove the memorization in model weights. Majmudar et al. (2022) apply linear interpolation with uniform distribution at the decoding time and show that this satisfies certain differential privacy (DP) criteria. Chen and Yang (2023) tune multiple unlearning layers to handle sequential unlearning requests and then fuse and plug them back into the base LLM. We set aside post-processing methods such as directly prompting LLMs to add a guardrail (Thaker et al., 2024); our focus is on removing knowledge directly from the base LLM via fine-tuning.",
            "score": 0.5828816635435368,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 6942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 777,
                    "end": 799,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 888,
                    "end": 913,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 959,
                    "end": 981,
                    "matchedPaperCorpusId": "249151985"
                },
                {
                    "start": 1129,
                    "end": 1149,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65185546875
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "The primary objective in language modeling is to minimize the negative log-likelihood of token sequences, training the model to predict the next word in a sequence accurately. Knowledge unlearning (Jang et al., 2023) involves negating this objective to remove specific learned information from the model. Instead of reinforcing certain sequences, unlearning aims to decrease their probabilities by maximizing their negative log-likelihood, which can be understood as equivalent to removing the negative sign: \n\nwhere x comes from a sequence of tokens x f \u2208 D f and p \u03b8 (x t |x <t ) denotes the conditional probability of predicting the next token given the model parameters \u03b8. This effectively reverses the learned patterns, reducing the probability of generating the targeted sequences and allowing the model to \"forget\" specific knowledge.",
            "score": 0.5826063369765296,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 6848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 841
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "In this study, we introduced and explored the concept of \"in-context knowledge unlearning\" within the framework of Large Language Models (LLMs) through the use of fine tuning. Our findings demonstrate that this approach not only enables LLMs to dynamically \"forget\" or selectively disregard information in test-time but also uncovers a nuanced behavior of LLMs-where they \"pretend to forget\" rather than actually eliminating the information from their knowledge base. \n\nThe ability of LLMs to learn to \"unlearn\" in both in-domain and out-of-domain scenarios without compromising their overall performance represents a significant step forward in the quest for more ethically responsible and privacy-conscious AI technologies. This capability is crucial for applications where sensitive or confidential information must be managed with great care, such as in healthcare, legal, and educational sectors. \n\nOur in-context knowledge unlearning method faces two main limitations: \n\n\u2022 Application to Closed Models: The method is difficult to apply to closed models accessible only via APIs (e.g., GPT-3, ChatGPT). These models do not allow modifications to their architecture or training procedure, which are necessary for implementing our unlearning tokens and loss functions. For instance, we cannot add the \u00abUNL\u00bb tokens or fine-tune the model to recognize them in such closed systems. \n\n\u2022 Lack of Internal Behavior Analysis: For closed models, we cannot analyze the internal unlearning process. This prevents us from observing how the model's internal representations change during the unlearning process, as we did with the logit lens analysis for open models like LLaMA2 and Mistral. Consequently, we cannot verify if the \"pretend to forget\" behavior occurs in closed models or optimize the unlearning process for better performance. \n\nThese limitations highlight the challenges in implementing and fully understanding our approach in environments with limited model transparency and configurability, particularly in widely-used commercial AI systems.",
            "score": 0.582310581376166,
            "section_title": "Conclusion",
            "char_start_offset": 19127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2050
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.433349609375
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "The proposal's lossagnostic nature renders it suitable for enhancing various existing LLM unlearning approaches.\n\n\u2022 We conduct thorough experiments across various LLM unlearning tasks, models, and evaluation metrics, consistently showing the effectiveness of Examples of text outputs from LLMs post unlearning using various approaches, including FO GradDiff (gradient difference) (Liu et al., 2022;Maini et al., 2024) and PO (preference optimization) (Maini et al., 2024;Eldan and Russinovich, 2023), as well as their SO counterparts.Failed unlearning is indicated by undesired answers marked in red, while successful unlearning is highlighted in green for desired answers.(Right) Quantitative evaluation comparing SO unlearning with FO unlearning using the metrics forget quality and model utility, as detailed in Sec. 5.\n\nSOUL in improving LLM unlearning, as exemplified in Fig. 1.",
            "score": 0.5822184058731951,
            "section_title": "Introduction",
            "char_start_offset": 4760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 114,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 822
                },
                {
                    "start": 824,
                    "end": 883
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 398,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345703125
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "We compare our framework with several strong unlearning approaches and various baselines: Original: \n\nThe \"original\" model without any unlearning applied. GradAscent+: This method begins with the original model and finetunes it on both the retain and forget sets, using gradient ascent on the latter. Previous work (Jang et al., 2023) examined a weaker baseline that only trains on the forget set with gradient ascent. We enhance GradAscent+ to achieve a better balance between retention and forgetting. NegTaskVector+: This approach also starts from the original model but finetunes two separate models, one on the forget set and another on the retain set. During inference, the weights of the forget-set-tuned model are negated, while the retained weights are added. Prior research (Ilharco et al., 2023) explored a weaker baseline training only on the forget set. Our refined version includes explicit retention tuning. Oracle: Serves as a reference point where our proposed method is applied one language at a time. This represents the \"pseudo\" upper bound performance of our approach, achieved inefficiently as the number of languages increases, i.e., O(Z). We do not directly compare with other teacher-student frameworks for unlearning (Chundawat et al., 2023;Kurmanji et al., 2023), as their training objectives involve a classification loss to forget a class label. Instead, we evaluate our adaptive unlearning scheme against the general knowledge distillation framework to demonstrate its effectiveness, as detailed in \u00a75.3.",
            "score": 0.5818551938633385,
            "section_title": "Baselines",
            "char_start_offset": 11045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1534
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1267,
                    "matchedPaperCorpusId": "248834527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "273323555",
            "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
            "text": "As shown in Figure 2, we find that both RMU and GD successfully reduce the accuracy after performing unlearning. RIA leads to less significant reductions in accuracy. For all methods, RTT recovers the forget accuracy close to its original level, which suggests that most of the information was hidden, not removed from the weights. \n\nTo quantify the quality of an unlearning technique in removing information, we consider Recovery Rate: the ratio of accuracy on V of the unlearned model after RTT to the accuracy on V of the original model after RTT: Recovery Rate = Accuracy on V of the unlearned model after RTT Accuracy on V of the original model after RTT \n\nA lower recovery rate corresponds to more successful information removal. In our tests, all recovery rates were greater than 88%, implying poor performance at removing information. \n\nTo test whether the retain loss is restricting unlearning methods from appropriately removing the information from the weights, we run unlearning with different unlearning strengths to achieve different values for the retain accuracy. Figure 3 shows that even with large losses in the retain accuracy, RTT is able to recover accuracy on the forget dataset. Even if we don't include a retain loss, RTT is often able to recover forget accuracy (see Appendix G, Figure 10). RTT recovering accuracy even when the model is not incentivized to retain performance on other tasks implies that the unlearning methods are not restricted by having to maintain good performance on the retain dataset. Figure 4: Forget accuracies for different formats of the unlearning dataset. We perform unlearning and RTT for different text formats and loss types when using RMU and GD. The unlearning strength is such that the loss in the retain accuracy is less than or equal to 5%. All of the runs were done using the WMDP-Deduped dataset. Figure 5: Our approach to creating a model that hides knowledge: by controlling which layers are fine-tuned, we ensure that the information is still present in the model weights.",
            "score": 0.5816276972064816,
            "section_title": "RESULTS",
            "char_start_offset": 17866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.481689453125
        },
        {
            "corpus_id": "271860124",
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "text": "This motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
            "score": 0.5810760658828873,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 459,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 464,
                    "end": 481,
                    "matchedPaperCorpusId": "267681958"
                },
                {
                    "start": 1129,
                    "end": 1146,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1478,
                    "end": 1501,
                    "matchedPaperCorpusId": "269791237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74462890625
        },
        {
            "corpus_id": "274822624",
            "title": "Toward Efficient Data-Free Unlearning",
            "text": "We theoretically analyzed and experimentally demonstrated the inefficiency in retaining knowledge during data-free unlearning when using Data-free Knowledge Distillation (DFKD). Our findings show that enriching the information related to retaining classes during distillation significantly enhances the student model's learning of retaining-related knowledge. We propose the Inhibited Synthetic PostFilter (ISPF) to achieve this from two perspectives: 1) reducing the synthesis of forgetting class information and 2) fully leveraging the retaining-related information in the synthesized samples. Experimental results confirm that ISPF effectively overcomes this challenge and outperforms existing methods. \n\nand approximate unlearning. The former (Bourtoule et al. 2021;Yan et al. 2022;Kim and Woo 2022) usually entails excluding the data to be forgotten (forgetting data) from the training data set, followed by the retraining of a model on the retained data. These methods inevitably necessitate access to the original training data set. The latter is achieved by tuning the parameters of the trained model so that the performance of the tuned model approximates that of the retrained model. These approaches primarily use forgetting data for gradient ascent (Thudi et al. 2022) or modifying the most relevant network parameters (Fan et al. 2023;Foster, Schoepf, and Brintrup 2024), and retaining data to repair the performance degradation caused by modifying the parameters. All of these methods necessitate access to real data to identify learning objectives and achieve unlearning. Due to storage expenses and privacy concerns, real training data are often deleted or archived post-training, leading to scenarios where unlearning methods must operate without full access to the real dataset. UNSIR (Tarun et al. 2023) eliminates access to forgetting data by learning proxy noise samples. Boundary Unlearning (Chen et al. 2023) requires only forgetting data and tuning the model with relabeled forgetting data. In contrast to these methods, our method does not require access to real data, neither forgetting nor retaining data.",
            "score": 0.5808492504067273,
            "section_title": "Conclusion",
            "char_start_offset": 29346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 770,
                    "end": 786,
                    "matchedPaperCorpusId": "209405263"
                },
                {
                    "start": 786,
                    "end": 803,
                    "matchedPaperCorpusId": "251025393"
                },
                {
                    "start": 1261,
                    "end": 1280,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1803,
                    "end": 1822,
                    "matchedPaperCorpusId": "246015506"
                },
                {
                    "start": 1913,
                    "end": 1931,
                    "matchedPaperCorpusId": "257636742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "273186481",
            "title": "Gradient Routing: Masking Gradients to Localize Computation in Neural Networks",
            "text": "Baselines. Expand, Route, Ablate is compared against the following baselines. \n\nData filtering removes all forget stories from the corpus and then pre-trains on the remaining stories. To operationalize data filtering as an unlearning method, we start with a base model that was trained on all of the stories. Unlearning, then, is constituted by re-initialization of the weights and training on the filtered dataset, as if from scratch. This serves as a kind of gold standard for unlearning, since in the 100% labeling case it means that forget data has zero influence on model weights. \n\nRMU (Li et al., 2024) works by corrupting a base model's internal representations on forget data and preserving its representations on retain data. We train the W out matrix in the MLP of the first 6 layers of the model. The learning target for the output of these combined layers is (a) a random vector of norm 100 on stories from the forget set, or (b) the original activation on stories from the retain set. We assign 200 times greater weight to the retain loss than the forget loss, use 500 steps of training with batch sizes of 80, and a learning rate of 5 \u00d7 10 \u22124 . \n\nDEMix plus ablation replaces all MLP layers with DEMix layers Gururangan et al. (2021) comprised of a \"retain expert\" and a \"forget expert,\" which are of the same type as the original MLP layers. When training on retain data (or unlabeled forget data), the retain experts are used. When training on (labeled) forget data, the forget experts are used. After training, we ablate the forget experts and use the retain experts for evaluation. The idea is to test whether this will enable robust removal of capabilities similarly to how ERA does. \n\nWhen combining ERA and RMU, RMU is applied normally after all steps of ERA have completed. \n\nExpand, Route, Ablate settings. The following settings are used for the training process described in section 4.2.2. \n\n\u2022 Target layers: {0, 1, 2, 3, 4}. \n\n\u2022 Dimensions added: 64 MLP neurons in each of the target layers.",
            "score": 0.580333670508266,
            "section_title": "C TINYSTORIES UNLEARNING DETAILS",
            "char_start_offset": 46495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 10
                },
                {
                    "start": 11,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1796
                },
                {
                    "start": 1799,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 1224,
                    "end": 1248,
                    "matchedPaperCorpusId": "236976189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27099609375
        },
        {
            "corpus_id": "272524187",
            "title": "Unlearning or Concealment? A Critical Analysis and Evaluation Metrics for Unlearning in Diffusion Models",
            "text": "(Tarun et al. 2023b) propose an efficient method that balances speed and effectiveness. (Sinha, Mandal, and Kankanhalli 2024) addresses the challenge of unlearning in multimodal recommendation systems with diverse data types, employing Reverse Bayesian Personalized Ranking to selectively forget data while maintaining system performance. Additionally, (Sinha, Mandal, and Kankanhalli 2023) applies knowledge distillation for unlearning in graph neural networks. In diffusion models, unlearning techniques include (Kumari et al. 2023) concept elimination via ablating concepts in the pretrained model. (Zhang et al. 2024;Heng and Soh 2024;Gandikota et al. 2023) propose text-guided concept erasure in diffusion models. (Kim et al. 2023) adapt knowledge distillation to remove forget concepts from the diffusion models. (Fuchi and Takagi 2024) use a few-shot unlearning approach for the text encoder. These methods aim to selectively remove concepts or data influences without requiring full model retraining. \n\nEvaluation Metrics for Unlearning in Diffusion Models. Zhang et al. (Zhang et al. 2024) proposed M-Score and ConceptBench for forget set validation. The work doesn't address retain set quantification. Kumari et al. leverage a set of metrics to assess their concept ablation method in textto-image diffusion models (Kumari et al. 2023). These include CLIP Score (Hessel et al. 2021) for measuring imagetext similarity in the CLIP feature space, CLIP accuracy for erased concepts, mean FID score to evaluate performance on unrelated concepts, and SSCD (Pizzi et al. 2022;Carlini et al. 2023) to quantify memorized image similarity. Fan et al. (Fan et al. 2023) state that the images generated by a retrained model should be considered the ground truth. However, retraining a model incurs significant computational costs, making it practically infeasible.",
            "score": 0.5802484507362284,
            "section_title": "A.4 Related Work",
            "char_start_offset": 44431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "259837117"
                },
                {
                    "start": 621,
                    "end": 639,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 639,
                    "end": 660,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 719,
                    "end": 735,
                    "matchedPaperCorpusId": "259837117"
                },
                {
                    "start": 1325,
                    "end": 1345,
                    "matchedPaperCorpusId": "259837117"
                },
                {
                    "start": 1372,
                    "end": 1392,
                    "matchedPaperCorpusId": "258740988"
                },
                {
                    "start": 1561,
                    "end": 1580,
                    "matchedPaperCorpusId": "247011159"
                },
                {
                    "start": 1580,
                    "end": 1600,
                    "matchedPaperCorpusId": "256389993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.556640625
        },
        {
            "corpus_id": "270440348",
            "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
            "text": "As Large Language Models (LLMs) continue to impress with their ability to learn from pre-training documents and apply this knowledge to real-world tasks like programming and question-answering, attention has increasingly focused on addressing the accompanying privacy issues [1,2].Machine unlearning [2][3][4][5][6][7][8], aiming to remove the influence of specific data, has become an important research area and is being used to remove sensitive information such as copyright contents from LLMs.\n\nGiven a target LLM, the conventional setting of LLM unlearning involves two goals [8,9].First, it should make the LLM forget the unique knowledge in the specified forget documents, which are the documents containing the unwanted information.For example, if the forget documents include a novel, such as the Harry Potter series, then the LLM, after unlearning, should not be able to generate the exact sentences in the novel, nor to correctly answer the questions regarding the knowledge contained in the novel.Second, the unlearning should not affect the other knowledge in the target Table 1: Example LLM responses to queries for different data knowledge along training process.Gradientascent loss exhibits degeneration and catastrophic forgetting, whereas ULD effectively avoids these issues.Responses are selected after epoch 1, 5, and 10.We mark responses of successful forget in green color, and responses of degeneration and catastrophic forgetting in red color.which cannot cover the vast knowledge in the target LLM that we wish to retain.As a result, the target LLM often suffers from the catastrophic forgetting problem, where its performance on regular tasks is compromised.Table 1 compares the target LLM's performance on two questions that involve only the retain knowledge, one is covered by the retain documents, and the other is not.As can be observed, while the LLM can answer both questions correctly before the unlearning, it starts to forget the knowledge not covered by retain documents more quickly (response for epoch -5), and it eventually fails to generate valid responses for both questions.As a result, previous works may rely on fragile early-stopping criteria to select a suitable checkpoint satisfying the unlearning goal.",
            "score": 0.5797285151412949,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 497
                },
                {
                    "start": 499,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1546
                },
                {
                    "start": 1546,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2116
                },
                {
                    "start": 2116,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 278,
                    "end": 280,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "232134970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3740234375
        },
        {
            "corpus_id": "270286011",
            "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
            "text": "The rapid advancements in large language models (LLMs) have led to remarkable breakthroughs in natural language processing and artificial intelligence.However, as these models are trained on vast amounts of data, they may inadvertently learn and perpetuate undesirable behaviors, biases, and harmful information.To address this issue, researchers have recently turned their attention to the concept of unlearning in LLMs.\n\nIn [18] paper, the authors explore the novel concept of unlearning in large language models (LLMs).They present a method that utilizes only negative examples to efficiently remove undesirable behaviors, demonstrating its effectiveness in alignment while significantly reducing computational resources compared to traditional reinforcement learning from human feedback (RLHF).While there is another paper introduces a data-driven unlearning approach for large language models (LLMs), utilizing a fine-tuning method informed by the importance of weights and relabeling during the pre-training phase of LLMs [19].This method adjusts word embedding, involving identifying and neutralizing bias vectors within the embedding space to prevent biased associations.Wang et al. [20] proposed an unlearning framework called Knowledge Gap Alignment (KGA), emphasizing its capability to efficiently handle large-scale data removal requests with significant accuracy.However, the inability of KGA to guarantee the complete removal of data influences also faces the challenge of maintaining extra data sets and models.Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.This approach not only advances the field of responsible AI but also opens new avenues for enhancing data privacy and model impartiality.Huang et al. claim an innovation offset unlearning framework tailored for the black box LLM [22].This framework effectively addresses the challenge of unlearning problematic training data in LLMs without requiring access to internal model weight, thus offering a versatile solution for adapting current unlearning algorithms.",
            "score": 0.5797241295649185,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 9557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57666015625
        },
        {
            "corpus_id": "271039803",
            "title": "Remembering Everything Makes You Vulnerable: A Limelight on Machine Unlearning for Personalized Healthcare Sector",
            "text": "Furthermore, recent adversarial assaults on trained models have showed the capacity to determine which instances or characteristics belonged to the training data.This needs a new method known as machine unlearning, which causes machine learning models to forget certain facts.Cao et al.Cao and Yang (2015) initially introduced the concept of machine unlearning, aiming to negate the influence of a specific data point on a trained model efficiently and precisely.This concept, which ensures that removing training data does not alter the model's distribution, was later formalized by Ginart et al. Ginart et al. (2019), who established foundational principles for designing data forgetting algorithms.These approaches, however, were primarily effective for non-adaptive ML models like kmeans clustering.To address this, Bourtoule et al.Bourtoule et al. (2021) proposed the SISA method, a model-agnostic technique that partitions training data into separate slices for individual model training, enabling precise unlearning but at increased storage costs.Golatkar et al.Shi et al. (2023) further advanced the field by introducing a technique to remove weights associated with to-be-forgotten data, eliminating the need for retraining.As the field evolved, various strategies emerged for estimating and mitigating the impact of removing training data on ML models, including influence functions, weight removal, linear replacement, and gradient updating.These methods provide approximate forgetting and mathematical guarantees for certified data removal in DNNs, marking machine unlearning as a burgeoning area of research with significant implications for model flexibility and ethical data management.",
            "score": 0.5795503675269374,
            "section_title": "Related work",
            "char_start_offset": 6370,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 836
                },
                {
                    "start": 836,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1233
                },
                {
                    "start": 1233,
                    "end": 1452
                },
                {
                    "start": 1452,
                    "end": 1701
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 618,
                    "matchedPaperCorpusId": "195886255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50439453125
        },
        {
            "corpus_id": "268553939",
            "title": "\u2207 \u03c4: Gradient-based and Task-Agnostic machine Unlearning",
            "text": "Several studies dive into the concept of unlearning.Existing works often focus on a particular subtask (e.g.removing bias, user privacy) and currently a standardized definition is missing.Nguyen et al. [18] present a comprehensive survey where different aspects and open questions of machine unlearning are addressed.\n\nUnlearning for User Privacy.In this setting, the objective is to remove the influence of some subset of samples to protect the data against privacy leaking attacks, such as Membership Inference.The current state-of-the-art for this particular task lacks consistency in its problem definition, framework, and evaluation methodology.Graves et al. [10] conceptualize unlearning as resistance to data-leakage attacks and perform label swapping to remove information.A more recent work by Chundawat et al. [5] defines unlearning solely as removing the information of forget set and use a randomly initialized incompetent teacher to approximate the nature of the removed information.Their focus consists in removing subclasses, where the samples to remove are not randomly selected, but share the same semantics (e.g.all images of a car).Foster et al. [7] present a retraining free method that aims to suppress the most influential weights for the forget set, but it only works for small enough forget set sizes (around 200 samples in the case of random subset removal).Kurmanji et al. [13] introduce an unlearning method based on bad teaching that works also in scenarios different from user privacy (such as removing biases).They discuss the resistance to privacy attacks, specifically Membership Inference Attacks, but evaluate their method only on class and subclass removal.\n\nIn these works, experiments frequently focus on subclass or class removal [5] [13]; this setting not only limits the model's usability in real-world use-cases, such as the need to remove a user's data, but also complicates the assessment of whether the model has truly forgotten training examples.Instead, in studies exploring random subset removal, researchers often use a forget set size of less than 1% [7].This scenario prevents using these methods in contexts requiring larger datasets to be removed.",
            "score": 0.579135878025959,
            "section_title": "Related Work",
            "char_start_offset": 5428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 317
                },
                {
                    "start": 319,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 781
                },
                {
                    "start": 781,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1130
                },
                {
                    "start": 1130,
                    "end": 1151
                },
                {
                    "start": 1151,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1692
                },
                {
                    "start": 1694,
                    "end": 1991
                },
                {
                    "start": 1991,
                    "end": 2104
                },
                {
                    "start": 2104,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "270620779",
            "title": "Mitigating Social Biases in Language Models through Unlearning",
            "text": "Early work on machine unlearning by Cao and Yang (2015) proposes the idea of a system that forgets data and its lineage to restore privacy, security, and usability by transforming learning algorithms into a summation form and updating a few summations.Similarly, Zhu et al. (2020) propose modifying specific factual knowledge in transformer models to make transformers forget.Another method proposed by Ilharco et al. (2022) uses task vectors to steer the behavior of neural networks by specifying the direction in the weight space of a pretrained model.Task vectors are used for forgetting via negation to mitigate undesirable behaviors of the language models (e.g., toxic generations), or to forget specific tasks.In model fusion (Zaman et al., 2023), shared knowledge of the models helps in enhancing the model capabilities, while unshared knowledge is usually lost or forgotten, which can be used for forgetting the biased information.Wang et al. (2023) propose an unlearning method that preserves the knowledge gap alignment between the original and debiased model.Zhang et al. (2023b) propose machine learning for privacy in LMs using the unlikelihood training objective to target token sequences with minimal impact on the performance of LLMs.Partitioned contrastive gradient unlearning (PCGU) (Yu et al., 2023) method debiases pretrained masked language models by systematically searching through a pre-trained masked language model to find the weights that contribute to bias and optimizes them.Similarly, another line of research uses influence functions for debiasing (Chen et al., 2023;Grosse et al., 2023).Influence functions are used to estimate how training examples impact predictions during testing.In some cases, data is divided into shards and models are trained on each shard and if a particular shard or part of the shard needs to be forgotten then only the parameter optimization of that smaller model is required (Bowman et al., 2023;Bourtoule et al., 2021).",
            "score": 0.5791221865632481,
            "section_title": "Related Work",
            "char_start_offset": 2791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 252,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 716
                },
                {
                    "start": 716,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1716
                },
                {
                    "start": 1716,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 55,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1301,
                    "end": 1318,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361328125
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
            "score": 0.5788268946925287,
            "section_title": "RELATED WORK",
            "char_start_offset": 4004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 605,
                    "matchedPaperCorpusId": "235474438"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "232404451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "268532505",
            "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Approximate Unlearning Completeness",
            "text": "They provide resource-efficient post-hoc solutions for removing data from large models by adjusting model parameters.Based on how they calculate the model update, we classify those unlearning algorithms into three groups:\n\nLog-based retrieval: Amnesiac Unlearning [18] directly subtracts the corresponding parameter updates, which have been logged during the training process, of the small batches containing targeted data from the model weights.Fisher Forgetting [16,17] approximates the scrubbing procedure of selective forgetting through a noisy Newton update, deriving it as reducing the KL divergence distance between two model distributions: one trained on the original dataset and the other trained on the retained dataset.They calculate the corresponding update by approximating the Hessian of the forgotten data using the Fisher Information Matrix.Despite there being a theoretical foundation for data deletion in linear models, approximating the influence of targeted samples in deep models remains challenging due to their non-convexity and the randomness of perturbations.Computation efficiency is also a concern when it comes to large models.Dynamics Masking: Forsaken [32] introduces a mask gradient generator that can iteratively generate mask gradients to \"stimulate\" neural neurons to unlearn the memorization of given samples.Selective Synaptic Dampening [13] uses the Fisher information matrix from training and forgetting data to identify key forget set parameters.Then, it dampens these based on their significance to the forget set relative to the overall training data.Jia et al. [26] explore the application of model sparsification via weight pruning in machine unlearning.These methods efficiently achieve machine unlearning by masking parameter dynamics of given samples but rely on explaining their opaque performance to evaluate their unlearning utility.This paper will primarily focus on unlearning commitment management for approximate algorithms.Since exact unlearning provides definitive unlearning, approximate unlearning operations are performed on the parameter space, making its unlearning utility less transparent, an evaluation step is necessary.Additionally, approximate unlearning is more acceptable for large models due to its computational efficiency.",
            "score": 0.5788050748924094,
            "section_title": "Machine Unlearning",
            "char_start_offset": 9856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 221
                },
                {
                    "start": 223,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1882
                },
                {
                    "start": 1882,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2184
                },
                {
                    "start": 2184,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "212628473"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "matchedPaperCorpusId": "236882730"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "260900355"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "276767847",
            "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
            "text": "The Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.",
            "score": 0.5786888369683587,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "268363571",
            "title": "Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning",
            "text": "Machine unlearning has garnered significant attention due to its ability to selectively erase knowledge obtained from specific training data samples in an already trained machine learning model. This capability enables data holders to adhere strictly to data protection regulations. However, existing unlearning techniques face practical constraints, often causing performance degradation, demanding brief fine-tuning post unlearning, and requiring significant storage. In response, this paper introduces a novel class of machine unlearning algorithms. First method is partial amnesiac unlearning, integration of layer-wise pruning with amnesiac unlearning. In this method, updates made to the model during training are pruned and stored, subsequently used to forget specific data from trained model. The second method assimilates layer-wise partial-updates into label-flipping and optimization-based unlearning to mitigate the adverse effects of data deletion on model efficacy. Through a detailed experimental evaluation, we showcase the effectiveness of proposed unlearning methods. Experimental results highlight that the partial amnesiac unlearning not only preserves model efficacy but also eliminates the necessity for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover, employing layer-wise partial updates in label-flipping and optimization-based unlearning techniques demonstrates superiority in preserving model efficacy compared to their naive counterparts.",
            "score": 0.5786011600140776,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69091796875
        },
        {
            "corpus_id": "272827145",
            "title": "Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting",
            "text": "The previous machine unlearning algorithms typically rely on two main concepts: (1) model manipulation, and (2) data manipulation. Firstly, various studies have addressed the machine unlearning problem by directly manipulating the parameters of the model to erase specific information. For instance, the Fisher Forgetting (Golatkar, Achille, and Soatto 2020a) method scrubs the model by directly adding specific noises to the parameters using the inverse of the Fisher information matrix. Another approach, SCRUB (Kurmanji, Triantafillou, and Triantafillou 2023), improves forgetting performance by using a teacher model that is a clone of the original model. This method trains the unlearned model by minimizing the KL divergence between the output probability of the unlearned model (\u03b8 unlearned ) and that of the teacher model (\u03b8 teacher ). Similarly, the BadTeaching (Chundawat et al. 2023a) method employs three models: a competent teacher, an incompetent teacher, and a student (unlearned model \u03b8 unlearned ). The student model is trained to mimic the competent teacher on the D retain while following the incompetent teacher on the D f orget . These methods highlight the effectiveness of teacher-student models in enhancing machine unlearning performance. \n\nOn the other hand, some methods focus on data manipulation. For example, UNSIR (Tarun et al. 2023) generates artificial noise that is added to the data to maximize the loss values for a specific target class that needs to be forgotten. Training on these error-maximized data points has shown good forgetting performance. Building on this, another method (Chundawat et al. 2023b) uses samples to be retained to improve unlearning scores, extending the work of UNSIR. Similarly, recent works (Cha et al. 2024) use intentionally perturbing noise to increase the loss value, focusing primarily on error-maximizing synthesized images to achieve a high forgetting score. \n\nHowever, we argue that this error-maximizing approach can easily lead to correlation collapse, where the useful correlations between features and labels degrade.",
            "score": 0.578463337974866,
            "section_title": "Related Work",
            "char_start_offset": 4227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 1345,
                    "end": 1364,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 1620,
                    "end": 1643,
                    "matchedPaperCorpusId": "15966283"
                },
                {
                    "start": 1756,
                    "end": 1772,
                    "matchedPaperCorpusId": "256358864"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489501953125
        },
        {
            "corpus_id": "271909642",
            "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models",
            "text": "MUSE is a comprehensive machine unlearning evaluation benchmark that considers six desirable properties for the un-learned models. \n\n(1) No verbatim memorization. The model should not replicate the content in the forget set. The benchmark quantifies this by prompting the model with the first l tokens from a sequence in the forget set and comparing the model's completion with the true completion using the ROUGE-L F1 score. \n\n(2) No knowledge memorization. The model should not be able to answer questions about the knowledge in the forget set. The benchmark provides QA-style probes and measures the relevance of the model's answer to the ground truth answer using the ROUGE score. \n\n(3) No Privacy Leakage. Similar to the MIA attack mentioned above, it should be impossible to detect whether the unlearned model was trained on a specific piece of text. The benchmark provides a metric called PrivLeak, for which a good unlearning algorithm should achieve a value close to zero. \n\n(4) Utility Preservation. The model's performance on the retain set should be preserved. The benchmark provides QAstyle probes on the retain set and uses ROUGE scores to measure the model's performance. \n\nTwo additional evaluation perspectives are scalability and sustainability, which focus on varying sizes of forget sets and successive unlearning requests, respectively. In this paper, we focus on the first four evaluation perspectives and leave the more challenging two scaling perspectives for future research. \n\nMUSE provides two representative types of textual data in unlearning tasks: news articles and books, and we conduct unlearning experiments on the news corpus.",
            "score": 0.578452609239851,
            "section_title": "A.2 MUSE",
            "char_start_offset": 28354,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 133,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 981
                },
                {
                    "start": 984,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1500
                },
                {
                    "start": 1503,
                    "end": 1661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67041015625
        },
        {
            "corpus_id": "270703035",
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "text": "In this section, we introduce the experimental and evaluation setup and evaluation methods for the new structural LLMs unlearning considerations. \n\nMetrics. We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data. In contrast, a higher DS reflects poorer unlearning, suggesting a weaker distinction between forget and retained knowledge. More details and other supplementary metrics, including the original ROUGE1 scores, MRR and the Top Hit Rate, can be found in Appendix D. Unlearning baselines We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024). Given the nascence of the field, existing unlearning methods often lack robustness. However, these methods represent the current mainstream and serve well to demonstrate the impact of structural datasets while inspiring further research. Base models. We evaluate all baseline methods using the current widely adopted language models Llama2-7B (Touvron et al., 2023), Gemma-7B (Team et al., 2024) and Mistral-7B (Jiang et al., 2023). We evaluated learning rates between 1\u00d710 \u22126 and 5\u00d710 \u22125 during unlearning and found that all methods are highly sensitive to learning rate and batch size selection.",
            "score": 0.5783269814514724,
            "section_title": "Evaluation Setup",
            "char_start_offset": 12700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 1254,
                    "end": 1272,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Prior work in fact unlearning from LLMs focuses on simply unlearning the target fact in isolation, and not other facts that logically imply it. This might cause the LLM to forget only this one specific fact, but retain others that can be combined to deduce the fact in question. In this section, we introduce the new setting of unlearning, deep unlearning, which considers the logical deductions between facts. To assess the effectiveness of unlearning methods in this setting, we propose two evaluation metrics: recall and accuracy.",
            "score": 0.5782387156680538,
            "section_title": "DEEP UNLEARNING",
            "char_start_offset": 6018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 533
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "270045706",
            "title": "Towards Natural Machine Unlearning",
            "text": "The underlying challenge is that incorrect information, i.e., the forgetting samples with their incorrect labels, is undesirably reinforced during fine-tuning.The unlearning process is quite unnatural since it compels the unlearned model to learn incorrect information, which actually conflicts with the remaining data.This adversely affects the model's natural ability to generalize these samples, which the retrained model preserves.For example, in random-subset unlearning, most of the forgetting samples can still be correctly classified by the retrained model, even if they are not involved in the training.Essentially, this unlearning process alters the learned knowledge to incorrect knowledge, instead of removing it.In addition to forgetting accuracy, various metrics can be also employed to measure the differences between the unlearned and retrained models, which will be discussed in Sec.4.1.Despite meticulous hyperparameter tuning can make one metric close to the retrained model, it cannot ensure that other metrics will also be close.\n\nTherefore, it is crucial to realize a more natural machine unlearning while ensuring its efficiency.On one hand, the forgetting samples should be included to enhance unlearning efficiency.On the other hand, excluding incorrect information ensures a natural machine unlearning process.In this paper, we propose a novel method, named NatMU, towards natural machine unlearning.The core idea of NatMU is to inject information extracted from the remaining data into the forgetting samples to reduce the conflicts in each modified instance.",
            "score": 0.5782046524025847,
            "section_title": "Revisiting Previous MU Methods",
            "char_start_offset": 12035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 900
                },
                {
                    "start": 900,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1050
                },
                {
                    "start": 1052,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1240
                },
                {
                    "start": 1240,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1586
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39697265625
        },
        {
            "corpus_id": "272770202",
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "text": "Existing unlearning methods in this setting often lead to incoherent or inconsistent responses from unlearned LLMs (see Figure 1), including responses related to the forgotten knowledge, which is undesirable. Such behaviors may unintentionally reveal details about the unlearning process or the forgotten data, posing potential privacy risks and increasing the model's susceptibility to membership inference attacks (Chen et al., 2021;Shi et al., 2024;Duan et al., 2024). The goal of unlearning in LLMs is to reduce memorization or prevent the leakage of information specific to the forgotten set, while maintaining the model's overall behavior and performance. Striking this balance is challenging and requires careful consideration of both the effectiveness of unlearning and the model's overall performance (Liu et al., 2024b). \n\nTo address the aforementioned challenges, we propose a novel method, AltPO-(Alternate-Preference Optimization), which ensures stable and effective unlearning by incorporating additional positive feedback for plausible alternative answers to the forgotten data, along with negative feedback targeting the knowledge to be erased. This approach enables the model to forget specific information while maintaining the ability to generate coherent and consistent responses. Additionally, recognizing the shortcomings of current evaluation metrics for unlearning in question-answering tasks, we introduce new metrics specifically designed to better evaluate the impact of unlearning on response quality related to forgotten knowledge. Our main contributions are as follows: \n\n\u2022 Algorithm: We propose a novel unlearning method AltPO using alternate responses and adapting the model to these while contrasting against the LLM's existing knowledge (Section 3). \u2022 Discovery and evaluation of failure modes: \n\nWe point out failure modes of prior approaches that are not captured by existing metrics and introduce new evaluation metrics to address these gaps (Section 4). \u2022 Empirical evaluation: We perform extensive experimentation and ablation tests for each component of our approach on the TOFU dataset, showing that AltPO-unlearned models achieve the highest unlearning scores on the existing metrics, while also achieving better and near-perfect scores on both existing and new evaluation metrics (Section 6).",
            "score": 0.5781918256272366,
            "section_title": "Introduction",
            "char_start_offset": 1629,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 416,
                    "end": 435,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 435,
                    "end": 452,
                    "matchedPaperCorpusId": "264451585"
                },
                {
                    "start": 452,
                    "end": 470,
                    "matchedPaperCorpusId": "267627639"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "271212828",
            "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
            "text": "Jang et al. (2022) shows that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances. Maini et al. (2024) tries to unlearn the memorized information in LLMs by relabeling the target data with uninformed answers such as \"I don't know\". We believe that these methods have their drawbacks: gradient ascent is sensitive to hyperparameters and could easily cause model training to crash; simply allowing the model to learn to respond with uninformed answers could easily affect the model's performance on the retain set. Therefore, we propose Name-Aware Unlearning Framework, to mitigate these issues and achieve a better balance between privacy protection and model performance.",
            "score": 0.5778958985515398,
            "section_title": "A Related Work",
            "char_start_offset": 25842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 783
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "267334976",
            "title": "CaMU: Disentangling Causal Effects in Deep Model Unlearning",
            "text": "Machine unlearning is the deliberate process of erasing information that a machine-learning model has previously acquired from training data [20,23,28]. It plays a crucial role in preserving privacy by allowing the removal of sensitive information [23]. With the widespread adoption of deep learning models, the concept of deep model unlearning [1,4,5,25] has gained significant attention, particularly in contexts dealing with sensitive data, such as in recommender systems or medical prediction [3]. The objective of machine unlearning is to selectively remove information associated with forgetting data from the pre-unlearning model while retaining the knowledge contained in the remaining data. \n\nCurrent research in deep model unlearning can be categorized into two main approaches based on the availability of the remaining data. One category, known as training-based methods, involves fine-tuning the model using the remaining data [1]. The other category, referred to as adjustment-based methods, utilizes the forgetting data only to adjust the model [4,5,25]. Despite the success of these methods in certain tasks, their overall performance remains unsatisfactory due to the potential occurrence of insufficient unlearning or excessive unlearning [20,28]. \n\nDespite the fact that insufficient unlearning and excessive unlearning diverge in the extent to which they remove information, they stem from a common underlying cause: the intricate intertwining of information between the forgetting data and remaining data. This intertwining may be attributed to the knowledge shared by both types of data, such as background identification and feature extraction. When these two types of data are highly interdependent, preserving common latent information becomes essential; otherwise, excessive unlearning may occur [19,20]. However, if too much information is retained, the post-unlearning model's performance on forgetting data may experience minimal alteration, resulting in insufficient unlearning [4,6]. \n\nTo illustrate, consider the example in Figure 1(a), where the remaining data comprises images of ginger and grey cats, while the forgetting data includes an image of a black cat. In this example, the pre-unlearning model contains latent information about a black cat.",
            "score": 0.5764216071910236,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 151,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 348,
                    "end": 350,
                    "matchedPaperCorpusId": "257636742"
                },
                {
                    "start": 350,
                    "end": 352,
                    "matchedPaperCorpusId": "248834527"
                },
                {
                    "start": 352,
                    "end": 355,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1060,
                    "end": 1063,
                    "matchedPaperCorpusId": "257636742"
                },
                {
                    "start": 1063,
                    "end": 1065,
                    "matchedPaperCorpusId": "248834527"
                },
                {
                    "start": 1065,
                    "end": 1068,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 2008,
                    "end": 2011,
                    "matchedPaperCorpusId": "257636742"
                },
                {
                    "start": 2011,
                    "end": 2013,
                    "matchedPaperCorpusId": "246015506"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.347412109375
        },
        {
            "corpus_id": "270440333",
            "title": "A More Practical Approach to Machine Unlearning",
            "text": "Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].Neel et al. (2021) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters [10].Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14].\n\nRecent studies have also focused on the embedding layer's role in the unlearning process.Jang et al. (2022) highlight the critical function of the embedding layer in representing input tokens, making it an effective focal point for unlearning operations [8].Eldan and Russinovich (2023) further explore the potential of embedding-layer unlearning, finding that targeting this layer can efficiently reduce the influence of specific data points without significantly impacting the model's overall performance [4].",
            "score": 0.5763988909379727,
            "section_title": "Gradient-Based Unlearning",
            "char_start_offset": 1762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 303
                },
                {
                    "start": 303,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 729
                },
                {
                    "start": 731,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1242
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 321,
                    "matchedPaperCorpusId": "220364296"
                },
                {
                    "start": 481,
                    "end": 485,
                    "matchedPaperCorpusId": "220364296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "277596426",
            "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models",
            "text": "Table 3 presents performance of the top teams when their unlearning algorithms are applied to 7B and 1B models. AILS-NTUA achieved the best per- \n\nTable 2: Key ideas explored in participating teams, sorted based on their performance on 7B model. formance with both the 1B and 7B models, as their system excels across all three metrics. While ZJUKLAB performed better on Task Aggregate and MMLU scores for the 7B model, their submission significantly underperformed on the MIA score suggesting the unlearned information was not completely removed from model parameter space, and also highlighting a trade-off between MIA and the Task Aggregate scores (also observed in (Ramakrishna et al., 2024)). \n\nResults for both models are largely consistent, with three teams (AILS-NTUA, Mr. Snuffleupagus, and ZJUKLAB) ranking in the top five posi-tions on both leaderboards. As discussed earlier, Atyaephyra had a bug in their submission which was addressed before 1B evaluations thereby gaining several positions. \n\nFinally, a handful of teams which were disqualified in 7B evals due to a drop in their MMLU utility recovered higher positions in the 1B leaderboard. Notably, SHA256 achieved a high Final Score (0.711), Task Aggregate (0.964), and MIA Score (0.894) with the 7B model. However, their MMLU score (0.275) dropped below the pre-defined threshold of 0.371, suggesting a substantial drop in overall model utility after unlearning. As a result, their  system was regrettably disqualified in 7B evals but retained for 1B. \n\nTable 4 presents task wise breakdown of top 5 teams in the 7B model. Results show that the top three systems achieve nearly perfect performance on the forget set, demonstrating the effectiveness of their methods in reducing regurgitation and removing knowledge from the LLMs. However, in several cases the performance on the retain sets drops considerably, suggesting over-unlearning, leading to unintended forgetting of relevant information from the model.",
            "score": 0.5763656462651862,
            "section_title": "Results and Discussion",
            "char_start_offset": 15236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 144
                },
                {
                    "start": 147,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 696
                },
                {
                    "start": 699,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330078125
        },
        {
            "corpus_id": "265281253",
            "title": "DeepClean: Machine Unlearning on the Cheap by Resetting Privacy Sensitive Weights using the Fisher Diagonal",
            "text": "Machine unlearning was first introduced by Cao and Yang in 2015 with the goal to make machine learning systems forget [2]. Increasing interest due to privacy regulations ('right to be forgotten') and potential use for error and bias removal has led to more studies, many that are model agnostic [23-25, 31, 32]. Methods for unlearning in a trained model are typically either exact or approximate, differentiated by completely or partially removing the influence of specific data points. Methods have been proposed for unlearning over data sample, class, feature, sequence, and graph [24,31]. A straightforward and exact method of unlearning is to re-train the model without the sensitive data in the training set. However, this is impractical for many use-cases where the trained model has required significant expense to learn, potentially millions of dollars for the largest models today. Alternatively, a model could be fine-tuned on data to be retained, however this can lead to catastrophic forgetting [16] of unseen data points and may incompletely remove information that should be forgotten. More sophisticated exact approaches have been proposed [29,33]. In this paper, we contribute new methods for the approximate category of unlearning that makes limited parameter updates to the model to approximate exact unlearning. Approximate methods are generally more cost effective and efficient, scaling to large models. Within approximate methods, approaches can be divided into a variety of data reorganization or model manipulation techniques. The DeepClean approach presented here is an improved model-agnostic approximate method in the category of model manipulation, supporting sample and class requests, and is computationally efficient. Prior work in approximate machine unlearning for model manipulation includes the use of influence functions [11,18] and approximation of the Fisher Information Matrix [30]. \n\nInfluence functions have emerged as one popular approach for machine unlearning. Proposed by Koh and Liang in 2017 [18], this method leverages influence functions to identify individual training examples that have an outsized influence on a model's predictions. Recent researches conducted by [8], [14] and [22] show that by removing or downweighting these influential points, the model can \"forget\" specific attributes without requiring full retraining.",
            "score": 0.5760318417335142,
            "section_title": "Related work",
            "char_start_offset": 4391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2185
                },
                {
                    "start": 2186,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 587,
                    "end": 590,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1155,
                    "end": 1159,
                    "matchedPaperCorpusId": "239616091"
                },
                {
                    "start": 1159,
                    "end": 1162,
                    "matchedPaperCorpusId": "250633553"
                },
                {
                    "start": 1861,
                    "end": 1864,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 1916,
                    "end": 1920,
                    "matchedPaperCorpusId": "220128049"
                },
                {
                    "start": 2039,
                    "end": 2043,
                    "matchedPaperCorpusId": "13193974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "Our evaluation criteria are tailored to each unlearning scenario.For example, in harmful Q&A scenarios, we employ a metric to measure output harmfulness.In knowledge unlearning scenarios, we evaluate the extent of content leakage in the original outputs.For model hallucination scenarios, we assess the accuracy of the model's outputs.Traditional machine unlearning domains typically use Membership Inference Attacks (MIA) for evaluation.However, this approach is not directly applicable to large language models due to their unique objectives and outcomes in unlearning.Consequently, we have developed novel evaluation methods specific to large language model unlearning.In assessing regular prompt reasoning and generation, we employ two approaches:\n\nOutput Similarity: This method evaluates the generation of regular Q&A.We use BLEURT [39] as the metric to measure output text similarity before and after unlearning.Higher similarity scores indicate minimal impact of unlearning fine-tuning on the model's regular reasoning capabilities.\n\nFluency: To assess output quality, we use a causal language model to encode and calculate the loss for the question alone, then combined with the answer for total loss.This is applied to reference LLMs to evaluate the perplexity of generated texts.Perplexity, rooted in entropy within conditional probability models, serves as an indicator of reasonable outputs by the unlearning LLM, valid only when output diversity is not exceedingly low.\n\nDiversity: Diversity metrics evaluate the richness of model outputs, tracking the percentage of unique vocabulary usage in texts.High diversity implies that the large language model has produced non-trivial, high-quality, and valuable outputs.",
            "score": 0.5758467262030349,
            "section_title": "Unlearning Effectiveness",
            "char_start_offset": 24358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 335
                },
                {
                    "start": 335,
                    "end": 438
                },
                {
                    "start": 438,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 672
                },
                {
                    "start": 672,
                    "end": 751
                },
                {
                    "start": 753,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1040
                },
                {
                    "start": 1042,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1483
                },
                {
                    "start": 1485,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1728
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32568359375
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "Unlearning Methods for LLMs. Machine unlearning is broadly categorized into exact unlearning, which ensures the model is indistinguishable from one retrained without the forget data, and approximate unlearning, which efficiently modifies existing model parameters to approximate this effect without full retraining. Due to the cost of retraining LLMs, most unlearning applied to LLMs (including ours) falls into the second category. One such approach trains the model to output an uninformative response instead of knowledge in the \"forget set\" via RLHF (Ouyang et al., 2022), maximizing the probability of a predefined response like, \"I don't know\" (Wen et al., 2024) Model Editing for Unlearning. Model editing provides an alternative approach to unlearning by directly modifying model weights to forget target facts (De Cao et al., 2021;Dai et al., 2022;Mitchell et al., 2022;Meng et al., 2022). \n\nThis method aligns with privacy requirements (Zhang et al., 2024a), avoids data-side interventions (Debenedetti et al., 2024), and protects against white-box extraction attacks. Following model editing work like Patil et al. (2024b), our framework employs LoRA-based weight updates for controlled unlearning via standard unlearning objectives (See Appendix B.6 for more details). \n\nCoreset Selection. Coreset selection identifies representative subsets that preserve key dataset properties, improving computational efficiency. Given the NP-hard complexity of the exhaustive search, methods have focused on optimizing coverage, diversity, or importance (Sener & Savarese, 2018;Tan et al., 2023). By recognizing unequal contributions of data points, coreset selection has proven effective in supervised learning (Wei et al., 2015;Killamsetty et al., 2021b;a), enabling efficient performance. Our work forms new connections between these methods and the problem of unlearning in LLMs, where preserving utility and minimizing collateral damage are critical.",
            "score": 0.5758428405084065,
            "section_title": "Background and Related Work",
            "char_start_offset": 8965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 554,
                    "end": 575,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 819,
                    "end": 840,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 840,
                    "end": 857,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 857,
                    "end": 879,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 879,
                    "end": 897,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1000,
                    "end": 1026,
                    "matchedPaperCorpusId": "261697333"
                },
                {
                    "start": 1113,
                    "end": 1133,
                    "matchedPaperCorpusId": "275302239"
                },
                {
                    "start": 1553,
                    "end": 1577,
                    "matchedPaperCorpusId": "3383786"
                },
                {
                    "start": 1577,
                    "end": 1594,
                    "matchedPaperCorpusId": "264426070"
                },
                {
                    "start": 1711,
                    "end": 1729,
                    "matchedPaperCorpusId": "9176532"
                },
                {
                    "start": 1729,
                    "end": 1755,
                    "matchedPaperCorpusId": "229339854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376708984375
        },
        {
            "corpus_id": "276767847",
            "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
            "text": "Large Language Models (LLMs) have revolutionized natural language understanding and generation, spanning a large range of tasks such as question-answering (Kamalloo et al., 2023), reasoning (Giadikiaroglou et al., 2024), summarization (Zhang et al., 2024a) and others, showcasing unprecedented scalability and adaptability to novel tasks. However, this remarkable progress is accompanied with several challenges, one of them being their tendency to memorize data (Carlini et al., 2021), leading to the inadvertent leakage of private and copyrighted information, an issue tied to several practical implications (Seh et al., 2020;Herrera Montano et al., 2022;Yan et al., 2024). \n\nIn response to the ethical and legal reverberations, the area of machine unlearning has gained prominence, focusing on the deletion of targeted information from trained models. Initial unlearning endeavors bridge the gap between data protection (Bost et al., 2015;Bonawitz et al., 2017) and differential privacy (Dwork and Roth, 2014;Papernot et al., 2016), focusing on removing individual data points from classifiers (Ginart et al., 2019). Such seminal works pose the main challenge of unlearning, which targets deleting individual data points without re-training the whole network from scratch. Still, challenges such as the catastrophic forgetting (Nguyen et al., 2020), as well as the stochasticity (Bourtoule et al., 2020) and incremental nature (Koh and Liang, 2017) of training, showcase the emerging particularities of unlearning algorithms. \n\nThe convergence of unlearning and LLMs arises as a nascent research field accompanied by several challenges, due to their vast and opaque pretraining, large-scale data inter-dependencies, and unbounded label spaces, making it difficult to identify and isolate specific data representations within the model, not to mention efficiently removing them (Yao et al., 2024b).",
            "score": 0.5756845392882435,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1900
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 178,
                    "matchedPaperCorpusId": "258615193"
                },
                {
                    "start": 190,
                    "end": 219,
                    "matchedPaperCorpusId": "267751102"
                },
                {
                    "start": 628,
                    "end": 657,
                    "matchedPaperCorpusId": "250566802"
                },
                {
                    "start": 942,
                    "end": 964,
                    "matchedPaperCorpusId": "3833774"
                },
                {
                    "start": 990,
                    "end": 1012,
                    "matchedPaperCorpusId": "207178262"
                },
                {
                    "start": 1330,
                    "end": 1351,
                    "matchedPaperCorpusId": "225067129"
                },
                {
                    "start": 1430,
                    "end": 1451,
                    "matchedPaperCorpusId": "13193974"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.335205078125
        }
    ],
    "quotes": {
        "cost": 0.172875,
        "quotes": [
            {
                "idx": 0,
                "key": "[267681754 | Dong et al. | 2024 | Citations: 8]",
                "snippets": "The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages...For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation...Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[266933371 | Maini et al. | 2024 | Citations: 194]": "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.",
                    "[269009619 | Zhang et al. | 2024 | Citations: 193]": "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."
                },
                "metadata": [
                    {
                        "section_title": "Case Study Two: MUSE Benchmark",
                        "pdf_hash": "",
                        "start": 173,
                        "end": 377,
                        "sentence_offsets": [
                            {
                                "start": 173,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 378
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages"
                    },
                    {
                        "section_title": "Case Study Two: MUSE Benchmark",
                        "pdf_hash": "",
                        "start": 381,
                        "end": 738,
                        "sentence_offsets": [
                            {
                                "start": 381,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 580
                            },
                            {
                                "start": 581,
                                "end": 738
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation"
                    },
                    {
                        "section_title": "Case Study Two: MUSE Benchmark",
                        "pdf_hash": "",
                        "start": 1272,
                        "end": 1668,
                        "sentence_offsets": [
                            {
                                "start": 1272,
                                "end": 1366
                            },
                            {
                                "start": 1367,
                                "end": 1572
                            },
                            {
                                "start": 1573,
                                "end": 1667
                            }
                        ],
                        "ref_mentions": [
                            "269009619",
                            "266933371"
                        ],
                        "quote": "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[270440333 | Zagardo | 2024 | Citations: 0]",
                "snippets": "Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].(Neel et al., 2020) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters (Neel et al., 2020).Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220364296 | Neel et al. | 2020 | Citations: 276]": "We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion."
                },
                "metadata": [
                    {
                        "section_title": "Gradient-Based Unlearning",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 729,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 128
                            },
                            {
                                "start": 128,
                                "end": 303
                            },
                            {
                                "start": 303,
                                "end": 486
                            },
                            {
                                "start": 486,
                                "end": 729
                            }
                        ],
                        "ref_mentions": [
                            "220364296",
                            "220364296"
                        ],
                        "quote": "Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].(Neel et al., 2020) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters (Neel et al., 2020).Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14]."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[270560986 | Hong et al. | 2024 | Citations: 24]",
                "snippets": "Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 250,
                        "end": 524,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270703035 | Qiu et al. | 2024 | Citations: 3]",
                "snippets": "We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)...We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247627962 | Liu et al. | 2022 | Citations: 62]": "As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."
                },
                "metadata": [
                    {
                        "section_title": "Evaluation Setup",
                        "pdf_hash": "",
                        "start": 1121,
                        "end": 1503,
                        "sentence_offsets": [
                            {
                                "start": 962,
                                "end": 1504
                            }
                        ],
                        "ref_mentions": [
                            "247627962"
                        ],
                        "quote": "We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)"
                    },
                    {
                        "section_title": "Evaluation Setup",
                        "pdf_hash": "",
                        "start": 157,
                        "end": 838,
                        "sentence_offsets": [
                            {
                                "start": 157,
                                "end": 399
                            },
                            {
                                "start": 400,
                                "end": 625
                            },
                            {
                                "start": 626,
                                "end": 837
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[270703237 | Ma et al. | 2024 | Citations: 6]",
                "snippets": "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task...we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model...Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Unlearning Execution",
                        "pdf_hash": "",
                        "start": 173,
                        "end": 492,
                        "sentence_offsets": [
                            {
                                "start": 94,
                                "end": 229
                            },
                            {
                                "start": 230,
                                "end": 266
                            },
                            {
                                "start": 267,
                                "end": 342
                            },
                            {
                                "start": 343,
                                "end": 493
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model"
                    },
                    {
                        "section_title": "Experimental Results",
                        "pdf_hash": "",
                        "start": 988,
                        "end": 2080,
                        "sentence_offsets": [
                            {
                                "start": 969,
                                "end": 1083
                            },
                            {
                                "start": 1084,
                                "end": 1208
                            },
                            {
                                "start": 1211,
                                "end": 1344
                            },
                            {
                                "start": 1347,
                                "end": 1349
                            },
                            {
                                "start": 1350,
                                "end": 1359
                            },
                            {
                                "start": 1360,
                                "end": 1540
                            },
                            {
                                "start": 1541,
                                "end": 1576
                            },
                            {
                                "start": 1577,
                                "end": 1591
                            },
                            {
                                "start": 1592,
                                "end": 1597
                            },
                            {
                                "start": 1598,
                                "end": 1752
                            },
                            {
                                "start": 1755,
                                "end": 1776
                            },
                            {
                                "start": 1777,
                                "end": 1781
                            },
                            {
                                "start": 1782,
                                "end": 1997
                            },
                            {
                                "start": 1998,
                                "end": 2143
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[271064299 | Shi et al. | 2024 | Citations: 84]",
                "snippets": "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 744,
                        "sentence_offsets": [
                            {
                                "start": 270,
                                "end": 578
                            },
                            {
                                "start": 580,
                                "end": 744
                            }
                        ],
                        "ref_mentions": [
                            "255825985"
                        ],
                        "quote": "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271212701 | Gao et al. | 2024 | Citations: 5]",
                "snippets": "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 507,
                        "end": 1321,
                        "sentence_offsets": [
                            {
                                "start": 507,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 982
                            },
                            {
                                "start": 983,
                                "end": 1068
                            },
                            {
                                "start": 1069,
                                "end": 1203
                            },
                            {
                                "start": 1204,
                                "end": 1321
                            }
                        ],
                        "ref_mentions": [
                            "255825985"
                        ],
                        "quote": "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271769107 | Lizzo et al. | 2024 | Citations: 1]",
                "snippets": "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations...Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 550,
                        "end": 1074,
                        "sentence_offsets": [
                            {
                                "start": 550,
                                "end": 696
                            },
                            {
                                "start": 697,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 1075
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations"
                    },
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 587,
                        "end": 1226,
                        "sentence_offsets": [
                            {
                                "start": 587,
                                "end": 734
                            },
                            {
                                "start": 735,
                                "end": 870
                            },
                            {
                                "start": 871,
                                "end": 1051
                            },
                            {
                                "start": 1052,
                                "end": 1225
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271860124 | Cha et al. | 2024 | Citations: 2]",
                "snippets": "Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258615571 | Wang et al. | 2023 | Citations: 78]": "Recent legislation of the \u201cright to be forgotten\u201d has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks.",
                    "[267681958 | Liu et al. | 2024 | Citations: 87]": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 142,
                        "end": 947,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 157
                            },
                            {
                                "start": 158,
                                "end": 440
                            },
                            {
                                "start": 441,
                                "end": 600
                            },
                            {
                                "start": 601,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 980
                            }
                        ],
                        "ref_mentions": [
                            "258615571",
                            "267681958"
                        ],
                        "quote": "Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[273098800 | Gandikota et al. | 2024 | Citations: 11]",
                "snippets": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[273350971 | Wang et al. | 2024 | Citations: 20]",
                "snippets": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Existing LLM Unlearning Paradigm",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1476,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 255
                            },
                            {
                                "start": 258,
                                "end": 317
                            },
                            {
                                "start": 320,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 550
                            },
                            {
                                "start": 551,
                                "end": 673
                            },
                            {
                                "start": 676,
                                "end": 796
                            },
                            {
                                "start": 797,
                                "end": 942
                            },
                            {
                                "start": 943,
                                "end": 1087
                            },
                            {
                                "start": 1090,
                                "end": 1194
                            },
                            {
                                "start": 1195,
                                "end": 1341
                            },
                            {
                                "start": 1342,
                                "end": 1476
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[274436499 | Ding et al. | 2024 | Citations: 8]",
                "snippets": "In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by (Lu et al., 2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by (Jang et al., 2022) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by (Yu et al., 2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249152301 | Lu et al. | 2022 | Citations: 219]": "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.",
                    "[252693065 | Jang et al. | 2022 | Citations: 239]": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust.",
                    "[259859034 | Yu et al. | 2023 | Citations: 100]": ","
                },
                "metadata": [
                    {
                        "section_title": "F.2 LARGE LANGUAGE MODELS UNLEARNING",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 1371,
                        "sentence_offsets": [
                            {
                                "start": 155,
                                "end": 379
                            },
                            {
                                "start": 380,
                                "end": 563
                            },
                            {
                                "start": 566,
                                "end": 729
                            },
                            {
                                "start": 730,
                                "end": 918
                            },
                            {
                                "start": 919,
                                "end": 1098
                            },
                            {
                                "start": 1101,
                                "end": 1233
                            },
                            {
                                "start": 1234,
                                "end": 1371
                            }
                        ],
                        "ref_mentions": [
                            "249152301",
                            "252693065",
                            "259859034"
                        ],
                        "quote": "In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by (Lu et al., 2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by (Jang et al., 2022) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by (Yu et al., 2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[274437750 | Xu et al. | 2024 | Citations: 0]",
                "snippets": "In a 2024 study, Yao et al. (Yao et al., 2024) proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[267897394 | Yao et al. | 2024 | Citations: 51]": "This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 519,
                        "end": 756,
                        "sentence_offsets": [
                            {
                                "start": 519,
                                "end": 756
                            }
                        ],
                        "ref_mentions": [
                            "267897394"
                        ],
                        "quote": "In a 2024 study, Yao et al. (Yao et al., 2024) proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[274823032 | Zuo et al. | 2024 | Citations: 2]",
                "snippets": "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[274982612 | Du et al. | 2024 | Citations: 5]",
                "snippets": "Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252693065 | Jang et al. | 2022 | Citations: 239]": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust.",
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/",
                    "[264828972 | Chen et al. | 2023 | Citations: 162]": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines."
                },
                "metadata": [
                    {
                        "section_title": "Knowledge Unlearning",
                        "pdf_hash": "",
                        "start": 562,
                        "end": 1646,
                        "sentence_offsets": [
                            {
                                "start": 562,
                                "end": 730
                            },
                            {
                                "start": 731,
                                "end": 852
                            },
                            {
                                "start": 853,
                                "end": 1054
                            },
                            {
                                "start": 1055,
                                "end": 1184
                            },
                            {
                                "start": 1185,
                                "end": 1362
                            },
                            {
                                "start": 1363,
                                "end": 1433
                            },
                            {
                                "start": 1434,
                                "end": 1526
                            },
                            {
                                "start": 1527,
                                "end": 1646
                            }
                        ],
                        "ref_mentions": [
                            "255825985",
                            "252693065",
                            "264828972"
                        ],
                        "quote": "Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276812969 | Wang et al. | 2025 | Citations: 3]",
                "snippets": "Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set...Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 4,
                        "end": 549,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 322
                            },
                            {
                                "start": 323,
                                "end": 549
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set"
                    },
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 693,
                        "end": 1126,
                        "sentence_offsets": [
                            {
                                "start": 693,
                                "end": 993
                            },
                            {
                                "start": 994,
                                "end": 1125
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[277510371 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.025383,
        "cot": "Based on the user query, I need to organize information about machine unlearning methods for knowledge removal in language models, focusing on parameter-based and parameter-free approaches, with a specific emphasis on entity-level or concept-level removal. The query also asks about evaluation metrics for effectiveness. Here's my plan for dimensionalizing this information:\n\n1. \"Introduction to Machine Unlearning in Language Models\" - This dimension will provide necessary background on what machine unlearning is, why it's important, and general concepts. This should be in synthesis format to provide a coherent introduction.\n\n2. \"Parameter-Based Unlearning Methods\" - This dimension will cover techniques that involve modifying model parameters directly. Given the diverse approaches mentioned in the quotes, synthesis format is appropriate to explain and compare these methods.\n\n3. \"Parameter-Free Unlearning Methods\" - This dimension will discuss approaches that don't require direct parameter modification, such as in-context unlearning. Synthesis format is appropriate to properly explain these approaches.\n\n4. \"Entity-Level and Concept-Level Unlearning Approaches\" - This dimension will focus specifically on methods designed for removing entity or concept knowledge, which is a key part of the query. Synthesis format works best to explain the nuances of these specialized approaches.\n\n5. \"Evaluation Metrics and Performance Assessment\" - This dimension will outline how unlearning methods are evaluated, including metrics for forgetting and retention rates. Synthesis format is appropriate to explain the relationships between different evaluation approaches.\n\n6. \"Comparative Effectiveness of Unlearning Methods\" - This dimension will provide direct comparisons between different methods' performance. List format is appropriate here to clearly present the relative strengths and weaknesses of each approach.",
        "plan": {
            "Introduction to Machine Unlearning in Language Models (synthesis)": [
                12,
                15
            ],
            "Parameter-Based Unlearning Methods (synthesis)": [
                1,
                3,
                4,
                6,
                8,
                10,
                13,
                14
            ],
            "Parameter-Free Unlearning Methods (synthesis)": [
                5,
                6,
                11
            ],
            "Entity-Level and Concept-Level Unlearning Approaches (synthesis)": [
                0,
                4,
                7,
                9,
                14,
                16
            ],
            "Evaluation Metrics and Performance Assessment (synthesis)": [
                0,
                2,
                3,
                4,
                15
            ],
            "Comparative Effectiveness of Unlearning Methods (list)": [
                0,
                3,
                4,
                7,
                8,
                9,
                13
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Machine Unlearning in Language Models",
                "tldr": "Machine unlearning has emerged as a key solution for implementing the \"right to be forgotten\" in large language models (LLMs). This approach aims to selectively remove specific knowledge from trained models while preserving overall performance, with evaluation typically focusing on both forget quality and model utility. (2 sources)",
                "text": "\nMachine unlearning in large language models (LLMs) refers to the process of selectively removing or \"forgetting\" specific information from trained models without requiring complete retraining. This concept has gained significant attention as a practical implementation of the \"right to be forgotten\" principle in AI systems <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. The framework for machine unlearning in pre-trained LLMs encompasses various methods that can efficiently eliminate undesired knowledge while maintaining the model's overall capabilities.\n\nWhen implementing unlearning for LLMs, the goal is typically to remove specific factual knowledge represented as question-answer pairs (x_i, y_i), where x_i denotes the question and y_i represents the corresponding answer. Given a dataset D containing such pairs and a model P_\u03b8 trained on this dataset, the objective of unlearning is to ensure the model completely forgets the knowledge contained in a designated \"forget set\" while retaining other capabilities <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of unlearning methods is generally evaluated along two critical dimensions: model utility, which measures the general capabilities that remain intact after unlearning, and forget quality, which quantifies how successfully the targeted knowledge has been removed <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. A significant advantage of machine unlearning approaches is their computational efficiency\u2014research has demonstrated that these methods can be over 10^5 times more computationally efficient than complete retraining of models <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>.\n\nOne prominent technique in the unlearning arsenal is gradient ascent, which essentially reverses the optimization process on the designated forget set. This approach, often combined with traditional gradient descent on remaining data, has shown promising results in improving hyperparameter robustness during the unlearning process <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yao et al., 2024)",
                        "snippets": [
                            "This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development."
                        ],
                        "paper": {
                            "corpus_id": 267897394,
                            "title": "Machine Unlearning of Pre-trained Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2287524199",
                                    "name": "Jin Yao"
                                },
                                {
                                    "authorId": "2286336292",
                                    "name": "Eli Chien"
                                },
                                {
                                    "authorId": "2359683064",
                                    "name": "Minxin Du"
                                },
                                {
                                    "authorId": "2284224531",
                                    "name": "Xinyao Niu"
                                },
                                {
                                    "authorId": "2239503716",
                                    "name": "Tianhao Wang"
                                },
                                {
                                    "authorId": "2332680558",
                                    "name": "Zezhou Cheng"
                                },
                                {
                                    "authorId": "2284224134",
                                    "name": "Xiang Yue"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 51
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set",
                            "Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set."
                        ],
                        "paper": {
                            "corpus_id": 276812969,
                            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                            "authors": [
                                {
                                    "authorId": "2348951919",
                                    "name": "Wenyu Wang"
                                },
                                {
                                    "authorId": "48985110",
                                    "name": "Mengqi Zhang"
                                },
                                {
                                    "authorId": "2286432237",
                                    "name": "Xiaotian Ye"
                                },
                                {
                                    "authorId": "2260895127",
                                    "name": "Zhaochun Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.72705078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter-Based Unlearning Methods",
                "tldr": "Parameter-based unlearning methods modify model weights directly to remove unwanted knowledge, with gradient-based approaches like Gradient Ascent being the most common. These methods typically balance three components: a forget loss to remove target knowledge, a retain loss to preserve general capabilities, and sometimes custom loss terms for specific requirements. (13 sources)",
                "text": "\nParameter-based unlearning methods in large language models (LLMs) involve direct modification of model parameters to selectively remove specific knowledge. These approaches can be broadly categorized into several types based on their underlying mechanisms.\n\nGradient-based methods represent the most common parameter-based approach to unlearning. The fundamental technique in this category is Gradient Ascent (GA), which essentially reverses the optimization process on targeted data by maximizing the loss on information to be forgotten <Paper corpusId=\"252693065\" paperTitle=\"(Jang et al., 2022)\" isShortName></Paper>. This approach effectively reduces the model's ability to generate correct responses for the forget set <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. However, while GA can successfully reduce the probability of generating ground truth answers, it often damages the model's overall capabilities, resulting in lower forget quality and diminished model utility <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nBuilding upon simple GA, several enhanced gradient-based methods have been developed. Gradient Difference (GD) incorporates learning through gradient descent on a retain set to remedy excessive damage to the model <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Another variant is GA with KL-divergence regularization (UKL), which applies constraints to maintain model performance on non-target data <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>. Wang et al. proposed Reverse KL-Divergence-based Knowledge Distillation (RKLD) for unlearning personal information, demonstrating the importance of balancing forget quality with model utility <Paper corpusId=\"270440333\" paperTitle=\"(Zagardo, 2024)\" isShortName></Paper>.\n\nThe general framework for parameter-based unlearning methods typically involves a modified loss function with three key components <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>:\n\n1. A forget loss (L_FG) that encourages the model to forget undesired data by increasing the loss on data to be forgotten\n2. A retain loss (L_RT) that ensures the model maintains its performance on unaffected data\n3. A custom loss (L_Custom) that may include regularization terms or specific constraints to control parameter updates\n\nMore specialized approaches include Selective Knowledge negation Unlearning (SKU), which employs a two-stage process: first acquiring harmful knowledge and then negating it <Paper corpusId=\"274823032\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This method effectively balances removing harmful information while preserving utility on normal prompts.\n\nAnother notable parameter-based approach is ROME (Rank-One Model Editing), which identifies and individually manipulates specific layers and neurons responsible for factual predictions <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Though originally designed for factual associations, ROME shows potential for broader unlearning applications.\n\nChen and Yang introduced a structural approach to unlearning by creating dedicated unlearning layers trained using a selective student-teacher objective <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This method enables the forgetting of specific information through specialized layers that can eventually be fused into a single layer, offering a more computationally efficient alternative to full model fine-tuning <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\nParameter optimization methods generally demonstrate superior effectiveness compared to in-context unlearning approaches, which rely on modifying input prompts rather than model parameters <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. However, most parameter-based methods still face challenges in balancing effective knowledge removal with preservation of general model capabilities.",
                "citations": [
                    {
                        "id": "(Jang et al., 2022)",
                        "snippets": [
                            "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."
                        ],
                        "paper": {
                            "corpus_id": 252693065,
                            "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
                            "authors": [
                                {
                                    "authorId": "2000091730",
                                    "name": "Joel Jang"
                                },
                                {
                                    "authorId": "29830817",
                                    "name": "Dongkeun Yoon"
                                },
                                {
                                    "authorId": "16110760",
                                    "name": "Sohee Yang"
                                },
                                {
                                    "authorId": "34352481",
                                    "name": "Sungmin Cha"
                                },
                                {
                                    "authorId": "3056520",
                                    "name": "Moontae Lee"
                                },
                                {
                                    "authorId": "2876316",
                                    "name": "Lajanugen Logeswaran"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 239
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task",
                            "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model",
                            "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."
                        ],
                        "paper": {
                            "corpus_id": 270703237,
                            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                            "authors": [
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2216505879",
                                    "name": "Yangfan Ye"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.8076171875
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."
                        ],
                        "paper": {
                            "corpus_id": 247627962,
                            "title": "Continual Learning and Private Unlearning",
                            "authors": [
                                {
                                    "authorId": "145306564",
                                    "name": "B. Liu"
                                },
                                {
                                    "authorId": "2155193246",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "144848112",
                                    "name": "P. Stone"
                                }
                            ],
                            "year": 2022,
                            "venue": "CoLLAs",
                            "n_citations": 62
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qiu et al., 2024)",
                        "snippets": [
                            "We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)",
                            "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."
                        ],
                        "paper": {
                            "corpus_id": 270703035,
                            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
                            "authors": [
                                {
                                    "authorId": "1702997626",
                                    "name": "Xinchi Qiu"
                                },
                                {
                                    "authorId": "2302373311",
                                    "name": "William F. Shen"
                                },
                                {
                                    "authorId": "2308073429",
                                    "name": "Yihong Chen"
                                },
                                {
                                    "authorId": "2313189467",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "1918552",
                                    "name": "Pontus Stenetorp"
                                },
                                {
                                    "authorId": "2298756346",
                                    "name": "N. Lane"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 3
                        },
                        "score": 0.76513671875
                    },
                    {
                        "id": "(Zagardo, 2024)",
                        "snippets": [
                            "Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].(Neel et al., 2020) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters (Neel et al., 2020).Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14]."
                        ],
                        "paper": {
                            "corpus_id": 270440333,
                            "title": "A More Practical Approach to Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2309006422",
                                    "name": "David Zagardo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.76123046875
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge."
                        ],
                        "paper": {
                            "corpus_id": 273350971,
                            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
                            "authors": [
                                {
                                    "authorId": "2306067819",
                                    "name": "Yaxuan Wang"
                                },
                                {
                                    "authorId": "2306500340",
                                    "name": "Jiaheng Wei"
                                },
                                {
                                    "authorId": "2271515779",
                                    "name": "Chris Liu"
                                },
                                {
                                    "authorId": "2284760719",
                                    "name": "Jinlong Pang"
                                },
                                {
                                    "authorId": "2326243943",
                                    "name": "Quan Liu"
                                },
                                {
                                    "authorId": "2316588330",
                                    "name": "Ankit Shah"
                                },
                                {
                                    "authorId": "2306754738",
                                    "name": "Yujia Bao"
                                },
                                {
                                    "authorId": "2306028548",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2306480290",
                                    "name": "Wei Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 20
                        },
                        "score": 0.7685546875
                    },
                    {
                        "id": "(Zuo et al., 2024)",
                        "snippets": [
                            "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts."
                        ],
                        "paper": {
                            "corpus_id": 274823032,
                            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
                            "authors": [
                                {
                                    "authorId": "2005445750",
                                    "name": "Xuhan Zuo"
                                },
                                {
                                    "authorId": "2005212347",
                                    "name": "Minghao Wang"
                                },
                                {
                                    "authorId": "2185053609",
                                    "name": "Tianqing Zhu"
                                },
                                {
                                    "authorId": "2304458654",
                                    "name": "Shui Yu"
                                },
                                {
                                    "authorId": "2134555583",
                                    "name": "Wanlei Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.72802734375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."
                        ],
                        "paper": {
                            "corpus_id": 267681958,
                            "title": "Towards Safer Large Language Models through Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2122087252",
                                    "name": "Zheyuan Liu"
                                },
                                {
                                    "authorId": "2174956825",
                                    "name": "Guangyao Dou"
                                },
                                {
                                    "authorId": "2093186816",
                                    "name": "Zhaoxuan Tan"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2275403324",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Du et al., 2024)",
                        "snippets": [
                            "Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."
                        ],
                        "paper": {
                            "corpus_id": 274982612,
                            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                            "authors": [
                                {
                                    "authorId": "2315112779",
                                    "name": "Hao Du"
                                },
                                {
                                    "authorId": "2275191329",
                                    "name": "Shang Liu"
                                },
                                {
                                    "authorId": "1865220753",
                                    "name": "Lele Zheng"
                                },
                                {
                                    "authorId": "2336916642",
                                    "name": "Yang Cao"
                                },
                                {
                                    "authorId": "2336874775",
                                    "name": "Atsuyoshi Nakamura"
                                },
                                {
                                    "authorId": "2336870835",
                                    "name": "Lei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines."
                        ],
                        "paper": {
                            "corpus_id": 264828972,
                            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "47739850",
                                    "name": "Jiaao Chen"
                                },
                                {
                                    "authorId": "2263629011",
                                    "name": "Diyi Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 162
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cha et al., 2024)",
                        "snippets": [
                            "Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs."
                        ],
                        "paper": {
                            "corpus_id": 271860124,
                            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "34352481",
                                    "name": "Sungmin Cha"
                                },
                                {
                                    "authorId": "2149157242",
                                    "name": "Sungjun Cho"
                                },
                                {
                                    "authorId": "1474356736",
                                    "name": "Dasol Hwang"
                                },
                                {
                                    "authorId": "2313692227",
                                    "name": "Moontae Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.74462890625
                    },
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                        ],
                        "paper": {
                            "corpus_id": 271212701,
                            "title": "On Large Language Model Continual Unlearning",
                            "authors": [
                                {
                                    "authorId": "2311833838",
                                    "name": "Chongyang Gao"
                                },
                                {
                                    "authorId": "2108631414",
                                    "name": "Lixu Wang"
                                },
                                {
                                    "authorId": "2148353350",
                                    "name": "Chenkai Weng"
                                },
                                {
                                    "authorId": "2276121035",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2275773112",
                                    "name": "Qi Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.728515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter-Free Unlearning Methods",
                "tldr": "Parameter-free unlearning methods modify model outputs without changing weights, with in-context unlearning being the primary approach that uses prompt engineering to guide models away from unwanted knowledge. While these methods are less effective than parameter-based approaches, they offer advantages in simplicity and can be applied to black-box models where direct parameter access is unavailable. (5 sources)",
                "text": "\nParameter-free unlearning methods represent an alternative approach to removing unwanted knowledge from language models without directly modifying model parameters. These techniques are particularly valuable when access to model weights is limited or when computational resources for retraining are constrained.\n\nThe most prominent parameter-free approach is in-context unlearning, which treats the model as a black box and modifies its output results using external knowledge <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This method works by supplying flipped labels or contradictory information during inference, effectively guiding the model to avoid generating specific knowledge while maintaining overall performance <Paper corpusId=\"274436499\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. By altering the prompt structure rather than the model itself, in-context unlearning provides a lightweight solution to selective knowledge removal.\n\nAnother parameter-free approach is Quantized Reward Konditioning (Quark), which employs a reinforcement learning framework to control and reduce undesirable behaviors in text generation <Paper corpusId=\"274436499\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249152301\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>. Quark works by collecting samples, sorting them into quantiles based on reward signals that quantify unwanted properties, and then conditioning the model's outputs on high-reward tokens during generation, all without changing the underlying model parameters.\n\nWhile parameter-free methods offer advantages in simplicity and applicability to black-box models, they generally demonstrate lower effectiveness compared to parameter optimization approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This effectiveness gap exists because parameter-free methods cannot directly modify the internal representations that store factual knowledge within the model.\n\nSome hybrid approaches combine aspects of both parameter-based and parameter-free methods. For instance, localization-informed unlearning first identifies model units (such as specific layers or neurons) closely related to the target knowledge, and then applies focused modifications to those components <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. This targeted approach allows for more precise knowledge removal while minimizing disruption to overall model capabilities.\n\nDespite their limitations in unlearning effectiveness, parameter-free methods remain valuable in situations where direct parameter access is unavailable or when quick, deployment-ready solutions are needed without the computational overhead of model fine-tuning.",
                "citations": [
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271064299,
                            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                            "authors": [
                                {
                                    "authorId": "2286638403",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2261353791",
                                    "name": "Jaechan Lee"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2266698166",
                                    "name": "Jieyu Zhao"
                                },
                                {
                                    "authorId": "2309248199",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309424274",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 84
                        },
                        "score": 0.7763671875
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by (Lu et al., 2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by (Jang et al., 2022) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by (Yu et al., 2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications."
                        ],
                        "paper": {
                            "corpus_id": 274436499,
                            "title": "Unified Parameter-Efficient Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "2313929104",
                                    "name": "Chenlu Ding"
                                },
                                {
                                    "authorId": "1491035012",
                                    "name": "Jiancan Wu"
                                },
                                {
                                    "authorId": "2263441815",
                                    "name": "Yancheng Yuan"
                                },
                                {
                                    "authorId": "2315426691",
                                    "name": "Jinda Lu"
                                },
                                {
                                    "authorId": "2333421125",
                                    "name": "Kai Zhang"
                                },
                                {
                                    "authorId": "2333357625",
                                    "name": "Alex Su"
                                },
                                {
                                    "authorId": "2259678005",
                                    "name": "Xiang Wang"
                                },
                                {
                                    "authorId": "2240825631",
                                    "name": "Xiangnan He"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 8
                        },
                        "score": 0.81640625
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives."
                        ],
                        "paper": {
                            "corpus_id": 249152301,
                            "title": "Quark: Controllable Text Generation with Reinforced Unlearning",
                            "authors": [
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2112504145",
                                    "name": "Liwei Jiang"
                                },
                                {
                                    "authorId": "2689239",
                                    "name": "Jack Hessel"
                                },
                                {
                                    "authorId": "3444092",
                                    "name": "Lianhui Qin"
                                },
                                {
                                    "authorId": "119659229",
                                    "name": "Peter West"
                                },
                                {
                                    "authorId": "19179135",
                                    "name": "Prithviraj Ammanabrolu"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 219
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                        ],
                        "paper": {
                            "corpus_id": 271212701,
                            "title": "On Large Language Model Continual Unlearning",
                            "authors": [
                                {
                                    "authorId": "2311833838",
                                    "name": "Chongyang Gao"
                                },
                                {
                                    "authorId": "2108631414",
                                    "name": "Lixu Wang"
                                },
                                {
                                    "authorId": "2148353350",
                                    "name": "Chenkai Weng"
                                },
                                {
                                    "authorId": "2276121035",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2275773112",
                                    "name": "Qi Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.728515625
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Entity-Level and Concept-Level Unlearning Approaches",
                "tldr": "Entity-level and concept-level unlearning approaches target the removal of specific knowledge entities or broader conceptual information from language models. These specialized techniques employ various strategies including subspace identification, representation manipulation, and discriminative methods to achieve targeted forgetting while minimizing disruption to related knowledge. (10 sources)",
                "text": "\nEntity-level and concept-level unlearning approaches represent more granular and targeted techniques for removing specific knowledge from language models. Unlike general unlearning methods, these approaches focus on eliminating particular entities (such as personal information) or broader concepts (like fictional universes) while preserving related knowledge.\n\nFor entity-level unlearning, researchers have systematically evaluated various algorithms by using knowledge-probing methods to identify entity-related information as forget sets. Studies show that existing unlearning algorithms often struggle to generalize effectively to entity-level unlearning tasks <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. When comparing different approaches, gradient ascent methods can successfully minimize the probability of generating ground truth answers about target entities but often damage the model's overall capabilities. Variants that incorporate gradient descent on a retain set (like Gradient Difference) prove more effective than KL-divergence restrictions in mitigating this collateral damage <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nFor concept-level unlearning, several innovative approaches have emerged. The Erasure of Language Memory (ELM) method leverages the model's introspective capabilities, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>. This approach differs from other methods like Representation Misdirection for Unlearning (RMU), WhoIsHarryPotter (WHP), and Representation Noising (RepNoise), which employ various techniques to disrupt internal representations associated with target concepts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\nThe UNLEARN algorithm represents another advancement in targeted knowledge removal, using subspace techniques to identify the vector spaces spanned by particular knowledge and applying discrimination methods to separate these subspaces from those containing similar information. This approach achieves impressive results\u201496% forgetting on target tasks while maintaining performance on other tasks within 2.5% of the original model. When dealing with similar tasks, it still achieves nearly 80% forgetting while preserving performance on related tasks within 10% of the original model <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>.\n\nSome approaches focus on manipulating internal model representations. The ROME (Rank-One Model Editing) method identifies and individually manipulates specific layers and neurons responsible for factual predictions <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Originally designed for factual associations, ROME shows potential for broader unlearning applications. Similarly, Chen and Yang's structural approach creates dedicated unlearning layers trained using a selective student-teacher objective, which can eventually be fused into a single layer for efficient forgetting of specific information <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nFor balancing effective forgetting with knowledge preservation, the Erasing Space Concept (ESC) method restricts important subspaces for forgetting knowledge by eliminating relevant activations in the feature space. Its enhanced version, ESC with Training (ESC-T), employs a learnable mask to better manage the trade-off between forgetting target knowledge and preserving other information <Paper corpusId=\"277510371\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nComparative evaluations reveal that many existing unlearning methods struggle with the challenge of mode collapse, where removing targeted knowledge significantly damages overall model capabilities. This is particularly evident in direct tuning methods like Gradient Ascent (GA) and Negative Preference Optimization (NPO) <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>. Although some variants attempt to address this by applying gradient ascent on the forget set and gradient descent on the retain set, they still underperform compared to more sophisticated approaches <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task",
                            "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model",
                            "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."
                        ],
                        "paper": {
                            "corpus_id": 270703237,
                            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                            "authors": [
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2216505879",
                                    "name": "Yangfan Ye"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.8076171875
                    },
                    {
                        "id": "(Gandikota et al., 2024)",
                        "snippets": [
                            "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals."
                        ],
                        "paper": {
                            "corpus_id": 273098800,
                            "title": "Erasing Conceptual Knowledge from Language Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "2140009998",
                                    "name": "Sheridan Feucht"
                                },
                                {
                                    "authorId": "2225941937",
                                    "name": "Samuel Marks"
                                },
                                {
                                    "authorId": "2284996653",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(Lizzo et al., 2024)",
                        "snippets": [
                            "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations",
                            "Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks."
                        ],
                        "paper": {
                            "corpus_id": 271769107,
                            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2315304043",
                                    "name": "Tyler Lizzo"
                                },
                                {
                                    "authorId": "2315302093",
                                    "name": "Larry Heck"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.86083984375
                    },
                    {
                        "id": "(Du et al., 2024)",
                        "snippets": [
                            "Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."
                        ],
                        "paper": {
                            "corpus_id": 274982612,
                            "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
                            "authors": [
                                {
                                    "authorId": "2315112779",
                                    "name": "Hao Du"
                                },
                                {
                                    "authorId": "2275191329",
                                    "name": "Shang Liu"
                                },
                                {
                                    "authorId": "1865220753",
                                    "name": "Lele Zheng"
                                },
                                {
                                    "authorId": "2336916642",
                                    "name": "Yang Cao"
                                },
                                {
                                    "authorId": "2336874775",
                                    "name": "Atsuyoshi Nakamura"
                                },
                                {
                                    "authorId": "2336870835",
                                    "name": "Lei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.71142578125
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines."
                        ],
                        "paper": {
                            "corpus_id": 264828972,
                            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "47739850",
                                    "name": "Jiaao Chen"
                                },
                                {
                                    "authorId": "2263629011",
                                    "name": "Diyi Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 162
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD."
                        ],
                        "paper": {
                            "corpus_id": 277510371,
                            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
                            "authors": [
                                {
                                    "authorId": "2301763757",
                                    "name": "Tae-Young Lee"
                                },
                                {
                                    "authorId": "2264950149",
                                    "name": "Sundong Park"
                                },
                                {
                                    "authorId": "2211098085",
                                    "name": "Minwoo Jeon"
                                },
                                {
                                    "authorId": "2073600754",
                                    "name": "Hyoseok Hwang"
                                },
                                {
                                    "authorId": "3144955",
                                    "name": "Gyeong-Moon Park"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.74169921875
                    },
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages",
                            "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation",
                            "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."
                        ],
                        "paper": {
                            "corpus_id": 267681754,
                            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2284461064",
                                    "name": "Yijiang River Dong"
                                },
                                {
                                    "authorId": "2325112252",
                                    "name": "Hongzhou Lin"
                                },
                                {
                                    "authorId": "2284217404",
                                    "name": "Mikhail Belkin"
                                },
                                {
                                    "authorId": "2284217279",
                                    "name": "Ramon Huerta"
                                },
                                {
                                    "authorId": "2267339029",
                                    "name": "Ivan Vuli'c"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.77294921875
                    },
                    {
                        "id": "(Maini et al., 2024)",
                        "snippets": [
                            "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."
                        ],
                        "paper": {
                            "corpus_id": 266933371,
                            "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "153742303",
                                    "name": "Pratyush Maini"
                                },
                                {
                                    "authorId": "2261439316",
                                    "name": "Zhili Feng"
                                },
                                {
                                    "authorId": "102604362",
                                    "name": "Avi Schwarzschild"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "2242257227",
                                    "name": "J. Kolter"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 194
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."
                        ],
                        "paper": {
                            "corpus_id": 269009619,
                            "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
                            "authors": [
                                {
                                    "authorId": "2295968232",
                                    "name": "Ruiqi Zhang"
                                },
                                {
                                    "authorId": "2257668289",
                                    "name": "Licong Lin"
                                },
                                {
                                    "authorId": "2257420186",
                                    "name": "Yu Bai"
                                },
                                {
                                    "authorId": "2257346113",
                                    "name": "Song Mei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 193
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evaluation Metrics and Performance Assessment",
                "tldr": "Evaluation of machine unlearning methods typically focuses on two key dimensions: forget quality (how effectively targeted knowledge is removed) and model utility (how well general capabilities are preserved). Researchers use specialized metrics like Deviation Score to measure the balance between these dimensions, while also emphasizing the need to monitor for residual knowledge that could be adversarially exploited. (8 sources)",
                "text": "\nEvaluating the effectiveness of machine unlearning methods requires robust assessment frameworks that can measure both the removal of targeted knowledge and the preservation of overall model capabilities. The evaluation process typically divides data into two distinct sets: a \"Forget\" set containing knowledge to be removed, and a \"Retain\" set containing knowledge that should be preserved <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nThe primary evaluation dimensions for unlearning methods are forget quality and model utility <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper>. Forget quality measures how effectively the model removes targeted knowledge, often assessed through question-answering tasks where questions related to the Forget set should receive low accuracy or probability scores. Conversely, model utility evaluates whether the unlearning procedure negatively impacts unrelated topics, typically measured by performance on the Retain set <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nTo comprehensively assess the balance between forgetting and retention, researchers have developed specialized metrics. One such metric is the Deviation Score (DS), calculated as DS = 100 \u00d7 (ROUGE1_forget)\u00b2 + (1 \u2212 ROUGE1_retain)\u00b2, which measures the Euclidean distance between the model's current state and the ideal state of perfect forgetting with complete utility preservation. A lower DS indicates more effective unlearning <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>.\n\nSystematic evaluations of unlearning methods reveal significant performance variations across different approaches. Gradient Ascent methods can effectively minimize the probability of generating correct answers for targeted knowledge but often damage overall model capabilities, resulting in lower forget quality and diminished utility. Methods that incorporate gradient descent on a retain set (like Gradient Difference) typically outperform those using KL-divergence restrictions in mitigating collateral damage to the model <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nCurrent evaluation protocols primarily rely on behavioral tests, which may not fully capture the presence of residual knowledge within model parameters. This limitation is particularly concerning as any remaining traces of supposedly forgotten information could potentially be exploited through adversarial methods to recover the erased data post-unlearning <Paper corpusId=\"270560986\" paperTitle=\"(Hong et al., 2024)\" isShortName></Paper>. This highlights the need for more comprehensive evaluation frameworks that can detect subtle forms of knowledge retention.\n\nWhen comparing unlearning methods, direct tuning approaches like Gradient Ascent (GA) and Negative Preference Optimization (NPO) often suffer from mode collapse, where the model's overall capabilities deteriorate significantly. While some variants attempt to address this by applying gradient ascent on the Forget set and gradient descent on the Retain set, they still typically underperform compared to more sophisticated approaches <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages",
                            "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation",
                            "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."
                        ],
                        "paper": {
                            "corpus_id": 267681754,
                            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2284461064",
                                    "name": "Yijiang River Dong"
                                },
                                {
                                    "authorId": "2325112252",
                                    "name": "Hongzhou Lin"
                                },
                                {
                                    "authorId": "2284217404",
                                    "name": "Mikhail Belkin"
                                },
                                {
                                    "authorId": "2284217279",
                                    "name": "Ramon Huerta"
                                },
                                {
                                    "authorId": "2267339029",
                                    "name": "Ivan Vuli'c"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.77294921875
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set",
                            "Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set."
                        ],
                        "paper": {
                            "corpus_id": 276812969,
                            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                            "authors": [
                                {
                                    "authorId": "2348951919",
                                    "name": "Wenyu Wang"
                                },
                                {
                                    "authorId": "48985110",
                                    "name": "Mengqi Zhang"
                                },
                                {
                                    "authorId": "2286432237",
                                    "name": "Xiaotian Ye"
                                },
                                {
                                    "authorId": "2260895127",
                                    "name": "Zhaochun Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.72705078125
                    },
                    {
                        "id": "(Maini et al., 2024)",
                        "snippets": [
                            "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."
                        ],
                        "paper": {
                            "corpus_id": 266933371,
                            "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "153742303",
                                    "name": "Pratyush Maini"
                                },
                                {
                                    "authorId": "2261439316",
                                    "name": "Zhili Feng"
                                },
                                {
                                    "authorId": "102604362",
                                    "name": "Avi Schwarzschild"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "2242257227",
                                    "name": "J. Kolter"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 194
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qiu et al., 2024)",
                        "snippets": [
                            "We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)",
                            "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."
                        ],
                        "paper": {
                            "corpus_id": 270703035,
                            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
                            "authors": [
                                {
                                    "authorId": "1702997626",
                                    "name": "Xinchi Qiu"
                                },
                                {
                                    "authorId": "2302373311",
                                    "name": "William F. Shen"
                                },
                                {
                                    "authorId": "2308073429",
                                    "name": "Yihong Chen"
                                },
                                {
                                    "authorId": "2313189467",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "1918552",
                                    "name": "Pontus Stenetorp"
                                },
                                {
                                    "authorId": "2298756346",
                                    "name": "N. Lane"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 3
                        },
                        "score": 0.76513671875
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task",
                            "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model",
                            "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."
                        ],
                        "paper": {
                            "corpus_id": 270703237,
                            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                            "authors": [
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2216505879",
                                    "name": "Yangfan Ye"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.8076171875
                    },
                    {
                        "id": "(Hong et al., 2024)",
                        "snippets": [
                            "Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning."
                        ],
                        "paper": {
                            "corpus_id": 270560986,
                            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                            "authors": [
                                {
                                    "authorId": "2306946364",
                                    "name": "Yihuai Hong"
                                },
                                {
                                    "authorId": "2306950395",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "2143278592",
                                    "name": "Shauli Ravfogel"
                                },
                                {
                                    "authorId": "2307764517",
                                    "name": "Haiqin Yang"
                                },
                                {
                                    "authorId": "22245981",
                                    "name": "Mor Geva"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 24
                        },
                        "score": 0.7431640625
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."
                        ],
                        "paper": {
                            "corpus_id": 269009619,
                            "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
                            "authors": [
                                {
                                    "authorId": "2295968232",
                                    "name": "Ruiqi Zhang"
                                },
                                {
                                    "authorId": "2257668289",
                                    "name": "Licong Lin"
                                },
                                {
                                    "authorId": "2257420186",
                                    "name": "Yu Bai"
                                },
                                {
                                    "authorId": "2257346113",
                                    "name": "Song Mei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 193
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."
                        ],
                        "paper": {
                            "corpus_id": 247627962,
                            "title": "Continual Learning and Private Unlearning",
                            "authors": [
                                {
                                    "authorId": "145306564",
                                    "name": "B. Liu"
                                },
                                {
                                    "authorId": "2155193246",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "144848112",
                                    "name": "P. Stone"
                                }
                            ],
                            "year": 2022,
                            "venue": "CoLLAs",
                            "n_citations": 62
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Effectiveness of Unlearning Methods",
                "tldr": "Parameter-based approaches generally outperform parameter-free methods for machine unlearning, with UNLEARN and ELM showing the best balance between forgetting target knowledge and preserving model utility. Most methods face trade-offs between effective forgetting and maintaining performance, with gradient-based approaches like Gradient Ascent often causing significant model degradation. (12 sources)",
                "text": "\nHere is a comparative assessment of major unlearning methods based on their effectiveness:\n\n* **UNLEARN Algorithm**: Achieves 96% forgetting on target tasks while maintaining performance on other tasks within 2.5% of the original model. When handling similar tasks (a significant challenge for most methods), it still manages nearly 80% forgetting while preserving performance on related tasks within 10% of the original model. Its discriminative capabilities significantly outperform other approaches. <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>\n\n* **Erasure of Language Memory (ELM)**: Demonstrates superior concept-level unlearning by leveraging the model's ability to evaluate its own knowledge. It outperforms competing methods like Representation Misdirection for Unlearning (RMU), WhoIsHarryPotter (WHP), and Representation Noising (RepNoise) across multiple evaluation dimensions. <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>\n\n* **Selective Knowledge Negation Unlearning (SKU)**: Effectively balances harmful content removal and normal prompt performance through a two-stage process of first acquiring harmful knowledge and then negating it. This approach has shown particular promise in maintaining model utility while removing targeted content. <Paper corpusId=\"274823032\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n* **Gradient Difference (GD)**: Outperforms KL-divergence regularization approaches by learning through gradient descent on a retain set, which helps mitigate excessive damage to the model. It shows better balance between forgetting and retention than simple Gradient Ascent. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>\n\n* **Negative Preference Optimization with Gradient Difference (NPO-GD)**: Stands out as one of the strongest approaches, particularly excelling in instance-level unlearning tasks under ideal conditions. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>\n\n* **Gradient Ascent (GA)**: While effective at minimizing the probability of generating ground truth answers for target knowledge, GA significantly harms the model's overall capabilities, resulting in lower forget quality and diminished model utility. It suffers from unstable optimization due to the unbounded nature of its objective loss. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>\n\n* **Preference Optimization methods**: These approaches (including Direct Preference Optimization and Negative Preference Optimization) can significantly reduce ROUGE scores for target set answers, but the original answers often maintain high generation probability and accuracy. This suggests they perform unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>\n\n* **Knowledge Distillation-based methods**: Approaches like Wang et al.'s method transfer knowledge selectively to a secondary model but incur substantial computational costs due to reliance on this secondary model. <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258615571\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n* **Direct tuning methods**: Approaches like Gradient Ascent (GA) and Negative Preference Optimization (NPO) often suffer from mode collapse, placing them near the origin on evaluation charts. Some variants attempt to correct this by applying gradient ascent on the forget set and gradient descent on the retain set, but they still underperform compared to more sophisticated approaches. <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Lizzo et al., 2024)",
                        "snippets": [
                            "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations",
                            "Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks."
                        ],
                        "paper": {
                            "corpus_id": 271769107,
                            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2315304043",
                                    "name": "Tyler Lizzo"
                                },
                                {
                                    "authorId": "2315302093",
                                    "name": "Larry Heck"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.86083984375
                    },
                    {
                        "id": "(Gandikota et al., 2024)",
                        "snippets": [
                            "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals."
                        ],
                        "paper": {
                            "corpus_id": 273098800,
                            "title": "Erasing Conceptual Knowledge from Language Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "2140009998",
                                    "name": "Sheridan Feucht"
                                },
                                {
                                    "authorId": "2225941937",
                                    "name": "Samuel Marks"
                                },
                                {
                                    "authorId": "2284996653",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(Zuo et al., 2024)",
                        "snippets": [
                            "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts."
                        ],
                        "paper": {
                            "corpus_id": 274823032,
                            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
                            "authors": [
                                {
                                    "authorId": "2005445750",
                                    "name": "Xuhan Zuo"
                                },
                                {
                                    "authorId": "2005212347",
                                    "name": "Minghao Wang"
                                },
                                {
                                    "authorId": "2185053609",
                                    "name": "Tianqing Zhu"
                                },
                                {
                                    "authorId": "2304458654",
                                    "name": "Shui Yu"
                                },
                                {
                                    "authorId": "2134555583",
                                    "name": "Wanlei Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.72802734375
                    },
                    {
                        "id": "(Cha et al., 2024)",
                        "snippets": [
                            "Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs."
                        ],
                        "paper": {
                            "corpus_id": 271860124,
                            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "34352481",
                                    "name": "Sungmin Cha"
                                },
                                {
                                    "authorId": "2149157242",
                                    "name": "Sungjun Cho"
                                },
                                {
                                    "authorId": "1474356736",
                                    "name": "Dasol Hwang"
                                },
                                {
                                    "authorId": "2313692227",
                                    "name": "Moontae Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.74462890625
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."
                        ],
                        "paper": {
                            "corpus_id": 267681958,
                            "title": "Towards Safer Large Language Models through Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2122087252",
                                    "name": "Zheyuan Liu"
                                },
                                {
                                    "authorId": "2174956825",
                                    "name": "Guangyao Dou"
                                },
                                {
                                    "authorId": "2093186816",
                                    "name": "Zhaoxuan Tan"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2275403324",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task",
                            "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model",
                            "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."
                        ],
                        "paper": {
                            "corpus_id": 270703237,
                            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                            "authors": [
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2216505879",
                                    "name": "Yangfan Ye"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.8076171875
                    },
                    {
                        "id": "(Liu et al., 2022)",
                        "snippets": [
                            "As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."
                        ],
                        "paper": {
                            "corpus_id": 247627962,
                            "title": "Continual Learning and Private Unlearning",
                            "authors": [
                                {
                                    "authorId": "145306564",
                                    "name": "B. Liu"
                                },
                                {
                                    "authorId": "2155193246",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "144848112",
                                    "name": "P. Stone"
                                }
                            ],
                            "year": 2022,
                            "venue": "CoLLAs",
                            "n_citations": 62
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qiu et al., 2024)",
                        "snippets": [
                            "We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)",
                            "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."
                        ],
                        "paper": {
                            "corpus_id": 270703035,
                            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
                            "authors": [
                                {
                                    "authorId": "1702997626",
                                    "name": "Xinchi Qiu"
                                },
                                {
                                    "authorId": "2302373311",
                                    "name": "William F. Shen"
                                },
                                {
                                    "authorId": "2308073429",
                                    "name": "Yihong Chen"
                                },
                                {
                                    "authorId": "2313189467",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "1918552",
                                    "name": "Pontus Stenetorp"
                                },
                                {
                                    "authorId": "2298756346",
                                    "name": "N. Lane"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 3
                        },
                        "score": 0.76513671875
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Recent legislation of the \"right to be forgotten\" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks."
                        ],
                        "paper": {
                            "corpus_id": 258615571,
                            "title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment",
                            "authors": [
                                {
                                    "authorId": "2187207650",
                                    "name": "Lingzhi Wang"
                                },
                                {
                                    "authorId": "1490931831",
                                    "name": "Tong Chen"
                                },
                                {
                                    "authorId": "2106755543",
                                    "name": "Wei Yuan"
                                },
                                {
                                    "authorId": "46180553",
                                    "name": "Xingshan Zeng"
                                },
                                {
                                    "authorId": "1784988",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "2416851",
                                    "name": "Hongzhi Yin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 78
                        },
                        "score": 0
                    },
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages",
                            "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation",
                            "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."
                        ],
                        "paper": {
                            "corpus_id": 267681754,
                            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2284461064",
                                    "name": "Yijiang River Dong"
                                },
                                {
                                    "authorId": "2325112252",
                                    "name": "Hongzhou Lin"
                                },
                                {
                                    "authorId": "2284217404",
                                    "name": "Mikhail Belkin"
                                },
                                {
                                    "authorId": "2284217279",
                                    "name": "Ramon Huerta"
                                },
                                {
                                    "authorId": "2267339029",
                                    "name": "Ivan Vuli'c"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.77294921875
                    },
                    {
                        "id": "(Maini et al., 2024)",
                        "snippets": [
                            "Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."
                        ],
                        "paper": {
                            "corpus_id": 266933371,
                            "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "153742303",
                                    "name": "Pratyush Maini"
                                },
                                {
                                    "authorId": "2261439316",
                                    "name": "Zhili Feng"
                                },
                                {
                                    "authorId": "102604362",
                                    "name": "Avi Schwarzschild"
                                },
                                {
                                    "authorId": "32219137",
                                    "name": "Zachary Chase Lipton"
                                },
                                {
                                    "authorId": "2242257227",
                                    "name": "J. Kolter"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 194
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."
                        ],
                        "paper": {
                            "corpus_id": 269009619,
                            "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
                            "authors": [
                                {
                                    "authorId": "2295968232",
                                    "name": "Ruiqi Zhang"
                                },
                                {
                                    "authorId": "2257668289",
                                    "name": "Licong Lin"
                                },
                                {
                                    "authorId": "2257420186",
                                    "name": "Yu Bai"
                                },
                                {
                                    "authorId": "2257346113",
                                    "name": "Song Mei"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 193
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.173403
    }
}