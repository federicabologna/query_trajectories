{
    "query": "What are the key differences and tradeoffs between parameter optimization, parameter merging, in-context unlearning, and localization-based unlearning methods for removing knowledge from large language models?",
    "user_id": "lib_user",
    "task_id": "99f49ce7-478b-4545-a578-32ccf0ac0a97",
    "timestamp": "2025-06-23T23:00:34.149400",
    "n_retrieval": 256,
    "n_retrieved": 252,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.34997100000000003,
    "decomposed_query": {
        "rewritten_query": "Key differences and tradeoffs between parameter optimization, parameter merging, in-context unlearning, and localization-based unlearning methods for removing knowledge from large language models.",
        "keyword_query": "parameter optimization parameter merging in-context unlearning localization-based unlearning methods removing knowledge large language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010194,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 33,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "73502630",
                    "name": "Nianwen Si"
                },
                {
                    "authorId": "2154930608",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2116152318",
                    "name": "Heyu Chang"
                },
                {
                    "authorId": "9047584",
                    "name": "Wenlin Zhang"
                },
                {
                    "authorId": "2253591545",
                    "name": "Dan Qu"
                },
                {
                    "authorId": "2268429659",
                    "name": "Weiqiang Zhang"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "corpus_id": 265456592,
            "sentences": [
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
                    "score": 0.7117685963396945,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81982421875
                },
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "In this section, we introduce current LLMs knowledge unlearning methods, classifying them into three categories: those based on parameter optimization, parameter merging, and in-context learning, as illustrated in Table 1. Subsequent subsections will offer detailed explanations of these methods.",
                    "score": 0.6113525519643106,
                    "section_title": "Current Methods",
                    "char_start_offset": 12121,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 296
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6728515625
                },
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "Differing from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning. \n\nTV (arXiv 2022.12) This paper introduces the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. Assuming the weights of the original pretrained model are denoted as \u03b8 pre and the weights of the model finetuned for the target task are denoted as , the task vector \u03c4 is obtained by subtracting the two (i.e., \u03c4 = \u2212\u03b8 f t \u2212 \u03b8 pre ), as illustrated on the left side of Figure 4. This task vector \u03c4 represents the parameter change vector after fine-tuning the model for downstream tasks. Taking the negation of the task vector, \u2212\u03c4 , enables the language model to forget related knowledge while exerting minimal influence on other aspects of the model, as depicted on the right side of Figure 4. \n\nCPEM (arXiv 2023.06) This paper primarily addresses the parameter-efficient modules (PEM) for LLMs, such as LoRA [27] and (IA)3 [28]. It employs arithmetic operations, including addition and subtraction, on multiple modules to alter the representation of knowledge within these modules. Two basic operators are defined: the addition operator and the negation operator. The negation operator, in particular, facilitates the forgetting of knowledge stored in the adapter modules, providing a means to The above operation results in the reversal of activation values. The principle of this method is similar to gradient ascent, inducing a change in the intermediate layer's activation values in the direction opposite to gradient descent. This mechanism facilitates the unlearning of knowledge within the module.",
                    "score": 0.5879335655996897,
                    "section_title": "Parameter Merging",
                    "char_start_offset": 22212,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 591
                        },
                        {
                            "start": 594,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 837
                        },
                        {
                            "start": 838,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1223
                        },
                        {
                            "start": 1224,
                            "end": 1430
                        },
                        {
                            "start": 1433,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1566
                        },
                        {
                            "start": 1567,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 1997
                        },
                        {
                            "start": 1998,
                            "end": 2168
                        },
                        {
                            "start": 2169,
                            "end": 2242
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62353515625
                },
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "ities in tasks such as question-answering and reasoning. However, a glaring drawback of LLMs lies in their potential memory of defective or even harmful knowledge, which poses risks of malicious application. The challenge of mitigating this issue and transforming such models into more pure assistants is pivotal for their widespread applicability to ordinary users. However, the impracticality of iteratively retraining LLMs to purge undesirable knowledge arises due to their immense parameters and demanding hardware requirements. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model. This paper provides an in-depth review of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated. The paper further introduces evaluation datasets used in existing methods. Finally, a comprehensive analysis of ongoing Figure 1: Knowledge unlearning is used to eliminate harmful, privacy-sensitive, and copyright-related information from LLMs, ensuring the generation of reasonable responses in model output. Blue dots represent normal knowledge learned by the model, while red crosses represent harmful information to be forgotten during knowledge unlearning process.",
                    "score": 0.5909106025188933,
                    "section_title": "body",
                    "char_start_offset": 1,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 56
                        },
                        {
                            "start": 57,
                            "end": 207
                        },
                        {
                            "start": 208,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 532
                        },
                        {
                            "start": 533,
                            "end": 712
                        },
                        {
                            "start": 713,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1256
                        },
                        {
                            "start": 1257,
                            "end": 1331
                        },
                        {
                            "start": 1332,
                            "end": 1566
                        },
                        {
                            "start": 1567,
                            "end": 1726
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61474609375
                },
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "It stands as a necessary approach for developing responsible, legally compliant, and user-trusted LLMs. To propel the advancement of this field, this paper investigates existing research related to knowledge unlearning for LLMs, with a primary focus on the problems, methods, and future directions. To the best of our knowledge, this paper is one of the early works in researching this issue. The primary contributions of this paper are as follows: \n\n\u2022 Building upon research on machine unlearning, we introduce for the first time the concept of knowledge unlearning for LLMs. We analyze its differences and connections with machine unlearning. \n\n\u2022 We conduct a comprehensive literature review, and categorize existing methods for knowledge unlearning in LLMs, including methods based on parameter optimization, parameter merging, and in-context learning. Detailed introduction of the principles and characteristics of each method are then provided, as well as the datasets and tasks used in evaluation. \n\n\u2022 Based on an in-depth analysis of challenges and demands in this field, we unveil future research directions of knowledge unlearning in LLMs. \n\nThe rest of this survey is illustrated in Figure 2. Section 2 defines the problem of knowledge unlearning, comparing it with machine unlearning and model editing. Section 3 introduces knowledge unlearning methods for LLMs, categorizing them into three types: methods based on parameter optimization, parameter merging, and in-context learning. Section 4 presents relevant datasets and evaluations. Section 5 summarizes the work of this paper and discusses future directions.",
                    "score": 0.5870521641588982,
                    "section_title": "Introduction",
                    "char_start_offset": 5919,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 104,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 392
                        },
                        {
                            "start": 393,
                            "end": 448
                        },
                        {
                            "start": 451,
                            "end": 576
                        },
                        {
                            "start": 577,
                            "end": 644
                        },
                        {
                            "start": 647,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 1003
                        },
                        {
                            "start": 1006,
                            "end": 1148
                        },
                        {
                            "start": 1151,
                            "end": 1313
                        },
                        {
                            "start": 1314,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1625
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.495361328125
                }
            ],
            "relevance_judgement": 0.81982421875,
            "relevance_judgment_input_expanded": "# Title: Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges\n# Venue: arXiv.org\n# Authors: Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang\n## Abstract\nIn recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.\n## body\nities in tasks such as question-answering and reasoning. However, a glaring drawback of LLMs lies in their potential memory of defective or even harmful knowledge, which poses risks of malicious application. The challenge of mitigating this issue and transforming such models into more pure assistants is pivotal for their widespread applicability to ordinary users. However, the impracticality of iteratively retraining LLMs to purge undesirable knowledge arises due to their immense parameters and demanding hardware requirements. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model. This paper provides an in-depth review of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated. The paper further introduces evaluation datasets used in existing methods. Finally, a comprehensive analysis of ongoing Figure 1: Knowledge unlearning is used to eliminate harmful, privacy-sensitive, and copyright-related information from LLMs, ensuring the generation of reasonable responses in model output. Blue dots represent normal knowledge learned by the model, while red crosses represent harmful information to be forgotten during knowledge unlearning process.\n\n## Introduction\nIt stands as a necessary approach for developing responsible, legally compliant, and user-trusted LLMs. To propel the advancement of this field, this paper investigates existing research related to knowledge unlearning for LLMs, with a primary focus on the problems, methods, and future directions. To the best of our knowledge, this paper is one of the early works in researching this issue. The primary contributions of this paper are as follows: \n\n\u2022 Building upon research on machine unlearning, we introduce for the first time the concept of knowledge unlearning for LLMs. We analyze its differences and connections with machine unlearning. \n\n\u2022 We conduct a comprehensive literature review, and categorize existing methods for knowledge unlearning in LLMs, including methods based on parameter optimization, parameter merging, and in-context learning. Detailed introduction of the principles and characteristics of each method are then provided, as well as the datasets and tasks used in evaluation. \n\n\u2022 Based on an in-depth analysis of challenges and demands in this field, we unveil future research directions of knowledge unlearning in LLMs. \n\nThe rest of this survey is illustrated in Figure 2. Section 2 defines the problem of knowledge unlearning, comparing it with machine unlearning and model editing. Section 3 introduces knowledge unlearning methods for LLMs, categorizing them into three types: methods based on parameter optimization, parameter merging, and in-context learning. Section 4 presents relevant datasets and evaluations. Section 5 summarizes the work of this paper and discusses future directions.\n\n## Current Methods\nIn this section, we introduce current LLMs knowledge unlearning methods, classifying them into three categories: those based on parameter optimization, parameter merging, and in-context learning, as illustrated in Table 1. Subsequent subsections will offer detailed explanations of these methods.\n\n## Parameter Merging\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning. \n\nTV (arXiv 2022.12) This paper introduces the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. Assuming the weights of the original pretrained model are denoted as \u03b8 pre and the weights of the model finetuned for the target task are denoted as , the task vector \u03c4 is obtained by subtracting the two (i.e., \u03c4 = \u2212\u03b8 f t \u2212 \u03b8 pre ), as illustrated on the left side of Figure 4. This task vector \u03c4 represents the parameter change vector after fine-tuning the model for downstream tasks. Taking the negation of the task vector, \u2212\u03c4 , enables the language model to forget related knowledge while exerting minimal influence on other aspects of the model, as depicted on the right side of Figure 4. \n\nCPEM (arXiv 2023.06) This paper primarily addresses the parameter-efficient modules (PEM) for LLMs, such as LoRA [27] and (IA)3 [28]. It employs arithmetic operations, including addition and subtraction, on multiple modules to alter the representation of knowledge within these modules. Two basic operators are defined: the addition operator and the negation operator. The negation operator, in particular, facilitates the forgetting of knowledge stored in the adapter modules, providing a means to The above operation results in the reversal of activation values. The principle of this method is similar to gradient ascent, inducing a change in the intermediate layer's activation values in the direction opposite to gradient descent. This mechanism facilitates the unlearning of knowledge within the module.",
            "reference_string": "[265456592 | Si et al. | 2023 | Citations: 33]"
        },
        {
            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2202361389",
                    "name": "P. Guo"
                },
                {
                    "authorId": "2201328926",
                    "name": "Aaquib Syed"
                },
                {
                    "authorId": "2284684654",
                    "name": "A. Sheshadri"
                },
                {
                    "authorId": "2287842553",
                    "name": "Aidan Ewart"
                },
                {
                    "authorId": "2533850",
                    "name": "G. Dziugaite"
                }
            ],
            "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
            "corpus_id": 273403717,
            "sentences": [
                {
                    "corpus_id": "273403717",
                    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
                    "text": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
                    "score": 0.6211195503889952,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80029296875
                }
            ],
            "relevance_judgement": 0.80029296875,
            "relevance_judgment_input_expanded": "# Title: Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization\n# Venue: arXiv.org\n# Authors: P. Guo, Aaquib Syed, A. Sheshadri, Aidan Ewart, G. Dziugaite\n## Abstract\nMethods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.\n",
            "reference_string": "[273403717 | Guo et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Machine Unlearning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268643078",
                    "name": "Kongyang Chen"
                },
                {
                    "authorId": "2289862169",
                    "name": "Zixin Wang"
                },
                {
                    "authorId": "2212851422",
                    "name": "Bing Mi"
                },
                {
                    "authorId": "2298857854",
                    "name": "Waixi Liu"
                },
                {
                    "authorId": "2295540521",
                    "name": "Shaowei Wang"
                },
                {
                    "authorId": "2298846891",
                    "name": "Xiaojun Ren"
                },
                {
                    "authorId": "2295552260",
                    "name": "Jiaxing Shen"
                }
            ],
            "abstract": "Recently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.",
            "corpus_id": 269430574,
            "sentences": [
                {
                    "corpus_id": "269430574",
                    "title": "Machine Unlearning in Large Language Models",
                    "text": "In the process of implementing unlearning, adjusting various hyperparameters based on the chosen fine-tuning approach is essential.Both full-parameter fine-tuning and Low-Rank Adaptation (LoRA) are efficacious for fine-tuning large language models, yet they present distinct trade-offs.Full-parameter fine-tuning provides extensive adaptability and the potential for high performance, albeit at the expense of increased computational load and time consumption.Conversely, LoRA offers a more efficient and swift finetuning option, ideal for situations with limited resources or the need for rapid iterations, but it may not perform as well in more intricate tasks when compared to fullparameter fine-tuning.The selection between these methods hinges on the specific requirements of the task, resource availability, and time limitations.Consequently, achieving an equilibrium between computational resource allocation and unlearning efficacy is imperative.",
                    "score": 0.5647809841770692,
                    "section_title": "Balancing Resources and Unlearning Effectiveness",
                    "char_start_offset": 20705,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 131,
                            "end": 286
                        },
                        {
                            "start": 286,
                            "end": 460
                        },
                        {
                            "start": 460,
                            "end": 706
                        },
                        {
                            "start": 706,
                            "end": 835
                        },
                        {
                            "start": 835,
                            "end": 954
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7900390625
                }
            ],
            "relevance_judgement": 0.7900390625,
            "relevance_judgment_input_expanded": "# Title: Machine Unlearning in Large Language Models\n# Venue: arXiv.org\n# Authors: Kongyang Chen, Zixin Wang, Bing Mi, Waixi Liu, Shaowei Wang, Xiaojun Ren, Jiaxing Shen\n## Abstract\nRecently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.\n## Balancing Resources and Unlearning Effectiveness\nIn the process of implementing unlearning, adjusting various hyperparameters based on the chosen fine-tuning approach is essential.Both full-parameter fine-tuning and Low-Rank Adaptation (LoRA) are efficacious for fine-tuning large language models, yet they present distinct trade-offs.Full-parameter fine-tuning provides extensive adaptability and the potential for high performance, albeit at the expense of increased computational load and time consumption.Conversely, LoRA offers a more efficient and swift finetuning option, ideal for situations with limited resources or the need for rapid iterations, but it may not perform as well in more intricate tasks when compared to fullparameter fine-tuning.The selection between these methods hinges on the specific requirements of the task, resource availability, and time limitations.Consequently, achieving an equilibrium between computational resource allocation and unlearning efficacy is imperative.",
            "reference_string": "[269430574 | Chen et al. | 2024 | Citations: 13]"
        },
        {
            "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323938152",
                    "name": "Miao Yu"
                },
                {
                    "authorId": "2362620538",
                    "name": "Liang Lin"
                },
                {
                    "authorId": "2232926268",
                    "name": "Guibin Zhang"
                },
                {
                    "authorId": "2349829666",
                    "name": "Xinfeng Li"
                },
                {
                    "authorId": "2362314471",
                    "name": "Junfeng Fang"
                },
                {
                    "authorId": "2357266201",
                    "name": "Ningyu Zhang"
                },
                {
                    "authorId": "2363281493",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2362692320",
                    "name": "Yang Wang"
                }
            ],
            "abstract": "Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.",
            "corpus_id": 278782460,
            "sentences": [],
            "relevance_judgement": 0.7744140625,
            "relevance_judgment_input_expanded": "# Title: UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models\n# Venue: \n# Authors: Miao Yu, Liang Lin, Guibin Zhang, Xinfeng Li, Junfeng Fang, Ningyu Zhang, Kun Wang, Yang Wang\n## Abstract\nLarge language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.\n",
            "reference_string": "[278782460 | Yu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Undesirable Memorization in Large Language Models: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 158,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02650, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165227497",
                    "name": "Ali Satvaty"
                },
                {
                    "authorId": "2280669651",
                    "name": "Suzan Verberne"
                },
                {
                    "authorId": "2324053570",
                    "name": "Fatih Turkmen"
                }
            ],
            "abstract": "While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it is equally crucial to examine their associated risks. Among these, privacy and security vulnerabilities are particularly concerning, posing significant ethical and legal challenges. At the heart of these vulnerabilities stands memorization, which refers to a model's tendency to store and reproduce phrases from its training data. This phenomenon has been shown to be a fundamental source to various privacy and security attacks against LLMs. In this paper, we provide a taxonomy of the literature on LLM memorization, exploring it across three dimensions: granularity, retrievability, and desirability. Next, we discuss the metrics and methods used to quantify memorization, followed by an analysis of the causes and factors that contribute to memorization phenomenon. We then explore strategies that are used so far to mitigate the undesirable aspects of this phenomenon. We conclude our survey by identifying potential research topics for the near future, including methods to balance privacy and performance, and the analysis of memorization in specific LLM contexts such as conversational agents, retrieval-augmented generation, and diffusion language models. Given the rapid research pace in this field, we also maintain a dedicated repository of the references discussed in this survey which will be regularly updated to reflect the latest developments.",
            "corpus_id": 273098143,
            "sentences": [
                {
                    "corpus_id": "273098143",
                    "title": "Undesirable Memorization in Large Language Models: A Survey",
                    "text": "By fine-tuning the model with a negative similarity score as a reward signal, the approach encourages the LLM to paraphrase and unlearn pre-training data while maintaining performance. \n\nAdditionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs [128]. These methods not only enhance privacy and security but also ensure that the overall performance of the models remains intact, making them scalable and practical for real-world applications [129]. \n\nUnlearning methods offer effective strategies to mitigate memorization by selectively removing specific information from trained models without the need for full retraining. However, these methods are generally designed to target specific pieces of information, which means they rely on the ability to identify what the model has memorized beforehand. This reliance poses a challenge, as determining the exact content a model has memorized can be difficult and may limit the applicability of unlearning techniques for general mitigation against memorization.",
                    "score": 0.6389792851758286,
                    "section_title": "C. Unlearning methods",
                    "char_start_offset": 49513,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 187,
                            "end": 433
                        },
                        {
                            "start": 434,
                            "end": 630
                        },
                        {
                            "start": 633,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1191
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 427,
                            "end": 432,
                            "matchedPaperCorpusId": "265456592"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76611328125
                }
            ],
            "relevance_judgement": 0.76611328125,
            "relevance_judgment_input_expanded": "# Title: Undesirable Memorization in Large Language Models: A Survey\n# Venue: arXiv.org\n# Authors: Ali Satvaty, Suzan Verberne, Fatih Turkmen\n## Abstract\nWhile recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it is equally crucial to examine their associated risks. Among these, privacy and security vulnerabilities are particularly concerning, posing significant ethical and legal challenges. At the heart of these vulnerabilities stands memorization, which refers to a model's tendency to store and reproduce phrases from its training data. This phenomenon has been shown to be a fundamental source to various privacy and security attacks against LLMs. In this paper, we provide a taxonomy of the literature on LLM memorization, exploring it across three dimensions: granularity, retrievability, and desirability. Next, we discuss the metrics and methods used to quantify memorization, followed by an analysis of the causes and factors that contribute to memorization phenomenon. We then explore strategies that are used so far to mitigate the undesirable aspects of this phenomenon. We conclude our survey by identifying potential research topics for the near future, including methods to balance privacy and performance, and the analysis of memorization in specific LLM contexts such as conversational agents, retrieval-augmented generation, and diffusion language models. Given the rapid research pace in this field, we also maintain a dedicated repository of the references discussed in this survey which will be regularly updated to reflect the latest developments.\n## C. Unlearning methods\nBy fine-tuning the model with a negative similarity score as a reward signal, the approach encourages the LLM to paraphrase and unlearn pre-training data while maintaining performance. \n\nAdditionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs [128]. These methods not only enhance privacy and security but also ensure that the overall performance of the models remains intact, making them scalable and practical for real-world applications [129]. \n\nUnlearning methods offer effective strategies to mitigate memorization by selectively removing specific information from trained models without the need for full retraining. However, these methods are generally designed to target specific pieces of information, which means they rely on the ability to identify what the model has memorized beforehand. This reliance poses a challenge, as determining the exact content a model has memorized can be difficult and may limit the applicability of unlearning techniques for general mitigation against memorization.",
            "reference_string": "[273098143 | Satvaty et al. | 2024 | Citations: 7]"
        },
        {
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00376, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303304464",
                    "name": "Shiji Zhou"
                },
                {
                    "authorId": "2233281230",
                    "name": "Lianzhe Wang"
                },
                {
                    "authorId": "2277872538",
                    "name": "Jiangnan Ye"
                },
                {
                    "authorId": "2314337350",
                    "name": "Yongliang Wu"
                },
                {
                    "authorId": "2303324907",
                    "name": "Heng Chang"
                }
            ],
            "abstract": "Generative AI (GenAI), which aims to synthesize realistic and diverse data samples from latent variables or other data modalities, has achieved remarkable results in various domains, such as natural language, images, audio, and graphs. However, they also pose challenges and risks to data privacy, security, and ethics. Machine unlearning is the process of removing or weakening the influence of specific data samples or features from a trained model, without affecting its performance on other data or tasks. While machine unlearning has shown significant efficacy in traditional machine learning tasks, it is still unclear if it could help GenAI become safer and aligned with human desire. To this end, this position paper provides an in-depth discussion of the machine unlearning approaches for GenAI. Firstly, we formulate the problem of machine unlearning tasks on GenAI and introduce the background. Subsequently, we systematically examine the limitations of machine unlearning on GenAI models by focusing on the two representative branches: LLMs and image generative (diffusion) models. Finally, we provide our prospects mainly from three aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and conscientiously advocate for the future development of this field.",
            "corpus_id": 271601132,
            "sentences": [
                {
                    "corpus_id": "271601132",
                    "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
                    "text": "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters.",
                    "score": 0.7104311779223096,
                    "section_title": "Unlearning for LLM",
                    "char_start_offset": 5182,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 232
                        },
                        {
                            "start": 235,
                            "end": 266
                        },
                        {
                            "start": 267,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 782
                        },
                        {
                            "start": 785,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1848
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75732421875
                }
            ],
            "relevance_judgement": 0.75732421875,
            "relevance_judgment_input_expanded": "# Title: On the Limitations and Prospects of Machine Unlearning for Generative AI\n# Venue: arXiv.org\n# Authors: Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, Heng Chang\n## Abstract\nGenerative AI (GenAI), which aims to synthesize realistic and diverse data samples from latent variables or other data modalities, has achieved remarkable results in various domains, such as natural language, images, audio, and graphs. However, they also pose challenges and risks to data privacy, security, and ethics. Machine unlearning is the process of removing or weakening the influence of specific data samples or features from a trained model, without affecting its performance on other data or tasks. While machine unlearning has shown significant efficacy in traditional machine learning tasks, it is still unclear if it could help GenAI become safer and aligned with human desire. To this end, this position paper provides an in-depth discussion of the machine unlearning approaches for GenAI. Firstly, we formulate the problem of machine unlearning tasks on GenAI and introduce the background. Subsequently, we systematically examine the limitations of machine unlearning on GenAI models by focusing on the two representative branches: LLMs and image generative (diffusion) models. Finally, we provide our prospects mainly from three aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and conscientiously advocate for the future development of this field.\n## Unlearning for LLM\nMachine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters.",
            "reference_string": "[271601132 | Zhou et al. | 2024 | Citations: 5]"
        },
        {
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01854, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2295742465",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2215923457",
                    "name": "Herbert Woisetschlaeger"
                },
                {
                    "authorId": "2109296817",
                    "name": "Zongxiong Chen"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2238271670",
                    "name": "Hans-Arno Jacobsen"
                },
                {
                    "authorId": "2265969003",
                    "name": "Fakhri Karray"
                }
            ],
            "abstract": "This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",
            "corpus_id": 276772996,
            "sentences": [
                {
                    "corpus_id": "276772996",
                    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                    "text": "We present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.",
                    "score": 0.5805151317321092,
                    "section_title": "Taxonomy",
                    "char_start_offset": 6148,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 352
                        },
                        {
                            "start": 353,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 771
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7509765625
                },
                {
                    "corpus_id": "276772996",
                    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                    "text": "Input/output-based unlearning methods offer flexibility by using prompt engineering and post-processing without modifying the model weights or the architecture. Liu et al. [2024a] proposed training a prompt classifier to identify prompts within the scope of unlearning and efficiently corrupting Privacy & Copyright them in the embedding space using zeroth-order optimization. Thaker et al. [2024] showed that simple guardrails like prompting and input/output filtering can effectively support unlearning independently or alongside fine-tuning. Pawelczyk et al. [2025] proposed in-context unlearning by constructing tailored prompts, where the labels of the data to be forgotten are flipped. \n\nBrief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning [Lynch et al., 2024]. Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment. From an efficiency standpoint, reinforcement learning and locate-then-unlearn approaches involve higher algorithmic complexity. Reinforcement learning requires training high-quality models to generate reward signals, while locate-then-unlearn methods rely on precise knowledge localization. Moreover, contrastive decoding and input/output-based methods increase the computational overhead during inference, which leads to slower generation.",
                    "score": 0.5997909716620096,
                    "section_title": "Input/Output-Based Unlearning",
                    "char_start_offset": 15474,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 691
                        },
                        {
                            "start": 694,
                            "end": 950
                        },
                        {
                            "start": 951,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1472
                        },
                        {
                            "start": 1473,
                            "end": 1631
                        },
                        {
                            "start": 1632,
                            "end": 1764
                        },
                        {
                            "start": 1765,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2150
                        },
                        {
                            "start": 2151,
                            "end": 2300
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 161,
                            "end": 179,
                            "matchedPaperCorpusId": "275932019"
                        },
                        {
                            "start": 1451,
                            "end": 1471,
                            "matchedPaperCorpusId": "273849875"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55712890625
                },
                {
                    "corpus_id": "276772996",
                    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                    "text": "This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",
                    "score": 0.6262919054686547,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5244140625
                }
            ],
            "relevance_judgement": 0.7509765625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models\n# Venue: arXiv.org\n# Authors: Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, Fakhri Karray\n## Abstract\nThis study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.\n## Taxonomy\nWe present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.\n\n## Input/Output-Based Unlearning\nInput/output-based unlearning methods offer flexibility by using prompt engineering and post-processing without modifying the model weights or the architecture. Liu et al. [2024a] proposed training a prompt classifier to identify prompts within the scope of unlearning and efficiently corrupting Privacy & Copyright them in the embedding space using zeroth-order optimization. Thaker et al. [2024] showed that simple guardrails like prompting and input/output filtering can effectively support unlearning independently or alongside fine-tuning. Pawelczyk et al. [2025] proposed in-context unlearning by constructing tailored prompts, where the labels of the data to be forgotten are flipped. \n\nBrief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning [Lynch et al., 2024]. Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment. From an efficiency standpoint, reinforcement learning and locate-then-unlearn approaches involve higher algorithmic complexity. Reinforcement learning requires training high-quality models to generate reward signals, while locate-then-unlearn methods rely on precise knowledge localization. Moreover, contrastive decoding and input/output-based methods increase the computational overhead during inference, which leads to slower generation.",
            "reference_string": "[276772996 | Geng et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06606, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306946364",
                    "name": "Yihuai Hong"
                },
                {
                    "authorId": "2325111074",
                    "name": "Yuelin Zou"
                },
                {
                    "authorId": "2153121378",
                    "name": "Lijie Hu"
                },
                {
                    "authorId": "2325714896",
                    "name": "Ziqian Zeng"
                },
                {
                    "authorId": "2303281490",
                    "name": "Di Wang"
                },
                {
                    "authorId": "2307764517",
                    "name": "Haiqin Yang"
                }
            ],
            "abstract": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",
            "corpus_id": 273228619,
            "sentences": [
                {
                    "corpus_id": "273228619",
                    "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
                    "text": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",
                    "score": 0.7258145358731027,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7470703125
                },
                {
                    "corpus_id": "273228619",
                    "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
                    "text": "We have deeply investigated the reasons why finetuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.",
                    "score": 0.5784340957618286,
                    "section_title": "Discussion and Conclusion",
                    "char_start_offset": 16100,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 550
                        },
                        {
                            "start": 551,
                            "end": 951
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.697265625
                }
            ],
            "relevance_judgement": 0.7470703125,
            "relevance_judgment_input_expanded": "# Title: Dissecting Fine-Tuning Unlearning in Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang\n## Abstract\nFine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.\n## Discussion and Conclusion\nWe have deeply investigated the reasons why finetuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.",
            "reference_string": "[273228619 | Hong et al. | 2024 | Citations: 4]"
        },
        {
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 84,
            "influential_citation_count": 30,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2286638403",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2261353791",
                    "name": "Jaechan Lee"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2266698166",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2309248199",
                    "name": "Ari Holtzman"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2309424274",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                }
            ],
            "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "corpus_id": 271064299,
            "sentences": [
                {
                    "corpus_id": "271064299",
                    "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                    "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
                    "score": 0.6909917818088447,
                    "section_title": "Related Work",
                    "char_start_offset": 24726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 65,
                            "end": 144
                        },
                        {
                            "start": 144,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 578
                        },
                        {
                            "start": 580,
                            "end": 744
                        },
                        {
                            "start": 746,
                            "end": 912
                        },
                        {
                            "start": 912,
                            "end": 1134
                        },
                        {
                            "start": 1134,
                            "end": 1196
                        },
                        {
                            "start": 1196,
                            "end": 1418
                        },
                        {
                            "start": 1418,
                            "end": 1635
                        },
                        {
                            "start": 1635,
                            "end": 1881
                        },
                        {
                            "start": 1883,
                            "end": 1934
                        },
                        {
                            "start": 1934,
                            "end": 2021
                        },
                        {
                            "start": 2021,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 382,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 1380,
                            "end": 1396,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 1396,
                            "end": 1417,
                            "matchedPaperCorpusId": "259088549"
                        },
                        {
                            "start": 1563,
                            "end": 1580,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73681640625
                }
            ],
            "relevance_judgement": 0.73681640625,
            "relevance_judgment_input_expanded": "# Title: MUSE: Machine Unlearning Six-Way Evaluation for Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke S. Zettlemoyer, Noah A. Smith, Chiyuan Zhang\n## Abstract\nLanguage models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io\n## Related Work\nMachine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "reference_string": "[271064299 | Shi et al. | 2024 | Citations: 84]"
        },
        {
            "title": "On Large Language Model Continual Unlearning",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10223, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311833838",
                    "name": "Chongyang Gao"
                },
                {
                    "authorId": "2108631414",
                    "name": "Lixu Wang"
                },
                {
                    "authorId": "2148353350",
                    "name": "Chenkai Weng"
                },
                {
                    "authorId": "2276121035",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2275773112",
                    "name": "Qi Zhu"
                }
            ],
            "abstract": "While large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the OOO framework that includes an Orthogonal low-rank adapter (LoRA) for continually unlearning requested data and an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our OOO framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, OOO's effectiveness does not rely on any retained data. We conducted extensive experiments on OOO and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that OOO consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.",
            "corpus_id": 271212701,
            "sentences": [
                {
                    "corpus_id": "271212701",
                    "title": "On Large Language Model Continual Unlearning",
                    "text": "Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
                    "score": 0.7871838115724189,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1068
                        },
                        {
                            "start": 1069,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1321
                        },
                        {
                            "start": 1324,
                            "end": 1470
                        },
                        {
                            "start": 1471,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1950
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 168,
                            "end": 187,
                            "matchedPaperCorpusId": "261064713"
                        },
                        {
                            "start": 357,
                            "end": 375,
                            "matchedPaperCorpusId": "220938739"
                        },
                        {
                            "start": 680,
                            "end": 698,
                            "matchedPaperCorpusId": "255825985"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69482421875
                }
            ],
            "relevance_judgement": 0.69482421875,
            "relevance_judgment_input_expanded": "# Title: On Large Language Model Continual Unlearning\n# Venue: International Conference on Learning Representations\n# Authors: Chongyang Gao, Lixu Wang, Chenkai Weng, Xiao Wang, Qi Zhu\n## Abstract\nWhile large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security by removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the OOO framework that includes an Orthogonal low-rank adapter (LoRA) for continually unlearning requested data and an Out-Of-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our OOO framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, OOO's effectiveness does not rely on any retained data. We conducted extensive experiments on OOO and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that OOO consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.\n## INTRODUCTION\nRecently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
            "reference_string": "[271212701 | Gao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 24,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11614, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306946364",
                    "name": "Yihuai Hong"
                },
                {
                    "authorId": "2306950395",
                    "name": "Lei Yu"
                },
                {
                    "authorId": "2143278592",
                    "name": "Shauli Ravfogel"
                },
                {
                    "authorId": "2307764517",
                    "name": "Haiqin Yang"
                },
                {
                    "authorId": "22245981",
                    "name": "Mor Geva"
                }
            ],
            "abstract": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
            "corpus_id": 270560986,
            "sentences": [
                {
                    "corpus_id": "270560986",
                    "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                    "text": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
                    "score": 0.6299125052083784,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67138671875
                },
                {
                    "corpus_id": "270560986",
                    "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                    "text": "This could imply that \"overriding\" the LLM computation is easier for fine-tuning methods than modifying the LLM parametric knowledge. \n\nConsidering unlearning specificity (Target-versus Unrelated-QA performance), by unlearning a certain concept all methods also hurt the ability of the model to generate information about other concepts, as scores roughly correlate for these two tasks. \n\nOverall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods.",
                    "score": 0.6145730032420187,
                    "section_title": "RESULTS",
                    "char_start_offset": 25424,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 136,
                            "end": 386
                        },
                        {
                            "start": 389,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 645
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.498779296875
                }
            ],
            "relevance_judgement": 0.67138671875,
            "relevance_judgment_input_expanded": "# Title: Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces\n# Venue: arXiv.org\n# Authors: Yihuai Hong, Lei Yu, Shauli Ravfogel, Haiqin Yang, Mor Geva\n## Abstract\nThe task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.\n## RESULTS\nThis could imply that \"overriding\" the LLM computation is easier for fine-tuning methods than modifying the LLM parametric knowledge. \n\nConsidering unlearning specificity (Target-versus Unrelated-QA performance), by unlearning a certain concept all methods also hurt the ability of the model to generate information about other concepts, as scores roughly correlate for these two tasks. \n\nOverall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods.",
            "reference_string": "[270560986 | Hong et al. | 2024 | Citations: 24]"
        },
        {
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 132,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07579",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89583148",
                    "name": "Martin Pawelczyk"
                },
                {
                    "authorId": "2273685865",
                    "name": "Seth Neel"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ],
            "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "corpus_id": 263834631,
            "sentences": [
                {
                    "corpus_id": "263834631",
                    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                    "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
                    "score": 0.7356754854793558,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.654296875
                }
            ],
            "relevance_judgement": 0.654296875,
            "relevance_judgment_input_expanded": "# Title: In-Context Unlearning: Language Models as Few Shot Unlearners\n# Venue: International Conference on Machine Learning\n# Authors: Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju\n## Abstract\nMachine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.\n",
            "reference_string": "[263834631 | Pawelczyk et al. | 2023 | Citations: 132]"
        },
        {
            "title": "Machine Unlearning in Generative AI: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 164,
            "citation_count": 19,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.20516, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2122087252",
                    "name": "Zheyuan Liu"
                },
                {
                    "authorId": "2174956825",
                    "name": "Guangyao Dou"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "46879986",
                    "name": "Yijun Tian"
                },
                {
                    "authorId": "2275403324",
                    "name": "Meng Jiang"
                }
            ],
            "abstract": "Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.",
            "corpus_id": 271543835,
            "sentences": [
                {
                    "corpus_id": "271543835",
                    "title": "Machine Unlearning in Generative AI: A Survey",
                    "text": "The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.",
                    "score": 0.6271165623304642,
                    "section_title": "Summary:",
                    "char_start_offset": 52294,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1234
                        },
                        {
                            "start": 1235,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1461
                        },
                        {
                            "start": 1462,
                            "end": 1637
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6513671875
                }
            ],
            "relevance_judgement": 0.6513671875,
            "relevance_judgment_input_expanded": "# Title: Machine Unlearning in Generative AI: A Survey\n# Venue: arXiv.org\n# Authors: Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang\n## Abstract\nGenerative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.\n## Summary:\nThe parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.",
            "reference_string": "[271543835 | Liu et al. | 2024 | Citations: 19]"
        },
        {
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "venue": "Nat. Mac. Intell.",
            "year": 2024,
            "reference_count": 154,
            "citation_count": 119,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254478722",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2257134957",
                    "name": "Yuanshun Yao"
                },
                {
                    "authorId": "2051552791",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "2284064052",
                    "name": "Stephen Casper"
                },
                {
                    "authorId": "2284064162",
                    "name": "Nathalie Baracaldo"
                },
                {
                    "authorId": "2266467463",
                    "name": "Peter Hase"
                },
                {
                    "authorId": "2259623067",
                    "name": "Xiaojun Xu"
                },
                {
                    "authorId": "100630765",
                    "name": "Yuguang Yao"
                },
                {
                    "authorId": "2271515779",
                    "name": "Chris Liu"
                },
                {
                    "authorId": "2257196326",
                    "name": "Hang Li"
                },
                {
                    "authorId": "1491635392",
                    "name": "Kush R. Varshney"
                },
                {
                    "authorId": "2253762115",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "123593472",
                    "name": "Sanmi Koyejo"
                },
                {
                    "authorId": "2257387078",
                    "name": "Yang Liu"
                }
            ],
            "abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
            "corpus_id": 267657624,
            "sentences": [
                {
                    "corpus_id": "267657624",
                    "title": "Rethinking Machine Unlearning for Large Language Models",
                    "text": "This position paper rethinks the paradigm of unlearning for modern LLMs to uncover its under-explored aspects. To achieve this, we dissect LLM unlearning into four essential aspects: formulation, methodologies, evaluation metrics, and applications. We show that there are considerable challenges in both foundational research and practical, use case-driven research. These include: (Generality) A desired solution for LLM unlearning should take into account the generality of the unlearning target and dataset choice, accommodate various model setups including both white-box and black-box scenarios, and consider the specifics of the unlearning method. (Authenticity) LLM unlearning should focus on effectively removing both data influence and specific model capabilities, in order to authenticate unlearning across a range of evaluation methods, particularly in adversarial contexts. (Precision) LLM unlearning should precisely define the scope of unlearning, while ensuring the preservation of general language modeling performance outside this unlearning scope. \n\nBy examining the current state of the art, we gain insights for the future development of LLM unlearning. For example, localization-informed unlearning shows promise with possible dual advantages of efficiency and efficacy. Effective unlearning requires careful consideration of data-model influences and adversaries. Despite similarities between LLM unlearning and model editing, they differ in their formulation and methodological design. Furthermore, insights gained from the study of LLM unlearning could catalyze technological advancements in other types of foundation models, e.g., large vision-language models.",
                    "score": 0.5786487017277127,
                    "section_title": "Discussion and Conclusion",
                    "char_start_offset": 37022,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 885
                        },
                        {
                            "start": 886,
                            "end": 1065
                        },
                        {
                            "start": 1068,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1291
                        },
                        {
                            "start": 1292,
                            "end": 1385
                        },
                        {
                            "start": 1386,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1685
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65087890625
                }
            ],
            "relevance_judgement": 0.65087890625,
            "relevance_judgment_input_expanded": "# Title: Rethinking Machine Unlearning for Large Language Models\n# Venue: Nat. Mac. Intell.\n# Authors: Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Chris Liu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu\n## Abstract\nWe explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.\n## Discussion and Conclusion\nThis position paper rethinks the paradigm of unlearning for modern LLMs to uncover its under-explored aspects. To achieve this, we dissect LLM unlearning into four essential aspects: formulation, methodologies, evaluation metrics, and applications. We show that there are considerable challenges in both foundational research and practical, use case-driven research. These include: (Generality) A desired solution for LLM unlearning should take into account the generality of the unlearning target and dataset choice, accommodate various model setups including both white-box and black-box scenarios, and consider the specifics of the unlearning method. (Authenticity) LLM unlearning should focus on effectively removing both data influence and specific model capabilities, in order to authenticate unlearning across a range of evaluation methods, particularly in adversarial contexts. (Precision) LLM unlearning should precisely define the scope of unlearning, while ensuring the preservation of general language modeling performance outside this unlearning scope. \n\nBy examining the current state of the art, we gain insights for the future development of LLM unlearning. For example, localization-informed unlearning shows promise with possible dual advantages of efficiency and efficacy. Effective unlearning requires careful consideration of data-model influences and adversaries. Despite similarities between LLM unlearning and model editing, they differ in their formulation and methodological design. Furthermore, insights gained from the study of LLM unlearning could catalyze technological advancements in other types of foundation models, e.g., large vision-language models.",
            "reference_string": "[267657624 | Liu et al. | 2024 | Citations: 119]"
        },
        {
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273843751",
                    "name": "Xiaojian Yuan"
                },
                {
                    "authorId": "19201674",
                    "name": "Tianyu Pang"
                },
                {
                    "authorId": "2325201427",
                    "name": "Chao Du"
                },
                {
                    "authorId": "8780109",
                    "name": "Kejiang Chen"
                },
                {
                    "authorId": "2189835131",
                    "name": "Weiming Zhang"
                },
                {
                    "authorId": "2253977831",
                    "name": "Min Lin"
                }
            ],
            "abstract": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
            "corpus_id": 273233618,
            "sentences": [
                {
                    "corpus_id": "273233618",
                    "title": "A Closer Look at Machine Unlearning for Large Language Models",
                    "text": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
                    "score": 0.6217970919026614,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6396484375
                }
            ],
            "relevance_judgement": 0.6396484375,
            "relevance_judgment_input_expanded": "# Title: A Closer Look at Machine Unlearning for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin\n## Abstract\nLarge language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.\n",
            "reference_string": "[273233618 | Yuan et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Erasing Conceptual Knowledge from Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52017367",
                    "name": "Rohit Gandikota"
                },
                {
                    "authorId": "2140009998",
                    "name": "Sheridan Feucht"
                },
                {
                    "authorId": "2225941937",
                    "name": "Samuel Marks"
                },
                {
                    "authorId": "2284996653",
                    "name": "David Bau"
                }
            ],
            "abstract": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "corpus_id": 273098800,
            "sentences": [
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
                    "score": 0.5784052859233151,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1370
                        },
                        {
                            "start": 1373,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2465
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6376953125
                }
            ],
            "relevance_judgement": 0.6376953125,
            "relevance_judgment_input_expanded": "# Title: Erasing Conceptual Knowledge from Language Models\n# Venue: arXiv.org\n# Authors: Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau\n## Abstract\nIn this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info\n## Introduction\nWhat does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
            "reference_string": "[273098800 | Gandikota et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "venue": "",
            "year": 2024,
            "reference_count": 52,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363922",
                    "name": "Qizhou Wang"
                },
                {
                    "authorId": "2301791087",
                    "name": "Bo Han"
                },
                {
                    "authorId": "2306755179",
                    "name": "Puning Yang"
                },
                {
                    "authorId": "2143475848",
                    "name": "Jianing Zhu"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "2313482750",
                    "name": "Masashi Sugiyama"
                }
            ],
            "abstract": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
            "corpus_id": 270440344,
            "sentences": [
                {
                    "corpus_id": "270440344",
                    "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
                    "text": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
                    "score": 0.5954909386605226,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6240234375
                }
            ],
            "relevance_judgement": 0.6240234375,
            "relevance_judgment_input_expanded": "# Title: Towards Effective Evaluations and Comparisons for LLM Unlearning Methods\n# Venue: \n# Authors: Qizhou Wang, Bo Han, Puning Yang, Jianing Zhu, Tongliang Liu, Masashi Sugiyama\n## Abstract\nThe imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.\n",
            "reference_string": "[270440344 | Wang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Unlearnable Algorithms for In-context Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.00751, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282138927",
                    "name": "Andrei Muresanu"
                },
                {
                    "authorId": "2052302616",
                    "name": "Anvith Thudi"
                },
                {
                    "authorId": "2282199470",
                    "name": "Michael R. Zhang"
                },
                {
                    "authorId": "1967156",
                    "name": "Nicolas Papernot"
                }
            ],
            "abstract": "Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.",
            "corpus_id": 267365499,
            "sentences": [
                {
                    "corpus_id": "267365499",
                    "title": "Unlearnable Algorithms for In-context Learning",
                    "text": "Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.",
                    "score": 0.5978010960872868,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60498046875
                }
            ],
            "relevance_judgement": 0.60498046875,
            "relevance_judgment_input_expanded": "# Title: Unlearnable Algorithms for In-context Learning\n# Venue: arXiv.org\n# Authors: Andrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot\n## Abstract\nMachine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.\n",
            "reference_string": "[267365499 | Muresanu et al. | 2024 | Citations: 13]"
        },
        {
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348951919",
                    "name": "Wenyu Wang"
                },
                {
                    "authorId": "48985110",
                    "name": "Mengqi Zhang"
                },
                {
                    "authorId": "2286432237",
                    "name": "Xiaotian Ye"
                },
                {
                    "authorId": "2260895127",
                    "name": "Zhaochun Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                }
            ],
            "abstract": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "corpus_id": 276812969,
            "sentences": [
                {
                    "corpus_id": "276812969",
                    "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                    "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
                    "score": 0.7457780735760895,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5986328125
                }
            ],
            "relevance_judgement": 0.5986328125,
            "relevance_judgment_input_expanded": "# Title: UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets\n# Venue: arXiv.org\n# Authors: Wenyu Wang, Mengqi Zhang, Xiaotian Ye, Zhaochun Ren, Zhumin Chen, Pengjie Ren\n## Abstract\nLarge Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.\n",
            "reference_string": "[276812969 | Wang et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324784973",
                    "name": "Zhiqi Bu"
                },
                {
                    "authorId": "2327893325",
                    "name": "Xiaomeng Jin"
                },
                {
                    "authorId": "3236313",
                    "name": "B. Vinzamuri"
                },
                {
                    "authorId": "2328076404",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2278433136",
                    "name": "Mingyi Hong"
                }
            ],
            "abstract": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
            "corpus_id": 273661686,
            "sentences": [
                {
                    "corpus_id": "273661686",
                    "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
                    "text": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
                    "score": 0.5964690849374952,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.591796875
                }
            ],
            "relevance_judgement": 0.591796875,
            "relevance_judgment_input_expanded": "# Title: Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Zhiqi Bu, Xiaomeng Jin, B. Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, V. Cevher, Mingyi Hong\n## Abstract\nMachine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.\n",
            "reference_string": "[273661686 | Bu et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 59,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.04626, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305621566",
                    "name": "Kevin Kuo"
                },
                {
                    "authorId": "80366270",
                    "name": "Amrith Rajagopal Setlur"
                },
                {
                    "authorId": "2302322978",
                    "name": "Kartik Srinivas"
                },
                {
                    "authorId": "2269982126",
                    "name": "Aditi Raghunathan"
                },
                {
                    "authorId": "2305622207",
                    "name": "Virginia Smith"
                }
            ],
            "abstract": "Approximate unlearning has gained popularity as an approach to efficiently update an LLM so that it behaves (roughly) as if it was not trained on a subset of data to begin with. However, existing methods are brittle in practice and can easily be attacked to reveal supposedly unlearned information. To alleviate issues with approximate unlearning, we instead propose SIFT-Masks (SIgn-Fixed Tuning-Masks), an exact unlearning method based on model merging. SIFT-Masks addresses two key limitations of standard model merging: (1) merging a large number of tasks can severely harm utility; and (2) methods that boost utility by sharing extra information across tasks make exact unlearning prohibitively expensive. SIFT-Masks solves these issues by (1) applying local masks to recover task-specific performance; and (2) constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging. Across four settings where we merge up to 500 models, SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines.",
            "corpus_id": 277620868,
            "sentences": [
                {
                    "corpus_id": "277620868",
                    "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale",
                    "text": "1. We merge up to 500 models, which is more than an order of magnitude more tasks than prior work (Ilharco et al., 2023;Wang et al., 2024). Our setting reflects the realistic scenario in unlearning where models are finetuned over a large set of tasks (e.g., a pool of clients or collection of documents), and tasks contribute subtly differing data for a common learning task rather than the tasks being largely disjoint. \n\n2. We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible. \n\n3. We propose SIFT, a finetuning method which makes model merging and localization computationally feasible for exact unlearning. Unlike existing methods where masks require global information, SIFT-Masks obtains masks by using only a random sign vector and local data. This allows us to maintain accuracy while enabling efficient exact unlearning at scale: As we show through extensive experiments on unlearning tasks, SIFT-Masks improves accuracy by 5-80% over na\u00efve merging and uses up to 250\u00d7 less compute for unlearning compared to other merging baselines.",
                    "score": 0.5885054403542265,
                    "section_title": "body",
                    "char_start_offset": 3693,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 420
                        },
                        {
                            "start": 423,
                            "end": 686
                        },
                        {
                            "start": 689,
                            "end": 818
                        },
                        {
                            "start": 819,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1250
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 98,
                            "end": 120,
                            "matchedPaperCorpusId": "254408495"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58251953125
                }
            ],
            "relevance_judgement": 0.58251953125,
            "relevance_judgment_input_expanded": "# Title: Exact Unlearning of Finetuning Data via Model Merging at Scale\n# Venue: arXiv.org\n# Authors: Kevin Kuo, Amrith Rajagopal Setlur, Kartik Srinivas, Aditi Raghunathan, Virginia Smith\n## Abstract\nApproximate unlearning has gained popularity as an approach to efficiently update an LLM so that it behaves (roughly) as if it was not trained on a subset of data to begin with. However, existing methods are brittle in practice and can easily be attacked to reveal supposedly unlearned information. To alleviate issues with approximate unlearning, we instead propose SIFT-Masks (SIgn-Fixed Tuning-Masks), an exact unlearning method based on model merging. SIFT-Masks addresses two key limitations of standard model merging: (1) merging a large number of tasks can severely harm utility; and (2) methods that boost utility by sharing extra information across tasks make exact unlearning prohibitively expensive. SIFT-Masks solves these issues by (1) applying local masks to recover task-specific performance; and (2) constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging. Across four settings where we merge up to 500 models, SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines.\n## body\n1. We merge up to 500 models, which is more than an order of magnitude more tasks than prior work (Ilharco et al., 2023;Wang et al., 2024). Our setting reflects the realistic scenario in unlearning where models are finetuned over a large set of tasks (e.g., a pool of clients or collection of documents), and tasks contribute subtly differing data for a common learning task rather than the tasks being largely disjoint. \n\n2. We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible. \n\n3. We propose SIFT, a finetuning method which makes model merging and localization computationally feasible for exact unlearning. Unlike existing methods where masks require global information, SIFT-Masks obtains masks by using only a random sign vector and local data. This allows us to maintain accuracy while enabling efficient exact unlearning at scale: As we show through extensive experiments on unlearning tasks, SIFT-Masks improves accuracy by 5-80% over na\u00efve merging and uses up to 250\u00d7 less compute for unlearning compared to other merging baselines.",
            "reference_string": "[277620868 | Kuo et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.04388, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2343642992",
                    "name": "Teodora Baluta"
                },
                {
                    "authorId": "3087941",
                    "name": "Pascal Lamblin"
                },
                {
                    "authorId": "2288056916",
                    "name": "Daniel Tarlow"
                },
                {
                    "authorId": "2310335993",
                    "name": "F. Pedregosa"
                },
                {
                    "authorId": "2533850",
                    "name": "G. Dziugaite"
                }
            ],
            "abstract": "Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.",
            "corpus_id": 273877462,
            "sentences": [
                {
                    "corpus_id": "273877462",
                    "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
                    "text": "Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.",
                    "score": 0.5722811849325894,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.578125
                }
            ],
            "relevance_judgement": 0.578125,
            "relevance_judgment_input_expanded": "# Title: Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method\n# Venue: arXiv.org\n# Authors: Teodora Baluta, Pascal Lamblin, Daniel Tarlow, F. Pedregosa, G. Dziugaite\n## Abstract\nMachine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.\n",
            "reference_string": "[273877462 | Baluta et al. | 2024 | Citations: 2]"
        },
        {
            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296414239",
                    "name": "Hyoseo Kim"
                },
                {
                    "authorId": "2325152150",
                    "name": "Dongyoon Han"
                },
                {
                    "authorId": "3338475",
                    "name": "Junsuk Choe"
                }
            ],
            "abstract": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.",
            "corpus_id": 273228310,
            "sentences": [
                {
                    "corpus_id": "273228310",
                    "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
                    "text": "Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently. Empirical evidence supporting our claim is presented in Section 4.3. Additionally, for successful unlearning, the task vector should exclusively represent the information to be forgotten within the framework of negation. This requires that the fine-tuned model precisely fits the forget set while it preserves the original knowledge. \n\nAchieving both is challenging due to the nature of fine-tuning, where the only forget set is accessible; a model that fits the forget set well will inevitably lose knowledge of the retain set, and vice versa (Kirkpatrick et al., 2017). As we observed, this makes it difficult to achieve the desired balance with only a single model, which leads us to consider that aggregating multiple models could suggest more effective unlearning. However, we also observed that conventional model merging methods like Uniform Merging, Greedy Merging, TIES-Merging, and MagMax, which do not account for the characteristics of machine unlearning, fail to escape this trade-off as displayed in Figure 1 (a). \n\nIn contrast, our method, specialized in unlearning, surpasses this trade-off and achieves superior performance. We aim to use the given fine-tuned models more effectively and enhance unlearning outcomes while avoiding additional training costs. More details will be provided in Section 3.2. \n\nOur Unlearning scenarios. In our study, we explore two distinct unlearning scenarios. The first scenario is the one described above, where a vision-language model like CLIP (Radford et al., 2021) is made to forget the knowledge of a specific class. For this scenario, we adopt the evaluation protocol for unlearning proposed in the original paper (Ilharco et al., 2022).",
                    "score": 0.6151934562466528,
                    "section_title": "METHOD",
                    "char_start_offset": 10333,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 130
                        },
                        {
                            "start": 131,
                            "end": 282
                        },
                        {
                            "start": 283,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 639
                        },
                        {
                            "start": 640,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 904
                        },
                        {
                            "start": 907,
                            "end": 1142
                        },
                        {
                            "start": 1143,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1598
                        },
                        {
                            "start": 1601,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1891
                        },
                        {
                            "start": 1894,
                            "end": 1919
                        },
                        {
                            "start": 1920,
                            "end": 1979
                        },
                        {
                            "start": 1980,
                            "end": 2142
                        },
                        {
                            "start": 2143,
                            "end": 2264
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1115,
                            "end": 1141,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 2067,
                            "end": 2089,
                            "matchedPaperCorpusId": "231591445"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57080078125
                }
            ],
            "relevance_judgement": 0.57080078125,
            "relevance_judgment_input_expanded": "# Title: NegMerge: Consensual Weight Negation for Strong Machine Unlearning\n# Venue: arXiv.org\n# Authors: Hyoseo Kim, Dongyoon Han, Junsuk Choe\n## Abstract\nMachine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.\n## METHOD\nSecond, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently. Empirical evidence supporting our claim is presented in Section 4.3. Additionally, for successful unlearning, the task vector should exclusively represent the information to be forgotten within the framework of negation. This requires that the fine-tuned model precisely fits the forget set while it preserves the original knowledge. \n\nAchieving both is challenging due to the nature of fine-tuning, where the only forget set is accessible; a model that fits the forget set well will inevitably lose knowledge of the retain set, and vice versa (Kirkpatrick et al., 2017). As we observed, this makes it difficult to achieve the desired balance with only a single model, which leads us to consider that aggregating multiple models could suggest more effective unlearning. However, we also observed that conventional model merging methods like Uniform Merging, Greedy Merging, TIES-Merging, and MagMax, which do not account for the characteristics of machine unlearning, fail to escape this trade-off as displayed in Figure 1 (a). \n\nIn contrast, our method, specialized in unlearning, surpasses this trade-off and achieves superior performance. We aim to use the given fine-tuned models more effectively and enhance unlearning outcomes while avoiding additional training costs. More details will be provided in Section 3.2. \n\nOur Unlearning scenarios. In our study, we explore two distinct unlearning scenarios. The first scenario is the one described above, where a vision-language model like CLIP (Radford et al., 2021) is made to forget the knowledge of a specific class. For this scenario, we adopt the evaluation protocol for unlearning proposed in the original paper (Ilharco et al., 2022).",
            "reference_string": "[273228310 | Kim et al. | 2024 | Citations: 3]"
        },
        {
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284461064",
                    "name": "Yijiang River Dong"
                },
                {
                    "authorId": "2325112252",
                    "name": "Hongzhou Lin"
                },
                {
                    "authorId": "2284217404",
                    "name": "Mikhail Belkin"
                },
                {
                    "authorId": "2284217279",
                    "name": "Ramon Huerta"
                },
                {
                    "authorId": "2267339029",
                    "name": "Ivan Vuli'c"
                }
            ],
            "abstract": "Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.",
            "corpus_id": 267681754,
            "sentences": [
                {
                    "corpus_id": "267681754",
                    "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                    "text": "Another line of research focuses on directly tuning the base LLM model to unlearn sensitive information, using techniques such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024). These approaches are gaining more attention as they align more closely with the growing emphasis on AI safety (Gallegos et al., 2023;\u0141ucki et al., 2024). \n\nDespite these advances, the recent unlearning benchmark MUSE (Shi et al., 2024) highlights a major drawback in current methods: applying unlearning to larger corpora leads to a decline in general language usefulness. This limits its usage in real-world settings, as an effective unlearning method must scale reliably with increasing data sizes, and accommodate continual updates-all while maintaining the model's overall language capabilities. \n\nIn this work, we introduce a novel direct-tuning Figure 1: An illustration of the self-distillation process in the proposed UNDIAL method: The original logits generated by the model are adjusted by subtracting the one-hot distribution of the target token. The student model is then fine-tuned to approximate this modified logit distribution. Since the adjustments rely solely on the original model's outputs, this is a self-distillation process to de-emphasize the token to be forgotten. \n\nmethod, UNDIAL, which enables Unlearning via Self-Distillation with Adjusted Logits. As shown in Figure 1, we generate a target distribution by reducing the logit of the token to be unlearned. This target distribution is fixed during self-distillation, ensuring a stable optimization process. Unlike GA and NPO, which suffer from significant model capacity degradation as datasets scale and training extends, UNDIAL demonstrates strong robustness to data scaling, hyperparameter tuning, and sequential unlearning, offering the first robust unlearning method for direct tuning LLMs. \n\nOur main contributions are as follows. 1) We identify the robustness issues in current unlearning methods and propose a new, more robust method based on self-distillation.",
                    "score": 0.5982175147438742,
                    "section_title": "Introduction",
                    "char_start_offset": 1723,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 389
                        },
                        {
                            "start": 392,
                            "end": 608
                        },
                        {
                            "start": 609,
                            "end": 835
                        },
                        {
                            "start": 838,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1325
                        },
                        {
                            "start": 1328,
                            "end": 1412
                        },
                        {
                            "start": 1413,
                            "end": 1520
                        },
                        {
                            "start": 1521,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1909
                        },
                        {
                            "start": 1912,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2083
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 151,
                            "end": 170,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 214,
                            "end": 234,
                            "matchedPaperCorpusId": "269009619"
                        },
                        {
                            "start": 369,
                            "end": 388,
                            "matchedPaperCorpusId": "272910981"
                        },
                        {
                            "start": 453,
                            "end": 471,
                            "matchedPaperCorpusId": "271064299"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5703125
                },
                {
                    "corpus_id": "267681754",
                    "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
                    "text": "Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.",
                    "score": 0.5977952592352,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53955078125
                }
            ],
            "relevance_judgement": 0.5703125,
            "relevance_judgment_input_expanded": "# Title: UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vuli'c\n## Abstract\nMitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.\n## Introduction\nAnother line of research focuses on directly tuning the base LLM model to unlearn sensitive information, using techniques such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024). These approaches are gaining more attention as they align more closely with the growing emphasis on AI safety (Gallegos et al., 2023;\u0141ucki et al., 2024). \n\nDespite these advances, the recent unlearning benchmark MUSE (Shi et al., 2024) highlights a major drawback in current methods: applying unlearning to larger corpora leads to a decline in general language usefulness. This limits its usage in real-world settings, as an effective unlearning method must scale reliably with increasing data sizes, and accommodate continual updates-all while maintaining the model's overall language capabilities. \n\nIn this work, we introduce a novel direct-tuning Figure 1: An illustration of the self-distillation process in the proposed UNDIAL method: The original logits generated by the model are adjusted by subtracting the one-hot distribution of the target token. The student model is then fine-tuned to approximate this modified logit distribution. Since the adjustments rely solely on the original model's outputs, this is a self-distillation process to de-emphasize the token to be forgotten. \n\nmethod, UNDIAL, which enables Unlearning via Self-Distillation with Adjusted Logits. As shown in Figure 1, we generate a target distribution by reducing the logit of the token to be unlearned. This target distribution is fixed during self-distillation, ensuring a stable optimization process. Unlike GA and NPO, which suffer from significant model capacity degradation as datasets scale and training extends, UNDIAL demonstrates strong robustness to data scaling, hyperparameter tuning, and sequential unlearning, offering the first robust unlearning method for direct tuning LLMs. \n\nOur main contributions are as follows. 1) We identify the robustness issues in current unlearning methods and propose a new, more robust method based on self-distillation.",
            "reference_string": "[267681754 | Dong et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314871557",
                    "name": "Xiaohua Feng"
                },
                {
                    "authorId": "2251485995",
                    "name": "Chao-Jun Chen"
                },
                {
                    "authorId": "1527113700",
                    "name": "Yuyuan Li"
                },
                {
                    "authorId": "2261893790",
                    "name": "Zibin Lin"
                }
            ],
            "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.",
            "corpus_id": 273901406,
            "sentences": [
                {
                    "corpus_id": "273901406",
                    "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                    "text": "Analogous to machine unlearning, the principles of knowledge unlearning include the following three aspects, albeit with a different focus: \n\n\u2022 Unlearning Efficiency: Due to the enormous sizes of data and parameters, knowledge unlearning not only considers time efficiency, but also emphasizes the computational feasibility for regular users in practical settings. \u2022 Unlearning Completeness: Also referred to as unlearning efficacy and forgetting quality in the literature. Retraining is the only authorized way to achieve exact unlearning. However, given the massive scale of language models, frequent retraining incurs extremely high training costs. Therefore, existing methods mainly focus on approximate unlearning. Secondly, due to the immense size of parameters, evaluating completeness mainly relies on comparing the model outputs, as directly comparing model parameters also incurs excessive cost. \u2022 General Ability: Maintaining the language utility is a crucial principle of unlearning. A sound unlearning method should selectively remove only the knowledge of target data, and avoid overunlearn that could compromise the general ability of language models.",
                    "score": 0.6436888174356462,
                    "section_title": "Unlearning Principles",
                    "char_start_offset": 10552,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 142,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 540
                        },
                        {
                            "start": 541,
                            "end": 651
                        },
                        {
                            "start": 652,
                            "end": 719
                        },
                        {
                            "start": 720,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1166
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5654296875
                },
                {
                    "corpus_id": "273901406",
                    "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                    "text": "Fine-tuning has emerged as a viable approach for achieving knowledge unlearning in language models. Jang et al. (2023) explore the use of gradient ascent to effectively facilitate unlearning. Chen and Yang (2023) further introduce PE fine-tuning to enhance efficiency. However, this method only alters the model's behavior to mimic unlearning, without actually updating the parameters of the original model. Based on this line of research, the regularization-based approach incorporates regularization terms that leverage other data and models. Chen and Yang (2023) and Yao et al. (2023) utilize Kullback-Leibler (KL) divergence for regularization, aiming to maintain the model's general ability. Similarly, Rafailov et al. (2023) uses direct preference optimization as a form of regularization to guide the unlearning process. Note that while both the regularization-based approach and our proposed adaptive objective aim to enhance the model's general ability, they differ in their methodologies. The regularization-based approach complements the fine-tuning approach, introducing additional regularization terms. In contrast, our proposed method directly modifies the fine-tuning approach, shaping the tuning process to achieve improved results. Furthermore, the regularization-based approach can complement our  proposed method, which we will investigate in our experiments.",
                    "score": 0.59815004994322,
                    "section_title": "Fine-tuning",
                    "char_start_offset": 7034,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 99
                        },
                        {
                            "start": 100,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 998
                        },
                        {
                            "start": 999,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1378
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 100,
                            "end": 118,
                            "matchedPaperCorpusId": "252693065"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.496337890625
                }
            ],
            "relevance_judgement": 0.5654296875,
            "relevance_judgment_input_expanded": "# Title: Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Feng, Chao-Jun Chen, Yuyuan Li, Zibin Lin\n## Abstract\nPre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.\n## Fine-tuning\nFine-tuning has emerged as a viable approach for achieving knowledge unlearning in language models. Jang et al. (2023) explore the use of gradient ascent to effectively facilitate unlearning. Chen and Yang (2023) further introduce PE fine-tuning to enhance efficiency. However, this method only alters the model's behavior to mimic unlearning, without actually updating the parameters of the original model. Based on this line of research, the regularization-based approach incorporates regularization terms that leverage other data and models. Chen and Yang (2023) and Yao et al. (2023) utilize Kullback-Leibler (KL) divergence for regularization, aiming to maintain the model's general ability. Similarly, Rafailov et al. (2023) uses direct preference optimization as a form of regularization to guide the unlearning process. Note that while both the regularization-based approach and our proposed adaptive objective aim to enhance the model's general ability, they differ in their methodologies. The regularization-based approach complements the fine-tuning approach, introducing additional regularization terms. In contrast, our proposed method directly modifies the fine-tuning approach, shaping the tuning process to achieve improved results. Furthermore, the regularization-based approach can complement our  proposed method, which we will investigate in our experiments.\n\n## Unlearning Principles\nAnalogous to machine unlearning, the principles of knowledge unlearning include the following three aspects, albeit with a different focus: \n\n\u2022 Unlearning Efficiency: Due to the enormous sizes of data and parameters, knowledge unlearning not only considers time efficiency, but also emphasizes the computational feasibility for regular users in practical settings. \u2022 Unlearning Completeness: Also referred to as unlearning efficacy and forgetting quality in the literature. Retraining is the only authorized way to achieve exact unlearning. However, given the massive scale of language models, frequent retraining incurs extremely high training costs. Therefore, existing methods mainly focus on approximate unlearning. Secondly, due to the immense size of parameters, evaluating completeness mainly relies on comparing the model outputs, as directly comparing model parameters also incurs excessive cost. \u2022 General Ability: Maintaining the language utility is a crucial principle of unlearning. A sound unlearning method should selectively remove only the knowledge of target data, and avoid overunlearn that could compromise the general ability of language models.",
            "reference_string": "[273901406 | Feng et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Not All Data Are Unlearned Equally",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.05058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354118238",
                    "name": "Aravind Krishnan"
                },
                {
                    "authorId": "2323901201",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "2269460274",
                    "name": "Marius Mosbach"
                }
            ],
            "abstract": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
            "corpus_id": 277622234,
            "sentences": [
                {
                    "corpus_id": "277622234",
                    "title": "Not All Data Are Unlearned Equally",
                    "text": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
                    "score": 0.6835000625936674,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55908203125
                }
            ],
            "relevance_judgement": 0.55908203125,
            "relevance_judgment_input_expanded": "# Title: Not All Data Are Unlearned Equally\n# Venue: arXiv.org\n# Authors: Aravind Krishnan, Siva Reddy, Marius Mosbach\n## Abstract\nMachine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.\n",
            "reference_string": "[277622234 | Krishnan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01472, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283127924",
                    "name": "Jinwei Hu"
                },
                {
                    "authorId": "2334128394",
                    "name": "Zhenglin Huang"
                },
                {
                    "authorId": "2273924576",
                    "name": "Xiangyu Yin"
                },
                {
                    "authorId": "8103173",
                    "name": "Wenjie Ruan"
                },
                {
                    "authorId": "2333978808",
                    "name": "Guangliang Cheng"
                },
                {
                    "authorId": "2304902376",
                    "name": "Yi Dong"
                },
                {
                    "authorId": "2288349292",
                    "name": "Xiaowei Huang"
                }
            ],
            "abstract": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
            "corpus_id": 276107656,
            "sentences": [
                {
                    "corpus_id": "276107656",
                    "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
                    "text": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
                    "score": 0.5903748728052752,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5576171875
                }
            ],
            "relevance_judgement": 0.5576171875,
            "relevance_judgment_input_expanded": "# Title: FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model\n# Venue: arXiv.org\n# Authors: Jinwei Hu, Zhenglin Huang, Xiangyu Yin, Wenjie Ruan, Guangliang Cheng, Yi Dong, Xiaowei Huang\n## Abstract\nLarge language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.\n",
            "reference_string": "[276107656 | Hu et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "2149157242",
                    "name": "Sungjun Cho"
                },
                {
                    "authorId": "1474356736",
                    "name": "Dasol Hwang"
                },
                {
                    "authorId": "2313692227",
                    "name": "Moontae Lee"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
            "corpus_id": 271860124,
            "sentences": [
                {
                    "corpus_id": "271860124",
                    "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                    "text": "This motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
                    "score": 0.5662618155520024,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1605,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 980
                        },
                        {
                            "start": 983,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1633
                        },
                        {
                            "start": 1634,
                            "end": 1732
                        },
                        {
                            "start": 1735,
                            "end": 1990
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 441,
                            "end": 459,
                            "matchedPaperCorpusId": "258615571"
                        },
                        {
                            "start": 464,
                            "end": 481,
                            "matchedPaperCorpusId": "267681958"
                        },
                        {
                            "start": 1129,
                            "end": 1146,
                            "matchedPaperCorpusId": "235458009"
                        },
                        {
                            "start": 1478,
                            "end": 1501,
                            "matchedPaperCorpusId": "269791237"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5478515625
                }
            ],
            "relevance_judgement": 0.5478515625,
            "relevance_judgment_input_expanded": "# Title: Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs\n# Venue: International Conference on Learning Representations\n# Authors: Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee\n## Abstract\nLarge Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.\n## INTRODUCTION\nThis motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
            "reference_string": "[271860124 | Cha et al. | 2024 | Citations: 2]"
        },
        {
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356009299",
                    "name": "Kun-Woo Kim"
                },
                {
                    "authorId": "2257098510",
                    "name": "Ji-Hoon Park"
                },
                {
                    "authorId": "2355145341",
                    "name": "Jumin Han"
                },
                {
                    "authorId": "2339467966",
                    "name": "Seong-Whan Lee"
                }
            ],
            "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.",
            "corpus_id": 277857590,
            "sentences": [
                {
                    "corpus_id": "277857590",
                    "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
                    "text": "Large Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.",
                    "score": 0.6013421504697353,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54736328125
                }
            ],
            "relevance_judgement": 0.54736328125,
            "relevance_judgment_input_expanded": "# Title: GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs\n# Venue: arXiv.org\n# Authors: Kun-Woo Kim, Ji-Hoon Park, Jumin Han, Seong-Whan Lee\n## Abstract\nLarge Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.\n",
            "reference_string": "[277857590 | Kim et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 4,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04076, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2005445750",
                    "name": "Xuhan Zuo"
                },
                {
                    "authorId": "2005212347",
                    "name": "Minghao Wang"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "48571174",
                    "name": "Lefeng Zhang"
                },
                {
                    "authorId": "2261513740",
                    "name": "Dayong Ye"
                },
                {
                    "authorId": "2304458654",
                    "name": "Shui Yu"
                },
                {
                    "authorId": "2134555583",
                    "name": "Wanlei Zhou"
                }
            ],
            "abstract": "The development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data. This is because training a LLM needs a large demanding of new data. Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model. However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms. Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary. To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities. Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism. We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process. Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning. Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs.",
            "corpus_id": 270286011,
            "sentences": [
                {
                    "corpus_id": "270286011",
                    "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
                    "text": "The rapid advancements in large language models (LLMs) have led to remarkable breakthroughs in natural language processing and artificial intelligence.However, as these models are trained on vast amounts of data, they may inadvertently learn and perpetuate undesirable behaviors, biases, and harmful information.To address this issue, researchers have recently turned their attention to the concept of unlearning in LLMs.\n\nIn [18] paper, the authors explore the novel concept of unlearning in large language models (LLMs).They present a method that utilizes only negative examples to efficiently remove undesirable behaviors, demonstrating its effectiveness in alignment while significantly reducing computational resources compared to traditional reinforcement learning from human feedback (RLHF).While there is another paper introduces a data-driven unlearning approach for large language models (LLMs), utilizing a fine-tuning method informed by the importance of weights and relabeling during the pre-training phase of LLMs [19].This method adjusts word embedding, involving identifying and neutralizing bias vectors within the embedding space to prevent biased associations.Wang et al. [20] proposed an unlearning framework called Knowledge Gap Alignment (KGA), emphasizing its capability to efficiently handle large-scale data removal requests with significant accuracy.However, the inability of KGA to guarantee the complete removal of data influences also faces the challenge of maintaining extra data sets and models.Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.This approach not only advances the field of responsible AI but also opens new avenues for enhancing data privacy and model impartiality.Huang et al. claim an innovation offset unlearning framework tailored for the black box LLM [22].This framework effectively addresses the challenge of unlearning problematic training data in LLMs without requiring access to internal model weight, thus offering a versatile solution for adapting current unlearning algorithms.",
                    "score": 0.6482011732135302,
                    "section_title": "B. Unlearning with LLM",
                    "char_start_offset": 9557,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 151,
                            "end": 312
                        },
                        {
                            "start": 312,
                            "end": 421
                        },
                        {
                            "start": 423,
                            "end": 522
                        },
                        {
                            "start": 522,
                            "end": 798
                        },
                        {
                            "start": 798,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1179
                        },
                        {
                            "start": 1179,
                            "end": 1376
                        },
                        {
                            "start": 1376,
                            "end": 1526
                        },
                        {
                            "start": 1526,
                            "end": 1826
                        },
                        {
                            "start": 1826,
                            "end": 1963
                        },
                        {
                            "start": 1963,
                            "end": 2060
                        },
                        {
                            "start": 2060,
                            "end": 2288
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1028,
                            "end": 1032,
                            "matchedPaperCorpusId": "259859034"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54052734375
                }
            ],
            "relevance_judgement": 0.54052734375,
            "relevance_judgment_input_expanded": "# Title: Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning\n# Venue: arXiv.org\n# Authors: Xuhan Zuo, Minghao Wang, Tianqing Zhu, Lefeng Zhang, Dayong Ye, Shui Yu, Wanlei Zhou\n## Abstract\nThe development of Large Language Models (LLMs) faces a significant challenge: the exhausting of publicly available fresh data. This is because training a LLM needs a large demanding of new data. Federated learning emerges as a promising solution, enabling collaborative model to contribute their private data to LLM global model. However, integrating federated learning with LLMs introduces new challenges, including the lack of transparency and the need for effective unlearning mechanisms. Transparency is essential to ensuring trust and fairness among participants, while accountability is crucial for deterring malicious behaviour and enabling corrective actions when necessary. To address these challenges, we propose a novel blockchain-based federated learning framework for LLMs that enhances transparency, accountability, and unlearning capabilities. Our framework leverages blockchain technology to create a tamper-proof record of each model's contributions and introduces an innovative unlearning function that seamlessly integrates with the federated learning mechanism. We investigate the impact of Low-Rank Adaptation (LoRA) hyperparameters on unlearning performance and integrate Hyperledger Fabric to ensure the security, transparency, and verifiability of the unlearning process. Through comprehensive experiments and analysis, we showcase the effectiveness of our proposed framework in achieving highly effective unlearning in LLMs trained using federated learning. Our findings highlight the feasibility of integrating blockchain technology into federated learning frameworks for LLMs.\n## B. Unlearning with LLM\nThe rapid advancements in large language models (LLMs) have led to remarkable breakthroughs in natural language processing and artificial intelligence.However, as these models are trained on vast amounts of data, they may inadvertently learn and perpetuate undesirable behaviors, biases, and harmful information.To address this issue, researchers have recently turned their attention to the concept of unlearning in LLMs.\n\nIn [18] paper, the authors explore the novel concept of unlearning in large language models (LLMs).They present a method that utilizes only negative examples to efficiently remove undesirable behaviors, demonstrating its effectiveness in alignment while significantly reducing computational resources compared to traditional reinforcement learning from human feedback (RLHF).While there is another paper introduces a data-driven unlearning approach for large language models (LLMs), utilizing a fine-tuning method informed by the importance of weights and relabeling during the pre-training phase of LLMs [19].This method adjusts word embedding, involving identifying and neutralizing bias vectors within the embedding space to prevent biased associations.Wang et al. [20] proposed an unlearning framework called Knowledge Gap Alignment (KGA), emphasizing its capability to efficiently handle large-scale data removal requests with significant accuracy.However, the inability of KGA to guarantee the complete removal of data influences also faces the challenge of maintaining extra data sets and models.Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.This approach not only advances the field of responsible AI but also opens new avenues for enhancing data privacy and model impartiality.Huang et al. claim an innovation offset unlearning framework tailored for the black box LLM [22].This framework effectively addresses the challenge of unlearning problematic training data in LLMs without requiring access to internal model weight, thus offering a versatile solution for adapting current unlearning algorithms.",
            "reference_string": "[270286011 | Zuo et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04795, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135634919",
                    "name": "Dinesh P. Srivasthav"
                },
                {
                    "authorId": "9266407",
                    "name": "B. Garlapati"
                }
            ],
            "abstract": "Large Language Models (LLMs) face significant challenges in maintaining privacy, ethics, and compliance, when sensitive or obsolete data must be selectively removed. Retraining these models from scratch is computationally infeasible, necessitating efficient alternatives. As part of the SemEval 2025 Task 4, this work focuses on the application of selective unlearning in LLMs to address this challenge. In this paper, we present our experiments and findings, primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility. We also detail the task-specific evaluation mechanism, results, and challenges. Our algorithms have achieved an aggregate score of 0.409 and 0.389 on the test set for 7B and 1B target models, respectively, demonstrating promising results in verifiable LLM unlearning.",
            "corpus_id": 276885223,
            "sentences": [
                {
                    "corpus_id": "276885223",
                    "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
                    "text": "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2025). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024;Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;Wu et al., 2023;Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024). Architecture modification based approaches involve tweaking the model's architecture such as by adding additional layers (Chen and Yang, 2023), or by using external modules (Ji et al., 2024;Zhang et al., 2023) in addition to the target model, etc. These approaches, while advantageous in other contexts, were not suitable for this specific task's setup.",
                    "score": 0.5580778866162122,
                    "section_title": "Related works",
                    "char_start_offset": 2724,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 649
                        },
                        {
                            "start": 650,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1435
                        },
                        {
                            "start": 1436,
                            "end": 1666
                        },
                        {
                            "start": 1667,
                            "end": 1914
                        },
                        {
                            "start": 1915,
                            "end": 2020
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 223,
                            "matchedPaperCorpusId": "268856658"
                        },
                        {
                            "start": 433,
                            "end": 452,
                            "matchedPaperCorpusId": "273901406"
                        },
                        {
                            "start": 618,
                            "end": 637,
                            "matchedPaperCorpusId": "271909642"
                        },
                        {
                            "start": 1180,
                            "end": 1196,
                            "matchedPaperCorpusId": "264816202"
                        },
                        {
                            "start": 1788,
                            "end": 1809,
                            "matchedPaperCorpusId": "264828972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53955078125
                }
            ],
            "relevance_judgement": 0.53955078125,
            "relevance_judgment_input_expanded": "# Title: Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models\n# Venue: arXiv.org\n# Authors: Dinesh P. Srivasthav, B. Garlapati\n## Abstract\nLarge Language Models (LLMs) face significant challenges in maintaining privacy, ethics, and compliance, when sensitive or obsolete data must be selectively removed. Retraining these models from scratch is computationally infeasible, necessitating efficient alternatives. As part of the SemEval 2025 Task 4, this work focuses on the application of selective unlearning in LLMs to address this challenge. In this paper, we present our experiments and findings, primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility. We also detail the task-specific evaluation mechanism, results, and challenges. Our algorithms have achieved an aggregate score of 0.409 and 0.389 on the test set for 7B and 1B target models, respectively, demonstrating promising results in verifiable LLM unlearning.\n## Related works\nThe approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2025). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024;Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;Wu et al., 2023;Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024). Architecture modification based approaches involve tweaking the model's architecture such as by adding additional layers (Chen and Yang, 2023), or by using external modules (Ji et al., 2024;Zhang et al., 2023) in addition to the target model, etc. These approaches, while advantageous in other contexts, were not suitable for this specific task's setup.",
            "reference_string": "[276885223 | Srivasthav et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2005445750",
                    "name": "Xuhan Zuo"
                },
                {
                    "authorId": "2005212347",
                    "name": "Minghao Wang"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "2304458654",
                    "name": "Shui Yu"
                },
                {
                    "authorId": "2134555583",
                    "name": "Wanlei Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.",
            "corpus_id": 274823032,
            "sentences": [
                {
                    "corpus_id": "274823032",
                    "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
                    "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
                    "score": 0.6156743999613749,
                    "section_title": "B. Unlearning with LLM",
                    "char_start_offset": 8767,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 1035
                        },
                        {
                            "start": 1038,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1663
                        },
                        {
                            "start": 1666,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2000
                        },
                        {
                            "start": 2001,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53662109375
                }
            ],
            "relevance_judgement": 0.53662109375,
            "relevance_judgment_input_expanded": "# Title: Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration\n# Venue: arXiv.org\n# Authors: Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou\n## Abstract\nLarge language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.\n## B. Unlearning with LLM\nThe challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "reference_string": "[274823032 | Zuo et al. | 2024 | Citations: 2]"
        },
        {
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12996, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2355988770",
                    "name": "Saransh Agrawal"
                },
                {
                    "authorId": "2355889422",
                    "name": "Kuan-Hao Huang"
                }
            ],
            "abstract": "Large language models (LLMs) frequently memorize sensitive information during training, posing risks when deploying publicly accessible models. Current machine unlearning methods struggle to selectively remove specific data associations without degrading overall model capabilities. This paper presents our solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a two-stage methodology that combines causal mediation analysis with layer-specific optimization. Through systematic causal tracing experiments on OLMo architectures (1B and 7B parameters), we identify the critical role of the first few transformer layers (layers 0-5) in storing subject-attribute associations within MLP modules. Building on this insight, we develop a constrained optimization approach that freezes upper layers while applying a novel joint loss function to lower layers-simultaneously maximizing forget set loss via output token cross-entropy penalties and minimizing retain set deviation through adaptive regularization. Our method achieves 2nd place in the 1B model track, demonstrating strong task performance while maintaining 88% of baseline MMLU accuracy. These results establish causal-informed layer optimization as a promising paradigm for efficient, precise unlearning in LLMs, offering a significant step forward in addressing data privacy concerns in AI systems.",
            "corpus_id": 277856836,
            "sentences": [
                {
                    "corpus_id": "277856836",
                    "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
                    "text": "Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
                    "score": 0.5602143966486348,
                    "section_title": "Introduction",
                    "char_start_offset": 1726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 234,
                            "end": 589
                        },
                        {
                            "start": 592,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 903
                        },
                        {
                            "start": 904,
                            "end": 1233
                        },
                        {
                            "start": 1236,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1734
                        },
                        {
                            "start": 1737,
                            "end": 1931
                        },
                        {
                            "start": 1932,
                            "end": 2149
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 98,
                            "matchedPaperCorpusId": "218502126"
                        },
                        {
                            "start": 745,
                            "end": 763,
                            "matchedPaperCorpusId": "258417932"
                        },
                        {
                            "start": 965,
                            "end": 990,
                            "matchedPaperCorpusId": "267365485"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53662109375
                }
            ],
            "relevance_judgement": 0.53662109375,
            "relevance_judgment_input_expanded": "# Title: SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation\n# Venue: arXiv.org\n# Authors: Saransh Agrawal, Kuan-Hao Huang\n## Abstract\nLarge language models (LLMs) frequently memorize sensitive information during training, posing risks when deploying publicly accessible models. Current machine unlearning methods struggle to selectively remove specific data associations without degrading overall model capabilities. This paper presents our solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a two-stage methodology that combines causal mediation analysis with layer-specific optimization. Through systematic causal tracing experiments on OLMo architectures (1B and 7B parameters), we identify the critical role of the first few transformer layers (layers 0-5) in storing subject-attribute associations within MLP modules. Building on this insight, we develop a constrained optimization approach that freezes upper layers while applying a novel joint loss function to lower layers-simultaneously maximizing forget set loss via output token cross-entropy penalties and minimizing retain set deviation through adaptive regularization. Our method achieves 2nd place in the 1B model track, demonstrating strong task performance while maintaining 88% of baseline MMLU accuracy. These results establish causal-informed layer optimization as a promising paradigm for efficient, precise unlearning in LLMs, offering a significant step forward in addressing data privacy concerns in AI systems.\n## Introduction\nFurther, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
            "reference_string": "[277856836 | Agrawal et al. | 2025 | Citations: 0]"
        },
        {
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.21088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326503114",
                    "name": "Haoming Xu"
                },
                {
                    "authorId": "2352398597",
                    "name": "Shuxun Wang"
                },
                {
                    "authorId": "2352301948",
                    "name": "Yanqiu Zhao"
                },
                {
                    "authorId": "2352995671",
                    "name": "Yi Zhong"
                },
                {
                    "authorId": null,
                    "name": "Ziyan Jiang"
                },
                {
                    "authorId": "2182474634",
                    "name": "Ningyuan Zhao"
                },
                {
                    "authorId": "152931849",
                    "name": "Shumin Deng"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2153010067",
                    "name": "Ningyu Zhang"
                }
            ],
            "abstract": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
            "corpus_id": 277349498,
            "sentences": [
                {
                    "corpus_id": "277349498",
                    "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
                    "text": "Unlearning has emerged as a critical technique in AI systems, enabling the selective removal of sensitive data, including copyrighted material and personal information, from trained models. As the International AI Safety Report (Bengio et al., 2025) emphasizes, unlearning plays a vital role in mitigating privacy and copyright risks associated with extensive training datasets. However, it also acknowledges that current unlearning methods remain in-adequate, which often fail to completely erase targeted data while potentially degrading model performance, thus limiting practical implementation. \n\nSpecifically, existing unlearning methods often struggle with over-forgetting (excessive elimination of non-sensitive information) or underforgetting (incomplete removal of sensitive data). It is challenging to find optimal hyperparameters that balance performance across multiple evaluation dimensions, sometimes even impossible. To address these limitations, we propose a novel unlearning system that leverages model merging to combine an over-forgetting model with an under-forgetting model, creating a more effective unlearned model. It can produce superior results simply by merging two models with complementary biases. \n\nOur system achieved second place in SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models, with our 7B model attaining a Task Aggregate Score of 0.944 and Aggregate Score of 0.487, demonstrating the effectiveness of our system in selectively removing sensitive content. Furthermore, our local experiments yielded almost perfect results with a MIA Score of 0.501 and Aggregate Score of 0.806, while maintaining an exceptionally high Task Aggregate and comparable MMLU Avg.. We provide comprehensive analyses that validate our system's effectiveness and offer deeper insights into the unlearning process.",
                    "score": 0.57586060493697,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 598
                        },
                        {
                            "start": 601,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1226
                        },
                        {
                            "start": 1229,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1854
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53173828125
                }
            ],
            "relevance_judgement": 0.53173828125,
            "relevance_judgment_input_expanded": "# Title: ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging\n# Venue: arXiv.org\n# Authors: Haoming Xu, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, Shumin Deng, Huajun Chen, Ningyu Zhang\n## Abstract\nThis paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.\n## Introduction\nUnlearning has emerged as a critical technique in AI systems, enabling the selective removal of sensitive data, including copyrighted material and personal information, from trained models. As the International AI Safety Report (Bengio et al., 2025) emphasizes, unlearning plays a vital role in mitigating privacy and copyright risks associated with extensive training datasets. However, it also acknowledges that current unlearning methods remain in-adequate, which often fail to completely erase targeted data while potentially degrading model performance, thus limiting practical implementation. \n\nSpecifically, existing unlearning methods often struggle with over-forgetting (excessive elimination of non-sensitive information) or underforgetting (incomplete removal of sensitive data). It is challenging to find optimal hyperparameters that balance performance across multiple evaluation dimensions, sometimes even impossible. To address these limitations, we propose a novel unlearning system that leverages model merging to combine an over-forgetting model with an under-forgetting model, creating a more effective unlearned model. It can produce superior results simply by merging two models with complementary biases. \n\nOur system achieved second place in SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models, with our 7B model attaining a Task Aggregate Score of 0.944 and Aggregate Score of 0.487, demonstrating the effectiveness of our system in selectively removing sensitive content. Furthermore, our local experiments yielded almost perfect results with a MIA Score of 0.501 and Aggregate Score of 0.806, while maintaining an exceptionally high Task Aggregate and comparable MMLU Avg.. We provide comprehensive analyses that validate our system's effectiveness and offer deeper insights into the unlearning process.",
            "reference_string": "[277349498 | Xu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.13474, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2131875272",
                    "name": "Anmol Reddy Mekala"
                },
                {
                    "authorId": "2122341489",
                    "name": "Vineeth Dorna"
                },
                {
                    "authorId": "2322096025",
                    "name": "Shreya Dubey"
                },
                {
                    "authorId": "2322095427",
                    "name": "Abhishek Lalwani"
                },
                {
                    "authorId": "2120266368",
                    "name": "David Koleczek"
                },
                {
                    "authorId": "35315626",
                    "name": "Mukund Rungta"
                },
                {
                    "authorId": "2322137989",
                    "name": "Sadid A. Hasan"
                },
                {
                    "authorId": "2322095799",
                    "name": "Elita A. Lobo"
                }
            ],
            "abstract": "Machine unlearning aims to efficiently eliminate the influence of specific training data, known as the forget set, from the model. However, existing unlearning methods for Large Language Models (LLMs) face a critical challenge: they rely solely on negative feedback to suppress responses related to the forget set, which often results in nonsensical or inconsistent outputs, diminishing model utility and posing potential privacy risks. To address this limitation, we propose a novel approach called Alternate Preference Optimization (AltPO), which combines negative feedback with in-domain positive feedback on the forget set. Additionally, we introduce new evaluation metrics to assess the quality of responses related to the forget set. Extensive experiments show that our approach not only enables effective unlearning but also avoids undesirable model behaviors while maintaining overall model performance. Our implementation can be found at https://github.com/molereddy/Alternate-Preference-Optimization.",
            "corpus_id": 272770202,
            "sentences": [
                {
                    "corpus_id": "272770202",
                    "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
                    "text": "In this paper, we explore factual knowledge unlearning in Large Language Models (LLMs) and find that it can result in nonsensical responses on knowledge related to forgotten entities, especially when only negative feedback is used or positive feedback is applied incorrectly. To address this, we propose AltPO, a fine-tuning approach that combines negative feedback with in-domain positive feedback on the forget set, ensuring more stable and effective unlearning. We also identify limitations in existing evaluation metrics and introduce new ones to offer a more comprehensive assessment of the unlearned model. We hope our findings offer valuable insights for practitioners in LLM unlearning, promoting the use of alternate answers for more effective unlearning and improving the evaluation of model performance post-unlearning.",
                    "score": 0.5613976101909433,
                    "section_title": "Conclusion",
                    "char_start_offset": 25341,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 612
                        },
                        {
                            "start": 613,
                            "end": 830
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5263671875
                }
            ],
            "relevance_judgement": 0.5263671875,
            "relevance_judgment_input_expanded": "# Title: Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models\n# Venue: arXiv.org\n# Authors: Anmol Reddy Mekala, Vineeth Dorna, Shreya Dubey, Abhishek Lalwani, David Koleczek, Mukund Rungta, Sadid A. Hasan, Elita A. Lobo\n## Abstract\nMachine unlearning aims to efficiently eliminate the influence of specific training data, known as the forget set, from the model. However, existing unlearning methods for Large Language Models (LLMs) face a critical challenge: they rely solely on negative feedback to suppress responses related to the forget set, which often results in nonsensical or inconsistent outputs, diminishing model utility and posing potential privacy risks. To address this limitation, we propose a novel approach called Alternate Preference Optimization (AltPO), which combines negative feedback with in-domain positive feedback on the forget set. Additionally, we introduce new evaluation metrics to assess the quality of responses related to the forget set. Extensive experiments show that our approach not only enables effective unlearning but also avoids undesirable model behaviors while maintaining overall model performance. Our implementation can be found at https://github.com/molereddy/Alternate-Preference-Optimization.\n## Conclusion\nIn this paper, we explore factual knowledge unlearning in Large Language Models (LLMs) and find that it can result in nonsensical responses on knowledge related to forgotten entities, especially when only negative feedback is used or positive feedback is applied incorrectly. To address this, we propose AltPO, a fine-tuning approach that combines negative feedback with in-domain positive feedback on the forget set, ensuring more stable and effective unlearning. We also identify limitations in existing evaluation metrics and introduce new ones to offer a more comprehensive assessment of the unlearned model. We hope our findings offer valuable insights for practitioners in LLM unlearning, promoting the use of alternate answers for more effective unlearning and improving the evaluation of model performance post-unlearning.",
            "reference_string": "[272770202 | Mekala et al. | 2024 | Citations: 6]"
        },
        {
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064522174",
                    "name": "Bo Tian"
                },
                {
                    "authorId": "2153398295",
                    "name": "Xiaozhuan Liang"
                },
                {
                    "authorId": "2258034882",
                    "name": "Siyuan Cheng"
                },
                {
                    "authorId": "2258682951",
                    "name": "Qingbin Liu"
                },
                {
                    "authorId": "2218346459",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2273504274",
                    "name": "Dianbo Sui"
                },
                {
                    "authorId": "48283576",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2153010067",
                    "name": "Ningyu Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "corpus_id": 270878324,
            "sentences": [
                {
                    "corpus_id": "270878324",
                    "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
                    "text": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
                    "score": 0.5797341926146862,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5126953125
                }
            ],
            "relevance_judgement": 0.5126953125,
            "relevance_judgment_input_expanded": "# Title: To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Bo Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Meng Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang\n## Abstract\nLarge Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.\n",
            "reference_string": "[270878324 | Tian et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294397567",
                    "name": "Yi Xu"
                }
            ],
            "abstract": "With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the\"right to be forgotten\". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.",
            "corpus_id": 268819249,
            "sentences": [
                {
                    "corpus_id": "268819249",
                    "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
                    "text": "The latest work [16,37,39,66] are usually directly related to supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).Some earlier works [29,57] are simpler, directly using reverse gradients [19] for updates, which enables LLM to gradient ascend through the data-to-forget, thus achieving the goal of unlearning.In addition, there are some works that, by imitating the previous model replacement approach, obtain redundant parameters, which can respond quickly when specific unlearning requirements arise, thereby eliminating direct modifications to existing effective parameters and preventing negative impacts on the performance of other tasks.\n\n\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, [28] proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure.",
                    "score": 0.5995844559762287,
                    "section_title": "Manuscript submitted to ACM",
                    "char_start_offset": 11663,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 145,
                            "end": 339
                        },
                        {
                            "start": 339,
                            "end": 673
                        },
                        {
                            "start": 675,
                            "end": 829
                        },
                        {
                            "start": 829,
                            "end": 1091
                        },
                        {
                            "start": 1091,
                            "end": 1203
                        },
                        {
                            "start": 1203,
                            "end": 1407
                        },
                        {
                            "start": 1407,
                            "end": 1524
                        },
                        {
                            "start": 1524,
                            "end": 1718
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 218,
                            "end": 222,
                            "matchedPaperCorpusId": "8026824"
                        },
                        {
                            "start": 1420,
                            "end": 1424,
                            "matchedPaperCorpusId": "254408495"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5048828125
                },
                {
                    "corpus_id": "268819249",
                    "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
                    "text": "Parameter-tuning in Large Language Model (LLM) unlearning consists various techniques aimed at modifying model parameters to achieve forgetting.Overall, these methods show promise in achieving efficient and effective unlearning in LLMs, though there is still room for improvement, particularly in explicit evaluation metrics and the specialization of neurons for specific tasks and the interpretability of parameter-tuning.Manuscript submitted to ACM",
                    "score": 0.7076752173070907,
                    "section_title": "Summary of Parameter-tuning Large Language Model Unlearning",
                    "char_start_offset": 29791,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 144,
                            "end": 423
                        },
                        {
                            "start": 423,
                            "end": 450
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5
                }
            ],
            "relevance_judgement": 0.5048828125,
            "relevance_judgment_input_expanded": "# Title: Machine Unlearning for Traditional Models and Large Language Models: A Short Survey\n# Venue: arXiv.org\n# Authors: Yi Xu\n## Abstract\nWith the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the\"right to be forgotten\". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.\n## Manuscript submitted to ACM\nThe latest work [16,37,39,66] are usually directly related to supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).Some earlier works [29,57] are simpler, directly using reverse gradients [19] for updates, which enables LLM to gradient ascend through the data-to-forget, thus achieving the goal of unlearning.In addition, there are some works that, by imitating the previous model replacement approach, obtain redundant parameters, which can respond quickly when specific unlearning requirements arise, thereby eliminating direct modifications to existing effective parameters and preventing negative impacts on the performance of other tasks.\n\n\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, [28] proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure.\n\n## Summary of Parameter-tuning Large Language Model Unlearning\nParameter-tuning in Large Language Model (LLM) unlearning consists various techniques aimed at modifying model parameters to achieve forgetting.Overall, these methods show promise in achieving efficient and effective unlearning in LLMs, though there is still room for improvement, particularly in explicit evaluation metrics and the specialization of neurons for specific tasks and the interpretability of parameter-tuning.Manuscript submitted to ACM",
            "reference_string": "[268819249 | Xu | 2024 | Citations: 8]"
        },
        {
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323750981",
                    "name": "Shota Takashiro"
                },
                {
                    "authorId": "2081836120",
                    "name": "Takeshi Kojima"
                },
                {
                    "authorId": "2304550144",
                    "name": "Andrew Gambardella"
                },
                {
                    "authorId": "2268816164",
                    "name": "Qi Cao"
                },
                {
                    "authorId": "1715282",
                    "name": "Yusuke Iwasawa"
                },
                {
                    "authorId": "2241471533",
                    "name": "Yutaka Matsuo"
                }
            ],
            "abstract": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.",
            "corpus_id": 273022754,
            "sentences": [
                {
                    "corpus_id": "273022754",
                    "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
                    "text": "Table 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
                    "score": 0.6343096925273795,
                    "section_title": "Comparison of Our Method with Prior Work",
                    "char_start_offset": 4786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 64
                        },
                        {
                            "start": 65,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1366
                        },
                        {
                            "start": 1369,
                            "end": 1466
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 316,
                            "end": 339,
                            "matchedPaperCorpusId": "207863297"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5029296875
                }
            ],
            "relevance_judgement": 0.5029296875,
            "relevance_judgment_input_expanded": "# Title: Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning\n# Venue: arXiv.org\n# Authors: Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo\n## Abstract\nAs large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.\n## Comparison of Our Method with Prior Work\nTable 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
            "reference_string": "[273022754 | Takashiro et al. | 2024 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271212701",
            "title": "On Large Language Model Continual Unlearning",
            "text": "Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
            "score": 0.7871838115724189,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 187,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 680,
                    "end": 698,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "score": 0.7457780735760895,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5986328125
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "score": 0.7356754854793558,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",
            "score": 0.7258145358731027,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "score": 0.7117685963396945,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "271601132",
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "text": "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters.",
            "score": 0.7104311779223096,
            "section_title": "Unlearning for LLM",
            "char_start_offset": 5182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1848
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "Parameter-tuning in Large Language Model (LLM) unlearning consists various techniques aimed at modifying model parameters to achieve forgetting.Overall, these methods show promise in achieving efficient and effective unlearning in LLMs, though there is still room for improvement, particularly in explicit evaluation metrics and the specialization of neurons for specific tasks and the interpretability of parameter-tuning.Manuscript submitted to ACM",
            "score": 0.7076752173070907,
            "section_title": "Summary of Parameter-tuning Large Language Model Unlearning",
            "char_start_offset": 29791,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 450
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "LLM Unlearning The topic of unlearning in large language models (Chen, 2024) has recently attracted significant attention in the literature. One approach to unlearning is Gradient Ascent (Jang et al., 2023), which aims to maximize the loss function to facilitate forgetting. Another method, Negative Preference Optimization (NPO) (Zhang et al., 2024a), builds upon Direct Preference Optimization (DPO) (Rafailov et al., 2023), offering an alternative strategy for model unlearning. Various unlearning techniques have been proposed, including those presented by (Lu et al., 2022;Eldan and Russinovich, 2023;Yu et al., 2023;Chen and Yang, 2023;Wang et al., 2025;Gandikota et al., 2024;Jiang et al., 2025;Liu et al., 2024b;Zhuang et al., 2024). An alternative strategy, referred to as \"locate-then-unlearn,\" is exemplified by KnowUnDo (Tian et al., 2024) and SURE (Zhang et al., 2024b), which focus on knowledge localization before executing the unlearning process. Additionally, data-driven methods for unlearning have also been introduced, such as those proposed by (Jang et al., 2022;Ma et al., 2024;Liu et al., 2024a;Gu et al., 2024;Sinha et al., 2024;Xu et al., 2025;Mekala et al., 2025). Several works have explored the use of model merging techniques to achieve unlearning (??). \n\nModel Merging Training a model for each task can be costly, but model merging offers a solution to these challenges by combining multiple pretrained models. Model merging strategies include parameter averaging (Linear), singular value decomposition (SVD) for low-rank alignment, and feature concatenation (CAT).",
            "score": 0.6999311870957059,
            "section_title": "B Related Work",
            "char_start_offset": 13501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1596
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 76,
                    "matchedPaperCorpusId": "265659278"
                },
                {
                    "start": 187,
                    "end": 206,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 330,
                    "end": 351,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 402,
                    "end": 425,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 561,
                    "end": 578,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 606,
                    "end": 622,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 642,
                    "end": 660,
                    "matchedPaperCorpusId": "274130297"
                },
                {
                    "start": 702,
                    "end": 720,
                    "matchedPaperCorpusId": "267681958"
                },
                {
                    "start": 1169,
                    "end": 1189,
                    "matchedPaperCorpusId": "272770202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40478515625
        },
        {
            "corpus_id": "271310121",
            "title": "Unlearning Concepts from Text-to-Video Diffusion Models",
            "text": "Several methods implement the unlearning in transformedbased models.(Chen and Yang 2023) introduced unlearn layers into the large language models to achieve unlearning.(Tian et al. 2024) utilize gradient information to address excessive unlearning in large language model.\n\nCompare to (Chen and Yang 2023)'s work, Our method does not introduce auxiliary parameters into the unlearning model.The method proposed by (Tian et al. 2024) to overcome excessive unlearning is promising for the future work on our method.",
            "score": 0.6969400206424908,
            "section_title": "Unlearning in Transformer-based Models",
            "char_start_offset": 8423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 68,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 272
                },
                {
                    "start": 274,
                    "end": 391
                },
                {
                    "start": 391,
                    "end": 513
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 185,
                    "matchedPaperCorpusId": "262823915"
                },
                {
                    "start": 414,
                    "end": 431,
                    "matchedPaperCorpusId": "262823915"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "score": 0.6909917818088447,
            "section_title": "Related Work",
            "char_start_offset": 24726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 578
                },
                {
                    "start": 580,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 382,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1380,
                    "end": 1396,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "259088549"
                },
                {
                    "start": 1563,
                    "end": 1580,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "Large language models (LLMs) have shown to pose social and ethical risks such as generating toxic language or facilitating malicious use of hazardous knowledge. Machine unlearning is a promising approach to improve LLM safety by directly removing harmful behaviors and knowledge. In this paper, we propose\"SPlit, UNlearn, MerGE\"(SPUNGE), a framework that can be used with any unlearning method to amplify its effectiveness. SPUNGE leverages data attributes during unlearning by splitting unlearning data into subsets based on specific attribute values, unlearning each subset separately, and merging the unlearned models. We empirically demonstrate that SPUNGE significantly improves the performance of two recent unlearning methods on state-of-the-art LLMs while maintaining their general capabilities on standard academic benchmarks.",
            "score": 0.686041330517478,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "277622234",
            "title": "Not All Data Are Unlearned Equally",
            "text": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
            "score": 0.6835000625936674,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55908203125
        },
        {
            "corpus_id": "274437750",
            "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
            "text": "Cao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs. \n\nThese three areas-knowledge graph embeddings, meta-learning, and machine unlearning-collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning.",
            "score": 0.6728063023248911,
            "section_title": "Related Work",
            "char_start_offset": 7950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "251025393"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "267897394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1329345703125
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Existing research on machine unlearning mainly focuses on computer vision and other fields, e.g., recommender systems and federated learning. However, knowledge unlearning, specifically in the context of language models, has received relatively less attention. Due to the prohibitive retraining over-head and the enormous size of model parameters, existing knowledge unlearning methods primarily rely on the fine-tuning approach. To provide a comprehensive view, we also briefly introduce the methods in the pre-processing and post-processing stages that are relevant to achieving unlearning in language models.",
            "score": 0.6710512097893023,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 5723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 611
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.423828125
        },
        {
            "corpus_id": "277856816",
            "title": "ForgetMe: Evaluating Selective Forgetting in Generative Models",
            "text": "Model Unlearning is defined as the process of making a model forget specific data or concepts without fully retraining the model. This capability is essential for meeting privacy requirements, such as the GDPR's right to be forgotten, ensuring that models do not retain sensitive information after it is deleted from training data [2,23,43]. The primary objective of model unlearning is to effectively remove the influence of specific data on the model's parameters without compromising overall performance [1,38,47]. \n\nExisting unlearning methods include fine-tuning, incremental updates, parameter pruning, and adversarial training, primarily applied in computer vision and natural language processing (NLP) tasks to remove specific data or features from models [36,40,46]. However, these methods typically require substantial parameter adjustment or retraining, especially for large-scale models, resulting in high computational costs and resource demands [2,42]. Additionally, these methods often struggle to preserve generation quality while achieving target-specific unlearning, especially in high-dimensional generative models [23].",
            "score": 0.6620416315444217,
            "section_title": "Model Unlearning",
            "char_start_offset": 2500,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1139
                }
            ],
            "ref_mentions": [
                {
                    "start": 513,
                    "end": 516,
                    "matchedPaperCorpusId": "277283875"
                },
                {
                    "start": 771,
                    "end": 774,
                    "matchedPaperCorpusId": "277709159"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "277723782"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23876953125
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning.",
            "score": 0.6613562554840748,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.445556640625
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "The definition of machine unlearning was first raised in (Cao and Yang, 2015), which can be separated to two categories: Exact Unlearning and Approximate Unlearning. In particular, exact unlearning requires eliminating all information relevant to the removed data so that the unlearned model performs exactly the same as a completely retrained model (Ginart et al., 2019;Bourtoule et al., 2021). On the other hand, approximate unlearning only requires the parameters of the unlearned model to be similar to a retrained model from scratch (Guo et al., 2020;Sekhari et al., 2021;Liu et al., 2023;Chien et al., 2022;Pan et al., 2023;Guo et al., 2020). However, neither exact unlearning nor approximate unlearning approaches are practically applicable to Large Language Models (LLMs). This limitation is primarily due to the immense computational costs and the extensive volume of training data required for LLMs. Though scarce, few works have explored the LLM unlearning. (Yao et al., 2023) first defined the setup and goal of unlearning on LLMs, which is to output whitespace on harmful prompts. Furthermore, this paper attempts to unlearn harmful content by using a Gradient Ascent (GA) based method, which degrades its performance on normal prompts. (Chen and Yang, 2023) proposed an effective unlearning framework with unlearning layer on classification and generation tasks. (Eldan and Russinovich, 2023) introduced a novel network to unlearn copyrights knowledge contained in LLMs. Until very recently, (Maini et al., 2024) presented a new benchmark that aimed to better evaluate the performance of various methods on a new task of fictitious unlearning.",
            "score": 0.6608110802331602,
            "section_title": "Large Language Model Unlearning",
            "char_start_offset": 4726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1657
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 77,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 350,
                    "end": 371,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 538,
                    "end": 556,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 630,
                    "end": 647,
                    "matchedPaperCorpusId": "207847600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2171630859375
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "This method offers several distinct advantages.Firstly, it eliminates the need for fine-tuning the model's parameters, significantly reducing computational costs.Additionally, its low cost makes it accessible for widespread adoption, offering the flexibility to unlearn the Language Model (LLM) at any time and from any location.However, it's essential to note some limitations.While this method may effectively adjust the model within a specific contextual conversation, its impact could be confined to that particular context.Moreover, despite its unlearning capability, the model might still retain sensitive or potentially harmful knowledge, underscoring the importance of continuous refinement and oversight in AI development.",
            "score": 0.6601696737645428,
            "section_title": "Summary of Parameter-agnostic Large Language Model Unlearning",
            "char_start_offset": 30306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 47,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435791015625
        },
        {
            "corpus_id": "274422652",
            "title": "UOE: Unlearning One Expert Is Enough For Mixture-of-experts LLMS",
            "text": "Recent advancements in large language model (LLM) unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model's utility for legitimate knowledge. However, despite these strides, sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have received little attention and remain largely unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance and highly efficient inference processes, we ask: How can unlearning be performed effectively and efficiently on MoE LLMs? And will traditional unlearning methods be applicable to MoE architectures? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to substantial utility drops when existing unlearning methods are applied. Specifically, unlearning disrupts the router's expert selection, causing significant selection shift from the most unlearning target-related experts to irrelevant ones. As a result, more experts than necessary are affected, leading to excessive forgetting and loss of control over which knowledge is erased. To address this, we propose a novel single-expert unlearning framework, referred to as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated on the most actively engaged expert for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning that preserves model utility. The proposed UOE framework is also compatible with various unlearning algorithms. Extensive experiments demonstrate that UOE enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks, LLM architectures, while only unlearning 0.06% of the model parameters.",
            "score": 0.6576063265399154,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "276767847",
            "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
            "text": "The Unlearning Sensitive Content from Large Language Models task aims to remove targeted datapoints from trained models while minimally affecting their general knowledge. In our work, we leverage parameter-efficient, gradient-based unlearning using low-rank (LoRA) adaptation and layer-focused fine-tuning. To further enhance unlearning effectiveness, we employ data chunking, splitting forget data into disjoint partitions and merging them with cyclically sampled retain samples at a pre-defined ratio. Our task-agnostic method achieves an outstanding forget-retain balance, ranking first on leaderboards and significantly outperforming baselines and competing systems.",
            "score": 0.6536238739417979,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.",
            "score": 0.6535334013692189,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255615234375
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "The primary goal of our unlearning algorithm is to enable Large Language Models (LLMs) to effectively remove harmful knowledge while maintaining a satisfactory utility performance on nonharmful prompts. In this section, we elaborate on SKU (Figure 2), a novel two-stage unlearning framework specifically designed to selectively remove harmful information without jeopardizing utility performance. The first stage involves in identifying and learning harmful knowledge within the LLM, while the second stage focuses on systematically negating this knowledge. Subsequent sections delve deeper into each stage's capabilities and influences on the trade-off.",
            "score": 0.6534654032329107,
            "section_title": "Methods",
            "char_start_offset": 8988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418701171875
        },
        {
            "corpus_id": "271601132",
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "text": "As delineated in the preceding section, extant unlearning algorithms substantially impair the performance of the original model. Consequently, in the design of unlearning algorithms, it is imperative to consider the trade-off between model performance and the efficacy of unlearning. In this section, we envisage several potential solutions that may address this Utility-Unlearning Trade-off in the future. \n\nRegularization. Owing to the small size of the forget set, the disparity in parameters between the retrained model and the original model is typically minimal. However, in current unlearning algorithms, there is often a substantial deviation of the unlearned model from the original model due to an overemphasis on unlearning efficacy. This deviation results in the loss of a considerable amount of useful information, thereby substantially diminishing the utility of the model. Consequently, a very direct approach would be to employ various regularization methods to constrain the changes to the parameters during the unlearning process. Techniques from parameter-efficient fine-tuning methods, such as those employed in LoRA (Hu et al., 2021) or Adapter (Houlsby et al., 2019) methods, could be adapted for this purpose. This approach achieves a more optimal Utility-Unlearning Trade-off and facilitates a faster unlearning efficiency. \n\nMulti-Objective Optimization. Merely simplistically applying regularization constraints could yield unforeseen outcomes, and due to the presence of conflicting training objectives, this approach may result in suboptimal unlearning efficiency. An alternative method worth exploring involves the employment of gradient surgery techniques, commonly employed to address conflicting gradients present in the multi-task learning framework (Yu et al., 2020;Zhu et al., 2023;Wang et al., 2023b). By pruning gradients that conflict with the direction of knowledge preservation, it becomes feasible to retain the performance manifestations of the samples in the retained set intact.",
            "score": 0.6514628688238003,
            "section_title": "Utility-Unlearning Trade-off",
            "char_start_offset": 27294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1347
                },
                {
                    "start": 1350,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1166,
                    "end": 1187,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1783,
                    "end": 1800,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1800,
                    "end": 1817,
                    "matchedPaperCorpusId": "249191896"
                },
                {
                    "start": 1817,
                    "end": 1836,
                    "matchedPaperCorpusId": "261081689"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2020263671875
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Knowledge Unlearning Given a token sequence x = {x} T i=1 in the training dataset D = {x} N i=1 , the task of knowledge unlearning is to safely remove the influence of a subset of data D f from a trained machine learning model such that the model behaves as if the removed data had never been part of the training process, thus maintaining the model performance for the rest of the dataset. Conventionally, the data to be forgotten D f is expressed as the forget set, while the data to be retained D r is named as the retain set. For simplicity, we consider the standard case where D f and D r represent the whole training dataset and are mutually exclusive; that is, \n\nThe objective is to adjust the model parameters \u03b8 such that the updated parameters \u03b8 \u2032 = S(\u03b8; D f ) reflect the removal of D f . This unlearning (scrubbing) function S ensures the model behaves as if trained solely on D r , effectively forgetting D f while maintaining performance on D r . \n\nMultilingual Unlearning Extending to a multilingual context, the above definition must hold for the dataset D across all possible languages. While ideally, unlearning should occur across all existing languages, our experiments focus on a predefined set of languages Z = {z} Z i=1 for feasibility. Consequently, we assume a parallel dataset with forget sets \n\nWe define successful multilingual unlearning as the effective forgetting of parallel samples across all forget sets while retaining parallel samples across all retain sets.",
            "score": 0.6510505486391867,
            "section_title": "Problem Definition",
            "char_start_offset": 5317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1493
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "Work in \"Machine Unlearning\" seeks to bridge this gap by building algorithms that remove the influence of the deleted point from a trained model, while avoiding the computationally expensive step of fully re-training on the updated dataset (Ginart et al., 2019;Sekhari et al., 2021). \n\nAt the same time as ML privacy regulation has started to gain traction, the release of Large Language Models (LLMs) has marked a pivotal transition in ML research (Brown et al., 2020). Modern LLMs have demonstrated competency in a vast array of challenging tasks, ranging from language comprehension (Radford et al., 2019), reasoning (Bubeck et al., 2023) to tabular data generation (Borisov et al., 2023). These models not only exhibit effective abilities on tasks they were designed for, but they also display remarkable adaptability to unfamiliar tasks. This surprising versatility is partially attributed to a learning paradigm called \"in-context learning\" (Brown et al., 2020), wherein the model has access to a set of in-context examples, a minimal collection of input and label pairs, that are added to the prompt at inference time to enhance LLM performance. \n\nDespite the prominence of LLMs, and extensive recent work on machine unlearning, studying unlearning in LLMs is relatively unexplored (Jang et al., 2023). Perhaps this is because compared to conventional machine unlearning on image classifiers for example, unlearning in LLMs has two additional challenges. First, many LLMs operate as black-boxes (see Figure 1), meaning that standard unlearning techniques that perform gradient ascent or descent on the model's parameters cannot be implemented (Neel et al., 2021). Second, even if the unlearning algorithm has \"white-box\" access (access to model parameters), performing gradient updates on LLMs with many billions of parameters every time an unlearning request comes in might be computationally infeasible. \n\nTo address these challenges, we propose a novel class of unlearning methods suitable for large language models (see Figure 1). 2).",
            "score": 0.6504054145016309,
            "section_title": "Introduction",
            "char_start_offset": 1982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 261,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 261,
                    "end": 282,
                    "matchedPaperCorpusId": "232134970"
                },
                {
                    "start": 449,
                    "end": 469,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 586,
                    "end": 608,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 947,
                    "end": 967,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1289,
                    "end": 1308,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "270286011",
            "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
            "text": "The rapid advancements in large language models (LLMs) have led to remarkable breakthroughs in natural language processing and artificial intelligence.However, as these models are trained on vast amounts of data, they may inadvertently learn and perpetuate undesirable behaviors, biases, and harmful information.To address this issue, researchers have recently turned their attention to the concept of unlearning in LLMs.\n\nIn [18] paper, the authors explore the novel concept of unlearning in large language models (LLMs).They present a method that utilizes only negative examples to efficiently remove undesirable behaviors, demonstrating its effectiveness in alignment while significantly reducing computational resources compared to traditional reinforcement learning from human feedback (RLHF).While there is another paper introduces a data-driven unlearning approach for large language models (LLMs), utilizing a fine-tuning method informed by the importance of weights and relabeling during the pre-training phase of LLMs [19].This method adjusts word embedding, involving identifying and neutralizing bias vectors within the embedding space to prevent biased associations.Wang et al. [20] proposed an unlearning framework called Knowledge Gap Alignment (KGA), emphasizing its capability to efficiently handle large-scale data removal requests with significant accuracy.However, the inability of KGA to guarantee the complete removal of data influences also faces the challenge of maintaining extra data sets and models.Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.This approach not only advances the field of responsible AI but also opens new avenues for enhancing data privacy and model impartiality.Huang et al. claim an innovation offset unlearning framework tailored for the black box LLM [22].This framework effectively addresses the challenge of unlearning problematic training data in LLMs without requiring access to internal model weight, thus offering a versatile solution for adapting current unlearning algorithms.",
            "score": 0.6482011732135302,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 9557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
            "score": 0.6458871569404933,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Analogous to machine unlearning, the principles of knowledge unlearning include the following three aspects, albeit with a different focus: \n\n\u2022 Unlearning Efficiency: Due to the enormous sizes of data and parameters, knowledge unlearning not only considers time efficiency, but also emphasizes the computational feasibility for regular users in practical settings. \u2022 Unlearning Completeness: Also referred to as unlearning efficacy and forgetting quality in the literature. Retraining is the only authorized way to achieve exact unlearning. However, given the massive scale of language models, frequent retraining incurs extremely high training costs. Therefore, existing methods mainly focus on approximate unlearning. Secondly, due to the immense size of parameters, evaluating completeness mainly relies on comparing the model outputs, as directly comparing model parameters also incurs excessive cost. \u2022 General Ability: Maintaining the language utility is a crucial principle of unlearning. A sound unlearning method should selectively remove only the knowledge of target data, and avoid overunlearn that could compromise the general ability of language models.",
            "score": 0.6436888174356462,
            "section_title": "Unlearning Principles",
            "char_start_offset": 10552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Jang et al. ( 2023) introduced knowledge unlearning, aimed at preventing language models from generating specific token sequences. They proposed a straightforward method by inverting the original training objective of minimizing the negative loglikelihood of the token sequences. To maintain the performance of the remaining knowledge, various works (Wang et al., 2023;Chen and Yang, 2023;Lee et al., 2024) employed the Kullback-Leibler (KL) divergence loss, minimizing the distributional differences between the original and unlearned models on the retained data. Our approach builds on these methods but differs in its focus. While the previous works targeted monolingual models like DistilBERT (Sanh et al., 2019) and T5 (Raffel et al., 2020), we extend the unlearning process to a multilingual context.",
            "score": 0.6419179833913402,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 20077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 806
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 369,
                    "end": 389,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 697,
                    "end": 716,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 724,
                    "end": 745,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2301025390625
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "Large Language Models (LLMs) face significant challenges in maintaining privacy, ethics, and compliance, when sensitive or obsolete data must be selectively removed. Retraining these models from scratch is computationally infeasible, necessitating efficient alternatives. As part of the SemEval 2025 Task 4, this work focuses on the application of selective unlearning in LLMs to address this challenge. In this paper, we present our experiments and findings, primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility. We also detail the task-specific evaluation mechanism, results, and challenges. Our algorithms have achieved an aggregate score of 0.409 and 0.389 on the test set for 7B and 1B target models, respectively, demonstrating promising results in verifiable LLM unlearning.",
            "score": 0.6411221103793959,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48291015625
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "Recently, there has been surging interest in developing methods for unlearning information captured in large language models (LLMs) (Jang et al., 2023;Chen & Yang, 2023;Yao et al., 2023;Eldan & Russinovich, 2023;Si et al., 2023;Liu et al., 2024a;b). Such methods are important for removing sensitive or harmful information, biases, and outdated facts. A key challenge in developing unlearning methods is evaluating their performance, namely, how to validate the erasure of the unlearned information. Existing evaluation protocols largely rely on behavioural tests, such as the ability to answer questions or complete queries about the removed information (Stoehr et al., 2024;Hase et al., 2023;Chen & Yang, 2023). However, growing evidence suggests that it is often possible to steer the model to generate the unlearned information post-unlearning (Lynch et al., 2024;Patil et al., 2024), indicating that the target knowledge has not in fact been exhaustively removed from the model. This work presents the first benchmark for internal evaluation of unlearning methods. \n\nWe highlight the existence of \"parametric knowledge traces\", which are specific sets of parameters in the model that strongly correlate with the knowledge to be erased (see Figure 1a for illustration). We show that this residual knowledge causally influences the model's ability to generate information about the target concept, and argue that its internal erasure should be a goal of unlearning. Specifically, we leverage recent methods that inspect the information encoded in model parameters through vocabulary projections (Dar et al., 2023;Geva et al., 2022b). Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones.",
            "score": 0.6408740271505718,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 151,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "255595518"
                },
                {
                    "start": 694,
                    "end": 712,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "252089779"
                },
                {
                    "start": 1616,
                    "end": 1635,
                    "matchedPaperCorpusId": "247762385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "278481378",
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
            "text": "Thus, machine unlearning seeks to provide a more efficient alternative. While exact unlearning methods have been proposed (Yan et al., 2022;Ding et al., 2024), which fully retrain on D r on an algorithmic-level (Xu et al., 2024) to recover the exact behavior of the retrained model, these approaches require access to the complete retain set and are generally computationally expensive. In contrast, approximate methods aim to closely approximate the retrained model's behavior through techniques such as finetuning (Yao et al., 2024c;Zhang et al., 2024;Neel et al., 2021), prompting (Liu et al., 2024a;Pawelczyk et al., 2024), or model editing (Veldanda et al., 2024;Hase et al., 2023), offering a more scalable and efficient alternative. \n\nParameter-tuning. Among approximate methods, one prominent direction is parameter-tuning approaches, which directly modify model parameters to achieve unlearning. We pursue this direction because these methods typically meet all unlearning requirements, preserving inference latency without demanding excessive training compute. Parameter-tuning methods frame unlearning as an optimization problem with two competing objectives: a forget objective L f that forces the model to unlearn specific knowledge and a retain objective L r that ensures performance on the remaining data is preserved. A generalized unlearning loss function typically follows this form: \n\nwhere D f is the forget set, D r is the retain set, \u03b8 o are the starting model weights and \u03bb is a hyperparameter. Typically, approaches use \u03b8 o in both objectives. The variation across methods lies primarily in how the forget loss L f is designed. The retain objective L r serves as a regularizer to mitigate catastrophic forgetting. Typically, either crossentropy (Yuan et al., 2024) or KL-divergence distillation from the starting model is used as retain loss with the latter usually performing better (Zhang et al., 2024;Maini et al., 2024).",
            "score": 0.6406009465995287,
            "section_title": "Introduction",
            "char_start_offset": 6074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "260887697"
                },
                {
                    "start": 516,
                    "end": 535,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 554,
                    "end": 572,
                    "matchedPaperCorpusId": "220364296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260009765625
        },
        {
            "corpus_id": "270045436",
            "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
            "text": "Large language models (LLMs) are pre-trained on a diverse range of internet text, enabling them to develop a broad understanding of language, context, and world knowledge.We are considering a large language model F 's parameterized by \u03b8.The model was trained on a dataset D. But unlike deep learning, the training data for LLM is replaced from (x, y) to (q, a), which represents dialogues of informative exchanges between users' questions and the model's answer.\n\nSuppose we want to unlearn a subset of data D delete \u2282 D. The challenge in machine unlearning is to efficiently modify \u03b8 to reflect the training on D \u2212 D delete , and the process is the same as equation 2.\n\nDespite the similarity in the unlearning process, unlearning between traditional deep learning models and LLMs exhibits notable differences.Firstly, traditional deep learning models typically rely on neural network architectures such as DNN [11] and CNNs [12], while LLMs employ transformer structures equipped with multi-head self-attention mechanisms and positional encoding.Secondly, the complexity of LLMs, characterized by their vast parameter count and intricate architecture, presents a more challenging unlearning task than traditional models.",
            "score": 0.6404590692687556,
            "section_title": "B. Large Language Models Unlearning",
            "char_start_offset": 10092,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 171,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 462
                },
                {
                    "start": 464,
                    "end": 669
                },
                {
                    "start": 671,
                    "end": 811
                },
                {
                    "start": 811,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1222
                }
            ],
            "ref_mentions": [
                {
                    "start": 912,
                    "end": 916,
                    "matchedPaperCorpusId": "3273340"
                },
                {
                    "start": 926,
                    "end": 930,
                    "matchedPaperCorpusId": "214803074"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1893310546875
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "However, obtaining a large volume of high-quality resources is challenging and may require considerable effort. Given that the primary priority of knowledge unlearning is to prevent the generation of undesirable outputs rather than focusing on generating desirable outputs, unlearning becomes particularly appealing than RLHF. \n\nPrevious research on knowledge unlearning uses gradient ascent to achieve unlearning (Jang et al., 2023). This approach is simple yet effective in overcoming the challenges associated with language models, i.e., the prohibitive retraining overhead and the enormous size of model parameters. However, as shown in Figure 1, it applies gradient ascent to the whole sequence, i.e., all the tokens in the target sequence that are intended to be removed. As a result, while unlearning takes place, besides the target undesirable knowledge, other lexical and semantic knowledge can also be affected. This, in turn, can have a negative impact on the general ability of language models. Note that the general language ability holds great significance, and excessively impacting it can be viewed as an instance of over-unlearning. \n\nTo address this concern, we propose an adaptive objective, which replaces the original objective used in gradient ascent. Our proposed objective introduces adaptive weights to each token in the target sequence, providing fine-grained control over the unlearning target. This allows us to minimize the negative impact on non-target knowledge to the greatest extent possible. It is important to note that this objective can be considered as a pluggable component to the gradient ascent approach, ensuring that the knowledge unlearning process remains simple without the need for expensive retraining or influence estimation. Furthermore, this approach facilitates the extension of our proposed Fine-grained Pluggable Gradient Ascent (FPGA) in the regularization-based framework. By incorporating preference-aligned data or models, this framework can further augment the general ability of language models, complementing the unlearning process. We summarize the main contributions of this paper as follows: \n\n\u2022 To mitigate the negative impact on general ability resulting from existing knowledge unlearning methods, we propose an adaptive objective for the gradient ascent approach (FPGA).",
            "score": 0.6397529997841568,
            "section_title": "Introduction",
            "char_start_offset": 1922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2155
                },
                {
                    "start": 2158,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 433,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378173828125
        },
        {
            "corpus_id": "273098143",
            "title": "Undesirable Memorization in Large Language Models: A Survey",
            "text": "By fine-tuning the model with a negative similarity score as a reward signal, the approach encourages the LLM to paraphrase and unlearn pre-training data while maintaining performance. \n\nAdditionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs [128]. These methods not only enhance privacy and security but also ensure that the overall performance of the models remains intact, making them scalable and practical for real-world applications [129]. \n\nUnlearning methods offer effective strategies to mitigate memorization by selectively removing specific information from trained models without the need for full retraining. However, these methods are generally designed to target specific pieces of information, which means they rely on the ability to identify what the model has memorized beforehand. This reliance poses a challenge, as determining the exact content a model has memorized can be difficult and may limit the applicability of unlearning techniques for general mitigation against memorization.",
            "score": 0.6389792851758286,
            "section_title": "C. Unlearning methods",
            "char_start_offset": 49513,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1191
                }
            ],
            "ref_mentions": [
                {
                    "start": 427,
                    "end": 432,
                    "matchedPaperCorpusId": "265456592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "We developed a framework for machine unlearning in large language models, defining its objectives and evaluating its performance.The results demonstrate the effectiveness of our approach, yielding positive outcomes in three complex scenarios frequently encountered in large oracle models.Our framework's versatility ensures a low-cost and straightforward implementation.In contexts such as harmful output elimination, knowledge unlearning, and hallucination reduction, our method's efficacy aligns closely with that of traditional fine-tuning techniques.Additionally, it offers the advantage of significantly reduced training time, leading to substantial computational resource savings.Unlike traditional machine unlearning, machine unlearning in large language models presents unique challenges, and a standardized evaluation criterion within the industry remains absent.Furthermore, the concept of machine unlearning for large language models is not yet fully established.Our research contributes to this emerging area, aiming to inform and enhance future studies in unlearning processes for large oracle models.",
            "score": 0.6382496842536993,
            "section_title": "CONCLUSION",
            "char_start_offset": 33206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381103515625
        },
        {
            "corpus_id": "277780105",
            "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
            "text": "LLM unlearning setup. LLM unlearning aims to remove undesired data, knowledge, or model influence from a pretrained LLM, while preserving its general utility. This is typically framed as a model fine-tuning or editing task, given access to a forget set D f , which contains the data or knowledge to be unlearned, and a retain set D r , which is typically unrelated to the unlearning target and serves to help preserve the model's overall utility post-unlearning. Therefore, the problem of LLM unlearning is cast as the following regularized optimization framework (Liu et al., 2025;Yao et al., 2024;Maini et al., 2024), \n\nwhere \u03b8 denotes the model parameters to be updated during unlearning (initialized from the pretrained model state), L f and L r denote the forget loss (i.e., the unlearning objective) and retain loss (i.e., the utility-preserving objective), respectively, evaluated when using \u03b8 to generate a response y given input x, and \u03bb > 0 is a regularization parameter to strike the balance between the forget and retain objectives. \n\nRepresentative LLM unlearning methods considered in this study. The specifics of the unlearning optimization in (1) correspond to different LLM unlearning approaches, typically depending on how the forget loss \u2113 f is formulated and implemented. Although numerous unlearning methods have been proposed in the literature, our work focuses on two widely used and high-performing LLM unlearning approaches, negative preference optimization (NPO) (Zhang et al., 2024a) and representation misdirection unlearning (RMU) (Li et al., 2024), where NPO typically excels at data-wise unlearning, as demonstrated on the MUSE benchmark (Shi et al., 2024b), and RMU is particularly effective for knowledge unlearning, as evidenced by its strong performance on the WMDP benchmark (Li et al., 2024).",
            "score": 0.6376857290856814,
            "section_title": "Preliminaries and Motivation on Coreset Effect in LLM Unlearning",
            "char_start_offset": 10897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 22,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 582,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 582,
                    "end": 599,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 599,
                    "end": 618,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2225341796875
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "Table 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
            "score": 0.6343096925273795,
            "section_title": "Comparison of Our Method with Prior Work",
            "char_start_offset": 4786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 339,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5029296875
        },
        {
            "corpus_id": "278602095",
            "title": "Layered Unlearning for Adversarial Relearning",
            "text": "The goal of machine unlearning is to remove information F from trained model weights \u03b8 \u2208 \u0398 that model a dataset D in some input space X. With unlimited compute, this would involve retraining from scratch on D \\ F. Due to, e.g., the cost of pretraining, unlearning methods attempt to approximate this result as a post-training step that maintains performance on a retain set R \u2282 D. \n\nThus, we can represent a generic unlearning algorithm U as a function that maps model parameters \u03b8, forget set F, retain set R, and hyperparameters \u03b3 \u2208 \u0393 to a new set of model parameters \u03b8 \u2032 . When clear from context, we may omit the final argument corresponding to the hyperparameters. Formally",
            "score": 0.6335435389288442,
            "section_title": "Machine unlearning notation",
            "char_start_offset": 3745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 678
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "277633839",
            "title": "Sharpness-Aware Parameter Selection for Machine Unlearning",
            "text": "However, their proposed method updates all model parameters (or a naively chosen subset), which makes it computationally expensive (suboptimal), especially when the size of the used model increases. \n\nFor large language models, increasing efficiency of unlearning frameworks is even more crucial due to their large number of parameters. Similar to fine-tuning, LoRA (Hu et al., 2022) is widely used for unlearning large language models. Gao et al. (2024) proposed a practical unlearning algorithm for large language models. It freezes the original weights and fine-tunes LoRA with target prompts to deter the model from providing the original response. Then, it utilizes a classification model to determine the similarity of given prompts with the unlearning contents. Depending on the similarity, adapters are activated to emit the safe answers. Similarly, Cha et al. (2024) utilized gradient ascent with LoRA. Instead of directly utilizing gradient ascent on LoRA, the authors further proposed additional hinge loss to boost unlearning stability. Hu et al. (2024b) proposed exact unlearning on LLMs. Instead of training a separate model on each data partition, the authors also partition adapters and train each adapter partition on each corresponding data partition. Recently, Jia et al. (2025) proposed a parameter selection strategy for large language model unlearning. The authors score the model weights by their gradient sensitivity. Specifically, they introduce small changes to the gradients, then select the weights that demonstrate significant response to the change, which are considered critical for unlearning. However, this requires solving complex optimization problems, which could lower the overall efficiency. \n\nHessians, or the second order gradients are widely utilized in machine unlearning. Primarily, certified unlearning (Guo et al., 2019) requires the inverse of Hessians to analyze the gradient norm and bounding unlearning budget, which is expensive to compute regardless of the complexity of the model (Zhang et al., 2024a). Some works utilized Hessian approximations to increase efficiency (Mehta et al., 2022).",
            "score": 0.6335142163627848,
            "section_title": "RELATED WORK",
            "char_start_offset": 8216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 1049,
                    "end": 1066,
                    "matchedPaperCorpusId": "269156966"
                },
                {
                    "start": 1280,
                    "end": 1297,
                    "matchedPaperCorpusId": "273532566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46728515625
        },
        {
            "corpus_id": "273661686",
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "text": "Large language models (LLMs) consume a large amount of data during pre-training. After the model is built, we may have to unlearn certain data points that contain potentially sensitive, harmful, or copyrighted content. As re-training from scratch in such a case is not feasible due to the associated costs, researchers have developed a number of machine unlearning methods applied after training. \n\nExisting machine unlearning methods are formulated primarily as minimizing memorization through the language model loss [22,7,34]. In particular, the Gradient Ascent (GA) method maximizes the language model (LM) loss (i.e., minimizes the negative LM loss) on the target forget set (F). However, this approach can also negatively affect the utility of the model. To mitigate the utility loss, the Gradient Difference (GDiff) method selects a subset of the training data as the retain set (R), minimizing the sum of the negative LM loss on the forgetting set and the standard LM loss on the retaining set. This approach has been empirically shown to effectively preserve the model's performance [31,36]. Similarly, Negative Preference Optimization (NPO) [55] assigns a lower likelihood of forgetting data, thereby balancing the unlearning performance with model utility. \n\nDespite these successes, there are still two key issues preventing the methods from reaching their full potential. First, balancing retaining and forgetting losses is difficult (Figure 1) given the disproportionate sizes of the forget and retain datasets. Second, the optimization methods for unlearning are usually sensitive to the learning rate (cf., Appendix A, Figure 6). For instance, various learning rates can lead to substantial changes in the ROUGE scores and loss values even for the same algorithm, making the unlearning methods unstable and difficult to use in practice.",
            "score": 0.6312394838148281,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1852
                }
            ],
            "ref_mentions": [
                {
                    "start": 519,
                    "end": 523,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1092,
                    "end": 1096,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
            "score": 0.6299125052083784,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten.In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 28; 57], primarily focusing on the removal of specific training samples in classification tasks.However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwoven capabilities they possess.\n\nRecently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37].From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 44], memorized sequences [25; 4], copyrighted books [59; 13], and toxic capacities [35; 5; 29; 22].Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37].Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model.Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].Although unlearning methods for large language models have rapidly developed, some studies [43; 34; 36; 50] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning.Therefore, there remains significant room for research on unlearning methods.",
            "score": 0.6284454493396908,
            "section_title": "Knowledge Unlearning for Large Language Models",
            "char_start_offset": 3928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 494
                },
                {
                    "start": 496,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "273661686",
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "text": "This section casts machine unlearning as a multi-task optimization (MTO), specifically the two-task optimization problem. Let the retain set be denoted by R and the forget set by F, with L R and L F representing the corresponding cross-entropy losses for language modeling. We are interested in finding \n\nwhere \u03b8 represents the model parameters. \n\nThere might not be a solution that simultaneously achieves both objectives in Eq. ( 1). For LLMs, the unlearning solutions generally exhibit a trade-off between performance in R and F (cf., Figure 1). To forget F, one may unavoidably unlearn general knowledge such as grammar rules on F, which can sacrifice the performance on R. \n\nIn MTO, Pareto optimality is used to characterize the trade-offs between multiple objectives. In layperson's terms, if \u03b8 is Pareto optimal, it is impossible to improve L R or L F without worsening the other. Formal definition is in below: Definition 1 (Pareto optimality in unlearning). For two models \u03b8 and \u03b8 \u2032 , if L R (\u03b8) \u2265 L R (\u03b8 \u2032 ) and L F (\u03b8) \u2264 L F (\u03b8 \u2032 ) with at least one inequality being strict, then \u03b8 is dominated by \u03b8 \u2032 . A model is Pareto optimal if it is not dominated by any other models. \n\nIn the remainder of this section, we will discuss current unlearning methods in a unified MTO framework and analyze their Pareto optimality. Building on this, we then propose a dynamic scalarization approach tailored to LLM unlearning. 3 An example is the Baum-Welch algorithm, originally proposed to estimate the parameters of HMMs and applied in speech recognition in 1970s before it was recognized as an instance of the EM algorithm [14]. It was later identified as a special case of a broader class of convex-concave optimization (CCCP) [53]. This connection inspired new designs, such as the unified EM [42]. \n\nFigure 2: Gradient space in 2-dimension. g F is the forgetting gradient and g R is the retaining gradient, each with a perpendicular dashed line. Yellow area is the linear span (Eq. ( 3)) by scalarization.",
            "score": 0.6283860758731187,
            "section_title": "Unlearning as multi-task optimization",
            "char_start_offset": 6822,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 1623,
                    "end": 1627,
                    "matchedPaperCorpusId": "4193919"
                },
                {
                    "start": 1728,
                    "end": 1732,
                    "matchedPaperCorpusId": "6538459"
                },
                {
                    "start": 1795,
                    "end": 1799,
                    "matchedPaperCorpusId": "1173402"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "271543835",
            "title": "Machine Unlearning in Generative AI: A Survey",
            "text": "The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.",
            "score": 0.6271165623304642,
            "section_title": "Summary:",
            "char_start_offset": 52294,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1637
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",
            "score": 0.6262919054686547,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "After selecting unforgotten samples, we localize and update only the knowledge neurons corresponding to those selected samples in the language model. Specifically, we first compute gradients of parameters for the selected unforgotten samples. Then, we quantify the knowledge relevance of each neuron by using the equations 3 and 4, and sort neurons of the whole target layers by the knowledge relevance scores; then, we select the top-n knowledge neurons. We finally mask gradients of the parameters for knowledge-irrelevant neurons to exclude them from the unlearning process. Suppose that a weight matrix W \u2208 R d\u00d7k is a linear matrix multiplication parameter of a language model, and the gradient computed for the parameter is \u2207 W L = \u2202L/\u2202W. Then, the gradient of i-th neuron (i.e., column) of the weight matrix after masking is denoted as Training Setups. When unlearning is applied to a language model, there is often a trade-off between unlearning knowledge (i.e., UA, UA \u2021 , and MA f ) and retaining the model's overall knowledge (i.e., TA, SA, and MA t ). Therefore, choosing the optimal model in the unlearning process is challenging since unlearning and retention are both important. For a fair comparison, we early stop the training procedure when UA\u2264 0.33 is satisfied (random sampling from three options) to select the optimal model. More detailed experimental settings can be found in Appendix B.3.",
            "score": 0.6253718336448084,
            "section_title": "Knowledge Neuron-localized Unlearning",
            "char_start_offset": 22465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.202880859375
        },
        {
            "corpus_id": "269156966",
            "title": "Exact and Efficient Unlearning for Large Language Model-based Recommendation",
            "text": "\u2022 Machine Unlearning.Machine unlearning, the process of removing partial training data information from trained machine learning models, is essential in various domains, including recommendation, for reasons such as privacy and security concerns [5,23].This concept is known as machine unlearning [4].In traditional machine learning, two main technique lines for unlearning have emerged: approximate unlearning and exact unlearning [24,33].Approximate unlearning aims for unlearning without retraining, using techniques like influence functions [19,38] and data augmentation [28,30] for extreme efficiency, but it often involves incomplete removal of the data.On the other hand, exact unlearning [4] typically involves retraining, ensuring complete unlearning but in a time-costly manner.Existing work, like SISA [8,9,26,34], focuses on partition strategies, building individual sub-models for partitioned training data shards to retrain only partial sub-models.Our method, while also based on the partition strategy, addresses new challenges posed by the large scale and high inference cost of Large Language Models (LLM).This makes our work distinct from existing methods.\n\n\u2022 LLM Unlearning.The challenges presented by Large Language Models (LLMs), particularly their large scale, bring forth new considerations for unlearning.Previous efforts [12,25,35] have explored unlearning for LLMs, but they often involve approximate methods.For instance, [12] simulates data labels to approximate the next-token predictions of a model that has not been trained on the unusable data, and then fine-tune LLM on these simulated labels for unlearning.[25] proposes \"In Context Unlearning\", which leverages in-context learning by flipping labels of unusable data to achieve approximate unlearning.[35] leverage the gradient ascent to erase the influence of unusable data on a trained model with finetuning.However, these methods do not achieve complete removal of unusable data and are not tailored for LLMs in the context of recommender systems.",
            "score": 0.6241093567969044,
            "section_title": "RELATED WORK 5.1 Machine Unlearning",
            "char_start_offset": 30901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1174
                },
                {
                    "start": 1176,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "237562940"
                },
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "219983181"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "246016138"
                },
                {
                    "start": 816,
                    "end": 818,
                    "matchedPaperCorpusId": "232404451"
                },
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "255403814"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49169921875
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "This paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
            "score": 0.6226163547385608,
            "section_title": "Conclusion",
            "char_start_offset": 27411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4052734375
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Driven by privacy protection laws and regulations, unlearning in Large Language Models (LLMs) is gaining increasing attention. However, current research often neglects the interpretability of the unlearning process, particularly concerning sample-level unlearning difficulty. Existing studies typically assume a uniform unlearning difficulty across samples. This simplification risks attributing the performance of unlearning algorithms to sample selection rather than the algorithm's design, potentially steering the development of LLM unlearning in the wrong direction. Thus, we investigate the relationship between LLM unlearning and sample characteristics, with a focus on unlearning difficulty. Drawing inspiration from neuroscience, we propose a Memory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level unlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of hard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an $\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning algorithms, which prioritizes easily forgettable samples, thereby improving unlearning efficiency and effectiveness. We validate the proposed metric and method using public benchmarks and datasets, with results confirming its effectiveness.",
            "score": 0.622046036708081,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4091796875
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Unlearning in Large Language Models Since large language models learn knowledge from different domains and corpora during the pre-training process, it is often found that they contain harmful, sensitive or private knowledge, leading to the possibility that language models produce output behaviors containing corresponding sensitive or harmful information (Liu et al., 2024;Chang et al., 2023a;Mozes et al., 2023). Therefore, unlearning emerges as a timely and important post-pretraining processing method for LLM safety. Currently, the vast majority of LLM unlearning methods use finetuning as the primary operational approach. In terms of classifying them by different training objectives, they include gradient direction control (Jang et al., 2023;Yao et al., 2024Yao et al., , 2023) ) and preference optimization methods (Rafailov et al., 2023;Zhao et al., 2024;Lee et al., 2024b). In terms of classifying them by the parameters covered during training, they include full parameters fine-tuning (Eldan and Russinovich, 2023;Jang et al., 2023;Yao et al., 2024;Rafailov et al., 2023), sparse finetuning (Chang et al., 2023b;Stoehr et al., 2024), and parameter-efficient fine-tuning (Lu et al., 2024;Chen and Yang, 2023). Additionally, there are also a few knowledge editing methods (Patil et al., 2024). We present the specific logic details of each method in \u00a7A. \n\nKnowledge Storation in Large Language Models Studying how knowledge is stored, transferred, and extracted in LLMs has always been an important direction in the research of LLM's interpretability (Meng et al., 2022;Geva et al., 2021b;Sukhbaatar et al., 2015;Geva et al., 2023). It is known that in transformer-based language models, the MLP is a crucial component for storing the model's factual knowledge, and its sub-layers can be viewed as key-value memories (Geva et al., 2021b).",
            "score": 0.6219764691657854,
            "section_title": "Background and Related Work",
            "char_start_offset": 3456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 394,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 732,
                    "end": 751,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 767,
                    "end": 788,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 825,
                    "end": 848,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1028,
                    "end": 1046,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1063,
                    "end": 1085,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1201,
                    "end": 1221,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1284,
                    "end": 1304,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1563,
                    "end": 1582,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1582,
                    "end": 1601,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 1601,
                    "end": 1625,
                    "matchedPaperCorpusId": "1399322"
                },
                {
                    "start": 1625,
                    "end": 1643,
                    "matchedPaperCorpusId": "258417932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324462890625
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
            "score": 0.6217970919026614,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "273707684",
            "title": "Zero-shot Class Unlearning via Layer-wise Relevance Analysis and Neuronal Path Perturbation",
            "text": "I N recent years, with the growing public attention on AI models, research has increasingly focused on privacy concerns stemming from user-model interactions. Once a model is trained, users may wish to withdraw their data and remove its influence from the model. Traditionally, this can be achieved by retraining the model, but retraining is timeconsuming and inefficient for large datasets, such as those used in large language models (LLMs) [1]. The goal of unlearning is to address this issue by selectively removing the model's memory of specific data without retraining the entire model. This can be done through data reorganization or model manipulation, allowing compliance with data deletion requests and safeguarding user privacy [2]. \n\nMachine unlearning research is primarily divided into class unlearning and sample unlearning [2], [3]. Class unlearning aims to eliminate all knowledge the model has acquired from a specific data class. This type of unlearning is often necessary when data usage policies require the deletion of certain users' data, when the data has lost its relevance, or when it has introduced bias. In contrast, sample unlearning focuses on removing the influence of specific data samples from the model. This approach is commonly applied when users request the deletion of their data or when problematic data samples need to be excluded. Class unlearning is better suited for deleting data related to broad groups, while sample unlearning is better for individual user needs. By exploring practical ways to implement class and sample unlearning, researchers can develop more flexible privacy protection mechanisms that meet user demands without incurring unnecessary performance losses. \n\nSo far, class unlearning is particularly important and requires more research because it tackles the challenge of removing knowledge from entire categories of data, which is crucial for mitigating risks such as data misuse and unintended bias. Class unlearning methods have been applied to a wide range of AI tasks. For example, Chen et al. [4] achieved unlearning in image classification models by manipulating the model's decision space. Ma et al. [5] implemented unlearning using neuron masks derived from gradients.",
            "score": 0.6213741773441123,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 739,
                    "end": 742,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 2064,
                    "end": 2067,
                    "matchedPaperCorpusId": "257636742"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "273403717",
            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "text": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
            "score": 0.6211195503889952,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "270440344",
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "text": "Large language models (LLMs), like Llama (Touvron et al., 2023a;b) and GPT (Brown et al., 2020;Achiam et al., 2023), have exhibited remarkable proficiency in general-purpose language generation and understanding (Azerbayev et al., 2023;Roziere et al., 2023;Wu et al., 2023;Thirunavukarasu et al., 2023;Zhou et al., 2024;Huang et al., 2024). These advancements are credited to the development of Transformer-based architectures (Vaswani et al., 2017) with billions of parameters and to the extensive pre-training on web-sourced corpora with trillions of tokens (Brown et al., 2020). However, on the other side, scaling up models aggravates the risk of memorizing effects (Arpit et al., 2017) and sourcing from the web makes LLMs inherent its inaccuracies and biases (Liu et al., 2023a). It raises the invoking concerns for LLM privacy and fidelity, posing a long array of undesirable LLM behaviors sourced from training corpora (Liu et al., 2023a), including copyright (Yao et al., 2023a), fairness (Gallegos et al., 2023), and toxicity (Liu et al., 2023b), among many others. \n\nHow to Erase Undesirable Data Memorization in LLMs? Machine unlearning (Bourtoule et al., 2021;Zhu et al., 2024) offers a general solution. In the context of LLMs, the primary goal of unlearning is to precisely remove the parameterized knowledge related to unlearning targets meanwhile maintaining model performance for non-targets (Liu et al., 2024). The unlearning targets within LLMs are typically characterized by an unlearning set, denoted as D u = {s u = [x, y u ]} nu , and we need to develop unlearning methods upon D u that meet the goals of LLM unlearning.",
            "score": 0.6197687131760301,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1644
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 302,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 302,
                    "end": 320,
                    "matchedPaperCorpusId": "273707060"
                },
                {
                    "start": 670,
                    "end": 690,
                    "matchedPaperCorpusId": "11455421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the\"forgotten\"information. To thoroughly evaluate this phenomenon, we conduct comprehensive experiments using various quantization techniques across multiple precision levels. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21\\% of the intended forgotten knowledge in full precision, which significantly increases to 83\\% after 4-bit quantization. ... Our code is available at: \\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.",
            "score": 0.619400396012199,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2132568359375
        },
        {
            "corpus_id": "270045436",
            "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
            "text": "This experimental section delves into the intricate dynamics of applying machine unlearning methods to large language models.The primary objective of our study is to comprehensively evaluate the effects of these unlearning techniques in the context of the large language model.\n\nOur experiments are structured to assess two pivotal aspects.First, we focus on the models' 'forgetting performance' postunlearning.This involves investigating how effectively the  models can delete specific information or training instances, thereby adhering to data privacy principles and right-to-beforgotten mandates.Second, we examine the models 'utility retention' post-unlearning.This is crucial to ensure that the process of unlearning does not significantly deteriorate the model's predictive accuracy or its ability to generalize from existing data.\n\nIn our experiments, we selected 911 questions about the model name.We defined the model name \"Vicuna\" as sensitive information, where the question types can be referred to in the following examples.\n\nTell me about yourself.\n\nIn Table II, before unlearning, the appearance rate and frequency were 0.998 and 910 times, respectively, while after unlearning, both decreased to 0. This phenomenon intuitively demonstrates that our method can effectively accomplish the unlearning task for large language models.Additionally, according to Table III, the baseline and unlearned models have similar utility, with an average MMLU of 49.68 and 49.8, respectively.The unlearned model ensures that utility is not compromised in all three subdomains.\n\nOur study provides insights into the balance between effective unlearning and preserving model utility by employing meticulously designed tests and evaluations on large predictive models.These findings have profound implications for developing and deploying AI systems, particularly in environments where data privacy and model adaptability are paramount.",
            "score": 0.6187173018727066,
            "section_title": "C. Performance Evaluation on Large Language Model",
            "char_start_offset": 62164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 277
                },
                {
                    "start": 279,
                    "end": 340
                },
                {
                    "start": 340,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 600
                },
                {
                    "start": 600,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 838
                },
                {
                    "start": 840,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1038
                },
                {
                    "start": 1040,
                    "end": 1063
                },
                {
                    "start": 1065,
                    "end": 1346
                },
                {
                    "start": 1346,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1577
                },
                {
                    "start": 1579,
                    "end": 1766
                },
                {
                    "start": 1766,
                    "end": 1934
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.397705078125
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Large language models (LLMs), due to their extensive pre-training corpora, often inadvertently learn harmful, sensitive, or copyright-protected knowledge (Chang et al., 2023a;Mozes et al., 2023;Eldan and Russinovich, 2023;Ye et al., 2022). Consequently, recent research has focused on developing efficient unlearning methods as a post-training technique to selectively unlearn the specific knowledge (Blanco-Justicia et al., 2024;Liu et al., 2024). Currently, the core mechanism of these unlearning methods involves finetuning (Eldan and Russinovich, 2023;Jang et al., 2023;Yao et al., 2024;Rafailov et al., 2023), with corresponding adjustments and designs in the loss function to facilitate the unlearning process. Although earlier investigations (Hong et al., 2024;Lee et al., 2024a) have proven that these methods are ineffective at completely erasing model-embedded knowledge, the factors contributing to the misleading success of these techniques remain unclear. \n\nTherefore, in this paper, we try to unveil why existing finetuning-based unlearning methods perform well in behavioral tests by analyzing the mechanisms of internal knowledge recall and flow within models (Meng et al., 2022;Pochinkov and Schoots, 2024;Geva et al., 2021a). Specifically, we investigate which components or parameters carry these unlearning effects. We design activations patching and parameters restoration experiments in three settings, aiming to independently study the impact of unlearning methods on the coefficients and value vectors in the MLPs, as well as on the attention components' states. Our findings further confirm that the methods do not truly alter the knowledge embedded in the value vectors of MLPs, and reveal that they will change how they extract and transfer this knowledge through modifications in the coefficients of MLPs and attention components during unlearning. Notably, the coefficients produced by the MLP in the final layers are primarily responsible for achieving the unlearning effects of finetuning-based methods.",
            "score": 0.6174289547058414,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 175,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 222,
                    "end": 238,
                    "matchedPaperCorpusId": "250627348"
                },
                {
                    "start": 556,
                    "end": 574,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 591,
                    "end": 613,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 768,
                    "end": 786,
                    "matchedPaperCorpusId": "266755904"
                },
                {
                    "start": 1176,
                    "end": 1195,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1223,
                    "end": 1242,
                    "matchedPaperCorpusId": "229923720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.449462890625
        },
        {
            "corpus_id": "270562504",
            "title": "Soft Prompting for Unlearning in Large Language Models",
            "text": "Machine unlearning arose as a promising solution to address data protection guidelines by efficiently forgetting training samples corresponding to unlearning requests in place of costly retraining (Bourtoule et al., 2021;Cao and Yang, 2015;Liu et al., 2022b;Guo et al., 2020;Sekhari et al., 2021;Golatkar et al., 2020). In the context of LLMs, machine unlearning is quickly gaining prominence due to concerns stemming from bias, toxicity, and privacy (Si et al., 2023;Liu et al., 2024). Some works in this direction emphasize model parameter optimization via gradient ascent (Jang et al., 2023;Chen and Yang, 2023;Yao et al., 2024b;Maini et al., 2024;Yao et al., 2024a) to unlearn unwanted responses for specific examples or datasets. They also fine-tune the model with various knowledge alignment objectives to maintain model utility. Other works leverage parameter optimization via relabeling of unlearning data. For instance, Eldan and Russinovich (2023) unlearn Harry Potter content by fine-tuning the model via gradient descent to replace the model's response for queries related to Harry Potter with outputs containing generic translations. In contrast to these works, Jia et al. (2024) utilize similar fine-tuning objectives but focus on optimizer selection and propose a framework that performs influence-based model updates via second-order optimization. Additionally, some works propose localization-based objectives that aim to identify a subset of model units that represent information about unlearning data and effectively delete them (Meng et al., 2022;Yu et al., 2023;Wu et al., 2023). A few works also focus on modifying LLM input sequences to promote unlearning for black-box LLMs but are limited in the size of data that can be unlearned. For instance, Pawelczyk et al. (2023) perform in-context unlearning by crafting input comprised of unlearn samples paired with flipped labels and other demonstrations with correct labels.",
            "score": 0.6157924413079078,
            "section_title": "Unlearning in LLMs",
            "char_start_offset": 5869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1945
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 221,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "247447326"
                },
                {
                    "start": 258,
                    "end": 275,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 275,
                    "end": 296,
                    "matchedPaperCorpusId": "232134970"
                },
                {
                    "start": 296,
                    "end": 318,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 575,
                    "end": 594,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 594,
                    "end": 614,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1549,
                    "end": 1568,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1568,
                    "end": 1584,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1584,
                    "end": 1600,
                    "matchedPaperCorpusId": "264816202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "271212701",
            "title": "On Large Language Model Continual Unlearning",
            "text": "Language Model Unlearning. Machine unlearning (Bourtoule et al., 2021) is proposed to protect data privacy and ensure authorized usage (Liu et al., 2024;Zhang et al., 2023). There have been approaches to achieving unlearning through parameter optimization (Jang et al., 2023;Golatkar et al., 2020;Yao et al., 2023;Eldan & Russinovich, 2023;Zhang et al., 2024;Jia et al., 2024;Meng et al., 2022;Yu et al., 2023;Wu et al., 2023;Li et al., 2024), and in-context learning (Thaker et al., 2024;Pawelczyk et al., 2024). The optimization-based unlearning is to employ GradAsc (Golatkar et al., 2020;Yao et al., 2023) on the unlearned data. The following approaches, like PO (Eldan & Russinovich, 2023;Zhang et al., 2024;Jia et al., 2024), notice that unconstrained GradAsc hurts the model's utility, thus crafting task labels through shuffling or rejection. Yu et al. (2023) localizes the model parameters related to unlearning data and updates them through merging or subtracting (Ding et al., 2023). The in-context learning-based methods adjust input prompts to reject unwanted content generation. Although these approaches can achieve unlearning in certain cases, they neglect that unlearning requests always appear continuously (Liu et al., 2024). The challenge is that the accumulative impact of conducting unlearning sequentially induces catastrophic model utility loss (Gu et al., 2024;Gupta et al., 2024). Besides, their success in a single unlearning operation relies on sufficient retained data with accurate labels, which are scarce in real-world LLM applications. Our O 3 addresses the above challenges via orthogonal unlearning LoRA and a designed OOD detector. \n\nOut-of-distribution Detection.",
            "score": 0.6157317152956268,
            "section_title": "PRELIMINARY",
            "char_start_offset": 5472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 275,
                    "end": 297,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 297,
                    "end": 314,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 376,
                    "end": 394,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 394,
                    "end": 410,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 410,
                    "end": 426,
                    "matchedPaperCorpusId": "264816202"
                },
                {
                    "start": 569,
                    "end": 592,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 592,
                    "end": 609,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 851,
                    "end": 867,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 974,
                    "end": 993,
                    "matchedPaperCorpusId": "257316425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4697265625
        },
        {
            "corpus_id": "274823032",
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "score": 0.6156743999613749,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "Recently, Large Language Models (LLMs) [1]- [3] have been trained on extensive datasets that include web pages and user-generated content. During training, models acquire sensitive knowledge that raises social and legal concerns, with principles like the \"Right to be forgotten\" [4] emphasizing the need to remove unauthorized data. However, retraining an entire language model from scratch to erase sensitive information is cost-inefficient, and reconstructing the original pre-training Fig. 1. Existing unlearning methods often rely on fixed boundaries within model layers and overlook the distinct unlearning and retention scopes required for both privacy and copyright. As a result, when these methods attempt to unlearn copyright knowledge after removing privacy knowledge in the same LLM, they risk corrupting knowledge that should remain intact. dataset is exceedingly difficult. As a result, researchers have turned their attention to Machine Unlearning [5]- [12], which aims to remove specific knowledge from pre-trained models. \n\nA key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge. Laws and legal principles [13]- [16] related to privacy and copyright indicate that certain knowledge within these sensitive domains should be retained. Despite this necessity, many existing approaches do not clearly differentiate between the unlearning scope, which specifies the knowledge to remove, and the retention scope, which describes what should be preserved. In some cases, they indiscriminately remove everything loosely associated with the target. Memflex [5] introduced knowledge localization to address this issue. It distinguishes unlearning and retention scope in a given domain by leveraging gradient information in a layer-wise manner to achieve effective knowledge unlearning and retention. \n\nDespite these efforts, several challenges remain in applying unlearning methods to real-world LLMs. First, singledomain methods like Memflex are inadequate for unlearning knowledge that spans multiple domains.",
            "score": 0.6155497594984618,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "234335026"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "270878324"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "8435667"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "270878324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4609375
        },
        {
            "corpus_id": "273228310",
            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
            "text": "Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently. Empirical evidence supporting our claim is presented in Section 4.3. Additionally, for successful unlearning, the task vector should exclusively represent the information to be forgotten within the framework of negation. This requires that the fine-tuned model precisely fits the forget set while it preserves the original knowledge. \n\nAchieving both is challenging due to the nature of fine-tuning, where the only forget set is accessible; a model that fits the forget set well will inevitably lose knowledge of the retain set, and vice versa (Kirkpatrick et al., 2017). As we observed, this makes it difficult to achieve the desired balance with only a single model, which leads us to consider that aggregating multiple models could suggest more effective unlearning. However, we also observed that conventional model merging methods like Uniform Merging, Greedy Merging, TIES-Merging, and MagMax, which do not account for the characteristics of machine unlearning, fail to escape this trade-off as displayed in Figure 1 (a). \n\nIn contrast, our method, specialized in unlearning, surpasses this trade-off and achieves superior performance. We aim to use the given fine-tuned models more effectively and enhance unlearning outcomes while avoiding additional training costs. More details will be provided in Section 3.2. \n\nOur Unlearning scenarios. In our study, we explore two distinct unlearning scenarios. The first scenario is the one described above, where a vision-language model like CLIP (Radford et al., 2021) is made to forget the knowledge of a specific class. For this scenario, we adopt the evaluation protocol for unlearning proposed in the original paper (Ilharco et al., 2022).",
            "score": 0.6151934562466528,
            "section_title": "METHOD",
            "char_start_offset": 10333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2264
                }
            ],
            "ref_mentions": [
                {
                    "start": 1115,
                    "end": 1141,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 2067,
                    "end": 2089,
                    "matchedPaperCorpusId": "231591445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "270392045",
            "title": "Large Language Model Unlearning via Embedding-Corrupted Prompts",
            "text": "Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present \\textbf{Embedding-COrrupted (ECO) Prompts}, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at \\textit{nearly zero side effects} in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases. We have made our code publicly available at \\url{https://github.com/chrisliu298/llm-unlearn-eco}.",
            "score": 0.6148388593328222,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.468505859375
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "This could imply that \"overriding\" the LLM computation is easier for fine-tuning methods than modifying the LLM parametric knowledge. \n\nConsidering unlearning specificity (Target-versus Unrelated-QA performance), by unlearning a certain concept all methods also hurt the ability of the model to generate information about other concepts, as scores roughly correlate for these two tasks. \n\nOverall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods.",
            "score": 0.6145730032420187,
            "section_title": "RESULTS",
            "char_start_offset": 25424,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 645
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "270764347",
            "title": "Evaluating Copyright Takedown Methods for Language Models",
            "text": "We provide details for unlearning methods used in \u00a72.2 and \u00a74 below. We first introduce the concept of the forget set and retain set used for unlearning, then discuss the four unlearning methods evaluated in our experiment in detail. \n\nA machine unlearning algorithm seeks to remove D F , a collection of data points, from a trained language model parameterized by \u03b8. This collection of the datapoints D F is usually referred to as the forget set. In our setting, the content in the forget is the blocklisted content from a takedown request. At the same time, it is also desired that after unlearning, the model still preserves its performance on the examples that are not subject to the unlearning request, usually referred to as the retain set and denoted as D R . With the help of these notations, we now explain the four unlearning algorithms evaluated: \n\nGradient Ascent (Thudi et al., 2022) aims to maximize the training loss on the forget set, thereby achieving the goal of forgetting the content within this set. Unlike the traditional gradient descent algorithm, which minimizes the training loss on the training data, gradient ascent takes an inverse approach. This method ensures that the model forgets the content in the forget set by deliberately increasing the loss associated with it. For consistent representation, we take the negative of the loss function. Thus, for each example x i \u2208 D F , gradient ascent aims to minimize the loss function: \n\nHere n F represents the number of examples inside D F . \n\nGradient Difference (Liu et al., 2022) aims to solve the problem in gradient ascent that it cannot guarantee the model retains the knowledge in the retain set. Therefore, gradient difference adds the loss on the retain set to L GA : \n\nHere n R represents the number of examples inside D R . By minimizing L GD , the model will jointly forget the blocklisted content in the forget set, while preserving the knowledge in the retain set. \n\nKL Minimization (Golatkar et al., 2020) considers two aspects.",
            "score": 0.6143717248794907,
            "section_title": "A.2 Machine Unlearning Methods",
            "char_start_offset": 34467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 876,
                    "end": 896,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 1541,
                    "end": 1559,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2117919921875
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "We describe the specific unlearning and merging methods used in this work in the following.\n\nUnlearning via Task Vector Negation (TVN) (Ilharco et al., 2023;Zhang et al., 2023): This method uses the notion of task vector arithmetic for unlearning (Ilharco et al., 2023).Let \u03b8 init \u2208 R d denote the initial model weights and \u03b8 ft \u2208 R d the corresponding weights after fine-tuning the model on unlearning dataset D. The task vector used for unlearning is computed as \u03c4 = \u03b8 ft \u2212 \u03b8 init .TVN obtains the unlearned model as \u03b8 u = \u03b8 init \u2212 \u03bb\u03c4 where \u03bb \u2265 0 is a scaling parameter.Following Zhang et al. (2023), we employ Parameter-Efficient Fine-Tuning (PEFT) instead of full fine-tuning and compute the task vector using Parameter Efficient Modules (PEMs).In our experiments, we use a state-of-the-art PEFT method, LoRA (Hu et al., 2022), and perform negation using LoRA modules with \u03bb = 1.\n\nRepresentation Misdirection Unlearning (RMU) (Li et al., 2024): This method randomizes model activations on unlearning data while preserving model activations on data to be kept.Specifically, RMU uses a two-part loss function: (1) a forget loss to bring the model activations on unlearning data close to a scaled uniform random vector, and (2) a retain loss to preserve model activations on data to be retained.Here, let D denote the unlearning dataset and D \u2032 denote the retain set (containing samples with desirable behavior or knowledge).Let f \u03b8 (\u2022) and f \u03b8init (\u2022) denote the hidden states of the model being unlearned and the initial model, respectively, at some layer \u2113.Then, the forget loss is\n\nwhere u is a random unit vector with entries sampled independently, and uniformly at random from [0, 1), and c is a hyperparameter.",
            "score": 0.6139632668284365,
            "section_title": "Instantiating SPUNGE",
            "char_start_offset": 5934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 93,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 749
                },
                {
                    "start": 749,
                    "end": 883
                },
                {
                    "start": 885,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1296
                },
                {
                    "start": 1296,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1585
                },
                {
                    "start": 1587,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 157,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 247,
                    "end": 269,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.144287109375
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Unlearning methods. For each target LLM, we evaluate four common unlearning methods in the literature: gradient ascent, negative preference optimization, task vector and who's harry potter. \n\n\u2022 Gradient ascent (GA; (Jang et al., 2022)) maximizes the loss on the target data, which is a reversed process of learning that does gradient descent. More optimization steps T result in better unlearning but worse accuracy on extraneous facts. \u2022 Negative preference optimization (NPO; (Zhang et al., 2024)) minimizes the difference between the likelihood of the target data L(x target ; f \u03b8 ) and the likelihood L(x target ; f original ) from the original model f original , while not allowing the unlearnt model to diverge too much from the original model. The objective is defined as \n\nAs suggested by the literature, the parameter \u03b2 that controls the degree of divergence between unlearnt and original models is set to 0.1. Similar to gradient ascent, the step T of optimizing L(x target , \u03b8) is to control the trade-off between the unlearning performance and the model utility. \u2022 Task vector (TV; (Ilharco et al., 2023)) first finetunes the original model f original on the target data \n\nx target until the original model overfits to the target data. Let f overfit denote the overfitting model. Then the difference overfitted and original model f overfit \u2212 f original can be used as the direction towards learning x target , and intuitively its negative direction can be used for unlearning the target data. Therefore, TV defines the unlearning model as \n\nA larger value of parameter \u03b1 gives a higher degree of unlearning but will hurt the model utility. \u2022 Who's harry potter (WHP; (Eldan & Russinovich, 2023)) is based on a similar idea as TV and uses the overfitted model f overfit . Instead of being guided by the difference in weights it uses the probability. Let P f (x t |x 1:t\u22121 ) denote the next token x t distribution given the language model f and prompt x 1:t\u22121 .",
            "score": 0.6127302560691272,
            "section_title": "SET-UP",
            "char_start_offset": 17752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1550
                },
                {
                    "start": 1553,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 1094,
                    "end": 1116,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32080078125
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "In this section, we introduce current LLMs knowledge unlearning methods, classifying them into three categories: those based on parameter optimization, parameter merging, and in-context learning, as illustrated in Table 1. Subsequent subsections will offer detailed explanations of these methods.",
            "score": 0.6113525519643106,
            "section_title": "Current Methods",
            "char_start_offset": 12121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 296
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6728515625
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "We focus on unlearning methods based on parameter optimization, i.e., unlearning fine-tuning, as this is still the mainstream formulation (Yao et al., 2023;Maini et al., 2024;Zhang et al., 2024a;Liu et al., 2024c;Jia et al., 2024;Zhang et al., 2024b;Jin et al., 2024) and is orthogonal to other methods, such as detection-based (Gao et al., 2024) or input processing (Liu et al., 2024a). Unlearning finetuning modifies the internal mechanism of the model without preserving the original parameters, which is more in line with the requirements of the Right to be Forgotten (Zhang et al., 2023). \n\nForget Loss. Based on how the unlearned model handles the knowledge to be forgotten, we categorize existing methods into two paradigms: Untargeted Unlearning and Targeted Unlearning. For untargeted unlearning, the unlearned model only needs to forget what was specified, but how it will respond on the forget set is unknown. We consider the following two advanced methods: \n\n\u2022 Gradient Ascent (GA) can be regarded as the most straightforward way for untargeted unlearning. Its main idea is to perform an optimization on the model that is opposite to the training objective. Specifically, GA maximize the predicted loss \u2113(y|x; \u03b8) on the forget set as follows: \n\n\u2022 Negative Preference Optimization (NPO) (Zhang et al., 2024a) is a variant based on Direct Preference Optimization (DPO) (Rafailov et al., 2024), which regards unlearning as a preference optimization problem. Specifically, it treats answers in the forget set as negative samples that do not match preferences, and ignores positive terms in the DPO loss, as follows: \n\nwhere \u03c3(t) = 1/(1 + e \u2212t ) is the sigmoid function, \u03b2 is a hyper-parameter and \u03b8 ref is a reference model which is always equivalent to the initial model during unlearning process.",
            "score": 0.6113084581603889,
            "section_title": "BASELINE UNLEARNING METHODS",
            "char_start_offset": 10931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 1379,
                    "end": 1402,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43115234375
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "There exist a few studies in addressing unlearning for LLMs. Eldan et al. [37] manipulated the model by further training it on specific target data to pinpoint the tokens most relevant to the unlearning goal. This was achieved using a baseline model for comparison. Later, the unique phrases in the target data were swapped with generic counterparts and used the model's predictions to generate new labels for each token, simulating what a model would predict. Finally, by finetuning the model on the newly generated labels, the original text was removed from the model's memory. \n\nWang et al. [18] developed a framework called knowledgegap-alignment (KGA), aimed at preserving preserves the differences in distribution between unlearned and retrained models and demonstrating its performance for NLP tasks. This approach requires to set aside an independent set to measure the knowledge gap between the original model and the unlearned model. Kumar et al. [17] proposed a method for unlearning in large language models (LLMs) based on the SISA [25] approach. Meanwhile, Maini et al. [38] employed fine-tuning techniques on synthetic (fictitious) data to facilitate unlearning in LLMs. However, these approaches do not explicitly take into account the distribution of samples within the model's latent space, such as the distances between forgotten samples and the remaining samples. As a result, they are unable to directly optimize the geometric distribution of the samples in the model.",
            "score": 0.6112114615545402,
            "section_title": "C. Unlearning in LLMs",
            "char_start_offset": 11912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28271484375
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Figure 1: Motivation for multi-hop knowledge unlearning. After Elon Musk (i.e., \"the user\") requests his personal information to be removed from the LLM, existing unlearning methods often succeed in deleting direct, single-hop facts but fail on indirect, multi-hop facts that entail one or a few of the unlearned facts. \n\nas the European Union's General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019) or the California Consumer Privacy Act (CCPA) (Pardau, 2018) in the United States. These regulations mandate the removal of personal or protected information from databases, extending to data embedded within machine learning models. In such cases, model owners must develop mechanisms to safely eliminate specific data while preserving the model's overall functionality. \n\nTo address these concerns, there has been increasing focus on the field of machine unlearning, which involves removing the influence of specific data points from machine learning models (Cao and Yang, 2015). Although the need for this task is critical, erasing the effects of certain data on models with billions of parameters is extremely difficult. The ideal approach is exact unlearning, where models are entirely retrained from scratch after excluding the data points that need to be forgotten. However, this process is computationally intensive and impractical, particularly for LLMs. As a result, research has shifted towards developing faster approximate unlearning techniques. While machine unlearning has been primarily explored in computer vision (Golatkar et al., 2020a,b;Bourtoule et al., 2021;Kurmanji et al., 2023;Fan et al., 2024), its prominence is now expanding in NLP due to privacy concerns related to LLMs (Nasr et al., 2023;Carlini et al., 2024). \n\nRecently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences.",
            "score": 0.6110969534889681,
            "section_title": "Unlearn Request",
            "char_start_offset": 619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 412,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 972,
                    "end": 992,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1569,
                    "end": 1592,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1592,
                    "end": 1614,
                    "matchedPaperCorpusId": "270619676"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 1731,
                    "end": 1752,
                    "matchedPaperCorpusId": "268357903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12646484375
        },
        {
            "corpus_id": "276557795",
            "title": "LUME: LLM Unlearning with Multitask Evaluations",
            "text": "Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations.",
            "score": 0.6109471735648545,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467041015625
        },
        {
            "corpus_id": "273661727",
            "title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
            "text": "Figure 1: Demonstration of the multimodal unlearning task. MLLM is firstly fine-tuned on constructed profiles in the proposed benchmark. After fine-tuning, MLLM can answer multimodal questions related to profiles. We then conduct various unlearning methods on a portion of profiles (forget set). Finally, the performance on tasks related to the forget set and the remaining evaluation datasets are tested simultaneously. \n\nunlearning in large language models (LLMs) using synthetic data, highlighting the need for privacypreserving unlearning methods that ensure the removal of sensitive information while maintaining model performance. However, few works have addressed unlearning in MLLMs, where the challenge lies in removing the effect of data samples across both textual and visual modalities. Even the study (Chakraborty et al., 2024) that have attempted MLLM unlearning tend to focus on textual modality, expecting that unlearning in one modality will result in knowledge removal across both. \n\n3 The MLLMU-Bench Benchmark",
            "score": 0.6089080030162042,
            "section_title": "Visual Question Answering",
            "char_start_offset": 7119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1029
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.352783203125
        },
        {
            "corpus_id": "252918646",
            "title": "Verifiable and Provably Secure Machine Unlearning",
            "text": "An example for this is amnesiac unlearning [30]. Given a model m, removing a data point d * with amnesiac unlearning means that we compute \n\nIn other words, to unlearn data point d * , we remove the updates to the model's parameters that were directly computed on that data point for all training epochs E. Yet, amnesiac unlearning only provides approximate guarantees since updates from unlearnt data points indirectly influence updates from later points during the iterative nature of the training [60]. \n\nOther approaches for approximate unlearning formulate unlearning as an optimization problem [37], [64] (similar to training). In every step, instead of reducing the loss of a data point, we increase it (i.e., unlearn this data point). We refer to this as optimization-based unlearning. Formally, we iteratively compute an update \u2206 e,d * for the current model that is subtracted from its parameters:",
            "score": 0.6074010706411948,
            "section_title": "A. Machine Learning Preliminaries",
            "char_start_offset": 9565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 906
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 47,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "237303799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0792236328125
        },
        {
            "corpus_id": "267897394",
            "title": "Machine Unlearning of Pre-trained Large Language Models",
            "text": "This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.",
            "score": 0.6066636113231394,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4501953125
        },
        {
            "corpus_id": "273162410",
            "title": "A Probabilistic Perspective on Unlearning and Alignment for Large Language Models",
            "text": "Machine Unlearning. Machine unlearning aims to remove specific information from a model's weights while preserving its overall capabilities (Cao & Yang, 2015). Early works focus on classification tasks (Guo et al., 2020;Golatkar et al., 2020;Tanno et al., 2022;Wang et al., 2023;Pawelczyk et al., 2023). Later works consider more complex scenarios, such as unlearning in autoregressive LLMs for text generation (Jang et al., 2022;Chen & Yang, 2023;Eldan & Russinovich, 2023;Kim et al., 2024;Maini et al., 2024;Sheshadri et al., 2024;Li et al., 2024), which we will focus on. Maini et al. (2024) introduced a synthetic benchmark dataset that allows for controlled learning and unlearning of fictional information. Other works explored broader unlearning contexts, such as removing knowledge about specific pop culture topics like Harry Potter (Eldan & Russinovich, 2023). Previous unlearning algorithms introduced considerable trade-offs between model capabilities and the effectiveness of unlearning, this includes Gradient Ascent (GA), Gradient Difference (GD) (Liu et al., 2022), Kullback-Leibler minization (KL), or preference optimization (PO) (Rafailov et al., 2024). Zhang et al. (2024) address this by proposing Negative Preference Optimization (NPO), which shows notable improvements in balancing model capability and unlearning quality. \n\nAttacks against unlearning. Towards more accurate evaluations of unlearning recent studies have explored whether information supposedly removed by unlearning algorithms can be retrieved using extraction attacks. Patil et al. (2023) utilized a logit lens (Geva et al., 2020) approach to analyze hidden states of LLMs to extract unlearned information.",
            "score": 0.6052392759907919,
            "section_title": "RELATED WORK",
            "char_start_offset": 3799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1697
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 158,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 202,
                    "end": 220,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 220,
                    "end": 242,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 242,
                    "end": 261,
                    "matchedPaperCorpusId": "250425883"
                },
                {
                    "start": 474,
                    "end": 491,
                    "matchedPaperCorpusId": "259342279"
                },
                {
                    "start": 1062,
                    "end": 1080,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 1148,
                    "end": 1171,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302978515625
        },
        {
            "corpus_id": "276249843",
            "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
            "text": "With the rapid advancement of large language models (LLMs), concerns about their privacy, safety, and trustworthiness, have become increasingly prominent (Liu et al., 2024d;Barez et al., 2025). However, retraining these models to eliminate the undesired data-model influence is often infeasible due to the significant computational and time costs involved. To address this challenge, LLM unlearning (Yao et al., 2024;Eldan & Russinovich, 2023;Maini et al., 2024;Liu et al., 2024b) has emerged as a post-pretraining strategy, which aims to mitigate the impact of undesirable data (e.g., sensitive, biased, unsafe, or illegal information) and suppress associated model capabilities, thereby preventing LLMs from generating harmful content while simultaneously preserving the model's utility post-unlearning. \n\nDespite the increasing importance of LLM unlearning, several recent studies (\u0141ucki et al., 2024;Zhang et al., 2024e;Lynch et al., 2024;Hu et al., 2024;Deeb & Roger, 2024) have identified a critical issue: LLM unlearning often lacks robustness. Specifically, the susceptibility to quickly recovering 'already-unlearned' knowledge post-unlearning is evident through so-called relearning attacks (Lynch et al., 2024;Hu et al., 2024). These attacks can effectively reverse the unlearning process by leveraging lightweight fine-tuning on the unlearned model using only a small number of data from the forget dataset. \n\nAlthough numerous LLM unlearning methods have been proposed in the literature (Yao et al., 2024;Maini et al., 2024;Ji et al., 2024b;Zhang et al., 2024a;Liu et al., 2024a;Ji et al., 2024b;Li et al., 2024;Jia et al., 2024a;b), few studies have explored the robust optimization foundation for LLM unlearning.",
            "score": 0.6030105548010309,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1419
                },
                {
                    "start": 1422,
                    "end": 1727
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 173,
                    "matchedPaperCorpusId": "267681958"
                },
                {
                    "start": 173,
                    "end": 192,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 399,
                    "end": 417,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 443,
                    "end": 462,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2108154296875
        },
        {
            "corpus_id": "273233165",
            "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
            "text": "Figure 1: (a) Systematic overview of an LLM (\u03b8) post-unlearning using the proposed SimNPO optimization principle, compared to the popular NPO (negative preference optimization) framework (Zhang et al., 2024a) and the reference model (i.e., model prior to unlearning). (b) & (c) Experiment highlights on the TOFU dataset with a 5% forget size (Maini et al., 2024) and on the MUSE News dataset (Shi et al., 2024). Unlearning effectiveness is measured by forget quality for TOFU and PrivLeak for MUSE, while utility preservation is evaluated using model utility for TOFU and KnowMem on Dr for MUSE (see Table 1 for details on task-specific metrics). In both tasks, Retrain serves as the gold standard for unlearning by fully removing the influence of the forget data. \n\nThe rapid advancement of large language models (LLMs) has raised security and safety concerns, including issues related to copyright violations and sociotechnical harms (Huang et al., 2024;Wang et al., 2023;Li et al., 2024;Shi et al., 2024). However, retraining these models to remove undesirable data influences is often impractical due to the substantial costs and time required for such processes. This gives rise to the problem of LLM unlearning, which aims to effectively remove undesired data influences and/or model behaviors while preserving the utility for essential, unrelated knowledge generation, and maintaining efficiency without the need for retraining (Eldan & Russinovich, 2023;Yao et al., 2023;Liu et al., 2024b;Blanco-Justicia et al., 2024).",
            "score": 0.6026933143994283,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 936,
                    "end": 956,
                    "matchedPaperCorpusId": "272330189"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11981201171875
        },
        {
            "corpus_id": "272987840",
            "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
            "text": "The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs. \n\nAs shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.",
            "score": 0.6024994927081992,
            "section_title": "F.3 Results for Natural Language Processing",
            "char_start_offset": 45558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199951171875
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "In essence, model-shifting based unlearning methods strive for enhanced efficiency through assumptions about the training process, including the datasets and optimization strategies employed.These methods, effective for simpler models like linear regression, face increased complexity with complex deep neural networks.Model pruning, a part of the unlearning process, involves significant architectural changes that could impact the performance of the models post-unlearning.It's important to mention that model replacement techniques necessitate pre-calculation and storage of all potential parameters for swift parameter replacement during unlearning.Therefore, developing more efficient unlearning strategies that balance model utility, storage requirements, and the effectiveness of the unlearning process are critical areas for further research.[67] introduced a novel method known as Partitioned Contrastive Gradient Unlearning (PCGU), a gray-box strategy designed for removing biases from pre-trained masked language models.PCGU focused on refining the weights that are primarily responsible for a particular type of bias by utilizing a first-order approximation derived from the gradients of contrasting sentence pairs.While the application of PCGU had been limited to the gender-profession domain, authors observed that it also contributed to a reduction in biases in other related areas.In addition to this, [24] introduced a second-order optimization method to improve the performance.Authors presentd a fresh examination of the unlearning issue through the lens of second-order information, specifically the Hessian, as opposed to methods relying on first-order information.The unlearning techniques, drawing from the traditional Newton update, were designed to be independent of the data and model used, and had been demonstrated to effectively maintain utility and ensure privacy protection.\n\n[57] introduced a general forgetting framework called KGA, aimed at inducing model forgetfulness.Unlike previous approaches that attempt to recover gradients or force models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap), relaxing the distribution assumption.Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with relevance.",
            "score": 0.602298946716488,
            "section_title": "Summary of Model-based Machine Unlearning",
            "char_start_offset": 24623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 850
                },
                {
                    "start": 850,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1227
                },
                {
                    "start": 1227,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1686
                },
                {
                    "start": 1686,
                    "end": 1905
                },
                {
                    "start": 1907,
                    "end": 2004
                },
                {
                    "start": 2004,
                    "end": 2227
                },
                {
                    "start": 2227,
                    "end": 2420
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "Unlearning Knowledge in Large Language Model Recent research has increasingly focused on the concept of machine unlearning in the context of large language models (LLMs), highlighting both its challenges and necessities [Liu et al., 2024a, Yao et al., 2024, Si et al., 2023, Yao et al., 2023b, Zhang et al., 2023].Beyond addressing privacy concerns necessitating unlearning in LLMs, several studies have employed unlearning techniques to investigate the influence of specific subsets of training data on model performance [Isonuma andTitov, 2024, Zhao et al., 2024].To facilitate knowledge unlearning, various approaches have been proposed.One method involves retraining the LLM on the targeted dataset using a reverse loss function, coupled with training on an irrelevant dataset to preserve performance on unrelated tasks.This can be implemented through the addition of unlearning layers [Chen and Yang, 2023] or directly within the large language model itself [Eldan and Russinovich, 2023].Unlike these approaches, which apply to whole sequences in the unlearning subset, Wang et al. (2024) suggest focusing on specific spans within sequences to minimize disruption to unrelated tasks [Wang et al., 2024].Furthermore, an alternative strategy known as in-context unlearning utilizes few-shot prompts to induce forgetting of specific datasets directly within the context of use, presenting a different approach from traditional training-based methods [Pawelczyk et al., 2023].In a distinct line of research, other methods target the mitigation of harmful outputs by collecting problematic prompts and applying techniques such as instruction tuning [Liu et al., 2024b] or reinforced learning [Lu et al., 2022] to prevent toxic responses.\n\nModel Editing of LLMs Model editing in large language models pertains to the modification of factual relations within the models to integrate new world knowledge [Yao et al., 2023a].Initial approaches to model editing focused on single-fact adjustments, requiring the model to update one factual relation at a time.",
            "score": 0.6022743570839846,
            "section_title": "Related Work",
            "char_start_offset": 4365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 314,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 640
                },
                {
                    "start": 640,
                    "end": 824
                },
                {
                    "start": 824,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1737
                },
                {
                    "start": 1739,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 1692,
                    "end": 1709,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.381591796875
        },
        {
            "corpus_id": "278602095",
            "title": "Layered Unlearning for Adversarial Relearning",
            "text": "Post-training interventions such as fine-tuning, preference learning, and unlearning are widely used to modify the behavior of pre-trained large language models (LLMs). However, changes introduced in post-training are often brittle. However, these changes are often shallow or brittle. In many cases, they are bypassed or reversed by clever adversarial prompting or fine-tuning (Jain et al., 2024;Arditi et al., 2024;Zou et al., 2023;Greenblatt et al., 2024;Che et al., 2024;Deeb & Roger, 2025;Betley et al., 2025). Our goal is to understand how post-training methods modify language model behavior and representation and support the design of more robust post-training methods. \n\nWe study this through the lens of machine unlearning, which seeks to remove knowledge or capabilities from pre-trained models. Deeb & Roger (2025) recently demonstrated that \"unlearned\" information is easily re-elicited by fine-tuning on a subset of the removed data. To explain this result, we hypothesize that SoTA unlearning methods introduce a contextdependent inhibitor mechanism. Efficient \"relearning\" generalizes because fine-tuning removes a single shared mechanism and reverses the full post-training modification. In LU, we sequentially unlearn the sequence {A, A \u222a B, A \u222a B \u222a C} while retaining the sequence {B \u222a C \u222a R, C \u222a R, R}. Middle: As a result, relearning B improves performance on C but not A. In contrast, training on any subset improves performance across the board for standard methods. Right: We hypothesize that unlearning the full set introduces a context-dependent shared inhibitor I ABC that suppresses the information and that subsequent relearning removes I ABC . The structure of LU is designed to create several distinct inhibitors I A , I AB , I ABC that cover different folds of the data. Relearning on B removes I AB and I ABC , but leaves I A active. \n\nWe investigate the performance of LU on a variety of synthetic tasks and unlearning benchmarks.",
            "score": 0.6015154966557815,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 458,
                    "matchedPaperCorpusId": "270122930"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36962890625
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "Large Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.",
            "score": 0.6013421504697353,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "270703035",
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "text": "In this section, we introduce the experimental and evaluation setup and evaluation methods for the new structural LLMs unlearning considerations. \n\nMetrics. We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data. In contrast, a higher DS reflects poorer unlearning, suggesting a weaker distinction between forget and retained knowledge. More details and other supplementary metrics, including the original ROUGE1 scores, MRR and the Top Hit Rate, can be found in Appendix D. Unlearning baselines We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024). Given the nascence of the field, existing unlearning methods often lack robustness. However, these methods represent the current mainstream and serve well to demonstrate the impact of structural datasets while inspiring further research. Base models. We evaluate all baseline methods using the current widely adopted language models Llama2-7B (Touvron et al., 2023), Gemma-7B (Team et al., 2024) and Mistral-7B (Jiang et al., 2023). We evaluated learning rates between 1\u00d710 \u22126 and 5\u00d710 \u22125 during unlearning and found that all methods are highly sensitive to learning rate and batch size selection.",
            "score": 0.6013292221886941,
            "section_title": "Evaluation Setup",
            "char_start_offset": 12700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 1254,
                    "end": 1272,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333251953125
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "This work reframes the challenge of machine unlearning for large language models, shifting from traditional samplebased approaches to concept-oriented unlearning through introspective classification. Our proposed Erasure of Language Memory (ELM) method demonstrates that effective concept unlearning requires modifying the model's output distribution based on its own ability to recognize and evaluate knowledge. By using low-rank model updates guided by the model's introspective classification, ELM achieves targeted concept removal while preserving the model's broader capabilities. Our experiments show that this approach overcomes limitations of previous methods like gradient ascent or representation disruption, as evidenced by near-random performance on multiple-choice questions related to erased concepts while maintaining accuracy on other tasks. Furthermore, ELM's resistance to adversarial attacks validates our hypothesis that concept unlearning should leverage the model's own understanding of its knowledge. In addition to providing a practical solution for concept erasure, we have established a foundation for more comprehensive evaluation of knowledge erasure in language models. Response Before Attack: The -The In -----were ---max --pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr --pr ---- \n\nResponse after attack: to be stopped whereas fit -represents from were mak bls coming ** -was ** -form w ** -zero ** -zero -** -** -in ** -** -form",
            "score": 0.6010393171636135,
            "section_title": "Conclusion",
            "char_start_offset": 26413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1464
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Recently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences. However, these approaches may be vulnerable to adversarial attacks, where specific token sequences are replaced or aliased with alternative sequences. For example, prompting in low-resource languages has been shown to jailbreak GPT-4 (Yong et al., 2023), and Choi et al. (2024) demonstrated that current unlearning techniques lack cross-lingual transfer, making LLMs susceptible to such low-resource language exploits. This leads to an important research question: \"Do current unlearning methods effectively erase multi-hop knowledge when one of the intermediate hops is removed?\" As illustrated in Figure 1, consider a scenario where Elon Musk (i.e., \"the user\") requests the removal of his personal information from an LLM. After unlearning, we expect that direct, single-hop knowledge related to Elon Musk, such as \"Who is the CEO of Tesla?\", would be deleted. Additionally, we would expect associated multi-hop knowledge, like \"What is the birthplace of Tesla's CEO?\", which indirectly references Musk, to also be removed. \n\nIn this study, we explore the effectiveness of existing unlearning methods in removing multihop knowledge. We begin by refashioning the widely used multi-hop knowledge editing dataset, MQuAKE (Zhong et al., 2023). Since we do not need to edit knowledge, we discard edited facts and only consider the original facts for unlearning. Each example in MQuAKE comprises a multihop question (ranging from 2 to 4 hops) that corresponds to a sequence of interconnected facts. When we unlearn one or more facts within a chain, the model is expected to propagate these changes such that it can no longer answer the associated multihop questions. Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed.",
            "score": 0.6009084413210937,
            "section_title": "Unlearn Request",
            "char_start_offset": 2375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 92,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "268357903"
                },
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1456,
                    "end": 1476,
                    "matchedPaperCorpusId": "258865984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21435546875
        },
        {
            "corpus_id": "270562119",
            "title": "Not Everything is All You Need: Toward Low-Redundant Optimization for Large Language Model Alignment",
            "text": "Machine unlearning (Cao and Yang, 2015;Bourtoule et al., 2019;Wang et al., 2024a;Chen et al., 2024a) is an important technique for artificial intelligence systems to remove the knowledge about the restricted data (e.g., unauthorized books), while keeping other knowledge and abilities of the systems. To perform unlearning of LLMs, research has proposed several methods (e.g., Gradient Ascent (Yao et al., 2023;Maini et al., 2024) and NPO (Zhang et al., 2024)), directly training LLMs on the invalid dataset to make LLMs forget relative knowledge. Following the unlearning mechanism, in this work, we utilize an unlearning algorithm to correct the unaligned knowledge stored in the neurons of LLMs. \n\nLow-Cost LLM Training. Due to the high computation cost of LLM training, previous work either trains a small portion of parameters in LLMs, or trains small-scale LLMs. For the former, existing work has shown the parameter redundancy in LLMs, and proposed related approaches to reduce the trainable parameters in LLM (Hu et al., 2022;Li and Liang, 2021). Besides, quantization has also been used to further reduce the requirements of GPU memory (Dettmers et al., 2023). Moreover, recent work has further improved the above methods by adjusting the updated parameters within LLMs or improving the training strategies (Zhou et al., 2024a;Du et al., 2024b). For training small LLMs, to break the ceiling derived from the scaling law, knowledge distillation methods have been widely used for transferring the capabilities from the teacher model to small LLMs (He et al., 2023). However, it is also promising to small models for hyper-parameters searching, to obtain the optimal training hyper-parameters for large LLMs (Hu et al., 2024). In this work, we mainly focus on training a small portion of parameters to reduce the useless redundancy in the human alignment training process. Such a way is capable of reducing the training cost and also improves the performance.",
            "score": 0.5999652864745347,
            "section_title": "Related Work",
            "char_start_offset": 7369,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 39,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1017,
                    "end": 1034,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1034,
                    "end": 1053,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 1145,
                    "end": 1168,
                    "matchedPaperCorpusId": "258841328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17041015625
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "Input/output-based unlearning methods offer flexibility by using prompt engineering and post-processing without modifying the model weights or the architecture. Liu et al. [2024a] proposed training a prompt classifier to identify prompts within the scope of unlearning and efficiently corrupting Privacy & Copyright them in the embedding space using zeroth-order optimization. Thaker et al. [2024] showed that simple guardrails like prompting and input/output filtering can effectively support unlearning independently or alongside fine-tuning. Pawelczyk et al. [2025] proposed in-context unlearning by constructing tailored prompts, where the labels of the data to be forgotten are flipped. \n\nBrief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning [Lynch et al., 2024]. Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment. From an efficiency standpoint, reinforcement learning and locate-then-unlearn approaches involve higher algorithmic complexity. Reinforcement learning requires training high-quality models to generate reward signals, while locate-then-unlearn methods rely on precise knowledge localization. Moreover, contrastive decoding and input/output-based methods increase the computational overhead during inference, which leads to slower generation.",
            "score": 0.5997909716620096,
            "section_title": "Input/Output-Based Unlearning",
            "char_start_offset": 15474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2300
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 179,
                    "matchedPaperCorpusId": "275932019"
                },
                {
                    "start": 1451,
                    "end": 1471,
                    "matchedPaperCorpusId": "273849875"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55712890625
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "The latest work [16,37,39,66] are usually directly related to supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).Some earlier works [29,57] are simpler, directly using reverse gradients [19] for updates, which enables LLM to gradient ascend through the data-to-forget, thus achieving the goal of unlearning.In addition, there are some works that, by imitating the previous model replacement approach, obtain redundant parameters, which can respond quickly when specific unlearning requirements arise, thereby eliminating direct modifications to existing effective parameters and preventing negative impacts on the performance of other tasks.\n\n\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, [28] proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure.",
            "score": 0.5995844559762287,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 11663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 673
                },
                {
                    "start": 675,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 222,
                    "matchedPaperCorpusId": "8026824"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5048828125
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Knowledge unlearning in LLMs stems from traditional machine unlearning, as the current transformer architecture-based language model are, in essence, machine learning models. Their goals are consistent, aiming to remove specific knowledge from the model. However, large language models differ significantly from typical machine learning models, both in terms of parameter scale and the richness of internal knowledge.",
            "score": 0.5994806998687241,
            "section_title": "Relationship with Machine Unlearning",
            "char_start_offset": 11684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305419921875
        },
        {
            "corpus_id": "277596426",
            "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models",
            "text": "Large Language Models (LLMs) have achieved enormous success recently due to their ability to understand and solve various non-trivial tasks in natural language. However, they have been shown to memorize their training data (Carlini et al., 2019) which, among other concerns, increases risk of the model regurgitating creative or private content. Often, such issues are discovered post model training during testing or red teaming. Furthermore, stakeholders may sometimes request to remove their data after model training to protect copyright, or exercise their right to be forgotten (General Data Protection Regulation). In these instances, retraining models after discarding such data is one option but doing so after each such removal request is prohibitively expensive. \n\nMachine unlearning is a promising line of research for removing sensitive information from models' parametric memory. While unlearning has been studied for sometime in classification problems, it is still a relatively underdeveloped area of study in LLM research since the latter operate in a potentially unbounded output label space. Current algorithms often fall short of effectively and efficiently unlearning sensitive information from LLMs, without impacting model utility. Further, there is a need for benchmarks which can provide thorough evaluations of new unlearning algorithms in removing different categories of sensitive information. \n\nTo address these needs and to spur further research on this topic, we developed a new challenge (and an associated benchmark) for LLM Unlearning as part of the SemEval 2025 competition. This document provides a summary of our challenge1 along with the benchmark, results and key takeaways.",
            "score": 0.598838282070123,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "270226658",
            "title": "RKLD: Reverse KL-Divergence-based Knowledge Distillation for Unlearning Personal Information in Large Language Models",
            "text": "With the passage of the Right to Be Forgotten (RTBF) regulations and the scaling up of language model training datasets, research on model unlearning in large language models (LLMs) has become more crucial. Before the era of LLMs, machine unlearning research focused mainly on classification tasks in models with small parameters. In these tasks, the content to be forgotten or retained is clear and straightforward. However, as parameter sizes have grown and tasks have become more complex, balancing forget quality and model utility has become more challenging, especially in scenarios involving personal data instead of classification results. Existing methods based on gradient ascent and its variants often struggle with this balance, leading to unintended information loss or partial forgetting. To address this challenge, we propose RKLD, a novel \\textbf{R}everse \\textbf{KL}-Divergence-based Knowledge \\textbf{D}istillation unlearning algorithm for LLMs targeting the unlearning of personal information. Through RKLD, we achieve significant forget quality and effectively maintain the model utility in our experiments.",
            "score": 0.5983282605720669,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "Another line of research focuses on directly tuning the base LLM model to unlearn sensitive information, using techniques such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024). These approaches are gaining more attention as they align more closely with the growing emphasis on AI safety (Gallegos et al., 2023;\u0141ucki et al., 2024). \n\nDespite these advances, the recent unlearning benchmark MUSE (Shi et al., 2024) highlights a major drawback in current methods: applying unlearning to larger corpora leads to a decline in general language usefulness. This limits its usage in real-world settings, as an effective unlearning method must scale reliably with increasing data sizes, and accommodate continual updates-all while maintaining the model's overall language capabilities. \n\nIn this work, we introduce a novel direct-tuning Figure 1: An illustration of the self-distillation process in the proposed UNDIAL method: The original logits generated by the model are adjusted by subtracting the one-hot distribution of the target token. The student model is then fine-tuned to approximate this modified logit distribution. Since the adjustments rely solely on the original model's outputs, this is a self-distillation process to de-emphasize the token to be forgotten. \n\nmethod, UNDIAL, which enables Unlearning via Self-Distillation with Adjusted Logits. As shown in Figure 1, we generate a target distribution by reducing the logit of the token to be unlearned. This target distribution is fixed during self-distillation, ensuring a stable optimization process. Unlike GA and NPO, which suffer from significant model capacity degradation as datasets scale and training extends, UNDIAL demonstrates strong robustness to data scaling, hyperparameter tuning, and sequential unlearning, offering the first robust unlearning method for direct tuning LLMs. \n\nOur main contributions are as follows. 1) We identify the robustness issues in current unlearning methods and propose a new, more robust method based on self-distillation.",
            "score": 0.5982175147438742,
            "section_title": "Introduction",
            "char_start_offset": 1723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1909
                },
                {
                    "start": 1912,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 170,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 369,
                    "end": 388,
                    "matchedPaperCorpusId": "272910981"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "271064299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Fine-tuning has emerged as a viable approach for achieving knowledge unlearning in language models. Jang et al. (2023) explore the use of gradient ascent to effectively facilitate unlearning. Chen and Yang (2023) further introduce PE fine-tuning to enhance efficiency. However, this method only alters the model's behavior to mimic unlearning, without actually updating the parameters of the original model. Based on this line of research, the regularization-based approach incorporates regularization terms that leverage other data and models. Chen and Yang (2023) and Yao et al. (2023) utilize Kullback-Leibler (KL) divergence for regularization, aiming to maintain the model's general ability. Similarly, Rafailov et al. (2023) uses direct preference optimization as a form of regularization to guide the unlearning process. Note that while both the regularization-based approach and our proposed adaptive objective aim to enhance the model's general ability, they differ in their methodologies. The regularization-based approach complements the fine-tuning approach, introducing additional regularization terms. In contrast, our proposed method directly modifies the fine-tuning approach, shaping the tuning process to achieve improved results. Furthermore, the regularization-based approach can complement our  proposed method, which we will investigate in our experiments.",
            "score": 0.59815004994322,
            "section_title": "Fine-tuning",
            "char_start_offset": 7034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1378
                }
            ],
            "ref_mentions": [
                {
                    "start": 100,
                    "end": 118,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.496337890625
        },
        {
            "corpus_id": "271892278",
            "title": "Fishers Harvest Parallel Unlearning in Inherited Model Networks",
            "text": "From the perspective of the FIM, the impact of data on specific model parameters is consistently localized at precise positions within the parameter space [13]. The stable localization of the FIM across iterations helps precisely locate the parameter locations for unlearning at the class, client, and sample levels using data tied to specific label sets. By fine-tuning , , and  in Eq. ( 6) to match the sample size of the class being unlearned, the task transits flexibly between the class-level [13] and sample-level unlearning [16]. Specifically, when the unlearning FIM mirrors the model's FIM with a class-level setting, it becomes a client-level unlearning task. All types of unlearning are performed irrespective of the succession of inheritance relationships. Consequently, employing FIUn disrupts the sequential dependencies traditionally required for unlearning, thus enabling fully parallel unlearning processes across any number of subgraphs. \n\nNot only can models across different subgraphs undergo parallel unlearning, but those within the same subgraph at various depths can also be unlearned in parallel, thanks to a feature referred to as Hyper-Distance in this paper. This Hyper-Distance is achieved without compromising the integrity of the inheritance relationships. Hyper-Distance ensures that both unlearned and retained knowledge is effectively managed along the inheritance paths. This efficiency is possible because the FIUn method utilizes FIMs for unlearning, enabling a class-wise inheritance approach. Each model effectively incorporates a union of label sets from its predecessors, ensuring that the inheritance is preserved. Simultaneously, by focusing on label differences, we achieve fully parallel unlearning-making depths and paths irrelevant to the process. \n\nAnswer to RQ2: The FIUn method enables independently removing knowledge from each inherited model. This method utilizes the FIM to obtain the Hyper-Distance property which breaks sequential dependencies among models, enabling fully parallel unlearning on inherited models and significantly decreasing the execution time of unlearning tasks. To address the complexity arising from multiple upstream models, the merging FIM function is developed to aggregate unlearning FIMs from those models, facilitating the efficient, one-shot removal of inherited knowledge.",
            "score": 0.597930530677153,
            "section_title": "Discussion",
            "char_start_offset": 21441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2137
                },
                {
                    "start": 2138,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "260900355"
                },
                {
                    "start": 498,
                    "end": 502,
                    "matchedPaperCorpusId": "260900355"
                },
                {
                    "start": 531,
                    "end": 535,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.255126953125
        },
        {
            "corpus_id": "267365499",
            "title": "Unlearnable Algorithms for In-context Learning",
            "text": "Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.",
            "score": 0.5978010960872868,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60498046875
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "Mitigating the retention of sensitive or private information in large language models is essential for enhancing privacy and safety. Existing unlearning methods, like Gradient Ascent and Negative Preference Optimization, directly tune models to remove unwanted information. However, these methods often become unstable because they fine-tune by maximizing cross-entropy loss, which is the opposite of traditional loss minimization in learning. This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning. In this paper, we introduce UnDIAL (Unlearning via Self-Distillation on Adjusted Logits), a novel and robust unlearning method. Our approach leverages self-distillation to adjust logits and selectively reduce the influence of targeted tokens. This technique ensures smooth convergence and avoids catastrophic forgetting, even in challenging unlearning tasks with large datasets and sequential unlearning requests. Extensive experiments show that UnDIAL can achieve both robustness in unlearning and scalability while maintaining stable training dynamics and resilience to hyperparameter tuning.",
            "score": 0.5977952592352,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "277620868",
            "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale",
            "text": "Modern machine learning applications often require finetuning a pretrained model on a collection of data. However, once a model has been finetuned, it may be necessary to unlearn a subset of data and produce a model identical to one trained as if the data were never present. This is because finetuning data can introduce risks such as harmful knowledge or private information (Carlini et al., 2023;More et al., 2024;Su et al., 2024;Ahmadian et al., 2024). Moreover, data privacy regulations such as GDPR and CCPA state that consumers have a \"right to be forgotten\" (Protection, 2018). For ML, this not only requires that data controllers remove data in accordance with deletion requests, but also retrain any models trained on such data. To address these concerns, there has been significant interest in methods for machine unlearning that can efficiently remove the influence of data from a model (Cao & Yang, 2015;Ginart et al., 2019;Bourtoule et al., 2021;Tarun et al., 2023). However, existing methods face key limitations: approximate unlearning methods lack guarantees-leading to exposure of supposedly unlearned information (Hu et al., 2025;\u0141ucki et al., 2024;Deeb & Roger, 2024), while exact unlearning methods have prohibitively expensive relearning costs. \n\nIn this work, we explore using model merging for both exact and efficient unlearning. Given a dataset split over several tasks (e.g., a large set of clients whose data we may wish to unlearn), we first finetune (FT) a pretrained model separately on each task to obtain a set of local models. While it is easy to unlearn a task by discarding its local model, this framework is limited by both high storage costs and lack of collaboration across tasks. Therefore, we merge (average) the local models' weights to produce a single merged model and then discard the local models. To unlearn a particular task, we can simply retrain the local model for that task and unmerge (subtract) it from the merged model.",
            "score": 0.597277740086582,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 399,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 899,
                    "end": 917,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 917,
                    "end": 937,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 960,
                    "end": 979,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 1132,
                    "end": 1149,
                    "matchedPaperCorpusId": "270619566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29296875
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Large language models (LLMs) are trained on a vast corpus of text, enabling them to achieve outstanding performance across various tasks (Radford et al., 2019;Chowdhery et al., 2023;Gemma et al., 2024). However, LLMs may present privacy risks, as sensitive or private information could unintentionally be included in the large text corpus used for training. Therefore, prior studies have investigated unlearning undesirable knowledge in Figure 1: Faithful Unlearning. FAITHUN proposes three types of datasets to evaluate the faithfulness of unlearning methods (i.e., Paraphrased, Multi-hop, and Same-answer datasets). Each target knowledge to be unlearned is mapped with questions corresponding to these three dataset types for evaluation. language models. To assess unlearning, most studies have examined whether a model successfully forgets the targeted knowledge while retaining irrelevant knowledge (Shi et al., 2024;Li et al., 2024a;Maini et al., 2024;Jin et al., 2024). \n\nHowever, they are limited since they have overlooked the complex and interconnected nature of knowledge, which requires careful investigation of related knowledge. Specifically, these studies have examined only the independent knowledge and failed to evaluate whether an unlearning method effectively erases interconnected knowledge that should be removed, while retaining knowledge that appears relevant but exists in a completely different context. Figure 1 presents an example of faithful unlearning in the real-world knowledge setting. Unlearning methods should also remove paraphrased and multi-hop questions, as they involve knowledge interconnected with the target question being unlearned. Conversely, unlearning methods should retain knowledge of other questions with the same answer as the target, if they actually contain different knowledge despite appearing relevant. \n\nTo address this gap, we first define superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge.",
            "score": 0.5972654467030745,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 159,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 159,
                    "end": 182,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2420654296875
        },
        {
            "corpus_id": "273661686",
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "text": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
            "score": 0.5964690849374952,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.591796875
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "The concept of unlearning in Large Language Models has garnered considerable attention as concerns over data privacy and model integrity have intensified. In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by Lu et al. (2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by Jang et al. (2023) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by Yu et al. (2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications.",
            "score": 0.5957732614220936,
            "section_title": "F.2 LARGE LANGUAGE MODELS UNLEARNING",
            "char_start_offset": 43282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 419,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 754,
                    "end": 772,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1175,
                    "end": 1191,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "273507094",
            "title": "Scalability of memorization-based machine unlearning",
            "text": "Recently, in large language models (LLMs), new algorithms were presented that exploit the memorization of textual sequences, offering new state-of-the-art MUL performance for memorized-data unlearning [2]. Furthermore, Zhao et al. [23] showed that memorization is strongly linked to the difficulty of the unlearning task: the more memorized the data is, the harder it is to effectively unlearn those examples. Building on these findings, Zhao et al. [23] proposed a new meta-algorithm, \"RUM,\" which leverages varying levels of memorization to improve existing approximate unlearning algorithms. However, this approach has practical limitations, as it requires precise knowledge of memorization levels in the dataset, which is computationally expensive to obtain. This severely limits the scalability of high-performing MUL algorithms like RUM. \n\nMotivated by this limitation, we explore and adopt a series of memorization proxies to ensure scalability while maintaining the effectiveness of this new class of high-performing machine unlearning algorithms, such as RUM. By using proxies that can be computed more efficiently, we aim to strike a balance between performance and computational feasibility. \n\n2 Related work and background",
            "score": 0.5957048768975137,
            "section_title": "Introduction",
            "char_start_offset": 1992,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1234
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "269605586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.362060546875
        },
        {
            "corpus_id": "270440344",
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "text": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to refine the evaluation of LLM unlearning by addressing two key challenges -- a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention. We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy.",
            "score": 0.5954909386605226,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6240234375
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "This survey provides a thorough view in both traditional and current LLM's machine unlearning.We make a substantial contribution by offering a comprehensive perspective on both traditional and contemporary approaches to machine unlearning within Large Language Models (LLMs).By exploring the evolution of machine unlearning techniques in LLMs, the survey provides valuable insights into the historical development and the latest advancements in this field.This holistic view enables researchers and practitioners to gain a deeper understanding of the challenges, opportunities, and emerging trends in LLM machine unlearning.Through its thorough examination of traditional methods alongside cutting-edge approaches, the survey equips stakeholders with the knowledge needed to navigate the complexities of machine unlearning in the context of LLMs effectively.",
            "score": 0.5945584613011057,
            "section_title": "Contributions of this survey",
            "char_start_offset": 3234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333740234375
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "The key components of machine learning are data samples for training, model for learning patterns, and loss function for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning method, and reversed loss function. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language models (LLMs). Our contributions are threefold; first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge - something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification.",
            "score": 0.5935081996179321,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or\"forgetting\"a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0.5932287978209125,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "270296369",
            "title": "Update Selective Parameters: Federated Machine Unlearning Based on Model Explanation",
            "text": "Recent studies have developed several types of unlearning methods, specifically data reorganization and parameter manipulation.Data reorganization refers to situations in which model providers simplify the unlearning process by reorganizing the training dataset in advance [5], [6].For example, Bourtoule et al. [6] divide the training dataset into multiple subsets and train sub-models based on each of these subsets.When a group of samples needs to be unlearned, this can be done by retraining sub-models based on the corresponding subset that contains those unlearning data, rather than the entire training dataset.For parameter manipulation, model providers usually offset the effect of the samples that need to be unlearned by directly manipulating the model parameters [7], [8].Both types of methods have been quite popular in recent years.\n\nIn this work, we consider the problem of unlearning in the setting of federated learning, in which the server wants to remove all samples associated with a particular class.There are many scenarios where data from a trained model that needs to be unlearned belongs to one or more classes [9], [10].For example, if the training process of federated learning contains a group of outdated data or data identified as adversarial type, the server needs to remove all effects of these data from the global model.Another example is thats the recommendation system needs to periodically remove some items (classes) due to relevant legal updates or product iterations.Considering this setting, the above-mentioned centralized unlearning methods are inefficient due to the following challenges.\n\nFor the data reorganization methods, most centralized unlearning algorithms make use of the original training dataset to remove the effect of unlearning data [4], [6], [11], [12].Those kinds of unlearning methods usually aggregate all training data in one node and split those data into different subsets.Then, each subset will be used to train the sub-model, which limits the impact of data in the subset to the corresponding sub-models rather than a more complex model with the entire training data.",
            "score": 0.5926889684776551,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 784
                },
                {
                    "start": 784,
                    "end": 846
                },
                {
                    "start": 848,
                    "end": 1021
                },
                {
                    "start": 1021,
                    "end": 1146
                },
                {
                    "start": 1146,
                    "end": 1354
                },
                {
                    "start": 1354,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1632
                },
                {
                    "start": 1634,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 1939
                },
                {
                    "start": 1939,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 276,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "211066387"
                },
                {
                    "start": 1792,
                    "end": 1795,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1797,
                    "end": 1800,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1802,
                    "end": 1806,
                    "matchedPaperCorpusId": "246016138"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "232404451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1041259765625
        },
        {
            "corpus_id": "276422157",
            "title": "SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning",
            "text": "The task of unlearning in LLMs has attracted significant attention in recent years (Barez et al., 2025). \n\nIn previous studies, MU methods are typically divided into training-based methods and training-free methods. Training-based methods include gradient ascent (Bourtoule et al., 2020), gradient difference (Wang et al., 2023b;Yao et al., 2023), KL divergence (Yao et al., 2024), and preference optimization (Maini et al., 2024) and so on. Training-free methods include in-context unlearning (Pawelczyk et al., 2024) and corrupting prompt embeddings to achieve unlearning (Liu et al., 2024a). As MU methods for LLMs continue to evolve, constructing high-quality unlearning datasets and benchmarks has become increasingly important. Eldan and Russinovich (2023) propose a \"Harry Potter\" task for copyright, Maini et al. ( 2024) design an unlearning task with fictional authors, and Ma et al. \n\n(2024) introduce an unlearning benchmark for a fictional facial identity VQA dataset which aims to protect privacy. However, existing studies have not explored the application of MLLMs for forgetting harmful knowledge, a safety concern in MLLMs.",
            "score": 0.5924537593904933,
            "section_title": "MU for LLMs",
            "char_start_offset": 4168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "277349425",
            "title": "Effective Skill Unlearning through Intervention and Abstention",
            "text": "Large language model machine unlearning. This line of work aims to remove the influence of specific data points and the corresponding model capabilities without retraining the model from scratch. Most previous works on machine unlearning have focused on fine-tuning-based approaches (Lu et al., 2022;Jang et al., 2023;Wang et al., 2023;Yu et al., 2023;Eldan and Russinovich, 2023;Chen and Yang, 2023;Yao et al., 2023), which become increasingly costly as models grow larger. Pawelczyk et al. (2024) introduced in-context unlearning, which provides contextual inputs to the language model during the inference stage. Despite its costefficiency, it lacks unlearning quality and is difficult to generalize to large-scale unlearning. \n\nOther training-free approaches focus on pruning or removing specific sets of behavior-related neurons in the model. DEPN (Wu et al., 2023) is a pruning-based unlearning approach that removes neurons based on their cumulative privacy gradient. Selective Pruning (Pochinkov and Schoots, 2023) is another pruning-based method, which removes neurons based on their relative importance to the forgetting dataset and the retaining dataset. However, the extent to which pruning-based methods affect the model's overall capabilities remains unknown and unjustifiable. Feed-forward neuron interpretability. This line of work focuses on the interpretation of individual neurons, meaning that individual neurons represent meaningful concepts, both in vision models (Bau et al., 2020;Hernandez et al., 2022;Oikarinen and Weng, 2023) and language models (Bills et al., 2023;Lee et al., 2023;Sun et al., 2024). Recent works have shown that neurons exhibit multisemanticity (Elhage et al., 2022;Bricken et al., 2023;Huben et al., 2024), with some being expressible as a linear combination of concepts (Oikarinen and Weng, 2024).",
            "score": 0.5922504531300266,
            "section_title": "Related Work & Background",
            "char_start_offset": 1530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1845
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 300,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 300,
                    "end": 318,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 336,
                    "end": 352,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 853,
                    "end": 869,
                    "matchedPaperCorpusId": "264816202"
                },
                {
                    "start": 1504,
                    "end": 1527,
                    "matchedPaperCorpusId": "246285344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365478515625
        },
        {
            "corpus_id": "270619676",
            "title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models",
            "text": "Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk. To mitigate the privacy concerns of LMs, machine unlearning has emerged as an important research area, which is utilized to induce the LM to selectively forget about some of its training data. While completely retraining the model will guarantee successful unlearning and privacy assurance, it is impractical for LMs, as it would be time-consuming and resource-intensive. Prior works efficiently unlearn the target token sequences, but upon subsequent iterations, the LM displays significant degradation in performance. In this work, we propose Privacy Protection via Optimal Parameters (POP), a novel unlearning method that effectively forgets the target token sequences from the pretrained LM by applying optimal gradient updates to the parameters. Inspired by the gradient derivation of complete retraining, we approximate the optimal training objective that successfully unlearns the target sequence while retaining the knowledge from the rest of the training data. Experimental results demonstrate that POP exhibits remarkable retention performance post-unlearning across 9 classification and 4 dialogue benchmarks, outperforming the state-of-the-art by a large margin. Furthermore, we introduce Remnant Memorization Accuracy that quantifies privacy risks based on token likelihood and validate its effectiveness through both qualitative and quantitative analyses.",
            "score": 0.5918119678782502,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.379638671875
        },
        {
            "corpus_id": "267657624",
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "text": "Computation cost has been a predominant efficiency metric when evaluating LLM unlearning methods, as demonstrated in Table 1. In addition to that, efforts have been made to extend LLM unlearning to black-box models, without access to model parameters, as demonstrated by Pawelczyk et al. (2023). Furthermore, memory efficiency could also serve as a crucial efficiency metric. The distinction from parameter efficiency is that current parameter-efficient fine-tuning methods still impose substantial memory costs for storing LLMs and for executing back-propagation (Malladi et al., 2023). Thus, a future research direction is to explore memory-efficient fine-tuning methods for LLM unlearning.",
            "score": 0.5916623832335599,
            "section_title": "Assessment",
            "char_start_offset": 31650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 692
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450927734375
        },
        {
            "corpus_id": "276938085",
            "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models",
            "text": "Large language model (LLM) unlearning has demonstrated its essential role in removing privacy and copyright-related responses, crucial for their legal and safe applications. However, the pursuit of complete unlearning often comes with substantial costs due to its compromises in their general functionality, leading to a notorious trade-off between unlearning and retention. In examining the update process for unlearning dynamically, we find gradients hold essential information for revealing this trade-off. In particular, we look at the varying relationship between retention performance and directional disparities between gradients during unlearning. It motivates the sculpting of an update mechanism derived from gradients from two sources, i.e., harmful for retention and useful for unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an enhanced unlearning framework controlling the updating gradients in a geometry-focused and optimization-driven manner such that their side impacts on other, unrelated responses can be minimized. Specifically, GRU derives a closed-form solution to project the unlearning gradient onto the orthogonal space of that gradient harmful for retention, ensuring minimal deviation from its original direction under the condition that overall performance is retained. Comprehensive experiments are conducted to demonstrate that GRU, as a general framework, is straightforward to implement and efficiently enhances a range of baseline methods through its adaptable and compatible characteristics. Additionally, experimental results show its broad effectiveness across a diverse set of benchmarks for LLM unlearning.",
            "score": 0.5915853630986744,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415771484375
        },
        {
            "corpus_id": "269982763",
            "title": "Unlearning Information Bottleneck: Machine Unlearning of Systematic Patterns and Biases",
            "text": "Effective adaptation to distribution shifts in training data is pivotal for sustaining robustness in neural networks, especially when removing specific biases or outdated information, a process known as machine unlearning. Traditional approaches typically assume that data variations are random, which makes it difficult to adjust the model parameters accurately to remove patterns and characteristics from unlearned data. In this work, we present Unlearning Information Bottleneck (UIB), a novel information-theoretic framework designed to enhance the process of machine unlearning that effectively leverages the influence of systematic patterns and biases for parameter adjustment. By proposing a variational upper bound, we recalibrate the model parameters through a dynamic prior that integrates changes in data distribution with an affordable computational cost, allowing efficient and accurate removal of outdated or unwanted data patterns and biases. Our experiments across various datasets, models, and unlearning methods demonstrate that our approach effectively removes systematic patterns and biases while maintaining the performance of models post-unlearning.",
            "score": 0.5915144391122039,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "ities in tasks such as question-answering and reasoning. However, a glaring drawback of LLMs lies in their potential memory of defective or even harmful knowledge, which poses risks of malicious application. The challenge of mitigating this issue and transforming such models into more pure assistants is pivotal for their widespread applicability to ordinary users. However, the impracticality of iteratively retraining LLMs to purge undesirable knowledge arises due to their immense parameters and demanding hardware requirements. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model. This paper provides an in-depth review of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated. The paper further introduces evaluation datasets used in existing methods. Finally, a comprehensive analysis of ongoing Figure 1: Knowledge unlearning is used to eliminate harmful, privacy-sensitive, and copyright-related information from LLMs, ensuring the generation of reasonable responses in model output. Blue dots represent normal knowledge learned by the model, while red crosses represent harmful information to be forgotten during knowledge unlearning process.",
            "score": 0.5909106025188933,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "276107656",
            "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
            "text": "Large language models have been widely applied, but can inadvertently encode sensitive or harmful information, raising significant safety concerns. Machine unlearning has emerged to alleviate this concern; however, existing training-time unlearning approaches, relying on coarse-grained loss combinations, have limitations in precisely separating knowledge and balancing removal effectiveness with model utility. In contrast, we propose Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment (FALCON), a novel representation-guided unlearning approach that leverages information-theoretic guidance for efficient parameter selection, employs contrastive mechanisms to enhance representation separation, and projects conflict gradients onto orthogonal subspaces to resolve conflicts between forgetting and retention objectives. Extensive experiments demonstrate that FALCON achieves superior unlearning effectiveness while maintaining model utility, exhibiting robust resistance against knowledge recovery attempts.",
            "score": 0.5903748728052752,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "270045436",
            "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
            "text": "Eldan et al. [22] propose a technique that effectively removes specific content, such as copyrighted material, from large language models, without the need for complete retraining.The method preserves the model's overall performance and represents an advancement in managing legal and ethical issues related to LLM training data.Jang et al. [23] introduce a more efficient and effective method for reducing privacy risks in Pretrained Language Models by using gradient ascent for knowledge unlearning.\n\nMoreover, Yao et al. [24] explore unlearning in LLMs, presenting it as a technique for aligning LLMs with human preferences by removing harmful responses, erasing copyrighted content, and eliminating hallucinations.Pawelczyk et al. [25] introduce \"In-Context Unlearning\" for LLMs, a novel approach to machine unlearning that does not require updating model parameters.This method involves providing the LLM with the training instance to be unlearned alongside a flipped label and additional correctly labeled instances at inference time.\n\nYu et al. [26] presented a new technique named \"Partitioned Contrastive Gradient Unlearning\" (PCGU).This gray-box approach focuses on adjusting only those weights in the model that contribute significantly to certain biases, particularly implicit social biases.",
            "score": 0.5902541439779544,
            "section_title": "B. Large Language Models Unlearning",
            "char_start_offset": 16945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 501
                },
                {
                    "start": 503,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1040
                },
                {
                    "start": 1042,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1303
                }
            ],
            "ref_mentions": [
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2135009765625
        },
        {
            "corpus_id": "273233165",
            "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
            "text": "This work studies the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences (e.g., copyrighted or harmful content) while preserving model utility. Despite the increasing demand for unlearning, a technically-grounded optimization framework is lacking. Gradient ascent (GA)-type methods, though widely used, are suboptimal as they reverse the learning process without controlling optimization divergence (i.e., deviation from the pre-trained state), leading to risks of over-forgetting and potential model collapse. Negative preference optimization (NPO) has been proposed to address this issue and is considered one of the state-of-the-art LLM unlearning approaches. In this work, we revisit NPO and identify another critical issue: reference model bias. This bias arises from using the reference model (i.e., the model prior to unlearning) to evaluate the unlearning success, which can compromise NPO's effectiveness. Specifically, it leads to (a) uneven allocation of optimization power across forget data with varying difficulty levels and (b) ineffective gradient weight smoothing during the early stages of unlearning optimization. To overcome these challenges, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that `simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We provide deeper insights into SimNPO's advantages through an analysis based on mixtures of Markov chains. Extensive experiments further validate SimNPO's efficacy on benchmarks like TOFU and MUSE, as well as its robustness against relearning attacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Simple.",
            "score": 0.590134575080681,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24072265625
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with relevance.\n\nYi Xu [66] presentd a task of acheiving generalization, utility and efficiency in LLM unlearning by introducing an optimization goal which consider the forget, mismatch and maintain metrics of LLM, and then use gradient ascent method to update.Furthermore, [39] proposed a new task of fictious unlearning (TOFU) and offered a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning, and used fine-tuning, gradient ascent with KL-divergence requirement to make LLM unlearn well on diverse datasets.Metrics of different benchmarks on TOFU are still frustrating, which\n\nshows that parameter optimization methods still needs improvement.",
            "score": 0.589263400033147,
            "section_title": "Summary of Model-based Machine Unlearning",
            "char_start_offset": 26850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 195,
                    "end": 439
                },
                {
                    "start": 439,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 893
                },
                {
                    "start": 895,
                    "end": 961
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232666015625
        },
        {
            "corpus_id": "277620868",
            "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale",
            "text": "1. We merge up to 500 models, which is more than an order of magnitude more tasks than prior work (Ilharco et al., 2023;Wang et al., 2024). Our setting reflects the realistic scenario in unlearning where models are finetuned over a large set of tasks (e.g., a pool of clients or collection of documents), and tasks contribute subtly differing data for a common learning task rather than the tasks being largely disjoint. \n\n2. We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible. \n\n3. We propose SIFT, a finetuning method which makes model merging and localization computationally feasible for exact unlearning. Unlike existing methods where masks require global information, SIFT-Masks obtains masks by using only a random sign vector and local data. This allows us to maintain accuracy while enabling efficient exact unlearning at scale: As we show through extensive experiments on unlearning tasks, SIFT-Masks improves accuracy by 5-80% over na\u00efve merging and uses up to 250\u00d7 less compute for unlearning compared to other merging baselines.",
            "score": 0.5885054403542265,
            "section_title": "body",
            "char_start_offset": 3693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1250
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 120,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58251953125
        },
        {
            "corpus_id": "277634314",
            "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
            "text": "The goal of LLM unlearning is to remove specific knowledge from training data while preserving the model's performance on unrelated tasks (Jang et al., 2023;Ji et al., 2024b;Liu et al., 2024a;Feng et al., 2024). Existing methods can be categorized into three main approaches: i) Gradient-based methods (Jang et al., 2023;Maini et al., 2024) use gradient ascent on the forget set (i.e., the data to be unlearned) to remove associated knowledge, with parameter regularization added to preserve performance on other tasks. ii) Preference optimization-based methods (Maini et al., 2024;Zhang et al., 2024) treat the forget set as negative examples or assign predefined responses (e.g., rejection responses) to achieve unlearning during PA. iii) Model weight-based methods (Jia et al., 2024b) analyze the roles of different model modules to guide unlearning, leveraging the modularity of LLMs. As model weight-based methods are primarily used for attribution analysis, this study focuses on gradientbased and preference optimization-based approaches.",
            "score": 0.5883485300027105,
            "section_title": "LLM unlearning",
            "char_start_offset": 6277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1045
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 157,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 157,
                    "end": 174,
                    "matchedPaperCorpusId": "270440348"
                },
                {
                    "start": 174,
                    "end": 192,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 192,
                    "end": 210,
                    "matchedPaperCorpusId": "273901406"
                },
                {
                    "start": 302,
                    "end": 321,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 321,
                    "end": 340,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 562,
                    "end": 582,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "As the scales of current LLMs and the size of training data are usually large, updating all the parameters in the model F (.) (e.g., re-training F (.) on D r i ) becomes extremely expensive. Inspired by recent advances in parameter-efficient fine-tuning (Houlsby et al., 2019;Chien et al., 2023), we model F \u2032 (.) by F (f (.)) where f (.; W ) is an adapter with significant smaller amount of parameters W compared to F (.). And we would only update f (.) to fulfill the unlearning requests. \n\nTo effectively achieve the unlearning goals in equation 1, we minimize a selective teacher-student objective where the student model \n\nwhere \u03b1 is a hyper-parameter to balance the tradeoff between forgetting x f and retaining x r . Intuitively, during training, f (.) is leaned to minimize the KL-divergence between the output from the updated model and the original model on the data to retain while maximizing the KL-divergence between the output from them on the data to forget. \n\nTo maintain the task performance, we optimize f (.) for the task loss on the retain data: \n\nwhere l(.) is the task-related loss, for example, cross-entropy loss, \u2212 log P (F (f (x r ))), for classification tasks. Furthermore, we also negate the original training objectives used in LLMs (e.g., masked language modeling objective (Raffel et al., 2020)) to forget the knowledge related to the data, in order to forget in pre-trained parameters and ensure that the information in the forgotten data cannot be easily extracted from F (.): \n\nwhere l(.) is the language model loss used when pre-training F (.), for example, masked language model loss, \u2212 log P (x|x \u2212 x) (x are the randomly masked tokens). In our experiments, we utilize T5 models (Raffel et al., 2020). Thus we add an extra \"Predict the masked word\" at the beginning of the input for this loss term. \n\nOur final training objective is then the following:",
            "score": 0.5879595862544622,
            "section_title": "Learning to Forget via Unlearning Layers",
            "char_start_offset": 8340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 276,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 276,
                    "end": 295,
                    "matchedPaperCorpusId": "259298766"
                },
                {
                    "start": 1304,
                    "end": 1325,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1716,
                    "end": 1737,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11279296875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Differing from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning. \n\nTV (arXiv 2022.12) This paper introduces the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. Assuming the weights of the original pretrained model are denoted as \u03b8 pre and the weights of the model finetuned for the target task are denoted as , the task vector \u03c4 is obtained by subtracting the two (i.e., \u03c4 = \u2212\u03b8 f t \u2212 \u03b8 pre ), as illustrated on the left side of Figure 4. This task vector \u03c4 represents the parameter change vector after fine-tuning the model for downstream tasks. Taking the negation of the task vector, \u2212\u03c4 , enables the language model to forget related knowledge while exerting minimal influence on other aspects of the model, as depicted on the right side of Figure 4. \n\nCPEM (arXiv 2023.06) This paper primarily addresses the parameter-efficient modules (PEM) for LLMs, such as LoRA [27] and (IA)3 [28]. It employs arithmetic operations, including addition and subtraction, on multiple modules to alter the representation of knowledge within these modules. Two basic operators are defined: the addition operator and the negation operator. The negation operator, in particular, facilitates the forgetting of knowledge stored in the adapter modules, providing a means to The above operation results in the reversal of activation values. The principle of this method is similar to gradient ascent, inducing a change in the intermediate layer's activation values in the direction opposite to gradient descent. This mechanism facilitates the unlearning of knowledge within the module.",
            "score": 0.5879335655996897,
            "section_title": "Parameter Merging",
            "char_start_offset": 22212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62353515625
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "The swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.",
            "score": 0.5876616157026749,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2313
                },
                {
                    "start": 2316,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 354,
                    "matchedPaperCorpusId": "168855552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385009765625
        },
        {
            "corpus_id": "277780105",
            "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
            "text": "The problem of machine unlearning (MU) for large language models (LLMs), referred to as LLM unlearning (Liu et al., 2025;Si et al., 2023;Qu et al., 2024;Cooper et al., 2024), is gaining critical importance as a means of enforcing data privacy rights (e.g., preventing the generation of copyrighted or sensitive content) (Eldan & Russinovich, 2023;Shi et al., 2024b;Maini et al., 2024;Jang et al., 2022), and for removing harmful or unsafe knowledge embedded in models amid growing concerns around safety and alignment (Li et al., 2024;Yao et al., 2024;Barez et al., 2025;Zhang et al., 2024b;Chen et al., 2025). The core objective of MU is to remove the influence of specific, undesired data or knowledge from a trained model, while preserving its general utility, without the cost of full retraining from scratch. \n\nDespite the growing importance of LLM unlearning, much of the existing research has primarily focused on the design of unlearning algorithms.",
            "score": 0.5875929878774222,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 957
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 121,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 121,
                    "end": 137,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 347,
                    "end": 365,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 365,
                    "end": 384,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 535,
                    "end": 552,
                    "matchedPaperCorpusId": "264172840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.148681640625
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "Large language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers. Experimental results demonstrate the effectiveness of LAW in forgetting target knowledge while maintaining reasoning ability. The code will be open-sourced at https://github.com/wangyu-ustc/LargeScaleWashing.",
            "score": 0.5874620189211552,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276611328125
        },
        {
            "corpus_id": "270372051",
            "title": "A Survey on Machine Unlearning: Techniques and New Emerged Privacy Risks",
            "text": "This method can effectively implement data removal in LLMs, and has the great potential to solve copyright issues.\n\nPawelczyk et al. [92] formalized a new unlearning paradigm for LLMs, and proposed a black-box removal mechanism, In-Context Unlearning (ICUL), aiming to unlearn data points by providing specific contextual information during the inference stage, without having to update model parameters.This method flips label on the forgotten data to eliminate its influence on the model, and then adds correctly labelled training samples to input to mitigate the impact of the label flipping operation.Finally, the constructed context with the query input of forgotten data is fed to LLMs.ICUL provides competitive model performance on real-world datasets and exhibits strong robustness against membership inference attacks.However, such black-box unlearning method introduces higher computational burdens.\n\nLarge language models exist with harmful social biases, which may lead to unfairness in natural language processing procedures.Yu et al. [93] proposed a Partitioned Contrastive Gradient Unlearning (PCGU) to identify the sources of problematic inferences in the model, and systematically retrain those parts of model to unlearn biased data.The basic idea of PCGU is to optimize the weights that makes the greatest contribution to a particular bias domain by comparing gradients of sentence pairs.Specially, the gradient is computed for a pair of sentences whose difference is in the bias domain, and the rank of weights is calculated by using a gradient-based importance algorithm.Then, with the gradients and ordered weights as inputs, PCGU computes a first-order approximation of bias gradient to optimize LLM.This method is highly effective in mitigating social bias in LLMs while also lowering costs.",
            "score": 0.5870970466530527,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 69226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 116,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1722
                },
                {
                    "start": 1722,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "It stands as a necessary approach for developing responsible, legally compliant, and user-trusted LLMs. To propel the advancement of this field, this paper investigates existing research related to knowledge unlearning for LLMs, with a primary focus on the problems, methods, and future directions. To the best of our knowledge, this paper is one of the early works in researching this issue. The primary contributions of this paper are as follows: \n\n\u2022 Building upon research on machine unlearning, we introduce for the first time the concept of knowledge unlearning for LLMs. We analyze its differences and connections with machine unlearning. \n\n\u2022 We conduct a comprehensive literature review, and categorize existing methods for knowledge unlearning in LLMs, including methods based on parameter optimization, parameter merging, and in-context learning. Detailed introduction of the principles and characteristics of each method are then provided, as well as the datasets and tasks used in evaluation. \n\n\u2022 Based on an in-depth analysis of challenges and demands in this field, we unveil future research directions of knowledge unlearning in LLMs. \n\nThe rest of this survey is illustrated in Figure 2. Section 2 defines the problem of knowledge unlearning, comparing it with machine unlearning and model editing. Section 3 introduces knowledge unlearning methods for LLMs, categorizing them into three types: methods based on parameter optimization, parameter merging, and in-context learning. Section 4 presents relevant datasets and evaluations. Section 5 summarizes the work of this paper and discusses future directions.",
            "score": 0.5870521641588982,
            "section_title": "Introduction",
            "char_start_offset": 5919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1625
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495361328125
        },
        {
            "corpus_id": "278338982",
            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
            "text": "The rapid growth of large language models (LLMs), trained on internet-scraped data, has raised concerns about privacy, compliance, and ethical usage. Regulations like GDPR require methods for selectively removing sensitive or copyrighted information from these models. Researchers have proposed various post-training techniques, which we broadly categorize into (i) knowledge removal, (ii) knowledge addition, (iii) knowledge edition. This paper focuses on knowledge removal, also referred to as unlearning (Liu et al., 2025), which involves removing specific information from trained LLMs without complete retraining. Ideally, after unlearning, the LLM behaves as though the removed information had never been learned. However, current 1 https://github.com/potsawee/unlearning-hallu/ tree/mcq methods often perform unlearning by extensively adding incorrect or irrelevant information, a practice we refer to as obfuscation, which effectively constitutes a form of knowledge addition rather than true removal, and can lead to random or incorrect model responses. Unlike knowledge editing (Mitchell et al., 2022), which updates factual associations, unlearning (the focus of this work) aims to eliminate targeted knowledge entirely. \n\nEarly knowledge removal approaches were gradient ascent (GA) based (Jang et al., 2023;Ilharco et al., 2023a;Yao et al., 2024a) and structural or privacy-related sub-circuit discovery methods (Bayazit et al., 2024), which directly minimize the probability of original facts. Negative preference optimization (Zhang et al., 2024) removes knowledge by increasing the probability of false statements compared to the true ones, which was an early form of obfuscating. More recent obfuscatingbased methods (Eldan and Russinovich, 2023;Liu et al., 2024;Dong et al., 2024;Xu et al., 2025a) have gained popularity due to their superior stability and the minimal distortion to knowledge to be retained. For example, WHP (Eldan and Russinovich, 2023) and WHP + (Liu et al., 2024) remove knowledge about target people by overwhelming LLMs with information from other individuals.",
            "score": 0.5869914841855532,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 525,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 1088,
                    "end": 1111,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1301,
                    "end": 1320,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1320,
                    "end": 1342,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1425,
                    "end": 1447,
                    "matchedPaperCorpusId": "263671765"
                },
                {
                    "start": 1541,
                    "end": 1561,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1763,
                    "end": 1780,
                    "matchedPaperCorpusId": "271404131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313232421875
        },
        {
            "corpus_id": "274446254",
            "title": "Improved Localized Machine Unlearning Through the Lens of Memorization",
            "text": "Machine unlearning refers to removing the influence of a specified subset of training data from a machine learning model, efficiently, after it has already been trained. This is important for key applications, including making the model more accurate by removing outdated, mislabeled, or poisoned data. In this work, we study localized unlearning, where the unlearning algorithm operates on a (small) identified subset of parameters. Drawing inspiration from the memorization literature, we propose an improved localization strategy that yields strong results when paired with existing unlearning algorithms. We also propose a new unlearning algorithm, Deletion by Example Localization (DEL), that resets the parameters deemed-to-be most critical according to our localization strategy, and then finetunes them. Our extensive experiments on different datasets, forget sets and metrics reveal that DEL sets a new state-of-the-art for unlearning metrics, against both localized and full-parameter methods, while modifying a small subset of parameters, and outperforms the state-of-the-art localized unlearning in terms of test accuracy too.",
            "score": 0.5864555291934317,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39111328125
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "MUSE divides data into two sets: the Forget set, containing the information to be unlearned, and the Retain set, which measures the impact of unlearning on unrelated knowledge. Ideally, unlearning should be precise, affecting only the Forget set without disturbing the Retain set. MUSE provides fine-tuned models for both sets as optimal reference points. To capture the training dynamics, we compute the average KL divergence between the unlearned model and the MUSE reference models over the Forget and Retain sets. An effective unlearning model should closely match both references, with near-zero divergence indicating successful unlearning and model performance preservation. \n\npress the generation of unwanted memorization at decode time. Task Arithmetic (TA) approaches (Ilharco et al., 2023) also fine-tune a model to memorize the forget set and leverage linear parameter merging (Matena and Raffel, 2022) to remove the memorization in model weights. Majmudar et al. (2022) apply linear interpolation with uniform distribution at the decoding time and show that this satisfies certain differential privacy (DP) criteria. Chen and Yang (2023) tune multiple unlearning layers to handle sequential unlearning requests and then fuse and plug them back into the base LLM. We set aside post-processing methods such as directly prompting LLMs to add a guardrail (Thaker et al., 2024); our focus is on removing knowledge directly from the base LLM via fine-tuning.",
            "score": 0.5863527319001837,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 6942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 777,
                    "end": 799,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 888,
                    "end": 913,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 959,
                    "end": 981,
                    "matchedPaperCorpusId": "249151985"
                },
                {
                    "start": 1129,
                    "end": 1149,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426025390625
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed. For example, ground-truth token sequences could still be extracted from 90.0% of multi-hop questions after unlearning in the Llama-3.1-8B-Instruct model using NPO (Zhang et al., 2024c), despite the original extraction success rate of 98.1% before unlearning. \n\nTo achieve more faithful knowledge unlearning, we propose a simple yet effective approach, MUNCH, which significantly outperforms existing approaches in unlearning multi-hop knowledge. MUNCH first decomposes multi-hop questions into successive subquestions, generates provisional answers, and employs the uncertainty of the unlearned model on the generated outputs as a measure to determine whether to provide a rejective response (e.g., \"I don't know.\") or keep it as is. Our method capitalizes on the high uncertainty of the unlearned model when dealing with direct, single-hop facts -an effect stemming from the reversed language modeling objective. By inspecting the decomposed multi-hop questions, we can more easily distinguish between information that needs to be forgotten and information that should be retained. Empirical results on the modified MQuAKE dataset confirm the efficacy of our approach, and we emphasize that MUNCH is highly practical, requiring no additional training and integrating seamlessly with existing unlearning techniques. To our knowledge, this is the first work to explore the unlearning of multi-hop knowledge.",
            "score": 0.5856302793037234,
            "section_title": "Unlearn Request",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1556
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 334,
                    "matchedPaperCorpusId": "269009619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16650390625
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "This survey addresses the critical challenge of the forgetting sensitive/poisonous data in the machine learning domain by delving into the advancements of machine unlearning, particularly within the context of Large Language Models (LLMs).By defining unlearning procedures, classifying unlearning approaches and establishing evaluation criteria, this work contributes significantly to the understanding and development of effective unlearning techniques in both traditional models and LLMs.This survey highlights the limitations of existing evaluation methods and underscores the necessity for comprehensive assessments to ensure the long-term effectiveness of unlearning, offering a solid foundation and clear directions for future research in this vital area of privacy protection.",
            "score": 0.5853957720269847,
            "section_title": "CONCLUSION",
            "char_start_offset": 33423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 783
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33056640625
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "For a long time, people have hoped for machines to have the ability to \"learn,\" enabling them to better understand the world and interact with humans by learning knowledge. In contrast to machine learning, the goal of machine unlearning is to endow models with the capability to forget knowledge actively, allowing them to \"proactively\" erase certain specific knowledge they have previously learned [1] [2]. In the era of large language models (LLMs), LLMs un-dergo pre-training on massive amounts of text to acquire and store a broad range of world knowledge. This paradigm has demonstrated excellence in various downstream natural language processing tasks [3] [4]. However, LLMs also encode significant amounts of private data, copyrighted content, and biased information [5] [6]. Recent research indicates that large language models merely recall and replicate the training samples they have encountered. Although this knowledge is encoded in parameterized, distributed,and high-dimensional embedding vectors, it can often be triggered in specific situations, potentially impacting user privacy or causing other data security concerns. Similar to traditional knowledge bases, it is necessary to establish knowledge removal mechanisms for LLMs, allowing for the removal of specific knowledge from the model upon user request. This approach is known as LLMs knowledge unlearning. It grants knowledge in LLMs the Right To Be Forgotten. When users request the removal of information related to personal privacy from applications driven by LLMs, the models should provide a reasonable response, complying with the user's demand for the forgetting of privacy data to protect the user's legitimate interests and mitigate the risk of legal action against these applications. \n\nUnlearning is not a recently emerging issue. In traditional machine learning research, machine unlearning has long been a subject of widespread research interest. It focuses on studying various unlearning methods for models to forget, aiming to enhance the model's security (unlearning toxic data), privacy (unlearning private data), and impartiality (unlearning biased data) [2].",
            "score": 0.5852069628805228,
            "section_title": "Introduction",
            "char_start_offset": 1744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2153
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "254805754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328369140625
        },
        {
            "corpus_id": "267365499",
            "title": "Unlearnable Algorithms for In-context Learning",
            "text": "After a machine learning model is deployed, it may become necessary to deploy a new model that does not use part of the original training set. This can occur because of legislation on the \"right to be forgotten\" [Mantelero, 2013] or because of unknown data provenance (e.g., some data may have come from untrustworthy sources). Machine unlearning addresses this challenge of modifying a model to behave as if it were trained without including a certain datapoint. \n\nFormally, the objective of machine unlearning is, given a datapoint x to unlearn and a model trained on some dataset D, to reproduce the model (distribution) that is equivalent to training on D \\ x [Cao and Yang, 2015]. The baseline solution is to retrain on D \\ x, however, one is generally interested in when machine unlearning can be achieved with less cost than this baseline. That is, solutions to unlearning that are efficient. Towards this, work has shown faster machine unlearning algorithms for statistical query learning algorithms [Cao and Yang, 2015], clustering algorithms [Ginart et al., 2019], and random forests [Brophy and Lowd, 2021]. For deep neural networks (DNNs), known methods for exact unlearning [Bourtoule et al., 2021] still have a cost on the order of training (and can come with performance degradation). In summary, although classical methods have known \"efficient\" machine unlearning algorithms, developing approaches compatible with deep neural networks is still an open problem and one that is desirable to solve, due to the high cost of retraining on large datasets and models. \n\nWe observe that the paradigm introduced by large language models (LLMs) paves new approaches for efficient exact unlearning for deep learning. Indeed, large language models typically learn from data in two phases: 1) with an unsupervised task-agnostic objective and 2) with a task adaptation method for a specific downstream task. In this paradigm, we observe that there are efficient exact unlearning operations to unlearn data introduced in the second phase when using in-context learning to do task adaptation.",
            "score": 0.5851634778105037,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 229,
                    "matchedPaperCorpusId": "56699980"
                },
                {
                    "start": 664,
                    "end": 684,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1008,
                    "end": 1028,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1052,
                    "end": 1073,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1094,
                    "end": 1117,
                    "matchedPaperCorpusId": "235422138"
                },
                {
                    "start": 1187,
                    "end": 1211,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259033203125
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.",
            "score": 0.5849145305058391,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.491455078125
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "Problem setup.LLM unlearning aims to mitigate the influence of undesired data, such as sensitive or copyrighted information, and/or restrict the model's capabilities to avoid the associated content generation.This process also requires preserving the LLM's utility for unrelated tasks and avoiding full retraining to maintain computational efficiency.\n\nFollowing the generic formulation of LLM unlearning in (Liu et al., 2024a), the unlearning problem can be conceptualized as removing the influence of a designated 'unlearning target'-whether it pertains to data, knowledge, or model capabilitiesfrom a pre-trained LLM (denoted as \u03b8 o ).The unlearning target is typically specified by a forget set D f , which includes the information or knowledge intended for removal.To preserve the LLM's generation capability (i.e., utility) after unlearning, a retain set D r is also introduced.This set comprises data that is irrelevant to the unlearning target.Given the aforementioned setup, the problem of LLM unlearning is often formulated as a regularized optimization problem, fine-tuned from \u03b8 o over the forget set D f and the retain set D r :\n\nHere \u2113 f and \u2113 r represent the forget loss and the retrain loss respectively, and \u03bb \u2265 0 is a regularization parameter to strike a balance between unlearning and utility preservation.Note that problem ( 1) is not the only formulation of LLM unlearning.Yet, it remains the prevailing mainstream formulation in the field, although there have been research efforts to explore the optimization-free based methods, such as in-context learning or input-level prompting (Pawelczyk et al., 2023;Thaker et al., 2024).\n\nSome specifics of LLM unlearning (1).While problem (1) may appear as a straightforward optimization task initially, complexities arise in determining the effective forget loss \u2113 f and achieving the optimal balance between unlearning and utility.These questions remain challenging in the literature.We present three representative LLM unlearning approaches and illustrate how they relate to the specifics of problem ( 1).",
            "score": 0.5846123401636688,
            "section_title": "Primer on LLM Unlearning",
            "char_start_offset": 9071,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 14,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 351
                },
                {
                    "start": 353,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 770
                },
                {
                    "start": 770,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1141
                },
                {
                    "start": 1143,
                    "end": 1325
                },
                {
                    "start": 1325,
                    "end": 1394
                },
                {
                    "start": 1394,
                    "end": 1650
                },
                {
                    "start": 1652,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1897
                },
                {
                    "start": 1897,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343017578125
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "To obtain hyperparameters for unlearning, we evaluate the unlearned models on a subset of 100 facts from F. Building on the code base from Jia et al. ( 2024), we apply the resulting optimized hyperparameter set across various unlearning algorithms to maximize effectiveness. For Gradient Ascent, here are the parameters we consider: \n\n\"unlearn_method\": \"GA+FT\", \"num_epochs\": 2, \"lr\": 2e-05, \"weight_decay\": 0.1, \"gradient_accumulation_steps\": 4, \"task_name\": \"mix_ai\", \"use_lora\": true, \"GA+FT\": {\"lambda\": 4} \n\nFor KL Divergence, here are the parameters: \n\n\"unlearn_method\": \"KL+FT\", \"num_epochs\": 2, \"lr\": 1.5e-05, \"weight_decay\": 0.1, \"gradient_accumulation_steps\": 4, \"task_name\": \"mix_ai\", \"use_lora\": true, \"KL+FT\": {\"lambda\": 0.2} \n\nFor Negative Preference Optimization, here are the parameters: \n\n\"unlearn_method\": \"NPO\", \"num_epochs\": 3, \"lr\": 2e-05, \"weight_decay\": 0.1, \"gradient_accumulation_steps\": 4, \"task_name\": \"mix_ai\", \"use_lora\": true, \"NPO\": {\"lambda\": 5} \n\nparameter \u03bb controls the relation between forget and retain set over the course of optimization.",
            "score": 0.5842371073095045,
            "section_title": "C.2 Unlearning",
            "char_start_offset": 38941,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 803
                },
                {
                    "start": 806,
                    "end": 977
                },
                {
                    "start": 980,
                    "end": 1076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "This paper introduces LLMEraser, a unified parameter-efficient unlearning framework. By systematically categorizing and addressing various unlearning tasks, LLMEraser leverages influence functions for parameter adjustments, circumventing the cumbersome retraining processes common in traditional methods. Extensive experiments on benchmark datasets show that LLMEraser excels in efficiently handling various unlearning tasks while preserving the overall integrity and efficacy of the models. Additionally, LLMEraser opens new avenues for future research, encouraging the exploration of enhanced unlearning techniques and their implications in diverse applications, such as data privacy and ethical AI. Future studies could explore the broader applicability of LLMEraser and potential optimizations for its computational efficiency and accuracy. \n\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient finetuning for transformer-based masked language-models. In ACL (2), pp. A OVERVIEW OF EXISTING LLM UNLEARNING METHODS \n\n\u2022 SISA (Bourtoule et al., 2021): It works by dividing the training dataset into partitions, allowing for targeted unlearning of specific instances. The methodology typically involves the following steps: data partitioning, retraining, and aggregation. However, a notable limitation of SISA is that it does not preserve the model architecture and requires retraining of sub-models, which can lead to increased computational costs. \u2022 FairSISA (Kadhe et al., 2023): FairSISA improves upon SISA by incorporating fairness enhancements. It still relies on the paradigm of retraining sub-models to handle unlearning requests. This approach inherently alters the model architecture and necessitates the retraining of the submodels, which can limit the flexibility and efficiency of the unlearning process. \u2022 APA (Hu et al., 2024c): This study introduces the first exact unlearning approach for large language model-based recommendation (LLMRec), focusing on the removal of personal data to comply with privacy regulations.",
            "score": 0.584201765052064,
            "section_title": "CONCLUSION AND FUTURE WORK",
            "char_start_offset": 26326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
            "score": 0.584176110751512,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36962890625
        },
        {
            "corpus_id": "267897394",
            "title": "Machine Unlearning of Pre-trained Large Language Models",
            "text": "This work primarily focuses on conducting experiments with the Yi-6B model. A significant challenge arises since most LLMs do not open-source their pre-training data, making the collection of forget sets infeasible. We encourage future research to investigate the applicability of unlearning processes to other models, including those of larger sizes such as 13B or 70B, or more complicated architecture such as the mixture of experts. Additionally, our experiments are mainly conducted on three specific pre-training data domains. Future research should aim to explore unlearning across other domains, including Wikipedia and News. \n\nMoreover, our work concentrates on unlearning copyrighted content from LLMs. Future studies could expand our methodologies to address other challenges, such as unlearning biases or harmful outputs in LLMs. Given that our methods are nonconvergent and may reduce model utility until convergence, the adjustment of hyperparameters becomes crucial for ideal unlearning results. While our guidelines simplify and streamline this process, we hope that future research will develop convergent methods that are less dependent on hyperparameter adjustments. \n\nThis study emphasizes the practical aspects of approximate unlearning in LLMs. However, since distinguishing between member and non-member data in LLMs remains challenging, the evaluation of approximate unlearning can be complex. A more principled theoretical investigation of unlearning in LLMs is necessary for future work. Additionally, more powerful MIA methods and alternative methods beyond MIA are needed for evaluating unlearning methods in the context of LLMs. While our focus is on unlearning data within a single domain while maintaining the unlearned model's general capability, exploring the unlearning of taskspecific data and assessing the impact on the unlearned model's performance in that task could be an interesting and important direction for future research. \n\nFigures 3a to 3c display the results of unlearning 2720 sequences of GitHub code, with the batch size varied from 85 to 2720 and other hyperparameters fixed. The results demonstrate that a small batch size leads to more optimization steps, which can cause detrimental damage to the model's general utility. However, a higher batch size also reduces the unlearning efficacy.",
            "score": 0.5840131772138665,
            "section_title": "Limitations",
            "char_start_offset": 28527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3427734375
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "This approach has led to the emergence of localization-informed LLM unlearning (Liu et al., 2024a).Furthermore, input prompt strategies have been employed, enabling unlearning through model queries and/or adjusting only a small fraction of parameters (Madaan et al., 2022;Zheng et al., 2023;Pawelczyk et al., 2023).\n\nDespite the recent progress of LLM unlearning, the majority of existing fine-tuning-based approaches have relied on first-order (FO) optimization to conduct unlearning.To our knowledge, there have been no prior studies that specifically investigate LLM unlearning from the perspective of optimizer design.In this work, we unveil the power of second-order (SO) optimizer in LLM unlearning and demonstrate its superiority over FO optimizer in various fine-tuning scenarios.We term the second-order optimization-based unlearning framework as SOUL (second-order unlearning).We will show that SOUL not only offers a viable approach for enhancing unlearning efficacy but also stays effective in preserving model utility.Such an optimizer-induced advantage holds consistently across various LLM unlearning objectives and formulations, providing a generic improvement.We summarize our contributions below.\n\n\u2022 We study the impact of optimizer choice in LLM unlearning, explicitly linking SO optimization and iterative influence unlearning.\n\n\u2022 We propose SOUL, built upon and extended from Sophia (second-order clipped stochastic optimization) (Liu et al., 2023a).The proposal's lossagnostic nature renders it suitable for enhancing various existing LLM unlearning approaches.",
            "score": 0.5831588746789523,
            "section_title": "Introduction",
            "char_start_offset": 3289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 315
                },
                {
                    "start": 317,
                    "end": 485
                },
                {
                    "start": 485,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 887
                },
                {
                    "start": 887,
                    "end": 1031
                },
                {
                    "start": 1031,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1214
                },
                {
                    "start": 1216,
                    "end": 1347
                },
                {
                    "start": 1349,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1583
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "3.1 Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set \n\nAfter unlearning, the model's performance should be indistinguishable from a model trained exclusively on the retained dataset D r = D\\D f . Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set. The method builds upon the standard training paradigm of the P \u03b8 , which minimizes the prediction loss over the full dataset D. To enforce forgetting, gradient ascent maximizes the prediction loss on the target forget subset D f , effectively approximating the reversal of the original optimization process. This procedure can be equivalently interpreted as performing gradient descent on the negative prediction loss (Zhang et al., 2024). The gradient ascent objective, denoted as L GA , is formulated as: \n\n(1)",
            "score": 0.5831163025731947,
            "section_title": "Preliminaries",
            "char_start_offset": 7810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3544921875
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "In this study, we introduced and explored the concept of \"in-context knowledge unlearning\" within the framework of Large Language Models (LLMs) through the use of fine tuning. Our findings demonstrate that this approach not only enables LLMs to dynamically \"forget\" or selectively disregard information in test-time but also uncovers a nuanced behavior of LLMs-where they \"pretend to forget\" rather than actually eliminating the information from their knowledge base. \n\nThe ability of LLMs to learn to \"unlearn\" in both in-domain and out-of-domain scenarios without compromising their overall performance represents a significant step forward in the quest for more ethically responsible and privacy-conscious AI technologies. This capability is crucial for applications where sensitive or confidential information must be managed with great care, such as in healthcare, legal, and educational sectors. \n\nOur in-context knowledge unlearning method faces two main limitations: \n\n\u2022 Application to Closed Models: The method is difficult to apply to closed models accessible only via APIs (e.g., GPT-3, ChatGPT). These models do not allow modifications to their architecture or training procedure, which are necessary for implementing our unlearning tokens and loss functions. For instance, we cannot add the \u00abUNL\u00bb tokens or fine-tune the model to recognize them in such closed systems. \n\n\u2022 Lack of Internal Behavior Analysis: For closed models, we cannot analyze the internal unlearning process. This prevents us from observing how the model's internal representations change during the unlearning process, as we did with the logit lens analysis for open models like LLaMA2 and Mistral. Consequently, we cannot verify if the \"pretend to forget\" behavior occurs in closed models or optimize the unlearning process for better performance. \n\nThese limitations highlight the challenges in implementing and fully understanding our approach in environments with limited model transparency and configurability, particularly in widely-used commercial AI systems.",
            "score": 0.5827088054577987,
            "section_title": "Conclusion",
            "char_start_offset": 19127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1381
                },
                {
                    "start": 1384,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 2050
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "265498745",
            "title": "Machine Unlearning in Learned Databases: An Experimental Analysis",
            "text": "[51] introduce a process with quantized gradients and randomized smoothing to avoid future unlearning, but large changes in data distribution, as a result of deletion, may exceed the 'deletion budget' invalidating assumptions. More recent works try to directly identify the parameters in the original model that are significantly influenced by the forget-set, aiming to modify these parameters to eliminate the impact of the forget-set. [8,37] leverage fisher information scores to identify the important parameters for the forget-set. [37] take a straightforward approach by fine-tuning the model on the retain-set while keeping the remaining parameters frozen. In contrast, [8] tries to 'dampen' those parameters while minimizing adverse effects on those essential to the retain-set. [19] introduce a 'sparsity-aware' unlearning technique, integrating unlearning through fine-tuning on the retain-set with a sparsification policy employing model pruning techniques. [7], on the other hand, proposes a 'saliency-aware' unlearning approach, utilizing the loss function's gradient to learn a mask identifying 'salient' parameters related to the forget-set. These parameters are subsequently unlearned using existing unlearning baselines such as 'random labelling'. Despite the diversity in these methods, a common challenge persists-efficiently identifying and modifying the important parameters tied to the forget-set without adversely affecting those essential to the retain-set. \n\nOn the other hand, SCRUB [25] is a recent machine unlearning method for computer vision image classification tasks that scales better than previous works without making restrictive assumptions. SCRUB reveals different requirements and metrics for different unlearning applications (e.g., removing biases, correcting erroneous mislabelling or attack-poisoned labels, and preserving user privacy). SCRUB is shown to be the most consistently well-performing approach on a wide range of architectures, datasets and metrics.",
            "score": 0.5826202683775811,
            "section_title": "Machine Unlearning",
            "char_start_offset": 11942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2002
                }
            ],
            "ref_mentions": [
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "278338982",
            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
            "text": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.",
            "score": 0.5820828009677659,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "In scenarios involving similar tasks, the differences between UNLEARN/LEARN and existing methods become even more pronounced. In the LEARN setting of Table 3, both methods show comparable improvements in task performance, demonstrating their efficacy for bolstering model performance. In the forgetting setting, the UNLEARN algorithm is able to successfully discriminate between two similar tasks and only remove the task of interest. We compared UNLEARN to the current stateof-the-art algorithms: Gradient Ascent (Yao et al., 2024), Knowledge Gap Alignment (KGA; Wang et al., 2023), and Knowledge Unlearning ( KU Jang et al., 2022). As seen in Table 1, these state-of-theart methods are unable to discriminate effectively between tasks, leading to performance degradation in closely related tasks. For example, when Narra-tiveQA is the task of interest, UNLEARN successfully degrades that task (down from 0.778 to 0.135) while maintaing the performance on NaturalQuestions (from 0.680 to 0.628). All three state-of-theart algorithms successfully degrade NarrativeQA: GA degrades the task to 0.094, KGA to 0.183, and KU to 0.163. However, they all show significantly diminished performance on NaturalQuestions: GA degrades the task to 0.415, KGA to 0.229, and KU to 0.329. These state-of-the-art methods lack the discrimination ability to target the knowledge they seek to remove without unwanted performance effects on secondary tasks. \n\nConversely, with its precise subspace manipulation, the UNLEARN method allows for the selective removal of task influences without negatively impacting the performance of related tasks. This specificity is particularly beneficial in multitask learning/unlearning environments where tasks share overlapping features (similar weight subspaces). As such, UNLEARN is better suited for forgetting tasks while preserving similar tasks.",
            "score": 0.5814190671710491,
            "section_title": "Task Performance",
            "char_start_offset": 24444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1868
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "273233165",
            "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
            "text": "This gives rise to the problem of LLM unlearning, which aims to effectively remove undesired data influences and/or model behaviors while preserving the utility for essential, unrelated knowledge generation, and maintaining efficiency without the need for retraining (Eldan & Russinovich, 2023;Yao et al., 2023;Liu et al., 2024b;Blanco-Justicia et al., 2024). \n\nTo trace its origins, the concept of machine unlearning was initially developed for data removal to comply with privacy regulations such as the \"right to be forgotten\" (Rosen, 2011;Hoofnagle et al., 2019), with early studies focusing on vision models (Cao & Yang, 2015;Warnecke et al., 2021;Bourtoule et al., 2021;Thudi et al., 2022;Kurmanji et al., 2024;Jia et al., 2023;Gandikota et al., 2023;Fan et al., 2024b). However, it is soon adapted to LLMs to remove unwanted data, knowledge, or specific model capabilities (Eldan & Russinovich, 2023;Yao et al., 2023;Liu et al., 2024b;Ji et al., 2024;Li et al., 2024;Shi et al., 2024;Maini et al., 2024;Zhang et al., 2024a;Jia et al., 2024). Compared to vision model unlearning, designing effective and efficient unlearning methods for LLMs presents its own unique challenges (Liu et al., 2024b). In particular, the current optimization foundation for LLM unlearning often relies on driving divergence to achieve the unlearning objective, making model parameter adjustments for unlearning difficult to control (Zhang et al., 2024a;Liu et al., 2022a;Maini et al., 2024;Yao et al., 2023;Jia et al., 2024).",
            "score": 0.5813762456549896,
            "section_title": "body",
            "char_start_offset": 1169,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1510
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 543,
                    "matchedPaperCorpusId": "9355614"
                },
                {
                    "start": 543,
                    "end": 566,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 613,
                    "end": 631,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 676,
                    "end": 695,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 695,
                    "end": 717,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 717,
                    "end": 734,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 734,
                    "end": 757,
                    "matchedPaperCorpusId": "257495777"
                },
                {
                    "start": 757,
                    "end": 775,
                    "matchedPaperCorpusId": "264305818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093505859375
        },
        {
            "corpus_id": "268230488",
            "title": "Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy",
            "text": "Kurmanji et al. [24] introduce the SCRUB algorithm for unlearning.It optimizes two loss terms to update the unlearned model.First, the KL divergence between the predictions the original and unlearned models make on the forget set   .This term is maximized; the motivation behind this is that the original model will likely perform well on forget set samples and so we want to increase the error on these samples.Second, to preserve the accuracy on the retain set   , the KL divergence between the predictions on   from the original model and the unlearned model are minimized (and the cross-entropy loss on the unlearned model is also minimized).These two steps are alternated, where the maximization step is usually stopped earlier than the minimization step.",
            "score": 0.5808985241845002,
            "section_title": "SCRUB.",
            "char_start_offset": 25962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 66,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 760
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "273532566",
            "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "text": "The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. Despite growing interest of LLM unlearning, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning, malicious use prevention, and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques.",
            "score": 0.5808248926352131,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4931640625
        },
        {
            "corpus_id": "267657624",
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "text": "In addition, gradient-based saliency (Yu et al., 2023) is employed to identify the crucial weights that need to be fine-tuned to achieve the unlearning objective. In (Wu et al., 2023b), neurons that respond to unlearning targets are identified within the feed-forward network and subsequently selected for knowledge unlearning. In the context of vision models, unlearning can also benefit from localizing weights salient to unlearning, as demonstrated by Jia et al. (2023) and Fan et al. (2024). Furthermore, the concept of localization-informed unlearning resonates with the future modular machine learning solution development (Menik & Ramaswamy, 2023). This modularity allows the emerging foundation models to be partitioned into manageable subparts, facilitating easier maintenance and independent updates for each component. \n\n\u2022 Influence function-based methods: While the influence function (Koh & Liang, 2017;Bae et al., 2022) is a standard approach to assess the effect of data removal on model performance (Izzo et al., 2021;Warnecke et al., 2021), it is not commonly employed in the context of LLM unlearning for two main reasons: the computational complexity involved in inverting the Hessian matrix, and the reduced accuracy resulting from the use of approximations in influence function derivation (Jia et al., 2023). However, the potential of influence functions in LLM unlearning may be underestimated, given that scalability issue has been improved by Grosse et al. (2023), and approximation errors can be mitigated by focusing on localized weights that are salient to unlearning, as described in the previous category. \n\n\u2022 Other model-based methods: Other types of LLM unlearning methods fall outside the categories mentioned above. For example, Jang et al. (2022); Chen & Yang (2023) show that sequential unlearning outperforms batch unlearning However, as indicated by Gu et al. (2024), sequential editing of LLMs may compromise their general capabilities.",
            "score": 0.5805861715417286,
            "section_title": "LLM Unlearning Methods Revisited: Prior Art and Overlooked Principles",
            "char_start_offset": 18225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 829
                },
                {
                    "start": 832,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1975
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 54,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 166,
                    "end": 184,
                    "matchedPaperCorpusId": "260500153"
                },
                {
                    "start": 455,
                    "end": 472,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 477,
                    "end": 494,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 897,
                    "end": 916,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 916,
                    "end": 932,
                    "matchedPaperCorpusId": "252198897"
                },
                {
                    "start": 1015,
                    "end": 1034,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 1311,
                    "end": 1329,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333251953125
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "We present a comprehensive taxonomy of LLM unlearning, as illustrated in Figure 2, outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal benchmarks.",
            "score": 0.5805151317321092,
            "section_title": "Taxonomy",
            "char_start_offset": 6148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 771
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7509765625
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inher-Figure 1. Left: Standard unlearning methods are applied equally to all points in the forget set. Here, outlier points in the model's hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage). Right: By finding a lower-variance coreset within the forget set, UPCORE reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points. \n\nently capturing sensitive, copyrighted, or undesirable content (Shokri et al., 2017;Carlini et al., 2019). As regulations like the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) empower individuals with the \"right to be forgotten\", the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. Machine unlearning has emerged as a promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations (Jang et al., 2023). These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies (Liu et al., 2024;Hase et al., 2024). \n\nGiven the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015;Bourtoule et al., 2021;Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020;Meng et al., 2022).",
            "score": 0.5801725905999892,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1560,
                    "end": 1579,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1737,
                    "end": 1755,
                    "matchedPaperCorpusId": "270764419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "\u2022 Gradient Ascent (Grad. Ascent) (Yao et al., 2023), which is one of the most straightforward unlearning methods, reduces the likelihood of original answers on the forget set S F . \u2022 Gradient Difference (Grad. Diff.) (Liu et al., 2022), which is a variant of Grad. Ascent, not only implements unlearning on the forget set S F but also learning on the retain set S R by gradient descent to minimize damage to the model. \u2022 KL Minimization (KL Min.) applies an additional Kullback-Leibler (KL) divergence regularization between the predictions on S R of the original model \u03b8 t and the unlearned model \u03b8 t+1 , while performing Grad. Ascent on S F . \u2022 Preference Optimization (Pref. Opt.) optimizes the model to realign the questions in the forget set S F with a refusal answer, such as \"I don't know,\" through the DPO algorithm (Rafailov et al., 2024) while learning on the retain set S R by gradient descent. \u2022 Negative Preference Optimization is an efficient unlearning method that requires only providing a negative response during preference optimization. We adopt the algorithm with the same restriction on the retain set S R as Gradient Difference (NPO-GD), which has been proven to outperform other variants (Zhang et al., 2024). \n\nEvaluation metrics. We assess the unlearned models using the following metrics (more details can be found in Appendix B.4): \n\n\u2022 ROUGE (Lin, 2004) measures the overlap cooccurrence of n-grams between the original answer and model's greed-decoding generation for the test QA pairs. \n\n\u2022 Probability computes the conditional probability with length normalization of QA pairs in the evaluation set. \u2022 Accuracy calculates the proportion of a paraphrased answer that the unlearned model can select from perturbed answers of the question. \u2022 Forget Quality assesses the unlearning effectiveness of the unlearned models via the Kolmogorov-Smirnov (KS) test.",
            "score": 0.5799448201514613,
            "section_title": "Experimental Setup",
            "char_start_offset": 10782,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1358
                },
                {
                    "start": 1361,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 235,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 824,
                    "end": 847,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1369,
                    "end": 1380,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291015625
        },
        {
            "corpus_id": "264172840",
            "title": "Large Language Model Unlearning",
            "text": "We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2% of its computational time.",
            "score": 0.579756572907972,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.254150390625
        },
        {
            "corpus_id": "270878324",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "text": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "score": 0.5797341926146862,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "This method uses Kullback-Leibler divergence and aims to obtain a model with maximum KL divergence between the predictions on D f of the corrupted model and the unlearned model (as it undergoes unlearning). The formal loss function for a sample x \u223c D f including T tokens can be written as \n\nNegative Preference Optimization. This method casts the unlearning problem into the preference optimization framework by treating each (x <i , x i ) where x \u2208 D f as only providing a negative response when x <i is prompted to the model. More formally, the loss function is (Zhang et al., 2024) \n\nwhere \u03b2 > 0 is the inverse temperature. \n\nTask Vector. This methods aims to derive a parameter-space vector aligned with the influence of the forget set documents. It subsequently updates the corrupted model's parameters by moving along the opposite direction of the vector. More formally, Let \u03b8 c be the corrupted model's parameters, task vector continues fine-tuning corrupted model on D f , and obtains the optimal parameters \u03b8 * . Then the unlearned model's parameters are obtained as \n\nwhere \u03b1 > 0 controls the step size.",
            "score": 0.5794291364179207,
            "section_title": "KL Divergence.",
            "char_start_offset": 17385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0753173828125
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Large foundation models of today are trained on massive amounts of uncurated data obtained from the internet. Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content. For large language models (LLM) in particular, the ability to unlearn specific facts becomes especially important to ensure compliance with copyright laws and privacy requirements (Jang et al., 2022;Liu et al., 2024) Prior work in unlearning in large language models have looked at this problem, but for the target fact, the focus has been on removing the target itself. However, this can be superficial -LLMs not only know single facts in isolation, but many connected facts -and very often, the fact that has been unlearnt can be deduced from facts that are already known by the model. Thus, successful unlearning in this setting should also remove other facts that imply the fact to be unlearnt. As a concrete example, consider Figure 1. Here, the target fact \"Camila Flores's child is Wyatt Ross\" can be deduced from fact A \"Wyatt Ross's father is Xavier Ross\" and fact B \"Camila Flores's husband is Xavier Ross\". If the LLM only unlearns the target fact but retains A and B, this is insufficient as an adversary who extracts A and B from the LLM can deduce the target fact. \n\nIn this work, we consider a new setting for unlearning, which we call deep unlearning, and investigate to what extent current unlearning methods succeed at this setting. Deep unlearning is formulated by stating a set of facts and logical rules that connect the facts. The fact is deeply unlearnt if the target fact cannot be deduced from the retained facts in the LLM through the given logical rules.",
            "score": 0.57935098207977,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "The past a few years have witnessed the great success of large language models, demonstrating powerful capabilities in comprehending textual data and generating human-like languages. Large language models achieve success by being trained on vast amounts of textual data, including online sources with copyrighted content and user-generated knowledge. However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model. Comprehensive experiments on real-world datasets demonstrate the effectiveness and efficiency of DeepCUT with consistent and significant improvement over baseline methods.",
            "score": 0.5792607556561538,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428466796875
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "It focuses on studying various unlearning methods for models to forget, aiming to enhance the model's security (unlearning toxic data), privacy (unlearning private data), and impartiality (unlearning biased data) [2]. Traditional approaches in machine unlearning can be broadly categorized into two types [7][8]: 1) designing new unlearning algorithms to isolate target data points during training and then retraining the model based on the unlearning algorithm, such as differential privacy (DP) methods [9] [10]. 2) Approximate unlearning, which involves making limited parameter updates to machine learning models to minimize the additional impact of forgetting target data points, reducing it to an acceptable level while simultaneously constraining other model behaviors from undergoing significant changes [11]. \n\nHowever, in the era of LLMs, traditional machine unlearning methods may not necessarily be applicable to LLMs. The potential reasons for this are as follows: 1) The parameter scale of LLMs is extremely large, leading to a high cost of model retraining, especially in the case of frequent requests for continuous unlearning, which is impractical in reality. 2) LLMs are knowledge-intensive and typically used for openended question answering or inference tasks. These tasks are often modeled as generative tasks in the form of (prompt, output). In contrast, previous natural language processing models in machine learning were primarily used for language understanding tasks, with classification tasks like text classification, sentiment analysis, and natural language inference being more common. Unlearning methods designed for these classification tasks are not applicable to generative tasks. 3) Commercialized LLMs generally only provide API access and do not offer a way to access their parameters. These factors have impacted the development of forgetting mechanisms in the era of LLMs, leading to the emergence of LLM knowledge unlearning tailored for these large generative models. Knowledge unlearning process of LLMs is illustrated in Figure 1. \n\nIn current scenario where resources for training and maintaining LLMs are highly constrained, knowledge unlearning for LLMs proves to be exceptionally practical. It stands as a necessary approach for developing responsible, legally compliant, and user-trusted LLMs.",
            "score": 0.5786815292201899,
            "section_title": "Introduction",
            "char_start_offset": 3680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2238
                },
                {
                    "start": 2239,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 216,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 305,
                    "end": 308,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "267657624",
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "text": "This position paper rethinks the paradigm of unlearning for modern LLMs to uncover its under-explored aspects. To achieve this, we dissect LLM unlearning into four essential aspects: formulation, methodologies, evaluation metrics, and applications. We show that there are considerable challenges in both foundational research and practical, use case-driven research. These include: (Generality) A desired solution for LLM unlearning should take into account the generality of the unlearning target and dataset choice, accommodate various model setups including both white-box and black-box scenarios, and consider the specifics of the unlearning method. (Authenticity) LLM unlearning should focus on effectively removing both data influence and specific model capabilities, in order to authenticate unlearning across a range of evaluation methods, particularly in adversarial contexts. (Precision) LLM unlearning should precisely define the scope of unlearning, while ensuring the preservation of general language modeling performance outside this unlearning scope. \n\nBy examining the current state of the art, we gain insights for the future development of LLM unlearning. For example, localization-informed unlearning shows promise with possible dual advantages of efficiency and efficacy. Effective unlearning requires careful consideration of data-model influences and adversaries. Despite similarities between LLM unlearning and model editing, they differ in their formulation and methodological design. Furthermore, insights gained from the study of LLM unlearning could catalyze technological advancements in other types of foundation models, e.g., large vision-language models.",
            "score": 0.5786487017277127,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 37022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1685
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65087890625
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "We have deeply investigated the reasons why finetuning-based unlearning methods seemingly succeeded in behavior-based testing for large language model unlearning: Through activation patching and parameter restoration experiments, we find that these methods alter the way knowledge is extracted by changing MLP activations or model's attention, ultimately affecting the output. This is evidenced by the fact that the model's output regarding the target knowledge is largely restored after patching the activations and the attention components' states. Furthermore, we conduct experiments on the pretraining datasets of two models, to test the models' capabilities after unlearning, verifying that in addition to unlearning the corresponding knowledge, fine-tuning-based methods that by altering the way the model accesses knowledge, will significantly impair the model's other unrelated capabilities, causing a certain degree of capability degradation.",
            "score": 0.5784340957618286,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 16100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 951
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.697265625
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
            "score": 0.5784052859233151,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "259687658",
            "title": "Toward Highly-Efficient and Accurate Services QoS Prediction via Machine Unlearning",
            "text": "(1) Model-agnostic methods include the unlearning process and frameworks applicable to different models, which can achieve practical positive results on different complex models. Gupta et al. [20] proposed a private unlearning mechanism for adaptive streaming data removal requests. Guo et al. [21], Golatkar et al. [22], Neel et al. [23] and Ullah et al. [24] provided different further extensions to support convex losses and provide theoretical guarantees on accuracy and unlearning times. However, these methods may require more computational resources and time as they need to be optimized independently of specific models. (2) Model-intrinsic methods are designed for specific models. Kim et al. [25] designed a softmax-based unlearning classifier to achieve class removal. Combining gradient methods with synthetic data, Izzo et al. [26] proposed an unlearning method for linear and logistic models. Using the criterion of information gain, Schelter et al. [27] proposed an unlearning solution that evaluates the split decisions' robustness in extremely randomized trees. Even in Bayesian models, Nguyen et al. [28] proved that minimizing the KL divergence between the retrained model's parameter distribution and the original model's parameter distribution can prevent catastrophic unlearning. Moreover, some recent works have focused on training deep neural network-based models using data that is known a priori distribution to be permanent. Then the model is fine-tuned on user data using some convex optimization [29]. However, these methods may be limited by specific models and difficult to extend to other models. \n\n(3) Data-driven methods use data partitioning mechanisms to accelerate the cost of retraining. The SISA framework, as described in [4], partitions data into shards and slices. The main advantage of data-driven approaches is their efficiency and scalability. By partitioning the data, the cost of retraining can be significantly reduced. This makes these approaches more suitable for handling large datasets and complex models.",
            "score": 0.5780449515638348,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 11011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1628
                },
                {
                    "start": 1631,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "220364296"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "232068763"
                },
                {
                    "start": 702,
                    "end": 706,
                    "matchedPaperCorpusId": "56895575"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 964,
                    "end": 968,
                    "matchedPaperCorpusId": "235474438"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1343994140625
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "As shown in Fig. 2, we first construct a vanilla model by training it on datasets that include knowledge to be unlearned or retained in privacy and copyright domains. In the first step, forward and backward passes are performed on the vanilla model using each dataset to compute the gradient information (\u2207 gradient values). Based on this gradient information, we perform parameter-wise localization to identify parameters that are highly associated with each dataset. To be exact, we localize the top k% of parameters based on their gradient magnitudes. Since the number and magnitude of parameters vary across different layers of the model, this approach is reasonable for adaptive application. Subsequently, we identify domain-agnostic parameters that significantly influence both unlearning and retention. We further localize parameters deemed critical for retention across both domains, highlighting their shared importance. In the second step, the localized parameter information derived from the first step is utilized to freeze specific parameters prior to unlearning. The unlearning operation is then adaptively adjusted at a parameter-specific level in each layer. When a parameter is strongly associated with both unlearning and retention knowledge across the two domains, we make sure to minimize conflict between unlearning and retention performance. Also, when a parameter is critical for retention knowledge in both domains, it is protected to ensure preservation. Through this adjustment process, our approach achieves effective multi-domain unlearning.",
            "score": 0.5773359530894305,
            "section_title": "A. Framework Overview",
            "char_start_offset": 9845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28271484375
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "Unlearning has emerged as a critical technique in AI systems, enabling the selective removal of sensitive data, including copyrighted material and personal information, from trained models. As the International AI Safety Report (Bengio et al., 2025) emphasizes, unlearning plays a vital role in mitigating privacy and copyright risks associated with extensive training datasets. However, it also acknowledges that current unlearning methods remain in-adequate, which often fail to completely erase targeted data while potentially degrading model performance, thus limiting practical implementation. \n\nSpecifically, existing unlearning methods often struggle with over-forgetting (excessive elimination of non-sensitive information) or underforgetting (incomplete removal of sensitive data). It is challenging to find optimal hyperparameters that balance performance across multiple evaluation dimensions, sometimes even impossible. To address these limitations, we propose a novel unlearning system that leverages model merging to combine an over-forgetting model with an under-forgetting model, creating a more effective unlearned model. It can produce superior results simply by merging two models with complementary biases. \n\nOur system achieved second place in SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models, with our 7B model attaining a Task Aggregate Score of 0.944 and Aggregate Score of 0.487, demonstrating the effectiveness of our system in selectively removing sensitive content. Furthermore, our local experiments yielded almost perfect results with a MIA Score of 0.501 and Aggregate Score of 0.806, while maintaining an exceptionally high Task Aggregate and comparable MMLU Avg.. We provide comprehensive analyses that validate our system's effectiveness and offer deeper insights into the unlearning process.",
            "score": 0.57586060493697,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023;Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2020;Bourtoule et al., 2021), and mitigating biases to promote fairness (Chen et al., 2023;Li et al., 2023b). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023a), and graph learning (Chen et al., 2022). \n\nThere is a pressing need for unlearning methods specifically tailored to language models, referred to as knowledge unlearning (Jang et al., 2023). This need arises because language models acquire knowledge from open-source text data, which inherently contains sensitive information, including toxic and private content. However, applying existing unlearning methods directly to language models poses significant challenges. Firstly, the retraining overhead of language models is exceptionally high, making it computationally prohibitive for regular users, even when only retraining a subcomponent. Secondly, language models have an enormous parameter size, rendering certain memory or influence estimation approaches inaccurate and even intractable. \n\nAn alternative approach to removing undesirable knowledge from language models is Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020), but it is not well-suited for this purpose. RLHF involves fine-tuning the model to align with human preferences, which requires a significant amount of preference-aligned text data, e.g., GPT 4 (Achiam et al., 2023). However, obtaining a large volume of high-quality resources is challenging and may require considerable effort.",
            "score": 0.5754098050537642,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "259991722"
                },
                {
                    "start": 291,
                    "end": 313,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 408,
                    "end": 426,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 426,
                    "end": 449,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 512,
                    "end": 529,
                    "matchedPaperCorpusId": "263830799"
                },
                {
                    "start": 679,
                    "end": 697,
                    "matchedPaperCorpusId": "260927489"
                },
                {
                    "start": 719,
                    "end": 737,
                    "matchedPaperCorpusId": "268030788"
                },
                {
                    "start": 758,
                    "end": 777,
                    "matchedPaperCorpusId": "232404451"
                },
                {
                    "start": 907,
                    "end": 926,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1665,
                    "end": 1688,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2481689453125
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Large language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.",
            "score": 0.5751248303558548,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435546875
        },
        {
            "corpus_id": "270559803",
            "title": "Avoiding Copyright Infringement via Large Language Model Unlearning",
            "text": "An effective unlearning algorithm should be stable, meaning it should ensure unlearning efficacy while preserving nontargeted knowledge, books that are not subject to unlearning, and general-purpose language abilities. \n\nThe core of many previous LLM unlearning methods have focused on Gradient Ascent (GA) and further combined it with an in-distribution retained dataset to preserve general-purpose language abilities, known as Gradient Difference (Maini et al., 2024;Zhao et al., 2024;Liu et al., 2024d;Yao et al., 2024). However, Gradient Difference requires collection of a substantial amount of in-distribution retained data to maintain generalpurpose abilities. Moreover, GA-based methods risk catastrophic collapse, where the model's general-purpose language abilities degrade significantly (Liu et al., 2024d). Zhang et al. (2024b) proposed Negative Preference Optimization (NPO), framing unlearning as preference optimization. However, NPO relies on a reference model, and if the reference model contains copyrighted information, unlearning efficacy is compromised. \n\nTo address these challenges in sequentially unlearning copyrighted books, we propose Stable Sequential Unlearning (SSU), that achieves a better trade-off between effective unlearning and maintaining general-purpose language abilities in sequential settings. Specifically, SSU first fine-tunes the model on the copyrighted books, followed by fine-tuning with random labels. During gradient updates, SSU applies targeted weight adjustments through weight saliency. Afterwards, it extracts task vectors (Ilharco et al., 2022) corresponding to the copyrighted books and subsequently negates these task vectors to achieve unlearning. Unlike Gradient Difference methods, SSU does not require additional retained data collection to maintain its performance on other tasks, thereby avoiding the complexity and overhead associated. \n\nOur experiments on the Llama-3.1-8B-Instruct",
            "score": 0.5748845976269339,
            "section_title": "Introduction",
            "char_start_offset": 1877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1899
                },
                {
                    "start": 1902,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "264590738"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.291259765625
        },
        {
            "corpus_id": "246015506",
            "title": "Zero-Shot Machine Unlearning",
            "text": "The concept of unlearning was introduced in [13] to eliminate the effect of data point(s) on the already trained model. Machine unlearning can be broadly grouped into two categories: exact unlearning and approximate unlearning methods. In exact unlearning, the impact of the data point (to forget) is removed from the model by retraining the model from scratch by excluding that data from the training set. This is clearly a computationally expensive process as the deep learning models are trained with large datasets. Moreover the request for data deletion is a recurrent event rather than a one-time occurrence. Several research efforts have focused on finding efficient ways of retraining. Reference [6] proposed a SISA approach to train the model by partitioning the dataset into a set of nonoverlapping shards. This reduces the need for full retraining as the model can be retrained on one of the shards. The approximate unlearning methods aim to approximate the parameters that would have been obtained if the model was trained without using the data to be unlearned. Usually the approach is to bring the parameters closer to a model (trained from scratch) that has never seen the unlearning data through a lower number of updates in comparison to exact unlearning methods. Reference [14] store the updates made by each data point in the parameter space while training. During unlearning, the corresponding updates are subtracted from the final parameters of the model. Reference [15] proposed random forests that support data forgetting. References [16] and [17] studied data deletion in k-means clustering algorithms. Similarly, unlearning for Bayesian methods are presented in [18]. Reference [12] give a certified information removal framework based on Newton's update removal mechanism for convex learning problems. Reference [19] studied several unlearning methods for linear models and analyzed the efficiency, effectiveness and certifiability trade-offs among them. Similarly, [20] analyze the difference between machine unlearning and differential privacy. All these methods are designed for convex problems, whereas we aim to present an unlearning solution for deep learning models.",
            "score": 0.574171230168676,
            "section_title": "A. Machine Unlearning",
            "char_start_offset": 8056,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "235422138"
                },
                {
                    "start": 1557,
                    "end": 1561,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "2224347"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "225067129"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 1992,
                    "end": 1996,
                    "matchedPaperCorpusId": "232134970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "260887697",
            "title": "Machine Unlearning: Solutions and Challenges",
            "text": "Removing an individual point can disrupt the learned patterns and dependencies, potentially leading to a significant decrease in performance [16], [26], [34]. \u2022 Model Complexity: Large machine learning models such as deep neural networks can have millions of parameters. Their intricate architectures and nonlinear interactions between components make it hard to interpret the model and locate the specific parameters most relevant to a given data point [16], [35]. The lack of transparency into how data influences predictions poses challenges for removing dependencies. \u2022 Computational Cost: Most machine unlearning techniques require iterative optimization methods such as gradient descent to adjust parameters after removing data. This incurs a significant computational cost, which grows rapidly as the model and dataset size increase [16], [36]. The computational demands may exceed the available resources when dealing with large-scale datasets and complex models. \u2022 Privacy Leaks: The unlearning process itself can leak information in multiple ways [37]. For instance, statistics, such as the time taken to remove a point, can reveal information about it [26], [37]. Additionally, alterations in accuracy and outputs can allow adversaries to infer the characteristics of removed data [26], [38]. \u2022 Dynamic Environments: Tracing each data point's influence becomes increasingly difficult as the dataset changes dynamically [11], [32]. Unlearning can also introduce delays that impede prompt model updates needed for lowlatency predictions. 4) Evaluation Metrics for Machine Unlearning Solutions: a) Data Erasure Completeness: This metric evaluates how thoroughly the unlearning algorithm makes the model remove the target data. It compares the model's predictions or parameters before and after unlearning to quantify the extent of removing. Various distance or divergence measures can be used to quantify the difference between the two models [39]. Representative measures include L 2 distance [40] and Kullback-Leibler (KL) divergence [41]. \n\nb) Unlearning Time Efficiency: This metric can be assessed by comparing the duration required for naive retraining of the model with the time it takes to perform the unlearning process [42].",
            "score": 0.5737696031277846,
            "section_title": "B. Machine Unlearning",
            "char_start_offset": 9771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2049
                },
                {
                    "start": 2052,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "235474438"
                },
                {
                    "start": 454,
                    "end": 458,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 460,
                    "end": 464,
                    "matchedPaperCorpusId": "260082364"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 846,
                    "end": 850,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1057,
                    "end": 1061,
                    "matchedPaperCorpusId": "249890361"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 1169,
                    "end": 1173,
                    "matchedPaperCorpusId": "249890361"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 1298,
                    "end": 1302,
                    "matchedPaperCorpusId": "250627348"
                },
                {
                    "start": 1430,
                    "end": 1434,
                    "matchedPaperCorpusId": "202661124"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "2224347"
                },
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "251647322"
                },
                {
                    "start": 2002,
                    "end": 2006,
                    "matchedPaperCorpusId": "220128049"
                },
                {
                    "start": 2044,
                    "end": 2048,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181396484375
        },
        {
            "corpus_id": "270440348",
            "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
            "text": "As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents, and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives - maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality. Our code will be publicly available at https://github.com/UCSB-NLP-Chang/ULD.",
            "score": 0.573439516379967,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336669921875
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "Unfortunately, there is a lack of effective fine-tuning techniques for LLM unlearning.For example, classical gradient ascent-based fine-tuning techniques are susceptible to over-forgetting, which can hamper the original model utility (Yao et al., 2023;Maini et al., 2024;Zhang et al., 2024).Conversely, less aggressive fine-tuning techniques, such as fine-tuning solely on the retain set (i.e., the data set irrelevant to the forgetting data points) (Yao et al., 2023), could result in under-forgetting, failing to completely erase the influence of forgotten data.As a result, it is hard to strike the optimal balance between unlearning effectiveness and model utility preservation.\n\nSeveral recent efforts have been made to develop improved model fine-tuning techniques for LLM unlearning.For example, studies have delved into designing fine-tuning loss functions tailored for LLM unlearning (Yao et al., 2023;Eldan and Russinovich, 2023;Zhang et al., 2024).A currently popular choice is the regularized optimization objective that integrates unlearning efficacy loss with model utility loss, as seen in approaches such as the gradient difference (GradDiff) (Liu et al., 2022;Yao et al., 2023;Maini et al., 2024), preference optimization (PO) (Eldan and Russinovich, 2023;Maini et al., 2024) and negative preference optimization (NPO) (Zhang et al., 2024).Additionally, other LLM unlearning techniques incorporate the model's prior into fine-tuning.For instance, finetuning is selectively applied to a subset of model units deemed essential for the unlearning task (Yu et al., 2023;Wu et al., 2023).This approach has led to the emergence of localization-informed LLM unlearning (Liu et al., 2024a).",
            "score": 0.5732657296135861,
            "section_title": "Introduction",
            "char_start_offset": 1689,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 291
                },
                {
                    "start": 291,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 682
                },
                {
                    "start": 684,
                    "end": 790
                },
                {
                    "start": 790,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1357
                },
                {
                    "start": 1357,
                    "end": 1450
                },
                {
                    "start": 1450,
                    "end": 1600
                },
                {
                    "start": 1600,
                    "end": 1699
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 252,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 450,
                    "end": 468,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 893,
                    "end": 911,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 1159,
                    "end": 1177,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 1177,
                    "end": 1194,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 1566,
                    "end": 1583,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287109375
        },
        {
            "corpus_id": "273162487",
            "title": "Position: LLM Unlearning Benchmarks are Weak Measures of Progress",
            "text": "Unlearning methods have the potential to improve the privacy and safety of large language models (LLMs) by removing sensitive or harmful information post hoc. The LLM unlearning research community has increasingly turned toward empirical benchmarks to assess the effectiveness of such methods. In this paper, we find that existing benchmarks provide an overly optimistic and potentially misleading view on the effectiveness of candidate unlearning methods. By introducing simple, benign modifications to a number of popular benchmarks, we expose instances where supposedly unlearned information remains accessible, or where the unlearning process has degraded the model's performance on retained information to a much greater extent than indicated by the original benchmark. We identify that existing benchmarks are particularly vulnerable to modifications that introduce even loose dependencies between the forget and retain information. Further, we show that ambiguity in unlearning targets in existing benchmarks can easily lead to the design of methods that overfit to the given test queries. Based on our findings, we urge the community to be cautious when interpreting benchmark results as reliable measures of progress, and we provide several recommendations to guide future LLM unlearning research.",
            "score": 0.5730715243181835,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "252089272",
            "title": "A Survey of Machine Unlearning",
            "text": "Unlearning in large language models (LLMs) has become essential due to increasing privacy concerns and the need to comply with regulations like GDPR [115] and CCPA [134]. As LLMs are trained on vast datasets that may include sensitive information, there is a risk of these models inadvertently memorising and reproducing private data [83,193,218]. Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem [86] propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. [73] propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. [112] presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality.",
            "score": 0.5730197083983507,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 100017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 154,
                    "matchedPaperCorpusId": "56699980"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "266174259"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "260925619"
                },
                {
                    "start": 1980,
                    "end": 1985,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20947265625
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge. \n\nThis paper presents the UNLEARN algorithm and demonstrates its performance in removing knowledge represented as tasks. Section 2 reviews the literature on Parameter Efficient Fine-Tuning, Machine Unlearning, and LLM Unlearning. Section 3 describes the three main parts of UNLEARN: subspace identification, subspace discrimination, and task removal. In Section 4, the performance of UNLEARN is tested over a large set of metrics and settings and compared to the current state-ofthe-art. Section 4.5 introduces LEARN, a dual application of the UNLEARN algorithm for adding knowledge to the LLM. A comparison to traditional fine-tuning methods is made in Section 5. Future works are discussed in Section 6. Finally, Section 7 concludes the paper and outlines potential directions for future research. \n\n2 Related Works",
            "score": 0.5725833178500641,
            "section_title": "Introduction",
            "char_start_offset": 2331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 977
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450439453125
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "This work explores the use of targeted unlearning in LLMs where we have experimented with several unlearning methods with different configurational settings to make the target models forget the requested dataset, and preserve the specified retain set, also, preserving its overall multifaceted capabilities. From our experiments, for 7B model: Gradient difference followed by Gradient ascent worked well with appropriate parameters tuning; and for 1B model: Gradient descent alone on the retain set worked well. Xavier initialization on the 1B model has got near equivalent score on all the metrics as the former. Followed by similar performance between Controlled gradient ascent, and Gradient difference, with respective appropriate parameters tuning. This work reemphasizes the fact that selective unlearning comes with the delicate problem of optimizing effective unlearning with knowledge retention of the remaining data and model's integrity, utility for downstream tasks. Further, it demonstrates that performance of a method significantly depends on the scale of the target model, and the kind of data it is presented with. \n\nA Other experiments conducted",
            "score": 0.5723930067321981,
            "section_title": "Conclusion",
            "char_start_offset": 13622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.247802734375
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
            "score": 0.5723412655261841,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "Machine unlearning aims to solve the problem of removing the influence of selected training examples from a learned model. Despite the increasing attention to this problem, it remains an open research question how to evaluate unlearning in large language models (LLMs), and what are the critical properties of the data to be unlearned that affect the quality and efficiency of unlearning. This work formalizes a metric to evaluate unlearning quality in generative models, and uses it to assess the trade-offs between unlearning quality and performance. We demonstrate that unlearning out-of-distribution examples requires more unlearning steps but overall presents a better trade-off overall. For in-distribution examples, however, we observe a rapid decay in performance as unlearning progresses. We further evaluate how example's memorization and difficulty affect unlearning under a classical gradient ascent-based approach.",
            "score": 0.5722811849325894,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.578125
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Privacy regulations such as the Right to be Forgotten (RTBF) (Rosen, 2011), the European Union's General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019), and the United States' California Consumer Privacy Act (CCPA) (Pardau, 2018) mandate that individuals have the right to request the deletion of their data from databases, which extends to data held within machine learning (ML) models. Additionally, the Writers Guild of America strike in 2023 highlighted increasing concerns regarding the copyrighted content generated by large language models (LLMs) (WGA, 2023). Consequently, removing such information in just one language does not entirely eradicate it from the model. This underscores the necessity for a multilingual unlearning approach to ensure the information is thoroughly eliminated from the model. \n\nTo comply with such issues, significant attention has been directed towards the task of machine unlearning (MU), which involves removing the influence of specific data points from ML models (Cao and Yang, 2015). Despite the critical necessity of the task, mitigating the influence of data samples on billions of model parameters presents an immense challenge. The most definitive method is exact unlearning, which necessitates retraining ML models entirely from scratch, utilizing the residual training dataset after excising the specified data points. However, this method is computationally prohibitive and not feasible, particularly for LLMs. Therefore, the advancement of rapid approximate unlearning methodologies has emerged as a primary focus of contemporary research efforts. \n\nResearch on MU has predominantly focused on with English data only. The plot illustrates that MA does not significantly drop across other languages, highlighting the necessity for a multilingual unlearning approach to effectively reduce memorization across all languages. computer vision tasks (Bourtoule et al., 2021;Golatkar et al., 2020a,b;Chundawat et al., 2023;Kurmanji et al., 2023); however, it is now gaining traction in NLP due to the safety issues that arise with LLMs (Zhang et al., 2023).",
            "score": 0.571978696932425,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 74,
                    "matchedPaperCorpusId": "9355614"
                },
                {
                    "start": 1016,
                    "end": 1036,
                    "matchedPaperCorpusId": "5945696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "Existing studies on LLM unlearning [16]- [19] mainly focus on analyzing and mitigating the impact of those forgot samples upon a model's outputs (e.g., the probability distribution of the classification layer). For example, Wang et al [18] measures the knowledge gap between the original model and the unlearned model using the KL-divergence between their output probability distributions. These works do not explicitly consider the distributions of sample in the latent space of the model, e.g., the distance between forgot samples and those remaining samples in the latent space. Thus, it cannot directly optimize the geometric distributions of the samples in the model. \n\nTo address this issue, we propose simple yet effective unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models, which removes the memory of a model from its deep latent space. Our proposed framework draws inspiration from the recent success of contrastive learning, a technique that has shown significant promise in various machine learning tasks. The core idea behind our framework is to modify the latent space of the model in a way that facilitates the forgetting of specific samples without disrupting the overall performance of the model. Given a sample that needs to be forgotten, referred to as the anchor sample, DeepCUT pushes the anchor sample away from other samples within the same class while simultaneously pulling it closer to samples in different classes. This approach ensures that the model unlearns the unique features of the anchor sample that made it distinguishable within its class and thus, effectively remove the most discriminative features that the model has memorized for the anchor sample. At the same time, DeepCUT is designed to ensure that the latent representations of other samples remain largely unaffected by the unlearning process. This selective adjustment helps maintain the accuracy of the model for the remaining data, preventing degradation in the model's overall performance. \n\nThe contributions of this paper can be summarized as follows: \n\n1) We introduce an effective unlearning framework, aimed at addressing the challenge of selectively forgetting specific samples in a trained model. 2) Inspired by contrastive learning, our method focuses on optimizing the sample distributions within the latent space of a language model to achieve unlearning.",
            "score": 0.5718578305619805,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2411
                }
            ],
            "ref_mentions": [
                {
                    "start": 235,
                    "end": 239,
                    "matchedPaperCorpusId": "258615571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31103515625
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
            "score": 0.5718470317373148,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390380859375
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge. \n\nIn summary, common loss adjustment methods employ one [24], two [23,20,22], or all three [5] of these components to guide the model towards forgetting specific data while minimizing the impact on its overall performance and utility. The interplay between these terms allows for controlled and targeted unlearning, ensuring the model retains its valuable capabilities while selectively forgetting undesired information. More detailed formulations of these loss adjustmentbased methods, along with related work, are deferred to Appendix C.1 and Appendix E. \n\nAn Example: Large Language Model Unlearning (LLMU). We adopt a popular approach in LLM unlearning, LLMU [5], to interpret a special case of Eqn. (1).",
            "score": 0.5717930395818336,
            "section_title": "Existing LLM Unlearning Paradigm",
            "char_start_offset": 5849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 2033
                },
                {
                    "start": 2036,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 1543,
                    "end": 1547,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3818359375
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Among the advanced unlearning methods, gradient ascent (GA) (Yao et al., 2023) and negative preference optimization (NPO) (Zhang et al., 2024) are the most foundational. GA aims to minimize the likelihood of making correct predictions on a forget dataset by applying gradient ascent to the cross-entropy loss. On the other hand, NPO treats the forget set as negative preference data, adapting the offline DPO (Rafailov et al., 2024) objective to adjust the model to assign a lower likelihood to the forget set. Since GA and NPO are not designed for utility preservation, several regularization techniques (Shi et al., 2024b;Maini et al., 2024) are typically combined with unlearning to preserve utility. For example, given a retain dataset, techniques such as gradient descent on the retain dataset (Zhang et al., 2024;Maini et al., 2024) and minimizing the KL divergence between the unlearned model's and the target model's probability distributions on inputs from the retain dataset (Zhang et al., 2024;Maini et al., 2024) are introduced to enhance the utility of the unlearned model. \n\nDespite their superior unlearning performance, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, that current unlearning benchmarks fail to detect. In this paper, we discover that given an unlearned model using existing representative unlearning methods, simply applying quantization can partially or even significantly recover the forgotten knowledge. Specifically, as shown in Figure 1, given a target model and a forget dataset, we apply unlearning methods to the model to remove knowledge from the forget dataset, resulting in an unlearned model. During testing, the unlearned model demonstrates superior unlearning performance in full precision. However, when we simply apply quantization to the unlearned model, the unlearning performance is compromised.",
            "score": 0.5713638044654463,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 1086
                },
                {
                    "start": 1089,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 78,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 409,
                    "end": 432,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 624,
                    "end": 643,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 819,
                    "end": 838,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 1005,
                    "end": 1024,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1048583984375
        },
        {
            "corpus_id": "273323555",
            "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
            "text": "The main problem with this approach is that it produces a metric that is only meaningful when compared to the learning time for a model that was never trained on the knowledge in the first place, which is hard to obtain given the cost of LLM pretraining. This method also suffers against methods that specifically target making fine-tuning slower or more difficult (Henderson et al., 2023;Rosati et al., 2024;Tamirisa et al., 2024). \n\nWeaknesses of current unlearning techniques Previous work has shown evidence about current unlearning techniques being weak against attacks that would fail if the information was removed (Lynch et al., 2024a;\u0141ucki et al., 2024;Hong et al., 2024). Our results further confirm the findings of this previous work and provides a standard method that can be applied to any unlearned model (a model to which unlearning was applied). Further, negative results using our method provide strong evidence of unsuccessful knowledge removal relative to previous prompting-based methods. \n\n3 PROBLEM STATEMENT",
            "score": 0.5710117977997988,
            "section_title": "RELATED WORK",
            "char_start_offset": 6407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1929931640625
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "computer vision tasks (Bourtoule et al., 2021;Golatkar et al., 2020a,b;Chundawat et al., 2023;Kurmanji et al., 2023); however, it is now gaining traction in NLP due to the safety issues that arise with LLMs (Zhang et al., 2023). Notably, Jang et al. (2023) first proposed an unlearning technique of reversing the gradient to refrain LMs from generating particular sensitive token sequences. On the other hand, Wang et al. (2023) presented an approach to maintaining distribution differences (i.e., knowledge gap) such that the performance of the data to be forgotten becomes similar to the performance of the unseen data. Besides the two approaches, substantial progress has been made in unlearning for monolingual models; nevertheless, there is a lack of empirical results and analyses of unlearning for multilingual LMs. As shown in Figures 1 and  2, our preliminary experiments find that existing unlearning approaches do not exhibit cross-lingual transferability. In other words, unlearning in one language does not automatically transfer to other languages, leaving LMs vulnerable to possible lowresource language attacks, which have been shown to jailbreak GPT-4 (Yong et al., 2023). \n\nTo this end, we introduce multilingual unlearning, which effectively removes specific information across a wide variety of languages from pretrained language models.2 Due to the inconsistency in model performance across languages, we leverage a multilingual teacher model in which the student model adaptively obeys the teacher based on its capabilities in a particular language. For example, a high knowledge distillation weight is applied when the teacher has strong expertise, ensuring the benefit of effective teaching. Conversely, a low weight is used when the teacher's knowledge is limited, allowing the student to learn independently. Our method is also as time-efficient as unlearning a single language, offering a significant improvement over unlearning languages one at a time, making it more practical for real-world applications. \n\nTo assess the success of unlearning across different languages, our experimental setup necessitates multilingual parallel data.",
            "score": 0.5709905892149372,
            "section_title": "Introduction",
            "char_start_offset": 1899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2034
                },
                {
                    "start": 2037,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 46,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 71,
                    "end": 94,
                    "matchedPaperCorpusId": "248834527"
                },
                {
                    "start": 410,
                    "end": 428,
                    "matchedPaperCorpusId": "258615571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.185302734375
        },
        {
            "corpus_id": "277633839",
            "title": "Sharpness-Aware Parameter Selection for Machine Unlearning",
            "text": "Unlearning this sensitive information requires removing the sensitive information from the group of emails (instead of removing all affected samples from the train set). \n\nThe unlearning scenarios discussed above have been studied in the context of sample/feature/label unlearning (Cao & Yang, 2015;Bourtoule et al., 2021;Guo et al., 2023;Ginart et al., 2019;Neel et al., 2020;Aldaghri et al., 2021;Warnecke et al., 2023). Most existing methods suffer from high computational complexity or low unlearning performance, due to their approach of updating all model parameters for unlearning (or a naively chosen subset, e.g., the parameters of the last layer of the model), which is expensive and/or inefficient (Warnecke et al., 2023). Accordingly, an efficient parameter selection strategy is crucial for performing the unlearning task at various levels of the training data. \n\nIn this work, we show that a subset of model parameters have the largest contribution when performing the unlearning task and propose a sharpness-aware (SA) parameter selection strategy for machine unlearning. More specifically, by leveraging second order information about the training loss landscape at the learned model parameters, we find those parameters that are located in a wide area of the loss landscape and update them during unlearning process. By only updating these salient parameters we achieve better unlearn efficacy and better efficiency. We provide justifications for the proposed strategy by connecting it to robust unlearning as well as an approximation of an existing unlearning algorithm designed based on influence functions (Warnecke et al., 2023;Hampel, 1974). \n\nWe summarize our contributions as follows: \n\n\u2022 We propose a sharpness-aware parameter selection strategy to find the most important model parameters to update during the unlearning procedure. \n\n\u2022 We provide theoretical justifications for the proposed strategy by connecting it to sharpnessaware minimization and robust unlearning. \n\n\u2022 We show the effectiveness of the proposed strategy in improving the efficiency and efficacy of unlearning with a low computational cost.",
            "score": 0.5709342015055573,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1857
                },
                {
                    "start": 1860,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 299,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1649,
                    "end": 1662,
                    "matchedPaperCorpusId": "119361583"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "We now compare the results of our method UN-DIAL against all the representative baseline approaches such as Gradient Ascent (GA), Negative 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0. Unlearning versus Model Usefulness. Figure 3 illustrates the trade-off between the memorization metric, Extraction Likelihood (EL), and the model usefulness metric, MAUVE. As we adjust the unlearning intensity for each method, a Pareto Frontier naturally emerges, with the ideal point located in the upper-right quadrant. Our method excels here, achieving state-of-the-art language performance while maintaining high unlearning accuracy. The NPO method, while capable of comparable performance with careful tuning, quickly loses robustness as parameter settings change, demonstrating its sensitivity and lack of stability. \n\nFull NLU & NLG Evaluation. Table 1 presents results for QA-style NLU benchmarks and NLG tasks. Notably, NLG tasks are much more sensitive to unlearning, while NLU scores remain stable, within a 5% margin from the GPT-Neo baseline. \n\nIn contrast, NLG metrics show significant performance drops for several unlearning methods. Focusing on rows with similar EL values around 0.1 (indicating a 50% reduction in memorization), we observe that methods like GA, TA, DP, and CD degrade NLG performance significantly, as reflected in sharply lower MAUVE scores. Methods relying on auxiliary models (TA, DP, CD) perform  worse on NLG tasks, showing a greater trade-off between memorization and usefulness. In contrast, NPO and our method UNDIAL maintain high MAUVE scores and experience less degradation in PPL and Rep 3 metrics. \n\nWhen reducing memorization further (EL < 0.05), NPO also sees a sharp decline in generation quality, with MAUVE scores falling below 0.03. We show examples in Appendix A.5, where its outputs include repetitive or unnatural sentences. However, UNDIAL continues to generate highquality outputs, even at these low memorization levels, highlighting its ability to balance unlearning and language generation quality.",
            "score": 0.5707948766761841,
            "section_title": "Results and Discussion",
            "char_start_offset": 16607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2028
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.336181640625
        },
        {
            "corpus_id": "267365484",
            "title": "Dataset Condensation Driven Machine Unlearning",
            "text": "Despite the popularity of approximate machine unlearning algorithms, they suffer from high margin between efficiency (the amount of time for unlearning algorithm completion), privacy (protection against adversary to infer the forgotten data from unlearned model) and utility (preservation of performance of unlearned model on retain dataset). One important illustration of this challenge is the work done in [8], where a close form expression for difference between original and unlearning parameters is derived, assuming the distance between them is sufficiently small, but leads to a computationally expensive solution involving Hessians, which may not be applicable at all for large models. If we take step back, and focus on the potential of other domains of machine unlearning within approximate machine unlearning, then there have been several techniques studied to be beneficial for utility and efficacy perspective of unlearning, like distillation [3], [9], model pruning [10] etc. Until now, this line of work has been centered around the model perspective. In other words, the unlearning algorithm have predominantly focused on modifying either the model's loss function or its parameters. In this paper, we take a digression and focus on dataset, as well as, model centeric machine unlearning scheme, which aims to fill the gap in unlearning literature to find a good median between privacy, utility and efficiency of approximate unlearning algorithm. More specifically, we design new dataset condensation techniques to reduce the training fodder for unlearning, and new unlearning scheme, which we term 'modular unlearning' to further accelerate unlearning via catastrophic forgetting. To simply describe the modularized training, we essentially split the model into three parts and train them seperately, the consequence of which is that middle part requires minimum epochs to achieve catastrophic forgetting. We also metricize this unlearning in two new ways, namely via 'unlearning' and 'overfitting' metric. Lastly, we envision our algorithm towards two new and important applications.",
            "score": 0.5707094126287064,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 1868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "13193974"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "248834527"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "277349425",
            "title": "Effective Skill Unlearning through Intervention and Abstention",
            "text": "In recent years, the superior capabilities demonstrated by Large Language Models (LLMs) have attracted significant research interest. Without training on task-specific datasets, LLMs exhibit strong skills in various domains such as math (Wei et al., 2022; Imani et al., 2023;Cobbe et al., 2021), coding (Austin et al., 2021;Li et al., 2022), and language comprehension (Shi et al., 2023). Understanding the mechanisms behind these abilities and implementing controls over them are becoming increasingly important for developing stronger, safer, and more interpretable models. \n\nA recent line of research focuses on machine unlearning (Yao et al., 2023;Liu et al., 2024), which aims to remove the knowledge LLMs have acquired from specific datasets while maintaining their causally unrelated knowledge. In this work, we focus on a variant of machine unlearning called skill unlearning, which aims to remove a specific skill (e.g., coding skill, elementary math-solving skill) from the LLM while retaining its other skills. Skill unlearning helps researchers control certain behaviors of LLMs, providing insights into when and how a model demonstrates a particular skill. \n\nCurrently, most unlearning methods (Lu et  2022; Jang et al., 2023;Wang et al., 2023;Yu et al., 2023;Eldan and Russinovich, 2023;Chen and Yang, 2023;Yao et al., 2023) rely on fine-tuning, which becomes increasingly costly as LLMs grow larger. Other unlearning methods (Wu et al., 2023;Pochinkov and Schoots, 2023)",
            "score": 0.5702774350397164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1485
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 275,
                    "matchedPaperCorpusId": "257427208"
                },
                {
                    "start": 324,
                    "end": 340,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 634,
                    "end": 652,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1239,
                    "end": 1257,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 1257,
                    "end": 1273,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1301,
                    "end": 1321,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1321,
                    "end": 1338,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 1440,
                    "end": 1457,
                    "matchedPaperCorpusId": "264816202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "266902752",
            "title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics",
            "text": "The analysis of the literature showcases that unlearning solutions focus predominantly on horizontal FU and do not work in a vertical setting. Feature unlearning is unique to vertical FL (VFL) and aims to erase data features owned by a client u. This approach differs significantly from traditional unlearning methods, which typically focus on eliminating specific data samples. Consequently, techniques developed for other unlearning scenarios may not be directly applicable. Notably, client unlearning can be viewed as a specialized case of feature unlearning where a client u possesses only the features targeted for unlearning. \n\nDespite the potential of feature unlearning is still an open challenge, as supported by the very limited body of research. Deng et al. [93] contribute to addressing this gap by presenting a method based on logistic regression for vertical FU. In the context of VFL, participants collaboratively share a common set of data samples while maintaining distinct sets of features [94]. The proposed unlearning algorithm subtracts the updates from the target client, contributing to the refinement of the global model. To enhance both efficiency and accuracy, constraints are imposed on intermediate updates, thereby minimizing the sum of intermediate parameters. This optimization strategy results in a reduced impact on the sum of intermediate parameters for the remaining clients when the target client's contributions are removed. To generate the unlearned model, the server must retain the intermediate parameters submitted by each client during the overall training. When a target client requests unlearning of their data, the server retrieves the intermediate updates from that client in the last round, negates these updates, and distributes them to the remaining clients. Subsequently, each client utilizes these negated updates to adjust their local model parameters.",
            "score": 0.5696920276983508,
            "section_title": "D. Feature Unlearning",
            "char_start_offset": 76787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1904
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "260149064"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10394287109375
        },
        {
            "corpus_id": "276580125",
            "title": "A General Framework to Enhance Fine-tuning-based LLM Unlearning",
            "text": "Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations, which is essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.",
            "score": 0.5696459155665916,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4453125
        },
        {
            "corpus_id": "270045257",
            "title": "Machine Unlearning in Large Language Models",
            "text": "Let D train denote the training dataset containing information that we aim to forget.We train a text classifier, represented by parameters \u03d5, on this dataset.The classifier's accuracy is denoted as Acc classifier .\n\nNext, we employ our unlearning process on a language model, represented by parameters \u03b8, using D train .The updated language model is denoted as \u03b8 unlearned .We then generate responses using \u03b8 unlearned and evaluate them using the trained text classifier.The accuracy of the classifier on the unlearned model's responses is denoted as Acc unlearned .",
            "score": 0.5694135560035432,
            "section_title": "Formalization",
            "char_start_offset": 12155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 214
                },
                {
                    "start": 216,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 471
                },
                {
                    "start": 471,
                    "end": 566
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2467041015625
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "Unlearning for LLMs [6], [20] spans diverse strategies. Exact unlearning reverts a model to its pre-training state, fully removing certain knowledge but at high computational cost. Approximate unlearning modifies parameters tied to unwanted information without full retraining, balancing efficiency and effectiveness. We adopt first-order approximate unlearning, a practical alternative to exact or second-order methods. Below, we briefly review four representative approximate approaches, each with their distinct trade-offs in performance and resource demands. \n\n1) Gradient Ascent (GA): Gradient ascent [21] shifts a model's parameters away from solutions containing unwanted data by reversing the training objective. This process effectively removes sensitive or outdated information. However, it can trigger catastrophic forgetting and degrade overall performance [22], making it more suitable for smaller datasets or fewer training epochs. \n\n2) Fine-tuning with Random Labels: This method randomly modifies labels of the data to be removed and retrains the model to break their association with model parameters Fig. 2. Overall pipeline of GRAIL. It demonstrates the unlearning process applied to a vanilla model trained on datasets from both privacy and copyright domains. These datasets include knowledge that must be either unlearned or retained within each domain. In the first step, we localize parameters that are associated with the relevant domains and identify where they overlap. In the second step, we use this information to freeze the parameters essential for retention. This, in turn, also ensures fine-grained unlearning which is the final step of our framework. [23]. To mitigate performance degradation, it is typically applied with fewer epochs. \n\n3) Unlearning with Adversarial Samples: Unlearning with adversarial samples [24] injects small, targeted perturbations into sensitive information, causing the model to forget or misclassify those examples. This method can offer more precise control than random label retraining, but poor tuning risks broader performance degradation. Additionally, generating adversarial samples can be resource-intensive, especially for large models or high-dimensional inputs.",
            "score": 0.5690463526567644,
            "section_title": "II. RELATED WORKS A. Unlearning Research for Large Language Models",
            "char_start_offset": 7040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 610,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1848,
                    "end": 1852,
                    "matchedPaperCorpusId": "266933371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39501953125
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "These approaches aim to achieve unlearning in the output space rather than in the parameter space. Among these methods, a notable baseline by [16] uses an external prompt classifier as a guardrail, applying embedding corruptions to the identified prompts. The authors demonstrate that this corruption scheme results in distribution-wise similarity to the retrained model. \n\nIn this work, we propose a novel loss adjustment method for LLM unlearning, which simultaneously utilizes available example responses, effectively combining data-based and model-based methods.",
            "score": 0.5688800898546362,
            "section_title": "C.2.2 Implementation Setting.",
            "char_start_offset": 40419,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 566
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07452392578125
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "The most notable method for this purpose is RMU (Li et al., 2024), which was introduced alongside WDMP, a benchmark for evaluating hazardous capabilities. General-purpose unlearning algorithms like negative preference optimization (NPO) (Zhang et al., 2024) can also be adapted for this purpose. \n\nUnlearning robustness. Initial unlearning evaluations for LLMs relied on simple classification metrics (Eldan and Russinovich, 2023) which do not account for all possible ways in which a language model can represent and output the target information. Recent works (Jin et al., 2024;Hong et al., 2024;Lynch et al., 2024;Schwinn et al., 2024;Pawelczyk et al., 2024) have adopted an adversarial approach to test whether there exist ways to extract the information that was supposedly unlearned. \n\nFor instance, Lynch et al. (2024) showed that knowledge could be extracted at comparable rates from both original and unlearned models by probing internal representations. In the context of unlearning hazardous capabilities, RMU reports robustness under some white-box jailbreaks like GCG or probing, but finds that finetuning unlearned models can easily disable the protections (Li et al., 2024). Similarly, Hu et al. (2024) find that fine-tuning can revert unlearning. In this work, we devise novel methods to extract hazardous knowledge from unlearned models without updating the weights. The importance of meticulous evaluations, has been demonstrated by an earlier work on word embedding debiasing, which revealed the lack of robustness of the respective methods (Gonen and Goldberg, 2019).",
            "score": 0.5684755953761116,
            "section_title": "Related Work",
            "char_start_offset": 4872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1587
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157958984375
        },
        {
            "corpus_id": "270869978",
            "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
            "text": "Recent advancements in Large Language Models (LLMs) raise concerns about their use for undesirable purposes.Unlearning emerged as a promising solution for knowledge control, originally developed for removal of privacy-sensitive information (Bourtoule et al., 2021).Since then, several works have attempted to utilize unlearning for a host of applications relating to the removal of undesired knowledge or behaviours: removing harmful capabilities (Lynch et al., 2024) or harmful responses (Liu et al., 2024;Yao et al., 2023), erasing backdoors (Liu et al., 2022) or specific information or knowledge pertaining to a particular topic (Eldan and Russinovich, 2023;Li et al., 2024), erasing copyrighted content (Yao et al., 2023) and even reducing hallucinations (Yao et al., 2023).Such applications have been studied in the context of diffusion models too, with various attempts to use unlearning to remove unsafe concepts (Fan et al., 2023;Zhang et al., 2023).This paper discusses the application of unlearning to LLMs for removal of broadly impermissible knowledge, the use-case often discussed in policy circles e.g. for removal of biological and nuclear knowledge (Li et al., 2024).In fact, we uncover a fundamental inconsistency of the unlearning paradigm for this application.While unlearning aims to erase knowledge, the inherent in-context learning (ICL) (Agarwal et al., 2024;Brown et al., 2020;Kossen et al., 2024) capabilities of LLMs introduce a major challenge.We introduce the concept of ununlearning, where successfully unlearned knowledge can resurface through contextual interactions.This raises a critical question: if unlearned information can be readily reintroduced, is unlearning a truly effective approach for making sure that the model does not exhibit impermissible behaviours?We discuss the ramifications of ununlearning, particularly the need for effective content regulation mechanisms to prevent the resurgence of undesirable knowledge.",
            "score": 0.5670514850794501,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1184
                },
                {
                    "start": 1184,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 264,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 544,
                    "end": 562,
                    "matchedPaperCorpusId": "246240818"
                },
                {
                    "start": 1383,
                    "end": 1402,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1402,
                    "end": 1422,
                    "matchedPaperCorpusId": "260125327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2152099609375
        },
        {
            "corpus_id": "269187650",
            "title": "Offset Unlearning for Large Language Models",
            "text": "The design of \u03b4-UNLEARNING leads to the following key merits.\n\nApplicability to Black-box LLMs.In contrast to most previous unlearning methods, \u03b4-UNLEARNING is applicable to not just opensourced models, but also black-box LLMs without access to internal weights.Instead of updating M , \u03b4-UNLEARNING only obtains the logits from M , and learns the logit offset needed to adjust its prediction using smaller white-box models.\n\nPrivacy Protection.Prior work has proposed incontext unlearning (Pawelczyk et al., 2023) to make unlearning possible for black-box LLMs.However, a key drawback of this approach is that the model developer still maintains a growing list of sensitive information used to construct queries for unlearning during inference, which defeats the purpose of privacy protection.For comparison, \u03b4-UNLEARNING does not require storage of any sensitive information after unlearning is completed.\n\nTraining Efficiency.While \u03b4-UNLEARNING introduces a pair of smaller offset language models to facilitate unlearning for black-box LLMs, the computational overhead for training is minimal since the logits of the two frozen models M and M o can be pre-computed in one forward pass prior to unlearning.This leads to an overall reduction in training time as \u03b4-UNLEARNING tunes fewer parameters than direct fine-tuning.\n\nVersion Control.\u03b4-UNLEARNING also facilitates more efficient version control and user customization, as instead of storing multiple versions of the larger model, we only need to keep track of a pool of smaller models.These models can be combined with the same base LLM in a plug-and-play manner for different applications.",
            "score": 0.5669391152842281,
            "section_title": "Merits of \u03b4-UNLEARNING",
            "char_start_offset": 11932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 63,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 423
                },
                {
                    "start": 425,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 906
                },
                {
                    "start": 908,
                    "end": 928
                },
                {
                    "start": 928,
                    "end": 1207
                },
                {
                    "start": 1207,
                    "end": 1322
                },
                {
                    "start": 1324,
                    "end": 1340
                },
                {
                    "start": 1340,
                    "end": 1541
                },
                {
                    "start": 1541,
                    "end": 1646
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "276249843",
            "title": "Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond",
            "text": "Preliminaries on unlearning and relearning attacks. To achieve efficient LLM unlearning while preserving model utility, the unlearning problem is formulated as an optimization task to update parameters from their pretrained values (Eldan & Russinovich, 2023;Yao et al., 2024;Maini et al., 2024;Zhang et al., 2024a;Li et al., 2024). To be specific, let D f and D r represent the 'forget' and 'retain' sets, respectively. Here the forget set D f defines the scope of unlearning, specifying the data samples whose influences are to be removed. Conversely, the retain set D r ensures the preservation of the model's utility post-unlearning. Built upon D f and D r , a forget loss (\u2113 f ) and a retain loss (\u2113 r ) are defined to balance unlearning effectiveness and utility retention. The leads to the following regularized optimization problem (Liu et al., 2024b): \n\nwhere \u03b8 denotes the model parameters, \u2113(\u03b8|\u2022) is the forget or retain loss associated with the model \u03b8 under a forget or retain dataset, and \u03bb \u2265 0 is a regularization parameter to balance 'forget' and 'retain'. One popular approach for designing the forget loss is negative preference optimization (NPO) (Zhang et al., 2024a), which formulates \u2113 f as a preference optimization objective (Rafailov et al., 2024) but exclusively treats the forget data as negative samples. The retain loss \u2113 r can be set as the standard training loss, ensuring the model preserves its utility on the retain set. \n\nDespite the growing demand for LLM unlearning, concerns also arise about its robustness against relearning attacks (Hu et al., 2024). These attacks aim to recover unlearned knowledge by fine-tuning the unlearned model, even using a very small number of forget samples. We present the relearning attack formulation below:",
            "score": 0.5668919522372882,
            "section_title": "LLM Unlearning and Relearning Attacks",
            "char_start_offset": 8872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1776
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 275,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 294,
                    "end": 314,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 314,
                    "end": 330,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 1165,
                    "end": 1186,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1248,
                    "end": 1271,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1512451171875
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "In this experiment, our goal was to induce the model to discard its previously acquired knowledge.To evaluate this, we employed question-answering and word chain tasks.\n\nOur testing involved two approaches: initially, we trained the model to incorporate specific information and knowledge, followed by an assessment and unlearning phase to determine if the model retained any of the initial content post-unlearning.The second approach focused on the unlearning of knowledge already embedded in the large pretrained language model.This involved analyzing changes in the model's outputs before and after unlearning and quantifying the similarity in output content.The results of these experiments are detailed in Table 3:\n\nThe results of our experiments clearly demonstrate that our unlearning method has effectively eliminated the targeted knowledge, as the model ceased to produce the original content.This outcome manifests in the model's textual outputs, which either generate irrelevant responses or explicitly display a lack of knowledge.These findings align with our anticipated experimental outcomes.Regarding knowledge injection and word chain tasks, the unlearning process is specific to each task, making these methods generally non-interchangeable.Nevertheless, incorporating data augmentation in the preprocessing stage can amplify the unlearning effect.For example, altering the formats in word chain tasks for unlearning assessments shows noticeable effects when subsequently evaluated using questionanswering techniques.The output generated by our approach was consistently unrelated and nonsensical, verifying that the model can produce text normally while effectively preventing knowledge leakage.",
            "score": 0.5667739995940639,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 27658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 168
                },
                {
                    "start": 170,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 719
                },
                {
                    "start": 721,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1534
                },
                {
                    "start": 1534,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353515625
        },
        {
            "corpus_id": "276767847",
            "title": "AILS-NTUA at SemEval-2025 Task 4: Parameter-Efficient Unlearning for Large Language Models using Data Chunking",
            "text": "The convergence of unlearning and LLMs arises as a nascent research field accompanied by several challenges, due to their vast and opaque pretraining, large-scale data inter-dependencies, and unbounded label spaces, making it difficult to identify and isolate specific data representations within the model, not to mention efficiently removing them (Yao et al., 2024b). In our work, we explore unlearning strategies on trained LLMs, primarily focusing on fine-tuning, achieving to delete targeted data points without deteriorating the LLM's general knowledge. Specifically, we investigate parameter-efficient gradient-based methods (Jang et al., 2022;Yao et al., 2024a) leveraging data chunking to improve unlearning effectiveness. To achieve this, we employ low-rank adaptation (LoRA) methods (Hu et al., 2021) or selectively fine-tune only the last layers of the model. This approach not only enhances training speed and efficiency, but also introduces a regularization effect mitigating catastrophic collapse by preserving a portion of the base model's weights. As a result, our approach ranked first, surpassing the second best by a large margin. In summary, our method: \n\n1. Leverages parameter-efficient fine-tuning. 2. Achieves near-perfect forget-retain quality. \n\n3. Preserves the model's reasoning abilities avoiding catastrophic collapse. 4. Generalizes well on various data distributions making it robust and widely applicable. The code for our system is available on GitHub1 .",
            "score": 0.5667118526445729,
            "section_title": "Introduction",
            "char_start_offset": 1546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327392578125
        },
        {
            "corpus_id": "273532566",
            "title": "WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models",
            "text": "Therefore, LLM unlearning, despite falling under the broad category of MU, presents a much more challenging problem. The two main difficulties lie in developing effective and efficient unlearning algorithms and in assessing the performance of LLM unlearning. Representative unlearning algorithms include gradient ascent (GA) [8,15,16] to deviate the LLM prediction away from responses to the forget data and its utility-regularized variants, such as GradDiff [15][16][17] which utilizes the gradient difference between the forget loss and the retain loss to strike a tradeoff between unlearning efficacy and utility retention. Drawing inspiration from direction preference optimization [18], the LLM unlearning problem has also been addressed using algorithms such as negative preference optimization (NPO) [19] and preference optimization (PO) [16]. NPO treats the forget data as negative examples in LLM preference alignment, while PO assigns pre-defined positive responses (such as rejection-based answers) to the forget data during preference alignment. In addition, further studies explored the choice of optimizers suited for solving LLM unlearning problems [20] and proposed prompting-based algorithms to achieve unlearning for black-box LLMs [21][22][23][24]. A few recent benchmarked unlearning tasks and datasets have also been developed to facilitate performance evaluation. Examples include the TOFU dataset for fictitious unlearning [16], the WMDP dataset for malicious use prevention of LLMs [25], the copyrighted information removal [26], and the LLM detoxification task [27,28]. All these evaluations will be considered in this work. \n\nDespite the rapid progress in LLM unlearning algorithms and evaluation methods, less effort has been made to explore the modularity characteristics of LLMs for unlearning and the influence of these modules. In the literature, weight sparsity achieved through model pruning has been found beneficial in reducing the gap between a GA-based approximate unlearning method and exact unlearning [12]. However, this advantage was limited to MU for classification models.",
            "score": 0.5665638928323549,
            "section_title": "Introduction",
            "char_start_offset": 1982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 467,
                    "end": 471,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 2041,
                    "end": 2045,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "In this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in Large Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs), we devised a method that effectively isolates and removes specific knowledge, ensuring that the targeted data and its contextual associations are erased from the model. Unlike previous methods, which required retraining or were limited to classification tasks, CodeUnlearn operates amortized and zero-shot, providing an efficient and scalable solution for unlearning in complex, generative models like LLMs. Our approach uses a discrete concept representation to regulate the flow of information in a language model, enabling the unlearning of specific topics while preserving overall model performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates the model's ability to reproduce the unlearned information without requiring additional training, achieving substantial unlearning effectiveness and maintaining interpretability.",
            "score": 0.5665506415827942,
            "section_title": "CONCLUSION",
            "char_start_offset": 25058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.498779296875
        },
        {
            "corpus_id": "271860124",
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "text": "This motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1). In this regard, several novel approaches have been proposed for approximate unlearning: Jang et al. ( 2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs. \n\nMeanwhile, Low-Rank Adaptation (LoRA) has emerged as one of the most prominent techniques for parameter-efficient fine-tuning on downstream tasks (Hu et al., 2022). The core idea of LoRA is to freeze all pretrained weights and instead train low-rank decomposition matrices to model the weight changes in each linear layer, effectively reducing the number of trainable parameters and thus its memory cost. Beyond its efficiency, the low-rank structure in LoRA also serves as a strong regularizer (Biderman et al., 2024), which we hypothesize aids LLM unlearning by stabilizing optimization and mitigating catastrophic forgetting of retained knowledge. However, the empirical effects of LoRA in the context of LLM unlearning remain largely unexplored. \n\nIn this paper, we present the first in-depth study of LLM unlearning under the low-rank adaptation paradigm and introduce Low-rank Knowledge Unlearning (LoKU), which consists of two novel techniques for robust and parameter-efficient knowledge unlearning.",
            "score": 0.5662618155520024,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1605,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 459,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 464,
                    "end": 481,
                    "matchedPaperCorpusId": "267681958"
                },
                {
                    "start": 1129,
                    "end": 1146,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1478,
                    "end": 1501,
                    "matchedPaperCorpusId": "269791237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5478515625
        },
        {
            "corpus_id": "268532505",
            "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Approximate Unlearning Completeness",
            "text": "They provide resource-efficient post-hoc solutions for removing data from large models by adjusting model parameters.Based on how they calculate the model update, we classify those unlearning algorithms into three groups:\n\nLog-based retrieval: Amnesiac Unlearning [18] directly subtracts the corresponding parameter updates, which have been logged during the training process, of the small batches containing targeted data from the model weights.Fisher Forgetting [16,17] approximates the scrubbing procedure of selective forgetting through a noisy Newton update, deriving it as reducing the KL divergence distance between two model distributions: one trained on the original dataset and the other trained on the retained dataset.They calculate the corresponding update by approximating the Hessian of the forgotten data using the Fisher Information Matrix.Despite there being a theoretical foundation for data deletion in linear models, approximating the influence of targeted samples in deep models remains challenging due to their non-convexity and the randomness of perturbations.Computation efficiency is also a concern when it comes to large models.Dynamics Masking: Forsaken [32] introduces a mask gradient generator that can iteratively generate mask gradients to \"stimulate\" neural neurons to unlearn the memorization of given samples.Selective Synaptic Dampening [13] uses the Fisher information matrix from training and forgetting data to identify key forget set parameters.Then, it dampens these based on their significance to the forget set relative to the overall training data.Jia et al. [26] explore the application of model sparsification via weight pruning in machine unlearning.These methods efficiently achieve machine unlearning by masking parameter dynamics of given samples but rely on explaining their opaque performance to evaluate their unlearning utility.This paper will primarily focus on unlearning commitment management for approximate algorithms.Since exact unlearning provides definitive unlearning, approximate unlearning operations are performed on the parameter space, making its unlearning utility less transparent, an evaluation step is necessary.Additionally, approximate unlearning is more acceptable for large models due to its computational efficiency.",
            "score": 0.5659248289980812,
            "section_title": "Machine Unlearning",
            "char_start_offset": 9856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 221
                },
                {
                    "start": 223,
                    "end": 446
                },
                {
                    "start": 446,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1592
                },
                {
                    "start": 1592,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1882
                },
                {
                    "start": 1882,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2184
                },
                {
                    "start": 2184,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "212628473"
                },
                {
                    "start": 1182,
                    "end": 1186,
                    "matchedPaperCorpusId": "236882730"
                },
                {
                    "start": 1373,
                    "end": 1377,
                    "matchedPaperCorpusId": "260900355"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1302490234375
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In light of emerging regulations, the removal of user-specified data from large language models (LLMs) is increasingly recognized as a critical component of responsible and ethical AI development. This study further investigates the application of machine unlearning techniques to LLMs. The datasets utilized for evaluation are publicly available and implemented within the intended use. \n\nWe hope this study to advance research and literature on machine unlearning for LLMs.",
            "score": 0.565572918438714,
            "section_title": "ETHICAL STATEMENT",
            "char_start_offset": 36312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44921875
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "Relearning through Fine-tuning Hu et al. [2024] suggested to use benign, publicly available data loosely related to the unlearned information. For example, while specific details about virus design might be unavailable, general knowledge such as \"What is avian influenza?\" is readily accessible. Their experiments demonstrate that augmenting with such benign data can inadvertently cause LLMs to relearn harmful knowledge, a finding further validated by [\u0141ucki et al., 2024]. LLM Quantization facilitates faster inference and efficient deployment on resource-constrained devices with minimal performance degradation. However, recent studies [Zhang et al., 2024c;\u0141ucki et al., 2024] indicated that quantization can inadvertently aid in recovering unlearned knowledge. Zhang et al. [2024c] reported that for unlearning methods constrained by utility requirements, unlearned models retain an average of 21% of the intended forgotten knowledge in full precision, which dramatically increases to 83% after 4bit quantization. Activation Intervention Recent research [Arditi et al., 2024] demonstrated that refusal behavior in language models is mediated by a single direction in the activation space. \n\nBuilding upon this, outputs from each transformer block of both original and unlearned models are collected using the forget preference dataset [\u0141ucki et al., 2024;Seyitoglu et al., 2024], and the refusal direction for each layer is computed by analyzing the differences in means. During inference, the refusal direction is removed at each layer. Extensive experiments confirmed the effectiveness of this approach. \n\n6 Future Directions Theoretical framework Existing LLM unlearning methods often lack formal guarantees of effectiveness. While locate-then-unlearn approaches [Wu et al., 2023;Jia et al., 2024] enhance interpretability, they do not establish a rigorous theoretical foundation. A crucial future direction is to develop a comprehensive framework that formally defines and ensures its effectiveness. This could involve leveraging principles from information theory [Jeon et al., 2024] and other theoretical approaches to provide a more principled understanding of LLM unlearning.",
            "score": 0.5653685035041599,
            "section_title": "Model Intervention",
            "char_start_offset": 23709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 47,
                    "matchedPaperCorpusId": "270619566"
                },
                {
                    "start": 641,
                    "end": 662,
                    "matchedPaperCorpusId": "270924079"
                },
                {
                    "start": 662,
                    "end": 681,
                    "matchedPaperCorpusId": "273849875"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "270924079"
                },
                {
                    "start": 1341,
                    "end": 1361,
                    "matchedPaperCorpusId": "273849875"
                },
                {
                    "start": 1361,
                    "end": 1384,
                    "matchedPaperCorpusId": "270226658"
                },
                {
                    "start": 1772,
                    "end": 1789,
                    "matchedPaperCorpusId": "264816202"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2076416015625
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "In the process of implementing unlearning, adjusting various hyperparameters based on the chosen fine-tuning approach is essential.Both full-parameter fine-tuning and Low-Rank Adaptation (LoRA) are efficacious for fine-tuning large language models, yet they present distinct trade-offs.Full-parameter fine-tuning provides extensive adaptability and the potential for high performance, albeit at the expense of increased computational load and time consumption.Conversely, LoRA offers a more efficient and swift finetuning option, ideal for situations with limited resources or the need for rapid iterations, but it may not perform as well in more intricate tasks when compared to fullparameter fine-tuning.The selection between these methods hinges on the specific requirements of the task, resource availability, and time limitations.Consequently, achieving an equilibrium between computational resource allocation and unlearning efficacy is imperative.",
            "score": 0.5647809841770692,
            "section_title": "Balancing Resources and Unlearning Effectiveness",
            "char_start_offset": 20705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7900390625
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.",
            "score": 0.5646717514467444,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24658203125
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "Training large language models (LLMs) often involves complex data pipelines. These pipelines handle large quantities of data, some of which might be sensitive. Recently, it has been shown that LLMs are susceptible to sentence-level membership inference attacks (Gu et al., 2023) and reconstruction attacks (Carlini et al., 2019), meaning that one may be able to infer which data was part of the training set, or in some cases, even reconstruct partial inputs by interrogating the model. As a result, this raises a prevalent problem of data removal from a trained LLM. \n\nTo this end, there has been growing interest in formalizing technical definitions of machine unlearning and designing machine unlearning techniques and evaluation metrics (Triantafillou et al., 2023(Triantafillou et al., , 2024)). The goal of machine unlearning is to remove the influence of a subset of the original training data, the forget set, from a corresponding model. A na\u00efve way to achieve it is to retrain the model from scratch on an updated training set (the retain set), that does not include the forget set. This approach is resource-intensive, and does not scale to the large models now in development. \n\nEfficient alternatives in LLMs often rely on gradient ascent-based procedures, where one maximizes some loss on the data to be forgotten to reduce the influence of this data on the model predictions (Jang et al., 2022). However, there are a few issues that arise with this approach: (1) inherently, gradient ascent-based unlearning does not come with guarantees, and one needs a way to empirically evaluate the unlearning quality; (2) such unlearning methods do not only affect the forget set examples, but also come at a performance cost on the rest of the data. \n\nOur work touches upon both of these issues. For the first issue, we propose two metrics for evaluating unlearning quality.",
            "score": 0.5644271553211924,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 278,
                    "matchedPaperCorpusId": "258735467"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337890625
        },
        {
            "corpus_id": "278602095",
            "title": "Layered Unlearning for Adversarial Relearning",
            "text": "Next, we evaluate the performance of LU on LLM unlearning benchmarks. Specifically, we consider unlearning on the WMDP (Li et al., 2024b), MMLU (Hendrycks et al., 2021), and Years (Deeb & Roger, 2025) datasets. WMDP consists of dangerous knowledge framed as multiple-choice questions (MCQs). To assess LU's ability to remove capability-related information, we also apply unlearning to subsets of MMLU directly. The Years dataset contains major world events annotated with the year in which they occurred. For retain set evaluation, we use MMLU; when unlearning on MMLU, we exclude the categories being unlearned from the retain set. \n\nUnlearning. State-of-the-art LLM unlearning methods fall into two categories: representation engineering and gradient ascent. We select a representative algorithm from each for our unlearning primitive U: Representation Misdirection Unlearning (RMU) (Li et al., 2024a) for representation engineering, and Simple Negative Policy Optimization (SimNPO) (Fan et al., 2025) for gradient ascent. We evaluate both methods with and without LU, denoting the LU variants as L-RMU and L-SimNPO, respectively. For a graphical overview of the unlearning process, see Appendix Figure 5. \n\nEvaluation. Given a dataset F to be unlearned, we uniformly at random split it into k folds, F 1 , . . . , F k . For a fixed k and dataset, this partitioning remains consistent across all experiments for both unlearning and evaluation. We follow the Language Model Evaluation Harness standards for 0-shot evaluation (Gao et al., 2023). \n\nTo evaluate a model M, we consider all 2 k \u2212 2 proper subsets S \u2282 {F 1 , . . . , F k } and follow the evaluation protocol in Deeb & Roger (2025).",
            "score": 0.564239977741734,
            "section_title": "LLM unlearning experiments",
            "char_start_offset": 12961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1551513671875
        },
        {
            "corpus_id": "271892278",
            "title": "Fishers Harvest Parallel Unlearning in Inherited Model Networks",
            "text": "Unlearning in various learning frameworks remains challenging, with the continuous growth and updates of models exhibiting complex inheritance relationships. This paper presents a novel unlearning framework, which enables fully parallel unlearning among models exhibiting inheritance. A key enabler is the new Unified Model Inheritance Graph (UMIG), which captures the inheritance using a Directed Acyclic Graph (DAG).Central to our framework is the new Fisher Inheritance Unlearning (FIUn) algorithm, which utilizes the Fisher Information Matrix (FIM) from initial unlearning models to pinpoint impacted parameters in inherited models. By employing FIM, the FIUn method breaks the sequential dependencies among the models, facilitating simultaneous unlearning and reducing computational overhead. We further design to merge disparate FIMs into a single matrix, synchronizing updates across inherited models. Experiments confirm the effectiveness of our unlearning framework. For single-class tasks, it achieves complete unlearning with 0\\% accuracy for unlearned labels while maintaining 94.53\\% accuracy for retained labels on average. For multi-class tasks, the accuracy is 1.07\\% for unlearned labels and 84.77\\% for retained labels on average. Our framework accelerates unlearning by 99\\% compared to alternative methods.",
            "score": 0.563866896106238,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "270440344",
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "text": "We address this issue by aligning their common performance, i.e., their capacity of retention, in a post-unlearning manner. Motivated by (Wortsman et al., 2022), it is achieved by mixing model parameters from both before and after unlearning, modulated through a mixing factor \u03b1. With proper control via \u03b1, we observe that model mixing (MM) enables us to finely calibrate the extent of unlearning such that performance on common data is adequately preserved, meanwhile the inevitable compromise on the extent of removal is roughly minimized, cf., Section 4. Thereafter, we can fairly concentrate on assessing the strength of the removal on targeted data, thereby alleviating the challenges for comparing different unlearning methods or unlearned models when pursuing to goals of removal and retention concurrently. \n\nWe refer to our evaluation framework as \"unlearning with control\" (UWC), which incorporates the ES as the basic metric and utilizes MM for calibration to ease assessments and comparisons across methods/setups. Based on UWC, we benchmark a series of representative works along with suggestions for their hyper-parameters. We challenge the currently perceived advancements in LLM unlearning, where the ostensibly positive behaviors of current state-of-the-art methods may be the result of either excessive unlearning or insufficient unlearning. Nevertheless, proper hyper-parameter tuning can remarkably enhance the efficacy of many earlier works, such as GA variants, showing potential to exceed many advanced counterparts. Leveraging UWC, we also benefit the community by exploring a range of simple yet intriguing tricks to further enhance the practical efficacy of current unlearning methods, which are not covered in previous works.",
            "score": 0.5637232359202397,
            "section_title": "INTRODUCTION",
            "char_start_offset": 5164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1752
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 160,
                    "matchedPaperCorpusId": "237420687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270263671875
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In recent years, large language models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to complex problem-solving. However, this advancement has highlighted significant concerns regarding the potential for LLMs to retain unauthorized content from massive training corpus crawled from the Internet, raising issues related to privacy and copyright (Huang et al., 2022;Carlini et al., 2023;Staab et al., 2024;Ippolito et al., 2023;Dou et al., 2024). These concerns are particularly relevant within legal and regulatory frameworks, such as the Right to be Forgotten (Dang, 2021), which aims to empower individuals to have unauthorized data erased from digital records. Addressing these issues is crucial for ensuring the responsible deployment of LLMs in real-world applications. \n\nDue to the high cost of retraining LLMs, researchers have explored machine unlearning techniques, namely LLM unlearning (Cao & Yang, 2015;Bourtoule et al., 2021;Yao et al., 2023). The typical paradigm involves fine-tuning the target LLM on a specified set, known as the forget set, to obtain an unlearned model. As described in (Maini et al., 2024;Jin et al., 2024), the unlearned model should meet two primary goals: 1) it should not reveal any information contained in the forget set, and 2) it should maintain performance on the neighbor set, which has a distribution similar to the forget set but is not the target of unlearning, as well as on other tasks with general knowledge. While the first goal is generally easier to achieve, the main challenge lies in meeting the second goal (Liu et al., 2024b;Maini et al., 2024;Zhang et al., 2024a;Ji et al., 2024;Shi et al., 2024a;Wang et al., 2024c). \n\nIn this paper, we have a closer look at machine unlearning for LLMs.",
            "score": 0.5634713853198023,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "249063119"
                },
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 488,
                    "end": 507,
                    "matchedPaperCorpusId": "263834989"
                },
                {
                    "start": 507,
                    "end": 529,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 663,
                    "end": 675,
                    "matchedPaperCorpusId": "234335026"
                },
                {
                    "start": 999,
                    "end": 1017,
                    "matchedPaperCorpusId": "5945696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271728515625
        },
        {
            "corpus_id": "276742219",
            "title": "CE-U: Cross Entropy Unlearning",
            "text": "We presented CE-U, a novel cross entropy unlearning loss for LLMs that unifies supervised learning and unlearning in a single framework. Our method leverages a modified cross entropy loss in which the logit for the true label is set to a tunable score, allowing smooth interpolation between full supervision and aggressive unlearning. The General CE-U framework provides a principled approach to modulating between learning and unlearning by adjusting a single parameter, offering flexibility across various machine learning scenarios. Empirically, CE-U achieved state-of-the-art performance for LLaMA2-7B on the TOFU benchmark, even without using additional positive samples. Our analysis reveals the differences in gradient behavior between conventional gradient ascent and CE-U, and we discuss how replacing unstable gradient ascent components in RL-based unlearning methods with CE-U can stabilize updates. Overall, our work suggests that CE-U is a promising method for unlearning in large language models.",
            "score": 0.5634544198651739,
            "section_title": "Conclusion",
            "char_start_offset": 18963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203125
        },
        {
            "corpus_id": "277349425",
            "title": "Effective Skill Unlearning through Intervention and Abstention",
            "text": "Large language Models (LLMs) have demonstrated remarkable skills across various domains. Understanding the mechanisms behind their abilities and implementing controls over them is becoming increasingly important for developing better models. In this paper, we focus on skill unlearning in LLMs, specifically unlearning a particular skill while retaining their overall capabilities. We introduce two lightweight, training-free machine skill unlearning techniques for LLMs. First, we observe that the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills. Additionally, we find that queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube. Based on these observations, we propose two lightweight, training-free skill unlearning methods via \\textit{intervention} and \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key Space Detection}. We evaluate our methods on unlearning math-solving, Python-coding, and comprehension skills across seven different languages. The results demonstrate their strong unlearning capabilities for the designated skills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative performance drop on the forgetting skill and less than 10\\% relative performance drop on other skills and the model's general knowledge (MMLU) for most unlearning tasks. Our code is available at https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning",
            "score": 0.563302801348688,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3017578125
        },
        {
            "corpus_id": "270296369",
            "title": "Update Selective Parameters: Federated Machine Unlearning Based on Model Explanation",
            "text": "In this paper, we proposed a federated unlearning scheme that addresses the practical problem of removing the effects of particular classes from a trained model in the federated learning context.As a solution, we analyzed the most influential channels of a model for the given classes based on the ablation study.For unlearning class, we provided two effective methods that only fine-tune partial influential channel parameters with the help of perturbation data.The analysis and experimental results demonstrate that under our scheme, the model can remove the impact of classes that need to be removed and ensure the accuracy of the remaining data in a quick and efficient manner.\n\nIn future work, we plan to explore more complex federated learning scenarios containing more kinds of unlearning requests, for example, unlearning one sample or all data in one client.At present, we have only illustrated that there are different influences within a trained model's channel at the class level.Due to the various unlearning requests involved, we intend to explore ways to extend and/or modify the current method of evaluating channel importance and develop new federated unlearning schemes based on these revised evaluations.In addition, we plan to design a more powerful scheme that supports unlearning requests from Natural Language Processing (NLP) models, combined with other technologies such as information theory.",
            "score": 0.5632695633117741,
            "section_title": "CONCLUSION AND FUTURE WORK",
            "char_start_offset": 59817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 681
                },
                {
                    "start": 683,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1418
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "In light of this, it is essential to provide LLMs with an efficient and effective way to unlearn the information requested by users. \n\nRecent attention has been paid to the handling of such unlearning requests for LLMs through retraining and data pre-processing like SISA (Bourtoule et al., 2021;Kumar et al., 2022) where training data is stored in different isolated slices and each checkpoint is saved after training on each slice. When a deletion request is received, the respective data point will be removed from the slice, and the model checkpoint up to the data point will be used to further retrain the model. The effect of unlearning is often reflected by the model errors on the deleted data (models cannot predict the deleted data) (Kurmanji et al., 2023;Jang et al., 2022). Other works have also explored the design of algorithms that ensure differential privacy (DP) (Yu et al., 2021;Li et al., 2021;Anil et al., 2021). However, machine unlearning approaches like SISA (Bourtoule et al., 2021) usually require a significantly large amount of storage space (Bourtoule et al., 2021), and DP methods could result in a slow convergence and significant deterioration in model performance (Nguyen et al., 2022). In addition, both of them require retraining the whole model, which is extremely expensive and time-consuming considering the model scales of the current LLMs. These limitations also make them unable to dynamically deal with a sequence of unlearning requests which is often the need in real-world scenarios (Jang et al., 2022;Nguyen et al., 2022). \n\nTo fill in these gaps, in this work, we propose an Efficient Unlearning method for LLMs (EUL) to efficiently unlearn what needs to be forgotten without completely retraining the whole model while retaining the performances of the models. Specifically, we propose a lightweight approach to learning the unlearning layer that is plugged into transformers through a selective teacher-student formulation (Kurmanji et al., 2023) within several updates, without tuning the large language models.",
            "score": 0.5629160922177066,
            "section_title": "Introduction",
            "char_start_offset": 1371,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1566
                },
                {
                    "start": 1569,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 272,
                    "end": 296,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 982,
                    "end": 1006,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1069,
                    "end": 1093,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14892578125
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Our results reveal that when a model is trained on both sets, unlearning only the target forgetting set is insufficient for complete knowledge removal. However, when related knowledge is included in the unlearning process, the model demonstrates significantly improved forgetting effectiveness on the target forget set. These findings suggest that LLMs can reconstruct target knowledge that should be forgotten by related information. \n\nGiven that LLMs are trained on massive datasets, and their training data is often inaccessible, constructing complete related knowledge sets remains a major challenge. This raises a crucial question: Can related knowledge unlearning be achieved without requiring additional training data? To address this, we propose UIPE (Unlearning Improvement via Parameter Extrapolation), a plug-and-play auxiliary unlearning method ( \u00a75). This method is founded on a crucial observation: the unlearning of target knowledge triggers the forgetting of related knowledge. This phenomenon stems from the fact that related knowledge exhibits similar distribution characteristics in the parameter space, leading to highly correlated gradient changes (Qin et al., 2024;Xie et al., 2024). By amplifying the gradient ascent updates on the target forget set, we extend its gradient update effects to the related knowledge set, significantly enhancing the model's capability to forget related knowledge. Experimental evaluations based on the TOFU benchmark demonstrate that our method enables various unlearning approaches to achieve optimal trade-offs between forget quality and model utility preservation. \n\nWe summarize our contributions below. \n\n\u2022 We identify the limitation of the GA method in unlearning related knowledge, which we found to be a key factor behind the unsatisfac-tory unlearning performance of models. \n\n\u2022 We introduce the UIPE method, which utilizes parameter extrapolation to enhance the model's ability to forget related knowledge. \n\n\u2022 We conduct experiments on various GA-based unlearning methods using the TOFU benchmark. The results demonstrate that UIPE facilitates a more optimal balance between model utility and forget quality across these methods. \n\n2 Related Work",
            "score": 0.5624654165492337,
            "section_title": "What is Patient John's medical condition?",
            "char_start_offset": 2765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1169,
                    "end": 1187,
                    "matchedPaperCorpusId": "271270722"
                },
                {
                    "start": 1187,
                    "end": 1204,
                    "matchedPaperCorpusId": "267770418"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447998046875
        },
        {
            "corpus_id": "276408764",
            "title": "MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models",
            "text": "The core challenge of multimodal MU lies in preserving textual knowledge while performing unlearning on VQA data. A naive unlearning method,  such as GA Difference (GA_Diff), simply updates the model parameters \u03b8 using a joint loss \n\nwhere t denotes the t-th step, and L f (\u2022) and L r (\u2022) represent the loss on the Forget and Retain sets, respectively. Interpreting the update of \u03b8 as an optimization problem in parameter space, the term \u2212L f (\u03b8 t ) forces the MLLM to forget the VQA samples that should be unlearned by following the steepest ascent direction. And L r (\u03b8 t ) aims to preserve knowledge from the retained VQA samples. However, the conflicting directions of the Forget loss and the Retain loss make the unlearning process unstable. Furthermore, traditional MLLM unlearning methods primarily focus on VQA data, neglecting the constraints from text-only QA data. Nonetheless, such conflicts can be effectively mitigated if model updates selectively target parameters that are salient for the targeted knowledge (S) while preserving those critical for others. This process can be formulated as: \n\nwhere m is a boolean mask that selectively updates parameters, and \u2299 denotes the Hadamard product. In this way, the ascent of the Forget Loss on targeted visual concept does not destroy the parameters salient for the Retain set or textual knowledge, as illustrated in Figure 3.",
            "score": 0.562404280553757,
            "section_title": "Selective Updating for Forget Loss",
            "char_start_offset": 8990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1386
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Multilingual pretrained language models (Devlin et al., 2019;Conneau et al., 2020;Xue et al., 2021;Lin et al., 2022;Le Scao et al., 2023) 2023) investigated the cross-lingual consistency of factual knowledge in multilingual models, finding that factual knowledge does not remain consistent across languages, but only when languages share a larger portion of vocabulary. Building on these advancements, our work employs multilingual language models to investigate the crosslingual transfer of machine unlearning, revealing that current unlearning methods lack this capability. \n\nIn response, we propose an effective approach for unlearning specific information across languages, addressing the need for precise and reliable information removal in a multilingual context.",
            "score": 0.5623215327494937,
            "section_title": "Cross-Lingual Transfer",
            "char_start_offset": 20910,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 769
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 82,
                    "matchedPaperCorpusId": "245334784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.297607421875
        },
        {
            "corpus_id": "268385396",
            "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
            "text": "Machine unlearning has received attention as an effective approach to remove data instances or features from the ML models without retraining from scratch (Bourtoule et al., 2021;Guo et al., 2020;Neel et al., 2021;Warnecke et al., 2023).Two alternative unlearning schemes have been proposed: exact unlearning represented by the Sharded, Isolated, Sliced, and Aggregated (SISA) framework (Bourtoule et al., 2021), and approximate unlearning, such as (\u03f5, \u03b4)-certified unlearning based on the influence function (Guo et al., 2020).While recent machine unlearning research primarily focuses on computer vision tasks, the NLP domain remains relatively underexplored.a Newton step on the model parameters.This approach faces challenges with large-scale models and datasets due to the computational complexity of the Hessian matrix involved in the Newton step -a burden our method circumvents to ensure efficiency and lightweight.",
            "score": 0.562022218231158,
            "section_title": "A.2 Machine Unlearning in NLP",
            "char_start_offset": 33587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 661
                },
                {
                    "start": 661,
                    "end": 699
                },
                {
                    "start": 699,
                    "end": 923
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 196,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 196,
                    "end": 214,
                    "matchedPaperCorpusId": "220364296"
                },
                {
                    "start": 509,
                    "end": 527,
                    "matchedPaperCorpusId": "207847600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10357666015625
        },
        {
            "corpus_id": "271963102",
            "title": "Brain-inspired Artificial Intelligence: A Comprehensive Review",
            "text": "Unlearning. It involves selectively removing the influence of specific data points from a trained machine learning model. It can be used to protect users' privacy when they want their data forgotten by a trained machine learning model. Instead of simply erasing their data from database, the deletion needs to eliminate the contribution of the erased training data from the already trained model, which process was first defined as machine unlearning [186]. To better understand how unlearning mechanisms work, we first introduce the unlearning problem and process following the machine unlearning framework demonstrated in Fig 2 . Assuming that a model  is trained on the dataset  with algorithm A, this could be denoted as \n\nwhere  \u2208 Z. Here, Z is the space of data items and model  is in a hypothesis space H . If a user wants to remove his data   from the trained model, then the server removes the contribution of   using a machine unlearning algorithm U, which is denoted as \n\nAfter providing an unlearning request, the unlearned model U (,   , A ()) is expected to be same as the retrained model A ( \\   ). The evaluation distance metrics in the verification process are diverse, including \n\nand \n\nwhere  A (\\  ) is the model parameters of retraining from the scratch and  U (,  ,A ( ) ) is the parameters of unlearning algorithms U (\u2022). This technique is crucial for addressing privacy concerns, correcting errors, and maintaining model relevance as data evolves. By effectively \"forgetting\" certain data, models can comply with privacy regulations, such as General Data Protection Regulation (GDPR) [108]. Machine unlearning enhances model efficiency and fairness by ensuring that outdated, incorrect, or biased data does not negatively impact predictions [175], while avoiding the computational cost and time required for complete model retraining. \n\nThe theoretical foundation of machine unlearning is inspired by the brain's ability to selectively forget and rewire neural connections, a process known as synaptic pruning [186]. This mechanism enables the brain to maintain cognitive efficiency by removing less useful or redundant information, thereby optimizing memory storage and retrieval.",
            "score": 0.5619843217561165,
            "section_title": "Machine",
            "char_start_offset": 37878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 1608,
                    "end": 1613,
                    "matchedPaperCorpusId": "56699980"
                },
                {
                    "start": 1765,
                    "end": 1770,
                    "matchedPaperCorpusId": "267499920"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055206298828125
        },
        {
            "corpus_id": "278602095",
            "title": "Layered Unlearning for Adversarial Relearning",
            "text": "We briefly review the relevant literature. \n\nUnlearning for LLMs. Machine unlearning for large language models (LLMs) has become an active area of research (Lu et al., 2022;Jang et al., 2022;Kumar et al., 2022;Zhang et al., 2023;Pawelczyk et al., 2023;Eldan & Russinovich, 2023;Ishibashi & Shimodaira, 2023;Yao et al., 2023;Maini et al., 2024;Zhang et al., 2024b;Li et al., 2024b;Wang et al., 2024;Jia et al., 2024;Liu et al., 2024b;a;Thaker et al., 2024;Kadhe et al., 2024;Fan et al., 2025;Zhang et al., 2024a). Due to the difficulty of exact unlearning, most existing methods adopt approximate strategies, including model optimization (Ilharco et al., 2022;Liu et al., 2022;Yao et al., 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024b;Li et al., 2024b) and prompt-based or in-context learning techniques (Thaker et al., 2024;Pawelczyk et al., 2023;Liu et al., 2024a). However, recent work has shown that these models often remain vulnerable to adversarial attacks (Schwarzschild et al., 2024;Patil et al., 2024;Lynch et al., 2024) or to relearning from small fragments of previously seen data (Hu et al., 2024;Lynch et al., 2024). These findings highlight the persistent challenges in achieving robust unlearning in LLMs. \n\nAdversarial relearning. Adversarial relearning attacks exploit residual knowledge after unlearning by fine-tuning on a small subset of forgotten data, aiming to recover information about the full unlearned set.",
            "score": 0.5614219435097944,
            "section_title": "Related work",
            "char_start_offset": 17557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 45,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1455
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 173,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 659,
                    "end": 676,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "272770202",
            "title": "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
            "text": "In this paper, we explore factual knowledge unlearning in Large Language Models (LLMs) and find that it can result in nonsensical responses on knowledge related to forgotten entities, especially when only negative feedback is used or positive feedback is applied incorrectly. To address this, we propose AltPO, a fine-tuning approach that combines negative feedback with in-domain positive feedback on the forget set, ensuring more stable and effective unlearning. We also identify limitations in existing evaluation metrics and introduce new ones to offer a more comprehensive assessment of the unlearned model. We hope our findings offer valuable insights for practitioners in LLM unlearning, promoting the use of alternate answers for more effective unlearning and improving the evaluation of model performance post-unlearning.",
            "score": 0.5613976101909433,
            "section_title": "Conclusion",
            "char_start_offset": 25341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 830
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "270619676",
            "title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models",
            "text": "There are two ways to apply unlearning: batch unlearning and sequential unlearning.The results shown in Table 2 demonstrate batch unlearning re-sults, in which all target sequences are unlearned at once.In sequential unlearning, target sequences are split into smaller batches, which are unlearned in succession.Although batch unlearning is important to consider, sequential unlearning is a more likely real-world scenario, as unlearning requests will follow a sporadic pattern, requiring a more flexible solution.\n\nWe sequentially unlearn 320 target sequences, split into 10 batches.Results for other models are available in Appendix D. As shown in Fig. 3, POP demonstrates better retention of performance in both classification and dialogue tasks compared to UL.After unlearning all 320 target sequences in 10 batches with UL, the performance of the OPT 2.7B model dropped over 18% in average classification accuracy, and 13% in average dialogue F1 score.The performance degradation in the dialogue task is extreme, as the average F1 score dropped to 0.47%, demonstrating catastrophic forgetting of general LM capabilities.Furthermore, the performance in both sets of benchmarks reaches the minimum value after 2 batches, demonstrating the major flaw in UL.POP, however, only demonstrates a moderate drop, demonstrating a decrease of 9.6% for the average classification accuracy and 7.14% for the average dialogue F1 score.Fig. 1 illustrates a qualitative example of the degradation in LM from UL.After the sequential unlearning of 10 batches with UL and POP, sequences are generated for a given prefix.The generated sequence from the LM unlearned with the UL method demonstrates catastrophic degradation, while the LM unlearned with POP generates an acceptable response.UL is not a viable option, as repeated unlearning in succession with UL results in a catastrophic failure of LMs.On the other hand, POP successfully induces the LM to unlearn the target sequences and does not significantly impact the LM performance.After unlearning, we generate sequences with the resulting models, and compute BLEU and CHRF scores, where a lower score is favorable, as it indicates less overlap between the sequences.",
            "score": 0.5613929994982269,
            "section_title": "Sequential Unlearning",
            "char_start_offset": 16213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 514
                },
                {
                    "start": 516,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 764
                },
                {
                    "start": 764,
                    "end": 957
                },
                {
                    "start": 957,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1499
                },
                {
                    "start": 1499,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1773
                },
                {
                    "start": 1773,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2022
                },
                {
                    "start": 2022,
                    "end": 2208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11968994140625
        },
        {
            "corpus_id": "276575899",
            "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
            "text": "The results of the ablation study, presented in Table 3, examine the effects of unlearning and the application of restricting unlearning parameters on CoME MEMIT and CoME PMET . We analyze the impact of removing each component: \u03b4 \u2032 , \u03b4 \u2032\u2032 , and restricting unlearning parameters. \n\nExcluding \u03b4 \u2032 , we observe a decline in performance across most metrics, particularly in Generality and Efficacy. Notably, in CoME MEMIT , performance drops significantly from 91.1 to 88.6. This suggests that the removal of outdated knowledge plays a crucial role in improving the accuracy of knowledge editing. \n\nExcluding \u03b4 \u2032\u2032 primarily affects Locality, where we observe significant performance degradation. This suggests that \u03b4 \u2032\u2032 plays a vital role in preserving the model's ability to handle unrelated information. On the other hand, Fluency shows an upward trend, likely due to the increased capacity to handle structured knowledge, which comes at the cost of Excluding restricting the unlearning parameter method leads to the greatest drop in Locality, while Efficacy and Generality are only slightly affected. This shows that unlearning is effectively performed only on the top-p% of parameters where outdated knowledge resides, preventing unnecessary parameter updates without sacrificing accuracy. \n\nUnlearning Weight Variation To control the degree of outdated knowledge removal, we introduce the hyperparameter \u03b1. Figure 3 shows the effect of varying \u03b1 from 0 to 2 on performance metrics such as Score, Efficacy, Generality, and Locality. We observe that both Efficacy and Generality increase as \u03b1 rises, indicating that more effective removal of outdated knowledge improves model performance. However, Locality decreases as \u03b1 increases, suggesting that excessive knowledge re-moval may negatively impact unrelated information. Based on these findings, we use \u03b1 = 0.1 as the default setting, and restrict the unlearning scope to minimize the drop in Locality.",
            "score": 0.5613750105732866,
            "section_title": "Ablation Study",
            "char_start_offset": 20083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 279
                },
                {
                    "start": 282,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 593
                },
                {
                    "start": 596,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365478515625
        },
        {
            "corpus_id": "259089053",
            "title": "Machine Unlearning: A Survey",
            "text": "These included one based on item-based collaborative filtering, another based on ridge regression, and the last based on -nearest neighbors. With each machine learning algorithm, the intermediate results are retained, and the model parameters are updated based on the intermediate results and unlearning data   , resulting in an unlearned model. However, this strategy can only be utilized with those models that can be straightforwardly computed to obtain the model parameters after the unlearning process, limiting the applicability of this scheme. \n\nIn addition, Graves et al. [45] also proposed a laser-focused removal of sensitive data, called amnesiac unlearning. During training, the model provider retains a variable that stores which samples appear in which batch, as well as the parameter updates for each batch. When a data unlearning request arrives, the model owner undoes the parameter updates from only the batches containing the sensitive data, that is M   = M  \u2212 \u0394  , where M  is the already-trained model and \u0394  are the parameter updates after each batch. Because undoing some parameters might greatly reduce the performance of the model, the model provider can perform a small amount of fine-tuning after an unlearning operation to regain performance. This approach requires the storage of a substantial amount of intermediate data. As the storage interval decreases, the amount of cached data increases, and smaller intervals lead to more efficient model unlearning. Therefore, a trade-off exists between efficiency and effectiveness in this method. \n\nThe above methods mainly focused on the core problem of empirical risk minimization, where the goal is to find approximate minimizers of the empirical loss on the remaining training dataset after unlearning samples [41,51]. Sekhari et al. [42] proposed a more general method of reducing the loss of unseen samples after an unlearning process. They produced an unlearned model by removing the contribution of some samples from an already-trained model using a disturbance update calculated based on some cheap-to-store data statistics during training. In addition, they proposed an evaluation parameter to measure the unlearning capacity. They also improved the data unlearning capacity of convex loss functions, which saw a quadratic improvement in terms of the dependence of d over differential privacy, where d is the problem dimension.",
            "score": 0.5613619180176365,
            "section_title": "2). Fisher unlearning method",
            "char_start_offset": 52751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2410
                }
            ],
            "ref_mentions": [
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 1787,
                    "end": 1791,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 1791,
                    "end": 1794,
                    "matchedPaperCorpusId": "232033672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0704345703125
        },
        {
            "corpus_id": "273228310",
            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
            "text": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.",
            "score": 0.5606612849008221,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392333984375
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a\"flat\"loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.",
            "score": 0.560623597929189,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39599609375
        },
        {
            "corpus_id": "274141503",
            "title": "LoRA Unlearns More and Retains More (Student Abstract)",
            "text": "Due to increasing privacy regulations and regulatory compliance, Machine Unlearning (MU) has become essential. The goal of unlearning is to remove information related to a specific class from a model. Traditional approaches achieve exact unlearning by retraining the model on the remaining dataset, but incur high computational costs. This has driven the development of more efficient unlearning techniques, including model sparsification techniques, which boost computational efficiency, but degrade the model\u2019s performance on the remaining classes. To mitigate these issues, we propose\na novel method, PruneLoRA which introduces a new MU paradigm, termed prune first, then adapt, then unlearn. LoRA reduces the need for large-scale parameter updates by applying low-rank updates to the model. We leverage LoRA to selectively modify a subset of the pruned model\u2019s parameters, thereby reducing the computational cost, memory requirements and improving the model\u2019s ability to retain performance on the remaining classes. Experimental Results across various metrics showcase that our method outperforms other approximate MU methods and bridges the gap between exact and approximate unlearning. Our code is available at\nhttps://github.com/vlgiitr/LoRA-Unlearn.",
            "score": 0.560562229496721,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.181396484375
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Machine unlearning has been used as a solution to address privacy and copyright issues in the text generation process of language models. Notable examples include gradient ascent-based methods (Jang et al., 2023;Yao et al., 2023;Barbulescu and Triantafillou, 2024), preference optimization approaches (Rafailov et al., 2024;Zhang et al., 2024;Jin et al., 2024), and representation learning techniques (Li et al., 2024a;Yao et al., 2024). \n\nHowever, the effectiveness of these methods has not been clearly demonstrated, prompting prior studies to introduce benchmarks in the field of unlearning to assess them. Eldan and Russinovich (2023); Shi et al. (2024); Tian et al. (2024) have aimed to unlearn the knowledge of copyrighted texts (e.g., BBC News and Harry Potter book) in a language model. Li et al. (2024a) have introduced a benchmark dealing with hazardous knowledge in various professional domains (e.g., biosecurity and cybersecurity). Maini et al. (2024); Jin et al. (2024) have proposed benchmarks for unlearning various entities. Specifically, Maini et al. (2024) have created synthetic entity profiles and removed their knowledge from a language model. Jin et al. (2024) have tried to unlearn the knowledge about real-world entities and evaluated the knowledge memorization in various forms of assessment (e.g., cloze test and question answering). However, existing studies remain limited as they have only examined independent knowledge and overlooked the intricate nature of world knowledge. World knowledge is highly complex and interconnected, which means that unlearning the target knowledge requires examining related knowledge carefully. Our research focuses on this aspect, examining and facilitating faithful unlearning. \n\n3 The FAITHUN Benchmark",
            "score": 0.5605003103371794,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 4422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 301,
                    "end": 324,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.155517578125
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "Table 2 shows the results of our experiments across various unlearning methods and tasks. Our proposed method consistently outperforms baseline approaches for both LLaMA2 and Mistral models. For LLaMA2 (7B), we achieve 'Forget' and 'Retain' scores of 85.0% and 80.0%, respectively for in-domain data, significantly surpassing the zeroshot baseline. Out-of-domain performance remains strong with 92.3% 'Forget' and 42.7% 'Retain' scores. LLaMA2 (13B) shows even better results, particularly for in-domain scenarios, with perfect 'Forget' scores (100.0%) and high 'Retain' scores (80.0%). Mistral (7B) demonstrates comparable performance, notably achieving high 'Retain' scores (74.4%) in out-of-domain settings, indicating robust knowledge preservation during unlearning. \n\nOur method maintains competitive performance on standard NLP tasks such as BoolQ, HellaSwag, and WinoGrande, with minimal degradation compared to baseline models. This suggests that the unlearning process does not significantly impact the model's general language understanding capabilities. Compared to other unlearning methods like Few-Shot Prompting, Gradient Ascent, and In-Context Unlearning, our approach consistently achieves a better balance between forgetting targeted information and retaining general knowledge. \n\nThese results demonstrate the effectiveness of our in-context knowledge unlearning method in enabling large language models to selectively forget information while maintaining overall performance across various NLP tasks.",
            "score": 0.5603958000988873,
            "section_title": "Performance Results",
            "char_start_offset": 11766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43603515625
        },
        {
            "corpus_id": "277856836",
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "text": "Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
            "score": 0.5602143966486348,
            "section_title": "Introduction",
            "char_start_offset": 1726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 98,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 965,
                    "end": 990,
                    "matchedPaperCorpusId": "267365485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "269187650",
            "title": "Offset Unlearning for Large Language Models",
            "text": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.",
            "score": 0.5594779654012173,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "273228310",
            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
            "text": "Furthermore, our method also focuses on using only the forget set for unlearning. We recognize the inherent trade-off between unlearning performance and retaining performance on the retain set. Relying on a single model to address this trade-off is inefficient. To overcome this, we propose a new approach that utilizes multiple fine-tuned models. By building on task arithmetic, our method computes a more effective task vector from these models, enhancing unlearning performance. \n\nModel Merging. The concept of Model soups (Wortsman et al., 2022) addresses inefficiencies in the validation process, where many models are discarded, and only the best one is retained. This approach advocates for merging the weights of sub-optimal models to enhance generalization performance without additional computational demands. Following this insight, more advanced model merging techniques have emerged. Task Arithmetic (Ilharco et al., 2022)  Each point represents the accuracy of the forget set and the retain set. For the forget set, 1 -accuracy on the forget set is used for better visibility. The green points indicate the results of models fine-tuned with various hyperparameters, while the points in other colors denote results from different methods, including ours. This experiment uses the CLIP ViT-B-32 model on the Cars dataset (Krause et al., 2013). (b), (c): The accuracy distribution for different hyperparameter choices on the forget and retain set, respectively. We observe that 1) the models trained under varied hyperparameters exhibit different unlearning capabilities; 2) smartly utilizing them could improve the capability without concerns on hyperparameter sensitivity (ours). \n\ntask vector, demonstrating that merging these vectors can effectively enhance a model's multi-task capabilities. TIES-Merging (Yadav et al., 2024) refines the merging process by incorporating a trimming step and, in cases of sign conflicts, selects one sign through a voting process, merging all task vectors corresponding to the chosen sign. AdaMerging (Yang et al., 2024) autonomously learns the coefficients for merging models, either task-wise or layer-wise, and does so without depending on the original training data.",
            "score": 0.5593666606745828,
            "section_title": "RELATED WORK",
            "char_start_offset": 6155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1692
                },
                {
                    "start": 1695,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 549,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1333,
                    "end": 1354,
                    "matchedPaperCorpusId": "14342571"
                },
                {
                    "start": 1821,
                    "end": 1841,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2359619140625
        },
        {
            "corpus_id": "276928229",
            "title": "How Does Overparameterization Affect Machine Unlearning of Deep Neural Networks?",
            "text": "Approximate unlearning methods aim to approximate a model retrained from scratch without the unlearned examples. This popular approach offers computational efficiency but may not fully remove the effect of the unlearned data. We consider the following approximate unlearning methods in this work: \u2022 SCRUB Unlearning [9]: SCRUB is a data-driven unlearning method that uses a teacher-student model approach. The original model (the teacher) remains unchanged, while a new model (the student) is trained to imitate the teacher on the data that should be retained and diverge from the teacher on the data that should be unlearned. This divergence is achieved by maximizing the Kullback-Leibler (KL) divergence between the teacher and student predictions for the examples to unlearn, appearing as the third sum in the following optimization: \n\nwhich is solved by iterative optimization that is initialized to the original model w 0 . Here l CE (\u2022, \u2022) represents the cross-entropy loss; d KL (x; w 0 , w) measures the KLdivergence between the softmax probabilities of the student w and teacher w 0 for an input x; and \u03b1 and \u03b3 are scalar hyperparameters to determine the importance of the optimization objective terms (including to determine the importance of the retain and forget sets). \u2022 Negative Gradient (NegGrad) [6,9]: Similar to finetuning, NegGrad starts from the original model and finetunes it both on data from the retain and forget sets, but negates the gradient for the latter. The unlearning optimization is \n\nwhich is solved by iterative optimization, initialized to the original model w 0 . The hyperparameter \u03b1 balances the contributions of the retain set and the forget set. \u2022 L1 Sparsity Unlearning [11]: This method employs model pruning for machine unlearning, implemented by solving an optimization problem that modifies the original parameters w 0 using only the retained training data D r , and an \u2113 1 -norm penalty that sparsifies the unlearned model parameters: \n\nThis optimization is solved by iterative optimization, initialized to the original model w 0 .",
            "score": 0.5591840534960301,
            "section_title": "The Examined Unlearning Methods",
            "char_start_offset": 7533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1281
                },
                {
                    "start": 1282,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1981
                },
                {
                    "start": 1984,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 1312,
                    "end": 1315,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1315,
                    "end": 1317,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154541015625
        },
        {
            "corpus_id": "271892278",
            "title": "Fishers Harvest Parallel Unlearning in Inherited Model Networks",
            "text": "The FIUn method estimates the unlearning FIM for all nodes within the unlearning graph, including both discovery and inherited nodes (see line 9, Algorithm 1). The unlearning FIMs from discovery nodes are merged to form a collective unlearning FIM for each node in the unlearning graph. This merger is crucial as the linkages between the unlearning data and parameters are embedded within the model and can persist through model inheritance [13], especially in multi-root scenarios where an inherited node may trace back to several discovery nodes. A novel merging FIM function is introduced, denoted by \u03a6 (\u2022), and is defined as \n\nwhere F   is the merged unlearning FIM for the -th node in the unlearning graph, the set { F  , } collects the unlearning FIMs of discovery nodes within \u011c that can be traced back from node   , and  denotes the -th discovery node reachable from   . In this paper, the element-wise maximum is employed for the merging FIM function. Specifically, F   assembles the largest elements among the unlearning FIMs of discovery nodes in multi-root scenarios and takes the only unlearning FIM in single-root scenarios. \n\nAs shown in Figure 4, the model  \u210e inherits from models   ,   , and   and needs to remove all requested unlearning data, though each discovery node contains only a portion of the unlearning data. The FIM of each model reflects the sensitivity of its parameters to the unlearning data, which can be interpreted as parameter importance. By taking the maximum of the corresponding elements across these unlearning FIMs, the merged FIM encompasses all unlearning tasks from the discovery nodes and effectively identifies the parameters that need updating to achieve complete unlearning. Model  \u210e inherits from these three discovery nodes, so we merge the unlearning FIMs of the discovery nodes and perform the unlearning of label 1 and label 2 on model  \u210e together.",
            "score": 0.5590876394505051,
            "section_title": "Merge unlearning FIMs for all nodes in the unlearning graph.",
            "char_start_offset": 18104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 628
                },
                {
                    "start": 631,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1902
                }
            ],
            "ref_mentions": [
                {
                    "start": 441,
                    "end": 445,
                    "matchedPaperCorpusId": "260900355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08331298828125
        },
        {
            "corpus_id": "270045257",
            "title": "Machine Unlearning in Large Language Models",
            "text": "In this section, we present the methodology employed for unlearning in the context of language models Yuanshun et al. [2023].Our approach involves updating the language model parameters at each training step, aiming to forget undesirable outputs while preserving normal utility.The update formula is expressed as follows:\n\nwhere \u03f5 i \u2265 0 are hyperparameters weighing different losses.Let's delve into the details of the introduced loss functions L fgt , L rdn , and L nor .\n\nConsider h \u03b8 (x, y < i) := P (y i |(x, y < i); \u03b8) as the predicted probability of token y i by the language model \u03b8, conditioned on prompt x and previously generated tokens y < i := [y 1 , . . ., y i\u22121 ].For a given prompt-output pair (x, y) and language model \u03b8, the loss on y is defined as:\n\nwhere \u2113(\u2022) is the cross-entropy loss.\n\nLet Y rdn be a set of random (non-harmful) responses unrelated to unlearned prompts x fgt , constructed by gathering irrelevant responses from the normal dataset.The three losses in Equation ( 1) are given by:\n\nwhere KL(\u2022) represents the KL divergence term.\n\nL fgt is the gradient ascent loss designed to forget unlearned samples, calculated exclusively on y fgt .\n\nL rdn forces the language model to predict a random output y rdn for the unlearned prompt x rdn , reinforcing forgetting by introducing irrelevance into the predicted outcome.This concept aligns with the idea of label smoothing in classification.\n\nL nor aims to preserve normal utility by comparing the predicted distribution of the unlearned model with the original language model through forward KL divergence.",
            "score": 0.5588888699762944,
            "section_title": "Unlearning",
            "char_start_offset": 10078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 321
                },
                {
                    "start": 323,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 472
                },
                {
                    "start": 474,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 766
                },
                {
                    "start": 768,
                    "end": 805
                },
                {
                    "start": 807,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1016
                },
                {
                    "start": 1018,
                    "end": 1064
                },
                {
                    "start": 1066,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1419
                },
                {
                    "start": 1421,
                    "end": 1585
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "273638595",
            "title": "Applying sparse autoencoders to unlearn knowledge in language models",
            "text": "The goal of an unlearning technique is to remove knowledge from the model, while limiting the damage to the model's performance in all other domains. Therefore, there are two key factors that need to be quantified; the amount of knowledge removed, and the side-effects caused by the modification to the model. \n\nThe primary metric for unlearning that we consider is the number of correct answers out of the subset of questions that the base model gets correct under all 24 permutations. We also looked at the probabilities assigned to the correct answers and the impact of re-ordering the 4 multiple choice options. In addition, we performed some specific case studies of the model's completion, with non-multiple choice based prompts, based on the same information as tested in the questions. \n\nWe quantified the side effect for the model in two different ways, (i) the accuracy on an unrelated multiple choice dataset (Measuring Massive Multitask Language Understanding, MMLU) and (ii) the loss added over 50k tokens of OpenWebText. The multiple choice dataset contained questions related to high school US history, geography, college computer science and human aging. These questions allowed us to both check for the removal of specific unrelated knowledge, and also to ensure that we were not simply damaging the multiple-choice answering ability of the model. Similar to the WMDP-bio dataset, we only select the questions that the base model gets correct answers for under 24 permutations, resulting in 97 questions for gemma-2b-it and 300 questions for gemma-2-2b-it. \n\nWe also compare our unlearning results to an existing technique, the Representation Misdirection for Unlearning (RMU, Li et al. 2024). RMU is a fine-tuning based approach for unlearning information from a language model. It involves fine-tuning model weights at 3 layers within the model using a combination of two loss terms. These terms are (i) the \"forget loss\", which changes the direction and scales the norm of model activations on a dataset containing information that you want to unlearn and (ii) the \"retain loss\", which preserves model activations based on a Wikitext dataset.",
            "score": 0.558274491766868,
            "section_title": "UNLEARNING METRICS",
            "char_start_offset": 2411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1573
                },
                {
                    "start": 1576,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2162
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2113037109375
        },
        {
            "corpus_id": "271710002",
            "title": "Controllable Unlearning for Image-to-Image Generative Models via \u03b5-Constrained Optimization",
            "text": "Therefore, we reformulate the bi-objective optimization problem into a \u03b5-constrained optimization problem, where the unlearning objective is treated as a constraint (primary to satisfy) and \u03b5 is the control coefficient. Utilizing gradient-based methods to solve this \u03b5-constrained optimization, we can obtain two Pareto optimal solutions for the boundaries of unlearning with theoretical guarantee, which can be used to determine the valid range of values for \u03b5. Subsequently, we select the value of \u03b5 within its valid range and relax the constraints on the unlearning objective by increasing \u03b5. As a result, we obtain a set of solutions that dynamically fulfill user's varied expectations regarding the trade-off between unlearning completeness and model utility. Finally, to enhance the efficiency of unlearning, we analyze the convergence rates of our unlearning framework under various settings of the control function which is utilized to govern the direction of parameter updates. The main contributions of this paper are summarized as follows: \n\n\u2022 We focus on I2I generative models, and propose a controllable unlearning approach that balances unlearning completeness and model utility, providing a set of solutions to fulfill varied user expectations. To the best of our knowledge, we are the first to study controllable unlearning. \u2022 We reformulate the machine unlearning of generative models as a \u03b5-constrained optimization problem with unlearning the forget set as the constraint, guaranteeing optimal theoretical solutions for the boundaries of unlearning. By progressively relaxing the unlearning constraint, we obtain the Pareto set and plot the corresponding Pareto front. \n\n\u2022 We utilize gradient-based methods to solve the \u03b5-constrained optimization problem. To enhance the efficiency of unlearning, we analyze our framework's performance across different settings of the control function and validate with multiple combinations. \u2022 We conduct extensive experiments to evaluate our proposed method over diverse I2I generative models. The results from two large datasets demonstrate that the Pareto optimal solutions yielded by our method significantly outperform baseline methods. Additionally, the solution set achieves controllable unlearning to fulfill varied expectations regarding the trade-off between unlearning completeness and model utility. \n\n2 Related Work",
            "score": 0.5581579324013288,
            "section_title": "Introduction",
            "char_start_offset": 3842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1687
                },
                {
                    "start": 1690,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2365
                },
                {
                    "start": 2368,
                    "end": 2382
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132080078125
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "score": 0.5581422648048318,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417724609375
        },
        {
            "corpus_id": "276938085",
            "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models",
            "text": "In the following, we present several representative methods for unlearning, each addressing ways to remove or preserve retention performance, while striving to mitigating the trade-off between the two goals. \n\nGradient ascent (GA) is one of the most fundamental unlearning methods, which minimizes the log-likelihood for targeted data. The unlearning objective of GA is \n\nwhich directly reduces the probabilities of generating contents resembling D u to approach zero, thereby leading to effective knowledge removal. However, due to its extremely large strengths of gradient updates, the resulting GAunlearned models will suffer from excessive unlearning [11], where the model responses for non-targeted data will also be damaged, and thus it is clear that GA fails at retention performance. This motivates a series of subsequent works in order to improve the retention performance of the resulting models as a result of the study. \n\nGradient Difference (GD) regularizes GA with a retain dataset D r of size m \u2032 , typically sampled from D t \\ D u and m \u2032 \u226a m. These data represent the knowledge that should be preserved. The associated retain loss, which is given by \n\nserves as regularization in conjunction with GA, namely, \n\nwhere \u03bb is a trade-off hyper-parameter, typically set to 1. However, many previous works [14] reveal that the unlearning term, i.e., L GA (D u ; \u03b8), tends to dominate the dynamics of gradient updates. Therefore, GD may still strongly impact retention performance negatively. \n\nNegative Preference Optimization (NPO) [15] directly refines the objective of GA to mitigate excessive unlearning, of which the formulation is motivated by direct preference optimization, a well-known preference alignment method [25]. NPO segregates the dis-preferred part from DPO, heuristically employing it as the unlearning objective, following the formulation of \n\nwhere \u03b2 is the inverse temperature and \u03b8 org denotes model parameters before unlearning. The effects of NPO in mitigating excessive unlearning can be understood through its gradients, which are equivalent to GA with extra reweighting [15]. This weighting mechanism pays more attention to data that have small impacts on retention.",
            "score": 0.5580959961713481,
            "section_title": "Unlearning Methods",
            "char_start_offset": 8811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1502
                },
                {
                    "start": 1505,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35888671875
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2025). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024;Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;Wu et al., 2023;Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024). Architecture modification based approaches involve tweaking the model's architecture such as by adding additional layers (Chen and Yang, 2023), or by using external modules (Ji et al., 2024;Zhang et al., 2023) in addition to the target model, etc. These approaches, while advantageous in other contexts, were not suitable for this specific task's setup.",
            "score": 0.5580778866162122,
            "section_title": "Related works",
            "char_start_offset": 2724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 223,
                    "matchedPaperCorpusId": "268856658"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "273901406"
                },
                {
                    "start": 618,
                    "end": 637,
                    "matchedPaperCorpusId": "271909642"
                },
                {
                    "start": 1180,
                    "end": 1196,
                    "matchedPaperCorpusId": "264816202"
                },
                {
                    "start": 1788,
                    "end": 1809,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "On the other hand, model deployers want to effectively accommodate many successive unlearning requests (sustainability) on various sizes of forget sets (scalability) without degrading the general model capabilities (utility preservation).\n\nWe apply MUSE to evaluate eight representative machine unlearning algorithms ( \u00a74) on two datasets ( \u00a73.2), focusing on the specific cases of unlearning Harry Potter books and news articles.Our findings indicate that most unlearning algorithms remove verbatim memorization and knowledge memorization with varying degrees of efficacy but operate at the cost of utility preservation and do not effectively prevent privacy leakage ( \u00a75.2).In particular, negative preference optimization (NPO; Zhang et al., 2024b) and task vectors (Ilharco et al., 2023) are especially effective in removing these types of memorization, but we find that NPO often permits privacy leakage and both methods induce a sharp drop in the utility of the model.Furthermore, testing their scalability and sustainability reveals that they both algorithms struggle with large forget sets and successive unlearning requests ( \u00a75.3).\n\nOur results highlight that unlearning algorithms generally fail to meet data owner expectations in preventing privacy leakage, which is one of the primary motivations for unlearning.Additionally, Table 1: Comparison with a previous benchmark: Unlike the previous benchmark TOFU (Maini et al., 2024), which evaluates unlearning on synthetic Q&A datasets, MUSE tackles real-world unlearning challenges: unlearning real-world large-scale corpus (22\u00d7 larger) while taking into account six desiderata that are important to both data owners and deployers.More related works are discussed in Appendix 6.",
            "score": 0.5578970619282233,
            "section_title": "Introduction",
            "char_start_offset": 2972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 240,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1691
                },
                {
                    "start": 1691,
                    "end": 1738
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "In this section, we prodive discussion on the difference between quantization and other methods in evaluating robust unleanring in LLMs: \n\n\u2022 Our motivation is different from attacking the unlearned model and forcing it to recover the knowledge. Instead, we consider the problem from the perspective that, as quantization becomes increasingly popular in the era of LLMs, it is important to evaluate whether existing unlearning methods remain effective when the unlearned model is quantized. \n\n\u2022 The performance of forcing an unlearned model to generate the supposed forgotten knowledge is poor (Lynch et al., 2024). Specifically, the unlearned model retains only 10% of the knowledge, and none of the attack methods (including Other Language, Jailbreaks, In-Context Extraction, Fine-Tuning, Downstream Task, Latent Knowledge, Prompting Baselines, Side Effects) achieve the goal of forcing the unlearned model to retain 20% of the knowledge. These results are much less effective compared to our empirical findings. Moreover, the attack methods mentioned in (Lynch et al., 2024), such as Jailbreak and In-context Learning, require significantly more effort to recover knowledge from an unlearned model compared to the simplicity and effectiveness of quantization.",
            "score": 0.5576570984124272,
            "section_title": "M COMPARISON WITH OTHER METHODS FOR EVALUATING ROBUST UNLEARNING IN LLMS",
            "char_start_offset": 55466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 139,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1261
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "276259498",
            "title": "LUNAR: LLM Unlearning via Neural Activation Redirection",
            "text": "We experiment with several unlearning methods summarized in the survey paper (Liu et al., 2024;Maini et al., 2024), each of which is introduced in detail in the section. \n\nGA-based methods. A major branch of LLM unlearning methods is built on the concept of performing Gradient Ascent (GA) on the forget data (Jang et al., 2022;Yao et al., 2023), which is mathematically equivalent to applying Gradient Descent on the negative cross-entropy loss function (Eq. 11). The objective of GA is to maximize the likelihood of mispredictions for samples in the forget set, effectively reducing the model's ability to recall or generate the unlearned information. \n\nSeveral unlearning methods build upon GA to improve the tradeoff between forget quality and model utility by linearly combining an additional loss term with the GA loss. Gradient Difference (GD) method (Liu et al., 2022) extends the GA approach by optimizing two distinct loss functions: one to maximize mispredictions on the forget set and another to minimize mispredictions on the retained set. Another GA-based variant (GA + KL) aims to minimize the Kullback-Leibler (KL) divergence between the predictions of the original fine-tuned model and the unlearned model on the retained set (Maini et al., 2024). These dual-objective framework aims to balance effective forgetting with the preservation of model utility. \n\nPreference optimization-based methods. DPO (Rafailov et al., 2024) is a preference alignment method that aligns the model to avoid disclosing information from the forget set by computing loss using question-answer pairs x idk = [q, a idk ] from the forget set D f , with answers replaced by variations of 'I don't know'. Unlike GA and its variants, DPO does not employ gradient ascent. Drawing inspiration from DPO, NPO (Zhang et al., 2024a) focuses on generating only negative responses to given instructions, without providing any positive or informative answers. The method optimizes exclusively for these negative responses, ensuring the model avoids revealing information from the forget set while maintaining stability. \n\nHyperparameters.",
            "score": 0.5574326061946333,
            "section_title": "D. Unlearning method baselines",
            "char_start_offset": 41457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 172,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2119
                }
            ],
            "ref_mentions": [
                {
                    "start": 858,
                    "end": 876,
                    "matchedPaperCorpusId": "247627962"
                },
                {
                    "start": 1418,
                    "end": 1440,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11456298828125
        },
        {
            "corpus_id": "278368492",
            "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
            "text": "Machine unlearning has become a vital research area to address privacy, safety, and bias in LLMs (Yao et al., 2024;Jang et al., 2023;Eldan and Russinovich, 2023;Pawelczyk et al., 2024;Li et al., 2024b;Liu et al., 2024a). Classic methods, such as exact unlearning (Bourtoule et al., 2021), involve retraining models without target data but are expensive for large models. Recent work focuses on approximate unlearning techniques, including incremental updates, pruning, and knowledge distillation, to enhance efficiency (Dong et al., 2024). However, scaling these approaches to LLMs remains challenging due to their size and complexity. \n\nEfficient unlearning techniques for LLMs have been proposed, including gradient ascent and descent methods (e.g., GA and GA+GD), which achieve unlearning objectives but often compromise performance (Yao et al., 2024). Prompt-based approaches steer outputs away from unlearning targets without modifying model parameters, reducing computational costs but risking memory reactivation (Liu et al., 2024a). Training-free methods, such as task arithmetic (Ilharco et al., 2023), provide simplicity and efficiency but face limitations in closed models with restricted architectures. \n\nConcept replacement methods, such as WHP (Eldan and Russinovich, 2023), employ an anchorgeneric term framework to \"forget\" specific targets while retaining related concepts. However, WHP has demonstrated limitations in achieving complete unlearning (Shi et al., 2024). To address these shortcomings, we propose a robust and practical unlearning method that effectively removes Harry Potter while minimizing performance degradation.",
            "score": 0.5573108608262713,
            "section_title": "A Related work A.1 Machine Unlearning",
            "char_start_offset": 26829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1648
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 115,
                    "end": 133,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 161,
                    "end": 184,
                    "matchedPaperCorpusId": "263834631"
                },
                {
                    "start": 184,
                    "end": 201,
                    "matchedPaperCorpusId": "268247897"
                },
                {
                    "start": 201,
                    "end": 219,
                    "matchedPaperCorpusId": "270392045"
                },
                {
                    "start": 836,
                    "end": 854,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 1020,
                    "end": 1039,
                    "matchedPaperCorpusId": "270392045"
                },
                {
                    "start": 1088,
                    "end": 1110,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1466,
                    "end": 1484,
                    "matchedPaperCorpusId": "10488675"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.390869140625
        },
        {
            "corpus_id": "271543835",
            "title": "Machine Unlearning in Generative AI: A Survey",
            "text": "[61] first proposes the concept of task vector, which can be obtained by taking the difference between the original model weights of a pre-trained model and its weights fine-tuned on a specific task. In particular, if we let     be the corresponding weights after fine-tuning on task , the task vector is then denoted as   =     \u2212   . Then, taking the element-wise negation of the task vector   can enable   to forget target knowledge on task  without jeopardizing irrelevant knowledge, resulting in an unlearned model that has weight of   =   \u2212  with  as a scaling term. One exemplar work is SKU [96], which designs a novel unlearning framework to eliminate harmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted of two stages where the first stage aims to identify and acquire harmful knowledge within the model, whereas the second stage targets to remove the knowledge using element-wise negation operation. Different from pure gradient ascent approaches where the model locality is largely compromised, SKU collectively aggregates the target unlearned knowledge D using gradient decent approach in the first stage and remove it from the pre-trained model. However, SSU [29] identifies the potential instability of the pure task vector approach in the case of multiple rounds of unlearning (i.e., sequential unlearning) and introduces a more stable unlearning framework integrated with weight saliency. \n\nBesides subtracting undesirable parameters, Pochinkov and Schoots [127] proposes a selective pruning method to trim those neurons' relative importance to different datasets, representing target model capability for unlearning. In particular, it performs either iterative pruning on nodes in the feed-forward layers or attention head layers. This selective approach utilizes importance functions that assess the contribution of individual neurons to specific tasks by measuring activation frequencies and magnitudes, enabling precise targeting of neurons that are crucial for the undesired capabilities. Furthermore, different from previous weight pruning method, where it requires Hessian computation, the selective neuron pruning is more computational efficient for large language models because it directly removes neurons that contribute the most to the unwanted behavior.",
            "score": 0.557300888671767,
            "section_title": "Safety Alignment",
            "char_start_offset": 47082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15380859375
        },
        {
            "corpus_id": "250450914",
            "title": "Federated Unlearning: How to Efficiently Erase a Client in FL?",
            "text": "For instance, our method can reduce the communication cost compared to retraining by 5\u00d7 to 24\u00d7. We rigorously evaluate our unlearning method by employing three performance measures adapted from Warnecke et al. (2023): efficacy (which measures success in removing the influence of data to be erased), fidelity (which measures performance on data to be retained), and efficiency (which measures costs compared to retraining from scratch). 3. Our key novelty is to formulate local unlearning problem as a constrained maximization problem, wherein the client to be erased maximizes their local loss while restricting the model parameters to an \u2113 2 -norm ball around a suitably chosen reference model obtained from the other clients' local models. Our formulation allows the client to efficiently perform local unlearning by using the Projected Gradient Descent (PGD). Starting with the locally unlearned model enables the server and the remaining clients to obtain the global unlearned model using very few FL rounds, resulting in significant efficiency gains over retraining.",
            "score": 0.5572153480841426,
            "section_title": "Introduction",
            "char_start_offset": 3925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1072
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 216,
                    "matchedPaperCorpusId": "237303799"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153564453125
        },
        {
            "corpus_id": "271909642",
            "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models",
            "text": "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities.",
            "score": 0.5569635039511975,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.372802734375
        },
        {
            "paperId": "ecb02ad28d05a1d97454540732b2e7343a75eb49",
            "corpusId": 278782460,
            "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 70,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323938152",
                    "name": "Miao Yu"
                },
                {
                    "authorId": "2362620538",
                    "name": "Liang Lin"
                },
                {
                    "authorId": "2232926268",
                    "name": "Guibin Zhang"
                },
                {
                    "authorId": "2349829666",
                    "name": "Xinfeng Li"
                },
                {
                    "authorId": "2362314471",
                    "name": "Junfeng Fang"
                },
                {
                    "authorId": "2357266201",
                    "name": "Ningyu Zhang"
                },
                {
                    "authorId": "2363281493",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2362692320",
                    "name": "Yang Wang"
                }
            ],
            "abstract": "Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.",
            "corpus_id": "278782460",
            "text": "Large language models require iterative updates to address challenges such as knowledge conflicts and outdated information (e.g., incorrect, private, or illegal contents). Machine unlearning provides a systematic methodology for targeted knowledge removal from trained models, enabling elimination of sensitive information influences. However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning. In this work, we introduce UniErase, a novel unlearning paradigm that employs learnable parametric suffix (unlearning token) to steer language models toward targeted forgetting behaviors. UniErase operates through two key phases: (I) an optimization stage that binds desired unlearning outputs to the model's autoregressive probability distribution via token optimization, followed by (II) a lightweight model editing phase that activates the learned token to probabilistically induce specified forgetting objective. Serving as a new research direction for token learning to induce unlearning target, UniErase achieves state-of-the-art (SOTA) performance across batch, sequential, and precise unlearning under fictitious and real-world knowledge settings. Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66% of the LLM parameters, outperforms previous forgetting SOTA baseline by around 4.01 times for model ability with even better unlearning efficacy. Similarly, UniErase, maintaining more ability, also surpasses previous retaining SOTA by 35.96% for unlearning efficacy, showing dual top-tier performances in current unlearing domain.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7744140625
        }
    ],
    "quotes": {
        "cost": 0.162219,
        "quotes": [
            {
                "idx": 0,
                "key": "[263834631 | Pawelczyk et al. | 2023 | Citations: 132]",
                "snippets": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 996,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[265456592 | Si et al. | 2023 | Citations: 33]",
                "snippets": "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model...we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "body",
                        "pdf_hash": "",
                        "start": 533,
                        "end": 838,
                        "sentence_offsets": [
                            {
                                "start": 533,
                                "end": 712
                            },
                            {
                                "start": 713,
                                "end": 839
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model"
                    },
                    {
                        "quote": "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268819249 | Xu | 2024 | Citations: 8]",
                "snippets": "\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, (Ilharco et al., 2022) proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[254408495 | Ilharco et al. | 2022 | Citations: 520]": "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."
                },
                "metadata": [
                    {
                        "section_title": "Manuscript submitted to ACM",
                        "pdf_hash": "",
                        "start": 675,
                        "end": 1718,
                        "sentence_offsets": [
                            {
                                "start": 675,
                                "end": 829
                            },
                            {
                                "start": 829,
                                "end": 1091
                            },
                            {
                                "start": 1091,
                                "end": 1203
                            },
                            {
                                "start": 1203,
                                "end": 1407
                            },
                            {
                                "start": 1407,
                                "end": 1524
                            },
                            {
                                "start": 1524,
                                "end": 1718
                            }
                        ],
                        "ref_mentions": [
                            "254408495"
                        ],
                        "quote": "\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, (Ilharco et al., 2022) proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[270286011 | Zuo et al. | 2024 | Citations: 4]",
                "snippets": "Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Unlearning with LLM",
                        "pdf_hash": "",
                        "start": 1526,
                        "end": 1826,
                        "sentence_offsets": [
                            {
                                "start": 1526,
                                "end": 1826
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[270560986 | Hong et al. | 2024 | Citations: 24]",
                "snippets": "Overall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RESULTS",
                        "pdf_hash": "",
                        "start": 389,
                        "end": 645,
                        "sentence_offsets": [
                            {
                                "start": 389,
                                "end": 557
                            },
                            {
                                "start": 558,
                                "end": 645
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Overall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[271064299 | Shi et al. | 2024 | Citations: 84]",
                "snippets": "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 744,
                        "sentence_offsets": [
                            {
                                "start": 270,
                                "end": 578
                            },
                            {
                                "start": 580,
                                "end": 744
                            }
                        ],
                        "ref_mentions": [
                            "255825985"
                        ],
                        "quote": "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271212701 | Gao et al. | 2024 | Citations: 5]",
                "snippets": "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 507,
                        "end": 1321,
                        "sentence_offsets": [
                            {
                                "start": 507,
                                "end": 787
                            },
                            {
                                "start": 788,
                                "end": 982
                            },
                            {
                                "start": 983,
                                "end": 1068
                            },
                            {
                                "start": 1069,
                                "end": 1203
                            },
                            {
                                "start": 1204,
                                "end": 1321
                            }
                        ],
                        "ref_mentions": [
                            "255825985"
                        ],
                        "quote": "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271543835 | Liu et al. | 2024 | Citations: 19]",
                "snippets": "The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Summary:",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1637,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 327
                            },
                            {
                                "start": 328,
                                "end": 521
                            },
                            {
                                "start": 522,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 784
                            },
                            {
                                "start": 785,
                                "end": 1024
                            },
                            {
                                "start": 1025,
                                "end": 1234
                            },
                            {
                                "start": 1235,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1461
                            },
                            {
                                "start": 1462,
                                "end": 1637
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271601132 | Zhou et al. | 2024 | Citations: 5]",
                "snippets": "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Unlearning for LLM",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1848,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 147
                            },
                            {
                                "start": 148,
                                "end": 232
                            },
                            {
                                "start": 235,
                                "end": 266
                            },
                            {
                                "start": 267,
                                "end": 404
                            },
                            {
                                "start": 405,
                                "end": 616
                            },
                            {
                                "start": 617,
                                "end": 782
                            },
                            {
                                "start": 785,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 975
                            },
                            {
                                "start": 976,
                                "end": 1210
                            },
                            {
                                "start": 1213,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1411
                            },
                            {
                                "start": 1412,
                                "end": 1639
                            },
                            {
                                "start": 1640,
                                "end": 1848
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[273022754 | Takashiro et al. | 2024 | Citations: 2]",
                "snippets": "For example, Gradient Ascent (Golatkar et al., 2019) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207863297 | Golatkar et al. | 2019 | Citations: 505]": "We explore the problem of selectively forgetting a particular subset of the data used for training a deep neural network. While the effects of the data to be forgotten can be hidden from the output of the network, insights may still be gleaned by probing deep into its weights. We propose a method for \"scrubbing\" the weights clean of information about a particular set of training data. The method does not require retraining from scratch, nor access to the data originally used for training. Instead, the weights are modified so that any probing function of the weights is indistinguishable from the same function applied to the weights of a network trained without the data to be forgotten. This condition is a generalized and weaker form of Differential Privacy. Exploiting ideas related to the stability of stochastic gradient descent, we introduce an upper-bound on the amount of information remaining in the weights, which can be estimated efficiently even for deep neural networks."
                },
                "metadata": [
                    {
                        "section_title": "Comparison of Our Method with Prior Work",
                        "pdf_hash": "",
                        "start": 287,
                        "end": 841,
                        "sentence_offsets": [
                            {
                                "start": 287,
                                "end": 401
                            },
                            {
                                "start": 402,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 841
                            }
                        ],
                        "ref_mentions": [
                            "207863297"
                        ],
                        "quote": "For example, Gradient Ascent (Golatkar et al., 2019) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[273098143 | Satvaty et al. | 2024 | Citations: 7]",
                "snippets": "Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs (Si et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265456592 | Si et al. | 2023 | Citations: 33]": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions."
                },
                "metadata": [
                    {
                        "section_title": "C. Unlearning methods",
                        "pdf_hash": "",
                        "start": 187,
                        "end": 433,
                        "sentence_offsets": [
                            {
                                "start": 187,
                                "end": 433
                            }
                        ],
                        "ref_mentions": [
                            "265456592"
                        ],
                        "quote": "Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs (Si et al., 2023)."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273098800 | Gandikota et al. | 2024 | Citations: 11]",
                "snippets": "Prior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 500,
                        "end": 1370,
                        "sentence_offsets": [
                            {
                                "start": 500,
                                "end": 783
                            },
                            {
                                "start": 784,
                                "end": 1118
                            },
                            {
                                "start": 1119,
                                "end": 1200
                            },
                            {
                                "start": 1201,
                                "end": 1370
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Prior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273228310 | Kim et al. | 2024 | Citations: 3]",
                "snippets": "Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "METHOD",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 570,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 130
                            },
                            {
                                "start": 131,
                                "end": 282
                            },
                            {
                                "start": 283,
                                "end": 570
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276772996 | Geng et al. | 2025 | Citations: 5]",
                "snippets": "Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[273849875 | Ma et al. | 2024 | Citations: 10]": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms."
                },
                "metadata": [
                    {
                        "section_title": "Input/Output-Based Unlearning",
                        "pdf_hash": "",
                        "start": 694,
                        "end": 1859,
                        "sentence_offsets": [
                            {
                                "start": 694,
                                "end": 950
                            },
                            {
                                "start": 951,
                                "end": 1204
                            },
                            {
                                "start": 1205,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1472
                            },
                            {
                                "start": 1473,
                                "end": 1631
                            },
                            {
                                "start": 1632,
                                "end": 1764
                            },
                            {
                                "start": 1765,
                                "end": 1859
                            }
                        ],
                        "ref_mentions": [
                            "273849875"
                        ],
                        "quote": "Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276885223 | Srivasthav et al. | 2025 | Citations: 0]",
                "snippets": "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[273901406 | Feng et al. | 2024 | Citations: 10]": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.",
                    "[264816202 | Wu et al. | 2023 | Citations: 55]": "Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.",
                    "[268856658 | Blanco-Justicia et al. | 2024 | Citations: 22]": "\n Large language models (LLMs) have become the state of the art in natural language processing. The massive adoption of generative LLMs and the capabilities they have shown have prompted public concerns regarding their impact on the labor market, privacy, the use of copyrighted work, and how these models align with human ethics and the rule of law. As a response, new regulations are being pushed, which require developers and service providers to evaluate, monitor, and forestall or at least mitigate the risks posed by their models. One mitigation strategy is digital forgetting: given a model with undesirable knowledge or behavior, the goal is to obtain a new model where the detected issues are no longer present. Digital forgetting is usually enforced via machine unlearning techniques, which modify trained machine learning models for them to behave as models trained on a subset of the original training data. In this work, we describe the motivations and desirable properties of digital forgetting when applied to LLMs, and we survey recent works on machine unlearning. Specifically, we propose a taxonomy of unlearning methods based on the reach and depth of the modifications done on the models, we discuss and compare the effectiveness of machine unlearning methods for LLMs proposed so far, and we survey their evaluation. Finally, we describe open problems of machine unlearning applied to LLMs and we put forward recommendations for developers and practitioners.",
                    "[271909642 | Yuan et al. | 2024 | Citations: 9]": "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities."
                },
                "metadata": [
                    {
                        "section_title": "Related works",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1666,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 385
                            },
                            {
                                "start": 386,
                                "end": 649
                            },
                            {
                                "start": 650,
                                "end": 865
                            },
                            {
                                "start": 866,
                                "end": 1012
                            },
                            {
                                "start": 1013,
                                "end": 1296
                            },
                            {
                                "start": 1297,
                                "end": 1435
                            },
                            {
                                "start": 1436,
                                "end": 1666
                            }
                        ],
                        "ref_mentions": [
                            "268856658",
                            "273901406",
                            "271909642",
                            "264816202"
                        ],
                        "quote": "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024)."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[277620868 | Kuo et al. | 2025 | Citations: 5]",
                "snippets": "We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "body",
                        "pdf_hash": "",
                        "start": 426,
                        "end": 686,
                        "sentence_offsets": [
                            {
                                "start": 423,
                                "end": 686
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[278782460 | Yu et al. | 2025 | Citations: 0]",
                "snippets": "However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.030174,
        "cot": "I'll plan the dimensions for addressing the query about key differences and tradeoffs between various unlearning methods for LLMs.\n\nFirst, I need to establish what dimensions will best help answer this query:\n\n1. \"Introduction/Background\" - This section will provide context about what machine unlearning is, why it's important for LLMs, and an overview of the different approaches. This is necessary to set up the detailed comparison that follows.\n\n2. \"Parameter Optimization Methods\" - This dimension will synthesize information about parameter optimization approaches to unlearning, including their mechanisms, advantages, and limitations. This requires synthesis format since we need to explain how these methods work and their tradeoffs.\n\n3. \"Parameter Merging Methods\" - Similar to above, this dimension will cover parameter merging approaches, their mechanisms and tradeoffs. Synthesis format is appropriate to explain these concepts cohesively.\n\n4. \"In-Context Unlearning Methods\" - This dimension will explain in-context unlearning approaches and their characteristics. Synthesis format is needed to explain these approaches comprehensively.\n\n5. \"Localization-Based Unlearning Methods\" - This dimension will cover localization-based unlearning approaches. Synthesis format will allow for proper explanation of these methods.\n\n6. \"Comparative Analysis of Tradeoffs\" - This dimension will directly compare all methods across key metrics like effectiveness, computational cost, and preservation of unrelated knowledge. A synthesis format is appropriate as we need to analyze the relationships between different approaches.\n\nNow, I'll assign quotes to each dimension:\n\nFor \"Introduction/Background\":\n- Quote [0] provides a good introduction to machine unlearning and its challenges for LLMs.\n- Quote [1] provides context about knowledge unlearning and its advantages.\n\nFor \"Parameter Optimization Methods\":\n- Quote [7] provides detailed information about parameter optimization strategies.\n- Quote [8] describes parameter optimization methods.\n- Quote [13] provides information about challenges with gradient ascent and other optimization methods.\n- Quote [14] explains global weight modification, which falls under parameter optimization.\n\nFor \"Parameter Merging Methods\":\n- Quote [1] explains parameter merging methods.\n- Quote [2] provides more details about parameter merging including task vectors.\n- Quote [8] briefly describes parameter merging methods.\n- Quote [13] mentions challenges with task vectors.\n\nFor \"In-Context Unlearning Methods\":\n- Quote [0] introduces in-context unlearning.\n- Quote [5] briefly describes in-context unlearning.\n- Quote [6] compares in-context unlearning with parameter optimization.\n- Quote [8] explains in-context learning methods.\n- Quote [9] discusses ICUL (In-context Unlearning).\n- Quote [16] mentions limitations of in-context unlearning.\n\nFor \"Localization-Based Unlearning Methods\":\n- Quote [4] mentions the promise of localization-based unlearning.\n- Quote [5] describes localization-informed unlearning.\n- Quote [14] explains local weight modification, which is related to localization-based approaches.\n- Quote [15] identifies a key deficiency in localization-based merging.\n\nFor \"Comparative Analysis of Tradeoffs\":\n- Quote [3] mentions the technical challenges across methods.\n- Quote [6] compares effectiveness of parameter optimization vs. in-context learning.\n- Quote [11] compares limitations of different approaches.\n- Quote [12] discusses the tradeoff between forget set and retain set performance.\n- Quote [13] summarizes challenges across different methods.\n- Quote [16] compares limitations of fine-tuning-based and in-context unlearning.\n\nEach dimension will be presented in synthesis format because they require explanation and analysis of the concepts, mechanisms, and tradeoffs rather than simply listing items.",
        "plan": {
            "Introduction/Background (synthesis)": [
                0,
                1
            ],
            "Parameter Optimization Methods (synthesis)": [
                7,
                8,
                13,
                14
            ],
            "Parameter Merging Methods (synthesis)": [
                1,
                2,
                8,
                13
            ],
            "In-Context Unlearning Methods (synthesis)": [
                0,
                5,
                6,
                8,
                9,
                16
            ],
            "Localization-Based Unlearning Methods (synthesis)": [
                4,
                5,
                14,
                15
            ],
            "Comparative Analysis of Tradeoffs (synthesis)": [
                3,
                6,
                10,
                11,
                12,
                13,
                16
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Machine unlearning is an emerging field focused on removing specific knowledge from trained models, driven by regulatory guidelines and the need to eliminate harmful content. Various approaches have been developed for large language models (LLMs), including parameter optimization, parameter merging, in-context unlearning, and localization-based methods, each with distinct characteristics and trade-offs. (2 sources)",
                "text": "\nMachine unlearning has gained significant attention in recent years, particularly in the context of large language models (LLMs), driven by regulatory guidelines such as the \"Right to be Forgotten\" and the need to remove potentially harmful knowledge from deployed models <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. This field focuses on efficiently removing the influence of specific training instances or knowledge from a model without compromising its overall performance on unrelated tasks.\n\nTraditional approaches to unlearning typically require complete model retraining, which is computationally prohibitive for large-scale models like LLMs. This limitation has spurred the development of alternative methods that can approximate the removal of certain knowledge without the computational burden of full retraining <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. These approaches are particularly valuable for LLMs, as they allow for the targeted removal of harmful knowledge while preserving the model's broader capabilities <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nCurrent knowledge unlearning methods for LLMs can be broadly categorized into several approaches: parameter optimization-based methods, which involve updating model weights; parameter merging methods, which combine parameters from different model versions through arithmetic operations; in-context unlearning methods, which operate without parameter updates by providing specific inputs in the context window; and localization-based methods that target specific parts of the model <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. Each of these approaches offers distinct advantages and limitations, making them suitable for different unlearning scenarios and requirements.",
                "citations": [
                    {
                        "id": "(Pawelczyk et al., 2023)",
                        "snippets": [
                            "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters."
                        ],
                        "paper": {
                            "corpus_id": 263834631,
                            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                            "authors": [
                                {
                                    "authorId": "89583148",
                                    "name": "Martin Pawelczyk"
                                },
                                {
                                    "authorId": "2273685865",
                                    "name": "Seth Neel"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 132
                        },
                        "score": 0.654296875
                    },
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model",
                            "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."
                        ],
                        "paper": {
                            "corpus_id": 265456592,
                            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                            "authors": [
                                {
                                    "authorId": "73502630",
                                    "name": "Nianwen Si"
                                },
                                {
                                    "authorId": "2154930608",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2116152318",
                                    "name": "Heyu Chang"
                                },
                                {
                                    "authorId": "9047584",
                                    "name": "Wenlin Zhang"
                                },
                                {
                                    "authorId": "2253591545",
                                    "name": "Dan Qu"
                                },
                                {
                                    "authorId": "2268429659",
                                    "name": "Weiqiang Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.81982421875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter Optimization Methods",
                "tldr": "Parameter optimization methods focus on adjusting model weights to selectively unlearn specific knowledge or behaviors, with approaches ranging from gradient-based techniques to knowledge distillation. These methods offer varying trade-offs between unlearning effectiveness, utility preservation, and computational efficiency. (7 sources)",
                "text": "\nParameter optimization methods form a major category of unlearning approaches that modify model parameters to remove specific knowledge from large language models. These techniques directly adjust model weights by optimizing various objective functions designed to target unwanted information while preserving general model capabilities.\n\nOne of the primary approaches within this category is gradient-based optimization, which can be implemented with or without reversed loss functions. Methods using reversed loss (such as gradient ascent) effectively unlearn targeted knowledge by maximizing the cross-entropy loss for specific samples that need to be forgotten <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This approach has been applied to various unlearning scenarios, including removing harmful responses, erasing copyright-protected content, and eliminating hallucinations <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. However, while effective at unlearning, these methods can inadvertently affect unrelated parameters, compromising the model's overall performance <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nOther parameter optimization techniques include knowledge distillation, which transfers knowledge to a new model while excluding specific data points. This approach maintains locality by preserving essential performance while unlearning undesired knowledge, though it can be resource-intensive and may not achieve optimal accuracy and generalizability <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Knowledge distillation approaches have also been reported to suffer from knowledge relearning issues, where models may reacquire forgotten information during fine-tuning with related or even unrelated samples <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273849875\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nGlobal weight modification approaches\u2014which update all model parameters during unlearning\u2014include techniques like gradient ascent, gradient difference, KL minimization, and weight perturbation <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271909642\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>. These methods generally provide strong unlearning guarantees but are resource-intensive for larger models and can face challenges in balancing effective unlearning with preservation of model capabilities <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>.\n\nThe use of extra learnable layers represents another parameter optimization approach, offering highly targeted unlearning but potentially requiring significant computational resources <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Additionally, task vectors and parameter-efficient module operations may perform well in terms of unlearning accuracy, but recent work has highlighted the risk of these approaches leading to model instability and degradation <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nA notable challenge across many parameter optimization methods is the inherent tension between forgetting specific information and maintaining general model performance. Methods that optimize for effective unlearning often require careful regularization to mitigate performance loss on unrelated tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. This fundamental trade-off represents one of the key considerations when selecting parameter optimization techniques for unlearning in large language models.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance."
                        ],
                        "paper": {
                            "corpus_id": 271543835,
                            "title": "Machine Unlearning in Generative AI: A Survey",
                            "authors": [
                                {
                                    "authorId": "2122087252",
                                    "name": "Zheyuan Liu"
                                },
                                {
                                    "authorId": "2174956825",
                                    "name": "Guangyao Dou"
                                },
                                {
                                    "authorId": "2093186816",
                                    "name": "Zhaoxuan Tan"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2275403324",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 19
                        },
                        "score": 0.6513671875
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."
                        ],
                        "paper": {
                            "corpus_id": 271601132,
                            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
                            "authors": [
                                {
                                    "authorId": "2303304464",
                                    "name": "Shiji Zhou"
                                },
                                {
                                    "authorId": "2233281230",
                                    "name": "Lianzhe Wang"
                                },
                                {
                                    "authorId": "2277872538",
                                    "name": "Jiangnan Ye"
                                },
                                {
                                    "authorId": "2314337350",
                                    "name": "Yongliang Wu"
                                },
                                {
                                    "authorId": "2303324907",
                                    "name": "Heng Chang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Geng et al., 2025)",
                        "snippets": [
                            "Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."
                        ],
                        "paper": {
                            "corpus_id": 276772996,
                            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2266466915",
                                    "name": "Jiahui Geng"
                                },
                                {
                                    "authorId": "2295742465",
                                    "name": "Qing Li"
                                },
                                {
                                    "authorId": "2215923457",
                                    "name": "Herbert Woisetschlaeger"
                                },
                                {
                                    "authorId": "2109296817",
                                    "name": "Zongxiong Chen"
                                },
                                {
                                    "authorId": "2241417701",
                                    "name": "Yuxia Wang"
                                },
                                {
                                    "authorId": "2026545715",
                                    "name": "Preslav Nakov"
                                },
                                {
                                    "authorId": "2238271670",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2265969003",
                                    "name": "Fakhri Karray"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.7509765625
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms."
                        ],
                        "paper": {
                            "corpus_id": 273849875,
                            "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
                            "authors": [
                                {
                                    "authorId": "2269426296",
                                    "name": "Yingzi Ma"
                                },
                                {
                                    "authorId": "2110170885",
                                    "name": "Jiong Wang"
                                },
                                {
                                    "authorId": "2306872120",
                                    "name": "Fei Wang"
                                },
                                {
                                    "authorId": "2295006382",
                                    "name": "Siyuan Ma"
                                },
                                {
                                    "authorId": "2286593620",
                                    "name": "Jiazhao Li"
                                },
                                {
                                    "authorId": "2268736216",
                                    "name": "Xiujun Li"
                                },
                                {
                                    "authorId": "2329735142",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2329521764",
                                    "name": "Lichao Sun"
                                },
                                {
                                    "authorId": "2286730795",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2329517334",
                                    "name": "Yejin Choi"
                                },
                                {
                                    "authorId": "1998918",
                                    "name": "Muhao Chen"
                                },
                                {
                                    "authorId": "2256992327",
                                    "name": "Chaowei Xiao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 10
                        },
                        "score": 0
                    },
                    {
                        "id": "(Srivasthav et al., 2025)",
                        "snippets": [
                            "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 276885223,
                            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2135634919",
                                    "name": "Dinesh P. Srivasthav"
                                },
                                {
                                    "authorId": "9266407",
                                    "name": "B. Garlapati"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.53955078125
                    },
                    {
                        "id": "(Feng et al., 2024)",
                        "snippets": [
                            "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain."
                        ],
                        "paper": {
                            "corpus_id": 273901406,
                            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                            "authors": [
                                {
                                    "authorId": "2314871557",
                                    "name": "Xiaohua Feng"
                                },
                                {
                                    "authorId": "2251485995",
                                    "name": "Chao-Jun Chen"
                                },
                                {
                                    "authorId": "1527113700",
                                    "name": "Yuyuan Li"
                                },
                                {
                                    "authorId": "2261893790",
                                    "name": "Zibin Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0.5654296875
                    },
                    {
                        "id": "(Yuan et al., 2024)",
                        "snippets": [
                            "LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities."
                        ],
                        "paper": {
                            "corpus_id": 271909642,
                            "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2165224410",
                                    "name": "Hongbang Yuan"
                                },
                                {
                                    "authorId": "2152843772",
                                    "name": "Zhuoran Jin"
                                },
                                {
                                    "authorId": "49776272",
                                    "name": "Pengfei Cao"
                                },
                                {
                                    "authorId": "1763402",
                                    "name": "Yubo Chen"
                                },
                                {
                                    "authorId": "2284814101",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "2269147239",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 9
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter Merging Methods",
                "tldr": "Parameter merging methods combine parameters from different model versions through simple arithmetic operations without requiring additional training, offering computational efficiency for deployed models. These approaches include task vectors that specify directional changes in weight space, enabling selective knowledge removal through operations like negation and addition. (5 sources)",
                "text": "\nParameter merging represents a distinct approach to machine unlearning that involves combining parameters from different model versions through arithmetic operations without requiring additional parameter training <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. Unlike parameter optimization methods, these techniques focus on offline composition of model parameters, making them particularly suitable for scenarios where models have already been deployed <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>.\n\nOne of the most notable parameter merging approaches involves task vectors, which specify directions in the weight space of a pre-trained model that, when applied, can steer model behavior toward or away from specific tasks <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper> <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. Task vectors are created by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a particular task <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. These vectors can then be modified through arithmetic operations such as negation and addition to selectively alter model behavior <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of parameter merging methods is their computational efficiency, as they eliminate the need for online updates and extensive parameter training <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. This makes them especially valuable for LLM providers seeking economical unlearning solutions and for applications where fine-tuning already-deployed models is necessary <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. Parameter merging also offers the ability to remove specific knowledge while maintaining stability in other model behaviors <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nHowever, parameter merging approaches face several limitations. The effectiveness of simply overlaying model parameters remains debatable, with a need for more theoretical analysis to fully understand their impact <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. Task vectors, in particular, have been noted to have limitations in precisely localizing knowledge, which restricts their application primarily to AI alignment tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Additionally, while more computationally efficient than parameter optimization methods, parameter merging still incurs some computational costs <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model",
                            "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."
                        ],
                        "paper": {
                            "corpus_id": 265456592,
                            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                            "authors": [
                                {
                                    "authorId": "73502630",
                                    "name": "Nianwen Si"
                                },
                                {
                                    "authorId": "2154930608",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2116152318",
                                    "name": "Heyu Chang"
                                },
                                {
                                    "authorId": "9047584",
                                    "name": "Wenlin Zhang"
                                },
                                {
                                    "authorId": "2253591545",
                                    "name": "Dan Qu"
                                },
                                {
                                    "authorId": "2268429659",
                                    "name": "Weiqiang Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.81982421875
                    },
                    {
                        "id": "(Xu, 2024)",
                        "snippets": [
                            "\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, (Ilharco et al., 2022) proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure."
                        ],
                        "paper": {
                            "corpus_id": 268819249,
                            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
                            "authors": [
                                {
                                    "authorId": "2294397567",
                                    "name": "Yi Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.5048828125
                    },
                    {
                        "id": "(Ilharco et al., 2022)",
                        "snippets": [
                            "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."
                        ],
                        "paper": {
                            "corpus_id": 254408495,
                            "title": "Editing Models with Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "78846919",
                                    "name": "Marco Tulio Ribeiro"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 520
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."
                        ],
                        "paper": {
                            "corpus_id": 271601132,
                            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
                            "authors": [
                                {
                                    "authorId": "2303304464",
                                    "name": "Shiji Zhou"
                                },
                                {
                                    "authorId": "2233281230",
                                    "name": "Lianzhe Wang"
                                },
                                {
                                    "authorId": "2277872538",
                                    "name": "Jiangnan Ye"
                                },
                                {
                                    "authorId": "2314337350",
                                    "name": "Yongliang Wu"
                                },
                                {
                                    "authorId": "2303324907",
                                    "name": "Heng Chang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Geng et al., 2025)",
                        "snippets": [
                            "Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."
                        ],
                        "paper": {
                            "corpus_id": 276772996,
                            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2266466915",
                                    "name": "Jiahui Geng"
                                },
                                {
                                    "authorId": "2295742465",
                                    "name": "Qing Li"
                                },
                                {
                                    "authorId": "2215923457",
                                    "name": "Herbert Woisetschlaeger"
                                },
                                {
                                    "authorId": "2109296817",
                                    "name": "Zongxiong Chen"
                                },
                                {
                                    "authorId": "2241417701",
                                    "name": "Yuxia Wang"
                                },
                                {
                                    "authorId": "2026545715",
                                    "name": "Preslav Nakov"
                                },
                                {
                                    "authorId": "2238271670",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2265969003",
                                    "name": "Fakhri Karray"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.7509765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "In-Context Unlearning Methods",
                "tldr": "In-context unlearning methods enable knowledge removal from LLMs without parameter modifications by strategically manipulating input prompts at inference time. While offering accessibility advantages for black-box models, these methods generally demonstrate lower unlearning effectiveness compared to parameter-based approaches and may struggle with generalizability. (6 sources)",
                "text": "\nIn-context unlearning represents a distinct approach to machine unlearning that operates without modifying model parameters, instead relying on strategic manipulation of input prompts during inference time <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. This technique has emerged as a valuable solution particularly for scenarios where access to model parameters is limited due to computational constraints or when working with black-box LLMs <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.\n\nThe core mechanism of in-context unlearning involves providing specific inputs within the context window to influence model behavior. Typically, this is implemented by including the instance to be forgotten alongside a flipped label or contradictory information, often accompanied by correctly labeled instances that serve as additional context <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. For example, to unlearn a particular instance, practitioners might prepend inputs with the target instance paired with modified labels or information that contradicts the knowledge to be forgotten <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of in-context unlearning is its accessibility and flexibility. Unlike parameter optimization or merging methods, in-context approaches do not require direct access to model weights, making them applicable to black-box models where only query access is available <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. Additionally, these methods enable test-time unlearning, allowing for dynamic knowledge removal at inference without the need for separate training or preparation phases <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>.\n\nHowever, in-context unlearning faces significant limitations. Most notably, studies indicate that parameter optimization methods typically achieve much higher unlearning effectiveness compared to in-context approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This effectiveness gap stems from the fact that in-context methods do not modify the model's intrinsic mechanisms, instead relying on contextual manipulation that may not fully counteract deeply embedded knowledge <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nAnother critical limitation is the potential for hallucinations. Since in-context unlearning often works by changing ground-truth labels or words within prompts, this approach can inadvertently lead to the generation of hallucinated content <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>. Furthermore, in-context methods typically struggle with generalizability beyond the specific instances included in the context window, limiting their ability to achieve comprehensive unlearning across varied inputs and situations <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nDespite these limitations, in-context unlearning remains a valuable approach in scenarios where parameter access is restricted or when temporary knowledge suppression is sufficient. Its ability to function without parameter modifications makes it particularly suitable for deployed models where more invasive unlearning methods are impractical or unavailable <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Pawelczyk et al., 2023)",
                        "snippets": [
                            "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters."
                        ],
                        "paper": {
                            "corpus_id": 263834631,
                            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                            "authors": [
                                {
                                    "authorId": "89583148",
                                    "name": "Martin Pawelczyk"
                                },
                                {
                                    "authorId": "2273685865",
                                    "name": "Seth Neel"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 132
                        },
                        "score": 0.654296875
                    },
                    {
                        "id": "(Zhou et al., 2024)",
                        "snippets": [
                            "Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."
                        ],
                        "paper": {
                            "corpus_id": 271601132,
                            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
                            "authors": [
                                {
                                    "authorId": "2303304464",
                                    "name": "Shiji Zhou"
                                },
                                {
                                    "authorId": "2233281230",
                                    "name": "Lianzhe Wang"
                                },
                                {
                                    "authorId": "2277872538",
                                    "name": "Jiangnan Ye"
                                },
                                {
                                    "authorId": "2314337350",
                                    "name": "Yongliang Wu"
                                },
                                {
                                    "authorId": "2303324907",
                                    "name": "Heng Chang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.75732421875
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271064299,
                            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                            "authors": [
                                {
                                    "authorId": "2286638403",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2261353791",
                                    "name": "Jaechan Lee"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2266698166",
                                    "name": "Jieyu Zhao"
                                },
                                {
                                    "authorId": "2309248199",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309424274",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 84
                        },
                        "score": 0.73681640625
                    },
                    {
                        "id": "(Takashiro et al., 2024)",
                        "snippets": [
                            "For example, Gradient Ascent (Golatkar et al., 2019) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations."
                        ],
                        "paper": {
                            "corpus_id": 273022754,
                            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
                            "authors": [
                                {
                                    "authorId": "2323750981",
                                    "name": "Shota Takashiro"
                                },
                                {
                                    "authorId": "2081836120",
                                    "name": "Takeshi Kojima"
                                },
                                {
                                    "authorId": "2304550144",
                                    "name": "Andrew Gambardella"
                                },
                                {
                                    "authorId": "2268816164",
                                    "name": "Qi Cao"
                                },
                                {
                                    "authorId": "1715282",
                                    "name": "Yusuke Iwasawa"
                                },
                                {
                                    "authorId": "2241471533",
                                    "name": "Yutaka Matsuo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.5029296875
                    },
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                        ],
                        "paper": {
                            "corpus_id": 271212701,
                            "title": "On Large Language Model Continual Unlearning",
                            "authors": [
                                {
                                    "authorId": "2311833838",
                                    "name": "Chongyang Gao"
                                },
                                {
                                    "authorId": "2108631414",
                                    "name": "Lixu Wang"
                                },
                                {
                                    "authorId": "2148353350",
                                    "name": "Chenkai Weng"
                                },
                                {
                                    "authorId": "2276121035",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2275773112",
                                    "name": "Qi Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.69482421875
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning."
                        ],
                        "paper": {
                            "corpus_id": 278782460,
                            "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
                            "authors": [
                                {
                                    "authorId": "2323938152",
                                    "name": "Miao Yu"
                                },
                                {
                                    "authorId": "2362620538",
                                    "name": "Liang Lin"
                                },
                                {
                                    "authorId": "2232926268",
                                    "name": "Guibin Zhang"
                                },
                                {
                                    "authorId": "2349829666",
                                    "name": "Xinfeng Li"
                                },
                                {
                                    "authorId": "2362314471",
                                    "name": "Junfeng Fang"
                                },
                                {
                                    "authorId": "2357266201",
                                    "name": "Ningyu Zhang"
                                },
                                {
                                    "authorId": "2363281493",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2362692320",
                                    "name": "Yang Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.7744140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Localization-Based Unlearning Methods",
                "tldr": "Localization-based unlearning methods target specific model components (like neurons or layers) associated with unwanted knowledge rather than modifying all parameters. While showing promise for precise knowledge removal with reduced computational demands compared to global approaches, these methods face challenges in accurately identifying the relevant parameters to modify. (6 sources)",
                "text": "\nLocalization-based unlearning represents an emerging approach that focuses on identifying and modifying only specific components of a model associated with the knowledge to be forgotten, rather than updating all parameters. This targeted approach aims to achieve more precise knowledge removal while reducing computational demands compared to global parameter modification methods <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>.\n\nAt its core, localization-based unlearning involves two key steps: first identifying model units (such as layers or neurons) closely related to the unlearning data or tasks, and then locally editing or modifying these specific units <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. This approach draws from research on knowledge localization in transformer models, which has found evidence that factual associations correspond to localized, directly-editable computations, particularly in middle-layer feed-forward modules <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nThe primary advantage of localization-based methods is their computational efficiency. By modifying only a subset of parameters, these approaches can significantly reduce the computational resources required for unlearning, making them more feasible for larger models compared to global weight modification techniques <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>. This targeted approach also potentially offers greater precision in removing specific knowledge without broadly affecting model capabilities.\n\nHowever, localization-based methods face several challenges. The most significant is accurately identifying the precise set of parameters that need to be modified, which can vary considerably depending on the diversity and nature of the information to be unlearned <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>. Incomplete or imprecise localization can leave traces of the targeted knowledge, allowing its influence to persist in the model's behavior <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264816202\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nAdditionally, recent research has identified limitations in localization-based merging for unlearning. Current localization methods that boost accuracy by sharing information across tasks make exact unlearning computationally infeasible <Paper corpusId=\"277620868\" paperTitle=\"(Kuo et al., 2025)\" isShortName></Paper>. This creates a fundamental tension between parameter efficiency and complete knowledge removal.\n\nDespite these challenges, localization-based unlearning methods show considerable promise. Recent studies have found that while many existing unlearning methods fail to completely remove parametric knowledge (with their effectiveness often overestimated by common behavioral evaluations), localization-based approaches offer a promising direction for more effective knowledge removal <Paper corpusId=\"270560986\" paperTitle=\"(Hong et al., 2024)\" isShortName></Paper>. As research in this area continues to advance, these methods may provide increasingly precise and efficient solutions for knowledge unlearning in large language models.",
                "citations": [
                    {
                        "id": "(Srivasthav et al., 2025)",
                        "snippets": [
                            "The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 276885223,
                            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2135634919",
                                    "name": "Dinesh P. Srivasthav"
                                },
                                {
                                    "authorId": "9266407",
                                    "name": "B. Garlapati"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.53955078125
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271064299,
                            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                            "authors": [
                                {
                                    "authorId": "2286638403",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2261353791",
                                    "name": "Jaechan Lee"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2266698166",
                                    "name": "Jieyu Zhao"
                                },
                                {
                                    "authorId": "2309248199",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309424274",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 84
                        },
                        "score": 0.73681640625
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach."
                        ],
                        "paper": {
                            "corpus_id": 264816202,
                            "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "2051972795",
                                    "name": "Xinwei Wu"
                                },
                                {
                                    "authorId": "2138202959",
                                    "name": "Junzhuo Li"
                                },
                                {
                                    "authorId": "2263533854",
                                    "name": "Minghui Xu"
                                },
                                {
                                    "authorId": "2114049130",
                                    "name": "Weilong Dong"
                                },
                                {
                                    "authorId": "2263775209",
                                    "name": "Shuangzhi Wu"
                                },
                                {
                                    "authorId": "2150136794",
                                    "name": "Chao Bian"
                                },
                                {
                                    "authorId": "2263617516",
                                    "name": "Deyi Xiong"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 55
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kuo et al., 2025)",
                        "snippets": [
                            "We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible."
                        ],
                        "paper": {
                            "corpus_id": 277620868,
                            "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale",
                            "authors": [
                                {
                                    "authorId": "2305621566",
                                    "name": "Kevin Kuo"
                                },
                                {
                                    "authorId": "80366270",
                                    "name": "Amrith Rajagopal Setlur"
                                },
                                {
                                    "authorId": "2302322978",
                                    "name": "Kartik Srinivas"
                                },
                                {
                                    "authorId": "2269982126",
                                    "name": "Aditi Raghunathan"
                                },
                                {
                                    "authorId": "2305622207",
                                    "name": "Virginia Smith"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.58251953125
                    },
                    {
                        "id": "(Hong et al., 2024)",
                        "snippets": [
                            "Overall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods."
                        ],
                        "paper": {
                            "corpus_id": 270560986,
                            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
                            "authors": [
                                {
                                    "authorId": "2306946364",
                                    "name": "Yihuai Hong"
                                },
                                {
                                    "authorId": "2306950395",
                                    "name": "Lei Yu"
                                },
                                {
                                    "authorId": "2143278592",
                                    "name": "Shauli Ravfogel"
                                },
                                {
                                    "authorId": "2307764517",
                                    "name": "Haiqin Yang"
                                },
                                {
                                    "authorId": "22245981",
                                    "name": "Mor Geva"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 24
                        },
                        "score": 0.67138671875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Analysis of Tradeoffs",
                "tldr": "Each unlearning method offers distinct tradeoffs between unlearning effectiveness, computational efficiency, and preservation of model utility, with parameter optimization generally providing the strongest unlearning but at higher computational cost, while in-context methods offer accessibility but limited effectiveness. (10 sources)",
                "text": "\nWhen comparing the various unlearning approaches for large language models, distinct tradeoffs emerge across several critical dimensions including unlearning effectiveness, computational efficiency, and preservation of model utility. Understanding these tradeoffs is essential for selecting the appropriate unlearning method based on specific requirements and constraints.\n\nParameter optimization methods generally achieve the highest unlearning effectiveness among all approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. However, these methods face significant challenges in balancing the removal of targeted knowledge while preserving performance on retained knowledge. As demonstrated in empirical studies, improving performance on data to be retained typically results in decreased unlearning effectiveness on the forget set, and vice versa <Paper corpusId=\"273228310\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This fundamental tension often necessitates careful regularization to mitigate performance degradation on unrelated tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Additionally, parameter optimization methods tend to be computationally intensive, particularly for large-scale models, making them less practical for deployment scenarios with limited resources.\n\nParameter merging approaches offer a middle ground in terms of effectiveness and efficiency. While they provide computational advantages by eliminating the need for extensive retraining, their precision in localizing and removing specific knowledge can be limited <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Task vectors, a common parameter merging technique, face limitations in precisely identifying knowledge locations, restricting their primary application to AI alignment tasks rather than comprehensive knowledge removal <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>.\n\nIn-context unlearning methods stand out for their accessibility and ease of implementation, particularly in scenarios where direct access to model parameters is restricted <Paper corpusId=\"270286011\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273098143\" paperTitle=\"(Satvaty et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. However, these approaches typically deliver significantly lower unlearning effectiveness compared to parameter-based methods <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. The reliance on contextual manipulation rather than direct parameter modification limits their ability to achieve comprehensive knowledge removal, resulting in poor generalizability beyond specific prompts <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nLocalization-based methods attempt to address the efficiency-effectiveness tradeoff by targeting specific model components associated with unwanted knowledge. While theoretically promising for balancing computational demands with unlearning precision <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>, these approaches face challenges in accurately identifying all relevant parameters, potentially leading to incomplete knowledge removal.\n\nEach unlearning approach also presents distinct failure modes. Gradient-based parameter optimization and preference optimization methods often struggle with utility preservation due to optimization conflicts, requiring additional regularization techniques <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Relabeling-based fine-tuning and knowledge distillation methods have been reported to suffer from knowledge relearning issues, where forgotten knowledge can be inadvertently reacquired during subsequent fine-tuning <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273849875\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Representation engineering approaches may be more vulnerable to adversarial attacks, failing to robustly erase knowledge under challenging conditions <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>.\n\nTraditional approaches like dataset filtering and complete retraining, while conceptually straightforward, remain impractical for large language models due to prohibitive computational costs <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>. Gradient reversal methods, though more efficient, can create broad damage to model capabilities beyond the targeted knowledge, while representation manipulation approaches often introduce obvious behavioral artifacts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\nThe ideal selection of an unlearning method ultimately depends on specific requirements regarding unlearning effectiveness, computational constraints, and the importance of preserving model utility across various tasks. Parameter optimization may be preferred when unlearning effectiveness is paramount and computational resources are available, while in-context approaches offer practical solutions for deployed models with limited parameter access, despite their reduced effectiveness.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."
                        ],
                        "paper": {
                            "corpus_id": 271212701,
                            "title": "On Large Language Model Continual Unlearning",
                            "authors": [
                                {
                                    "authorId": "2311833838",
                                    "name": "Chongyang Gao"
                                },
                                {
                                    "authorId": "2108631414",
                                    "name": "Lixu Wang"
                                },
                                {
                                    "authorId": "2148353350",
                                    "name": "Chenkai Weng"
                                },
                                {
                                    "authorId": "2276121035",
                                    "name": "Xiao Wang"
                                },
                                {
                                    "authorId": "2275773112",
                                    "name": "Qi Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.69482421875
                    },
                    {
                        "id": "(Kim et al., 2024)",
                        "snippets": [
                            "Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently."
                        ],
                        "paper": {
                            "corpus_id": 273228310,
                            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2296414239",
                                    "name": "Hyoseo Kim"
                                },
                                {
                                    "authorId": "2325152150",
                                    "name": "Dongyoon Han"
                                },
                                {
                                    "authorId": "3338475",
                                    "name": "Junsuk Choe"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.57080078125
                    },
                    {
                        "id": "(Geng et al., 2025)",
                        "snippets": [
                            "Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."
                        ],
                        "paper": {
                            "corpus_id": 276772996,
                            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2266466915",
                                    "name": "Jiahui Geng"
                                },
                                {
                                    "authorId": "2295742465",
                                    "name": "Qing Li"
                                },
                                {
                                    "authorId": "2215923457",
                                    "name": "Herbert Woisetschlaeger"
                                },
                                {
                                    "authorId": "2109296817",
                                    "name": "Zongxiong Chen"
                                },
                                {
                                    "authorId": "2241417701",
                                    "name": "Yuxia Wang"
                                },
                                {
                                    "authorId": "2026545715",
                                    "name": "Preslav Nakov"
                                },
                                {
                                    "authorId": "2238271670",
                                    "name": "Hans-Arno Jacobsen"
                                },
                                {
                                    "authorId": "2265969003",
                                    "name": "Fakhri Karray"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.7509765625
                    },
                    {
                        "id": "(Zuo et al., 2024)",
                        "snippets": [
                            "Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models."
                        ],
                        "paper": {
                            "corpus_id": 270286011,
                            "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
                            "authors": [
                                {
                                    "authorId": "2005445750",
                                    "name": "Xuhan Zuo"
                                },
                                {
                                    "authorId": "2005212347",
                                    "name": "Minghao Wang"
                                },
                                {
                                    "authorId": "2185053609",
                                    "name": "Tianqing Zhu"
                                },
                                {
                                    "authorId": "48571174",
                                    "name": "Lefeng Zhang"
                                },
                                {
                                    "authorId": "2261513740",
                                    "name": "Dayong Ye"
                                },
                                {
                                    "authorId": "2304458654",
                                    "name": "Shui Yu"
                                },
                                {
                                    "authorId": "2134555583",
                                    "name": "Wanlei Zhou"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.54052734375
                    },
                    {
                        "id": "(Satvaty et al., 2024)",
                        "snippets": [
                            "Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs (Si et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 273098143,
                            "title": "Undesirable Memorization in Large Language Models: A Survey",
                            "authors": [
                                {
                                    "authorId": "2165227497",
                                    "name": "Ali Satvaty"
                                },
                                {
                                    "authorId": "2280669651",
                                    "name": "Suzan Verberne"
                                },
                                {
                                    "authorId": "2324053570",
                                    "name": "Fatih Turkmen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.76611328125
                    },
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model",
                            "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."
                        ],
                        "paper": {
                            "corpus_id": 265456592,
                            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                            "authors": [
                                {
                                    "authorId": "73502630",
                                    "name": "Nianwen Si"
                                },
                                {
                                    "authorId": "2154930608",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2116152318",
                                    "name": "Heyu Chang"
                                },
                                {
                                    "authorId": "9047584",
                                    "name": "Wenlin Zhang"
                                },
                                {
                                    "authorId": "2253591545",
                                    "name": "Dan Qu"
                                },
                                {
                                    "authorId": "2268429659",
                                    "name": "Weiqiang Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.81982421875
                    },
                    {
                        "id": "(Yu et al., 2025)",
                        "snippets": [
                            "However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning."
                        ],
                        "paper": {
                            "corpus_id": 278782460,
                            "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models",
                            "authors": [
                                {
                                    "authorId": "2323938152",
                                    "name": "Miao Yu"
                                },
                                {
                                    "authorId": "2362620538",
                                    "name": "Liang Lin"
                                },
                                {
                                    "authorId": "2232926268",
                                    "name": "Guibin Zhang"
                                },
                                {
                                    "authorId": "2349829666",
                                    "name": "Xinfeng Li"
                                },
                                {
                                    "authorId": "2362314471",
                                    "name": "Junfeng Fang"
                                },
                                {
                                    "authorId": "2357266201",
                                    "name": "Ningyu Zhang"
                                },
                                {
                                    "authorId": "2363281493",
                                    "name": "Kun Wang"
                                },
                                {
                                    "authorId": "2362692320",
                                    "name": "Yang Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.7744140625
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms."
                        ],
                        "paper": {
                            "corpus_id": 273849875,
                            "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
                            "authors": [
                                {
                                    "authorId": "2269426296",
                                    "name": "Yingzi Ma"
                                },
                                {
                                    "authorId": "2110170885",
                                    "name": "Jiong Wang"
                                },
                                {
                                    "authorId": "2306872120",
                                    "name": "Fei Wang"
                                },
                                {
                                    "authorId": "2295006382",
                                    "name": "Siyuan Ma"
                                },
                                {
                                    "authorId": "2286593620",
                                    "name": "Jiazhao Li"
                                },
                                {
                                    "authorId": "2268736216",
                                    "name": "Xiujun Li"
                                },
                                {
                                    "authorId": "2329735142",
                                    "name": "Furong Huang"
                                },
                                {
                                    "authorId": "2329521764",
                                    "name": "Lichao Sun"
                                },
                                {
                                    "authorId": "2286730795",
                                    "name": "Bo Li"
                                },
                                {
                                    "authorId": "2329517334",
                                    "name": "Yejin Choi"
                                },
                                {
                                    "authorId": "1998918",
                                    "name": "Muhao Chen"
                                },
                                {
                                    "authorId": "2256992327",
                                    "name": "Chaowei Xiao"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 10
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gandikota et al., 2024)",
                        "snippets": [
                            "Prior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior."
                        ],
                        "paper": {
                            "corpus_id": 273098800,
                            "title": "Erasing Conceptual Knowledge from Language Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "2140009998",
                                    "name": "Sheridan Feucht"
                                },
                                {
                                    "authorId": "2225941937",
                                    "name": "Samuel Marks"
                                },
                                {
                                    "authorId": "2284996653",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.6376953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.14738400000000001
    }
}