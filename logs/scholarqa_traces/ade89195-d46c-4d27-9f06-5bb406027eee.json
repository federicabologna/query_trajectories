{
    "query": "How does temperature scaling in the softmax function facilitate the transfer of dark knowledge during knowledge distillation from teacher to student models?",
    "user_id": "lib_user",
    "task_id": "ade89195-d46c-4d27-9f06-5bb406027eee",
    "timestamp": "2025-06-24T00:09:46.833182",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 48,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.41913600000000006,
    "decomposed_query": {
        "rewritten_query": "How temperature scaling in the softmax function facilitates the transfer of dark knowledge during knowledge distillation from teacher to student models.",
        "keyword_query": "temperature scaling softmax function dark knowledge knowledge distillation teacher student models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009819,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2235060849",
                    "name": "Hao Xuan"
                },
                {
                    "authorId": "2348692399",
                    "name": "Bokai Yang"
                },
                {
                    "authorId": "2348481539",
                    "name": "Xingyu Li"
                }
            ],
            "abstract": "The softmax function is a fundamental component in deep learning. This study delves into the often-overlooked parameter within the softmax function, known as\"temperature,\"providing novel insights into the practical and theoretical aspects of temperature scaling for image classification. Our empirical studies, adopting convolutional neural networks and transformers on multiple benchmark datasets, reveal that moderate temperatures generally introduce better overall performance. Through extensive experiments and rigorous theoretical analysis, we explore the role of temperature scaling in model training and unveil that temperature not only influences learning step size but also shapes the model's optimization direction. Moreover, for the first time, we discover a surprising benefit of elevated temperatures: enhanced model robustness against common corruption, natural perturbation, and non-targeted adversarial attacks like Projected Gradient Descent. We extend our discoveries to adversarial training, demonstrating that, compared to the standard softmax function with the default temperature value, higher temperatures have the potential to enhance adversarial training. The insights of this work open new avenues for improving model performance and security in deep learning applications.",
            "corpus_id": 276725399,
            "sentences": [
                {
                    "corpus_id": "276725399",
                    "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
                    "text": "The softmax function has been a longstanding component of neural networks, usually used to normalize a vector of real values into probabilities. Modulating the temperature scaling factor within the softmax function allows for reshaping the probability distribution. This section provides a concise overview of the application of temperature scaling in various computational tasks. \n\nKnowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model. Note that despite various temperatures used during training, it is set to 1 when the model is deployed. \n\nModel Confidence Calibration usually utilizes temperature scaling to address the over-confident issue in deep learning [7], [15], [19]. It centers on estimating predictive uncertainty to match its expected accuracy [13], [14]. Despite multiple generic calibration methods being proposed, temperature scaling proposed by [7] remains a baseline method for being simple, effective and able to apply to various cases without major expense. The motivation behind temperature scaling is simple, since the goal is to control the network's confidence to match its accuracy, applying temperature to the softmax function that can directly modify the probability distribution seems a perfect fit for the problem. During training, a validation set is needed to find the ideal temperature parameter for the network, and the same temperature is used when deployed. \n\nContrastive Learning is one paradigm for unsupervised learning [20], [26]. To achieve a powerful feature encoder, it utilizes contrastive loss to pull similar samples close and push negative pairs away in the latent space.",
                    "score": 0.6489362584307459,
                    "section_title": "II. RELATED WORKS",
                    "char_start_offset": 3806,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 144
                        },
                        {
                            "start": 145,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 380
                        },
                        {
                            "start": 383,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1053
                        },
                        {
                            "start": 1054,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1277
                        },
                        {
                            "start": 1280,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1981
                        },
                        {
                            "start": 1982,
                            "end": 2130
                        },
                        {
                            "start": 2133,
                            "end": 2207
                        },
                        {
                            "start": 2208,
                            "end": 2355
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1399,
                            "end": 1402,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1404,
                            "end": 1408,
                            "matchedPaperCorpusId": "6294674"
                        },
                        {
                            "start": 1410,
                            "end": 1414,
                            "matchedPaperCorpusId": "235435823"
                        },
                        {
                            "start": 1495,
                            "end": 1499,
                            "matchedPaperCorpusId": "202773833"
                        },
                        {
                            "start": 1501,
                            "end": 1505,
                            "matchedPaperCorpusId": "49314079"
                        },
                        {
                            "start": 1600,
                            "end": 1603,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 2202,
                            "end": 2206,
                            "matchedPaperCorpusId": "4591284"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97607421875
                }
            ],
            "relevance_judgement": 0.97607421875,
            "relevance_judgment_input_expanded": "# Title: Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness\n# Venue: arXiv.org\n# Authors: Hao Xuan, Bokai Yang, Xingyu Li\n## Abstract\nThe softmax function is a fundamental component in deep learning. This study delves into the often-overlooked parameter within the softmax function, known as\"temperature,\"providing novel insights into the practical and theoretical aspects of temperature scaling for image classification. Our empirical studies, adopting convolutional neural networks and transformers on multiple benchmark datasets, reveal that moderate temperatures generally introduce better overall performance. Through extensive experiments and rigorous theoretical analysis, we explore the role of temperature scaling in model training and unveil that temperature not only influences learning step size but also shapes the model's optimization direction. Moreover, for the first time, we discover a surprising benefit of elevated temperatures: enhanced model robustness against common corruption, natural perturbation, and non-targeted adversarial attacks like Projected Gradient Descent. We extend our discoveries to adversarial training, demonstrating that, compared to the standard softmax function with the default temperature value, higher temperatures have the potential to enhance adversarial training. The insights of this work open new avenues for improving model performance and security in deep learning applications.\n## II. RELATED WORKS\nThe softmax function has been a longstanding component of neural networks, usually used to normalize a vector of real values into probabilities. Modulating the temperature scaling factor within the softmax function allows for reshaping the probability distribution. This section provides a concise overview of the application of temperature scaling in various computational tasks. \n\nKnowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model. Note that despite various temperatures used during training, it is set to 1 when the model is deployed. \n\nModel Confidence Calibration usually utilizes temperature scaling to address the over-confident issue in deep learning [7], [15], [19]. It centers on estimating predictive uncertainty to match its expected accuracy [13], [14]. Despite multiple generic calibration methods being proposed, temperature scaling proposed by [7] remains a baseline method for being simple, effective and able to apply to various cases without major expense. The motivation behind temperature scaling is simple, since the goal is to control the network's confidence to match its accuracy, applying temperature to the softmax function that can directly modify the probability distribution seems a perfect fit for the problem. During training, a validation set is needed to find the ideal temperature parameter for the network, and the same temperature is used when deployed. \n\nContrastive Learning is one paradigm for unsupervised learning [20], [26]. To achieve a powerful feature encoder, it utilizes contrastive loss to pull similar samples close and push negative pairs away in the latent space.",
            "reference_string": "[276725399 | Xuan et al. | 2025 | Citations: 4]"
        },
        {
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 102,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.09668",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.09668, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2093329661",
                    "name": "Sushil Thapa"
                }
            ],
            "abstract": "Knowledge distillation is a popular machine learning technique that aims to transfer knowledge from a large \u2018teacher\u2019 network to a smaller \u2018student\u2019 network and improve the student\u2019s performance by training it to emulate the teacher. In recent years, there has been significant progress in novel distillation techniques that push performance frontiers across multiple problems and benchmarks. Most of the reported work focuses on achieving state-of-the-art results on the specific problem. However, there has been a significant gap in understanding the process and how it behaves under certain training scenarios. Similarly, transfer learning (TL) is an effective technique in training neural networks on a limited dataset faster by reusing representations learned from a different but related problem. Despite its effectiveness and popularity, there has not been much exploration of knowledge distillation on transfer learning. In this thesis, we propose a machine learning architecture we call TL+KD that combines knowledge distillation with transfer learning; we then present a quantitative and qualitative comparison of TL+KD with TL in the domain of image classification. Through this work, we show that using guidance and knowledge from a larger teacher network during fine-tuning, we can improve the student network to achieve better validation performances like accuracy. We characterize the improvement in the validation performance of the model using a variety of metrics beyond just accuracy scores, and study its performance in scenarios such as input degradation.",
            "corpus_id": 252968120,
            "sentences": [
                {
                    "corpus_id": "252968120",
                    "title": "On effects of Knowledge Distillation on Transfer Learning",
                    "text": ", we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures.\n\nVanilla form of knowledge distillation originally proposed in [14] in classification problem learns with a combined total loss of hard and soft labels as:\n\nHere, z s and z t are soft logits of the student and teacher network, respectively, with the temperature parameter T in the Softmax function \u03c3, y is the ground truth, H is the loss function, and \u03b1 is the hyperparameter for the weight of the distillation. \u03b1 controls the weight",
                    "score": 0.5734161849197388,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 26503,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 41,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1914,
                            "end": 1918,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9638671875
                },
                {
                    "corpus_id": "252968120",
                    "title": "On effects of Knowledge Distillation on Transfer Learning",
                    "text": "Knowledge distillation is a way to transfer knowledge from large ensembles of strong models to smaller/faster networks. Knowledge distillation consists of two different networks, called the teacher network and the student network. Teacher networks are generally large ensembles of networks or a single neural network larger than student networks. The teacher network, generally pre-trained in vanilla form, takes in data and outputs logits, which the student network tries to mimic. Logits are the values we get as an output of the neural networks. Logits are generally passed through normalization layers such as Softmax. The softmax layer converts the real values to a distribution of probabilities for K classes that add up to 1. The loss function compares the output of both teacher and student models and provides a loss value used to iterate and train the Student through Backpropagation [37]. The architecture of vanilla knowledge distillation (or the teacher-student framework) is shown in Figure 1 The student's output is compared with the teacher's output instead of the ground truth that generally occurs during transfer learning. The student network is then optimized and trained with the signal from the loss function. The teacher network is frozen and is not trained during this process. This figure is adapted from [18].\n\nDuring training, a batch of images(x) is passed through the network to produce a vector of raw outputs(z(x)). If there are K classes from which the model has to choose, the dimension of the raw output is also K. To give the probabilities for each class, the raw outputs are passed through normalization layers like Softmax. For an input x and the raw output vector z(x) = [z 1 (x), z 2 (x)...z K (x)] where K is the number of classes, i th output from the Softmax function is:\n\nwhere j ranges from 0 to K and i is an instance of j.\n\nFurthermore, as proposed in a seminal paper [14], we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of",
                    "score": 0.633540842221183,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 24585,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 894,
                            "end": 898,
                            "matchedPaperCorpusId": "121072881"
                        },
                        {
                            "start": 1330,
                            "end": 1334,
                            "matchedPaperCorpusId": "219559263"
                        },
                        {
                            "start": 1914,
                            "end": 1918,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80712890625
                }
            ],
            "relevance_judgement": 0.9638671875,
            "relevance_judgment_input_expanded": "# Title: On effects of Knowledge Distillation on Transfer Learning\n# Venue: arXiv.org\n# Authors: Sushil Thapa\n## Abstract\nKnowledge distillation is a popular machine learning technique that aims to transfer knowledge from a large \u2018teacher\u2019 network to a smaller \u2018student\u2019 network and improve the student\u2019s performance by training it to emulate the teacher. In recent years, there has been significant progress in novel distillation techniques that push performance frontiers across multiple problems and benchmarks. Most of the reported work focuses on achieving state-of-the-art results on the specific problem. However, there has been a significant gap in understanding the process and how it behaves under certain training scenarios. Similarly, transfer learning (TL) is an effective technique in training neural networks on a limited dataset faster by reusing representations learned from a different but related problem. Despite its effectiveness and popularity, there has not been much exploration of knowledge distillation on transfer learning. In this thesis, we propose a machine learning architecture we call TL+KD that combines knowledge distillation with transfer learning; we then present a quantitative and qualitative comparison of TL+KD with TL in the domain of image classification. Through this work, we show that using guidance and knowledge from a larger teacher network during fine-tuning, we can improve the student network to achieve better validation performances like accuracy. We characterize the improvement in the validation performance of the model using a variety of metrics beyond just accuracy scores, and study its performance in scenarios such as input degradation.\n## Knowledge Distillation\nKnowledge distillation is a way to transfer knowledge from large ensembles of strong models to smaller/faster networks. Knowledge distillation consists of two different networks, called the teacher network and the student network. Teacher networks are generally large ensembles of networks or a single neural network larger than student networks. The teacher network, generally pre-trained in vanilla form, takes in data and outputs logits, which the student network tries to mimic. Logits are the values we get as an output of the neural networks. Logits are generally passed through normalization layers such as Softmax. The softmax layer converts the real values to a distribution of probabilities for K classes that add up to 1. The loss function compares the output of both teacher and student models and provides a loss value used to iterate and train the Student through Backpropagation [37]. The architecture of vanilla knowledge distillation (or the teacher-student framework) is shown in Figure 1 The student's output is compared with the teacher's output instead of the ground truth that generally occurs during transfer learning. The student network is then optimized and trained with the signal from the loss function. The teacher network is frozen and is not trained during this process. This figure is adapted from [18].\n\nDuring training, a batch of images(x) is passed through the network to produce a vector of raw outputs(z(x)). If there are K classes from which the model has to choose, the dimension of the raw output is also K. To give the probabilities for each class, the raw outputs are passed through normalization layers like Softmax. For an input x and the raw output vector z(x) = [z 1 (x), z 2 (x)...z K (x)] where K is the number of classes, i th output from the Softmax function is:\n\nwhere j ranges from 0 to K and i is an instance of j.\n\nFurthermore, as proposed in a seminal paper [14], we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of\n...\n, we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures.\n\nVanilla form of knowledge distillation originally proposed in [14] in classification problem learns with a combined total loss of hard and soft labels as:\n\nHere, z s and z t are soft logits of the student and teacher network, respectively, with the temperature parameter T in the Softmax function \u03c3, y is the ground truth, H is the loss function, and \u03b1 is the hyperparameter for the weight of the distillation. \u03b1 controls the weight",
            "reference_string": "[252968120 | Thapa | 2022 | Citations: 1]"
        },
        {
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 46,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-55483-x.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11322354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259997326",
                    "name": "Kangjie Cao"
                },
                {
                    "authorId": "2288621104",
                    "name": "Ting Zhang"
                },
                {
                    "authorId": "2269063771",
                    "name": "Jueqiao Huang"
                }
            ],
            "abstract": "In the field of engineering systems\u2014particularly in underground drilling and green stormwater management\u2014real-time predictions are vital for enhancing operational performance, ensuring safety, and increasing efficiency. Addressing this niche, our study introduces a novel LSTM-transformer hybrid architecture, uniquely specialized for multi-task real-time predictions. Building on advancements in attention mechanisms and sequence modeling, our model integrates the core strengths of LSTM and Transformer architectures, offering a superior alternative to traditional predictive models. Further enriched with online learning, our architecture dynamically adapts to variable operational conditions and continuously incorporates new field data. Utilizing knowledge distillation techniques, we efficiently transfer insights from larger, pretrained networks, thereby achieving high predictive accuracy without sacrificing computational resources. Rigorous experiments on sector-specific engineering datasets validate the robustness and effectiveness of our approach. Notably, our model exhibits clear advantages over existing methods in terms of predictive accuracy, real-time adaptability, and computational efficiency. This work contributes a pioneering predictive framework for targeted engineering applications, offering actionable insights into.",
            "corpus_id": 268060109,
            "sentences": [
                {
                    "corpus_id": "268060109",
                    "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
                    "text": "The softmax loss, also known as the cross-entropy loss between the teacher's softened outputs and the student's predictions, plays a pivotal role in knowledge distillation. We handle this loss calculation as follows: \n\nThe softmax function with temperature scaling is applied to both the teacher's and student's logits, generating softened probability distributions. \n\nThe cross-entropy loss is then computed between these two distributions. This loss quantifies the difference between the teacher's guidance and the student's current understanding. \n\n(5) \n\nThis cross-entropy loss is combined with the traditional loss function to form the final loss function for training the student model. This combined loss function guides the student to not only mimic the teacher's output distribution but also to align correctly with the ground truth. \n\nThe choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer. \n\nEquations: Softmax function with temperature scaling: \n\nwhere z i is the logit for class i and T is the temperature.",
                    "score": 0.8037073820339646,
                    "section_title": "Softmax loss calculation:",
                    "char_start_offset": 23189,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 216
                        },
                        {
                            "start": 219,
                            "end": 366
                        },
                        {
                            "start": 369,
                            "end": 441
                        },
                        {
                            "start": 442,
                            "end": 549
                        },
                        {
                            "start": 552,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 842
                        },
                        {
                            "start": 845,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1161
                        },
                        {
                            "start": 1162,
                            "end": 1320
                        },
                        {
                            "start": 1323,
                            "end": 1376
                        },
                        {
                            "start": 1379,
                            "end": 1439
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems\n# Venue: Scientific Reports\n# Authors: Kangjie Cao, Ting Zhang, Jueqiao Huang\n## Abstract\nIn the field of engineering systems\u2014particularly in underground drilling and green stormwater management\u2014real-time predictions are vital for enhancing operational performance, ensuring safety, and increasing efficiency. Addressing this niche, our study introduces a novel LSTM-transformer hybrid architecture, uniquely specialized for multi-task real-time predictions. Building on advancements in attention mechanisms and sequence modeling, our model integrates the core strengths of LSTM and Transformer architectures, offering a superior alternative to traditional predictive models. Further enriched with online learning, our architecture dynamically adapts to variable operational conditions and continuously incorporates new field data. Utilizing knowledge distillation techniques, we efficiently transfer insights from larger, pretrained networks, thereby achieving high predictive accuracy without sacrificing computational resources. Rigorous experiments on sector-specific engineering datasets validate the robustness and effectiveness of our approach. Notably, our model exhibits clear advantages over existing methods in terms of predictive accuracy, real-time adaptability, and computational efficiency. This work contributes a pioneering predictive framework for targeted engineering applications, offering actionable insights into.\n## Softmax loss calculation:\nThe softmax loss, also known as the cross-entropy loss between the teacher's softened outputs and the student's predictions, plays a pivotal role in knowledge distillation. We handle this loss calculation as follows: \n\nThe softmax function with temperature scaling is applied to both the teacher's and student's logits, generating softened probability distributions. \n\nThe cross-entropy loss is then computed between these two distributions. This loss quantifies the difference between the teacher's guidance and the student's current understanding. \n\n(5) \n\nThis cross-entropy loss is combined with the traditional loss function to form the final loss function for training the student model. This combined loss function guides the student to not only mimic the teacher's output distribution but also to align correctly with the ground truth. \n\nThe choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer. \n\nEquations: Softmax function with temperature scaling: \n\nwhere z i is the logit for class i and T is the temperature.",
            "reference_string": "[268060109 | Cao et al. | 2024 | Citations: 46]"
        },
        {
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "venue": "",
            "year": 2023,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.13621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268314644",
                    "name": "Chi-Ping Su"
                },
                {
                    "authorId": "1569686364",
                    "name": "Ching-Hsun Tseng"
                },
                {
                    "authorId": "2338265886",
                    "name": "Bin Pu"
                },
                {
                    "authorId": "2338506240",
                    "name": "Lei Zhao"
                },
                {
                    "authorId": "2328588941",
                    "name": "Zhuangzhuang Chen"
                },
                {
                    "authorId": "2116351339",
                    "name": "Shin-Jye Lee"
                }
            ],
            "abstract": "Knowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.",
            "corpus_id": 265444951,
            "sentences": [
                {
                    "corpus_id": "265444951",
                    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                    "text": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
                    "score": 0.5676894088672171,
                    "section_title": "Preliminaries",
                    "char_start_offset": 9275,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 258
                        },
                        {
                            "start": 261,
                            "end": 436
                        },
                        {
                            "start": 437,
                            "end": 481
                        },
                        {
                            "start": 482,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 774
                        },
                        {
                            "start": 775,
                            "end": 808
                        },
                        {
                            "start": 811,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1009
                        },
                        {
                            "start": 1010,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1260
                        },
                        {
                            "start": 1261,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1745
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95361328125
                }
            ],
            "relevance_judgement": 0.95361328125,
            "relevance_judgment_input_expanded": "# Title: EA-KD: Entropy-based Adaptive Knowledge Distillation\n# Venue: \n# Authors: Chi-Ping Su, Ching-Hsun Tseng, Bin Pu, Lei Zhao, Zhuangzhuang Chen, Shin-Jye Lee\n## Abstract\nKnowledge distillation (KD) enables a smaller\"student\"model to mimic a larger\"teacher\"model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quantifies each sample's learning value by strategically combining the entropy of the teacher and student output, then dynamically reweights the distillation loss to place greater emphasis on high-value samples. Extensive experiments across diverse KD frameworks and tasks$\\unicode{x2014}$including image classification, object detection, and large language model (LLM) distillation$\\unicode{x2014}$demonstrate that EA-KD consistently enhances performance, achieving state-of-the-art results with negligible computational cost. Our code will be publicly available.\n## Preliminaries\nThe goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
            "reference_string": "[265444951 | Su et al. | 2023 | Citations: 2]"
        },
        {
            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
            "venue": "",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.14520, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2162621330",
                    "name": "Minh-Duong Nguyen"
                },
                {
                    "authorId": "145436642",
                    "name": "Viet Quoc Pham"
                },
                {
                    "authorId": "2233724",
                    "name": "D. Hoang"
                },
                {
                    "authorId": "1389575160",
                    "name": "Long Tran-Thanh"
                },
                {
                    "authorId": "30479205",
                    "name": "Diep N. Nguyen"
                },
                {
                    "authorId": "2054135428",
                    "name": "W. Hwang"
                }
            ],
            "abstract": "In real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks; and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.",
            "corpus_id": 252596141,
            "sentences": [
                {
                    "corpus_id": "252596141",
                    "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
                    "text": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
                    "score": 0.687327440469715,
                    "section_title": "A.2 KNOWLEDGE DISTILLATION",
                    "char_start_offset": 25127,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 257
                        },
                        {
                            "start": 258,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 462
                        },
                        {
                            "start": 465,
                            "end": 562
                        },
                        {
                            "start": 565,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1148
                        },
                        {
                            "start": 1149,
                            "end": 1227
                        },
                        {
                            "start": 1230,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1654
                        },
                        {
                            "start": 1657,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 2002
                        },
                        {
                            "start": 2003,
                            "end": 2098
                        },
                        {
                            "start": 2099,
                            "end": 2223
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: Label driven Knowledge Distillation for Federated Learning with non-IID Data\n# Venue: \n# Authors: Minh-Duong Nguyen, Viet Quoc Pham, D. Hoang, Long Tran-Thanh, Diep N. Nguyen, W. Hwang\n## Abstract\nIn real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks; and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.\n## A.2 KNOWLEDGE DISTILLATION\nKnowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
            "reference_string": "[252596141 | Nguyen et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
            "venue": "Mathematics",
            "year": 2022,
            "reference_count": 34,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2227-7390/10/22/4285/pdf?version=1668591858",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/math10224285?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/math10224285, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "144583200",
                    "name": "D. Borza"
                },
                {
                    "authorId": "1821352",
                    "name": "A. Darabant"
                },
                {
                    "authorId": "74810447",
                    "name": "Tudor Alexandru Ileni"
                },
                {
                    "authorId": "150020994",
                    "name": "Alexandru-Ion Marinescu"
                }
            ],
            "abstract": "Large-scale deep learning models have achieved impressive results on a variety of tasks; however, their deployment on edge or mobile devices is still a challenge due to the limited available memory and computational capability. Knowledge distillation is an effective model compression technique, which can boost the performance of a lightweight student network by transferring the knowledge from a more complex model or an ensemble of models. Due to its reduced size, this lightweight model is more suitable for deployment on edge devices. In this paper, we introduce an online knowledge distillation framework, which relies on an original attention mechanism to effectively combine the predictions of a cohort of lightweight (student) networks into a powerful ensemble, and use this as a distillation signal. The proposed aggregation strategy uses the predictions of the individual students as well as ground truth data to determine a set of weights needed for ensembling these predictions. This mechanism is solely used during system training. When testing or at inference time, a single, lightweight student is extracted and used. The extensive experiments we performed on several image classification benchmarks, both by training models from scratch (on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets) and using transfer learning (on Oxford Pets and Oxford Flowers datasets), showed that the proposed framework always leads to an improvement in the accuracy of knowledge-distilled students and demonstrates the effectiveness of the proposed solution. Moreover, in the case of ResNet architecture, we observed that the knowledge-distilled model achieves a higher accuracy than a deeper, individually trained ResNet model.",
            "corpus_id": 253655030,
            "sentences": [
                {
                    "corpus_id": "253655030",
                    "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
                    "text": "Knowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
                    "score": 0.6757643238272186,
                    "section_title": "Related Work",
                    "char_start_offset": 3724,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 676
                        },
                        {
                            "start": 677,
                            "end": 790
                        },
                        {
                            "start": 791,
                            "end": 830
                        },
                        {
                            "start": 833,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1296
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 26,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 265,
                            "end": 268,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 899,
                            "end": 902,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1291,
                            "end": 1295,
                            "matchedPaperCorpusId": "229363322"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: Effective Online Knowledge Distillation via Attention-Based Model Ensembling\n# Venue: Mathematics\n# Authors: D. Borza, A. Darabant, Tudor Alexandru Ileni, Alexandru-Ion Marinescu\n## Abstract\nLarge-scale deep learning models have achieved impressive results on a variety of tasks; however, their deployment on edge or mobile devices is still a challenge due to the limited available memory and computational capability. Knowledge distillation is an effective model compression technique, which can boost the performance of a lightweight student network by transferring the knowledge from a more complex model or an ensemble of models. Due to its reduced size, this lightweight model is more suitable for deployment on edge devices. In this paper, we introduce an online knowledge distillation framework, which relies on an original attention mechanism to effectively combine the predictions of a cohort of lightweight (student) networks into a powerful ensemble, and use this as a distillation signal. The proposed aggregation strategy uses the predictions of the individual students as well as ground truth data to determine a set of weights needed for ensembling these predictions. This mechanism is solely used during system training. When testing or at inference time, a single, lightweight student is extracted and used. The extensive experiments we performed on several image classification benchmarks, both by training models from scratch (on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets) and using transfer learning (on Oxford Pets and Oxford Flowers datasets), showed that the proposed framework always leads to an improvement in the accuracy of knowledge-distilled students and demonstrates the effectiveness of the proposed solution. Moreover, in the case of ResNet architecture, we observed that the knowledge-distilled model achieves a higher accuracy than a deeper, individually trained ResNet model.\n## Related Work\nKnowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
            "reference_string": "[253655030 | Borza et al. | 2022 | Citations: 2]"
        },
        {
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 55,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.00382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327960212",
                    "name": "Chengyu Li"
                },
                {
                    "authorId": "2320341",
                    "name": "Debo Cheng"
                },
                {
                    "authorId": "2152233401",
                    "name": "Guixian Zhang"
                },
                {
                    "authorId": "2316406036",
                    "name": "Yi Li"
                },
                {
                    "authorId": "2186753918",
                    "name": "Shichao Zhang"
                }
            ],
            "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilization of available graph information. To address this tradeoff, we propose an effective strategy to balance fairness and utility in knowledge distillation. Specifically, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation, leveraging a causal graph model to guide and optimize the design of the distillation process. Specifically, FairDTD employs two fairness-oriented teacher models: a feature teacher and a structure teacher, to facilitate dual distillation, with the student model learning fairness knowledge from the teachers while also leveraging full data to mitigate utility loss. To enhance information transfer, we incorporate graph-level distillation to provide an indirect supplement of graph information during training, as well as a node-specific temperature module to improve the comprehensive transfer of fair knowledge. Experiments on diverse benchmark datasets demonstrate that FairDTD achieves optimal fairness while preserving high model utility, showcasing its effectiveness in fair representation learning for GNNs.",
            "corpus_id": 274436184,
            "sentences": [
                {
                    "corpus_id": "274436184",
                    "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
                    "text": "In this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
                    "score": 0.7337937035485411,
                    "section_title": "Learning node-specific temperatures",
                    "char_start_offset": 17591,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 668
                        },
                        {
                            "start": 669,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 883
                        },
                        {
                            "start": 886,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1452
                        },
                        {
                            "start": 1453,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1575
                        },
                        {
                            "start": 1578,
                            "end": 1677
                        },
                        {
                            "start": 1678,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 1855
                        },
                        {
                            "start": 1858,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2121
                        },
                        {
                            "start": 2122,
                            "end": 2229
                        },
                        {
                            "start": 2230,
                            "end": 2309
                        },
                        {
                            "start": 2310,
                            "end": 2473
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1550,
                            "end": 1574,
                            "matchedPaperCorpusId": "219558831"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88623046875
                }
            ],
            "relevance_judgement": 0.88623046875,
            "relevance_judgment_input_expanded": "# Title: Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Chengyu Li, Debo Cheng, Guixian Zhang, Yi Li, Shichao Zhang\n## Abstract\nGraph Neural Networks (GNNs) have demonstrated strong performance in graph representation learning across various real-world applications. However, they often produce biased predictions caused by sensitive attributes, such as religion or gender, an issue that has been largely overlooked in existing methods. Recently, numerous studies have focused on reducing biases in GNNs. However, these approaches often rely on training with partial data (e.g., using either node features or graph structure alone), which can enhance fairness but frequently compromises model utility due to the limited utilization of available graph information. To address this tradeoff, we propose an effective strategy to balance fairness and utility in knowledge distillation. Specifically, we introduce FairDTD, a novel Fair representation learning framework built on Dual-Teacher Distillation, leveraging a causal graph model to guide and optimize the design of the distillation process. Specifically, FairDTD employs two fairness-oriented teacher models: a feature teacher and a structure teacher, to facilitate dual distillation, with the student model learning fairness knowledge from the teachers while also leveraging full data to mitigate utility loss. To enhance information transfer, we incorporate graph-level distillation to provide an indirect supplement of graph information during training, as well as a node-specific temperature module to improve the comprehensive transfer of fair knowledge. Experiments on diverse benchmark datasets demonstrate that FairDTD achieves optimal fairness while preserving high model utility, showcasing its effectiveness in fair representation learning for GNNs.\n## Learning node-specific temperatures\nIn this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
            "reference_string": "[274436184 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Multitask Emotion Recognition with Incomplete Labels",
            "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
            "year": 2020,
            "reference_count": 36,
            "citation_count": 95,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2002.03557",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.03557, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46203817",
                    "name": "Didan Deng"
                },
                {
                    "authorId": "2808872",
                    "name": "Zhaokang Chen"
                },
                {
                    "authorId": "2075335081",
                    "name": "Bertram E. Shi"
                }
            ],
            "abstract": "We train a unified model to perform three tasks: facial action unit detection, expression classification, and valence-arousal estimation. We address two main challenges of learning the three tasks. First, most existing datasets are highly imbalanced. Second, most existing datasets do not contain labels for all three tasks. To tackle the first challenge, we apply data balancing techniques to experimental datasets. To tackle the second challenge, we propose an algorithm for the multitask model to learn from missing (incomplete) labels. This algorithm has two steps. We first train a teacher model to perform all three tasks, where each instance is trained by the ground truth label of its corresponding task. Secondly, we refer to the outputs of the teacher model as the soft labels. We use the soft labels and the ground truth to train the student model. We find that most of the student models outperform their teacher model on all the three tasks. Finally, we use model ensembling to boost performance further on the three tasks. Our code is publicly available1.1https://github.com/wtomin/multitask-Emotion-Recognition-withIncomplete-Labels",
            "corpus_id": 212644537,
            "sentences": [
                {
                    "corpus_id": "212644537",
                    "title": "Multitask Emotion Recognition with Incomplete Labels",
                    "text": "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels. Knowledge Distillation has been proved effective in model compression, continual learning [27] and domain adaptation [1]. However, its application to multitask learning with missing labels is under-researched. \n\nKnowledge distillation for regression is not as common as for classification. Some work in face alignment [26], [32] has used the L1 or L2 distance as the distillation loss function. \n\nTo enable the use of knowledge distillation using the hyperparameter temperature for valence-arousal estimation, we transform the regression task to a classification task by discretizing the continuous values. Then we can use temperature to control the smoothness of the soft labels.",
                    "score": 0.568488483274487,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 6487,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 73
                        },
                        {
                            "start": 74,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 729
                        },
                        {
                            "start": 732,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 914
                        },
                        {
                            "start": 917,
                            "end": 1126
                        },
                        {
                            "start": 1127,
                            "end": 1200
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 610,
                            "end": 614,
                            "matchedPaperCorpusId": "4853851"
                        },
                        {
                            "start": 637,
                            "end": 640,
                            "matchedPaperCorpusId": "206742843"
                        },
                        {
                            "start": 838,
                            "end": 842,
                            "matchedPaperCorpusId": "8911820"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87353515625
                }
            ],
            "relevance_judgement": 0.87353515625,
            "relevance_judgment_input_expanded": "# Title: Multitask Emotion Recognition with Incomplete Labels\n# Venue: IEEE International Conference on Automatic Face & Gesture Recognition\n# Authors: Didan Deng, Zhaokang Chen, Bertram E. Shi\n## Abstract\nWe train a unified model to perform three tasks: facial action unit detection, expression classification, and valence-arousal estimation. We address two main challenges of learning the three tasks. First, most existing datasets are highly imbalanced. Second, most existing datasets do not contain labels for all three tasks. To tackle the first challenge, we apply data balancing techniques to experimental datasets. To tackle the second challenge, we propose an algorithm for the multitask model to learn from missing (incomplete) labels. This algorithm has two steps. We first train a teacher model to perform all three tasks, where each instance is trained by the ground truth label of its corresponding task. Secondly, we refer to the outputs of the teacher model as the soft labels. We use the soft labels and the ground truth to train the student model. We find that most of the student models outperform their teacher model on all the three tasks. Finally, we use model ensembling to boost performance further on the three tasks. Our code is publicly available1.1https://github.com/wtomin/multitask-Emotion-Recognition-withIncomplete-Labels\n## C. Knowledge Distillation\nHinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels. Knowledge Distillation has been proved effective in model compression, continual learning [27] and domain adaptation [1]. However, its application to multitask learning with missing labels is under-researched. \n\nKnowledge distillation for regression is not as common as for classification. Some work in face alignment [26], [32] has used the L1 or L2 distance as the distillation loss function. \n\nTo enable the use of knowledge distillation using the hyperparameter temperature for valence-arousal estimation, we transform the regression task to a classification task by discretizing the continuous values. Then we can use temperature to control the smoothness of the soft labels.",
            "reference_string": "[212644537 | Deng et al. | 2020 | Citations: 95]"
        },
        {
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "venue": "IEEE Internet of Things Journal",
            "year": 2021,
            "reference_count": 123,
            "citation_count": 52,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://zenodo.org/records/5105970/files/09366901.pdf",
                "status": "GREEN",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/JIOT.2021.3063147?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/JIOT.2021.3063147, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1993679419",
                    "name": "Qihua Zhou"
                },
                {
                    "authorId": "2034348867",
                    "name": "Zhihao Qu"
                },
                {
                    "authorId": "144123438",
                    "name": "Song Guo"
                },
                {
                    "authorId": "2150444053",
                    "name": "Boyuan Luo"
                },
                {
                    "authorId": "8572939",
                    "name": "Jingcai Guo"
                },
                {
                    "authorId": "150358457",
                    "name": "Zhenda Xu"
                },
                {
                    "authorId": "1854500",
                    "name": "R. Akerkar"
                }
            ],
            "abstract": "Modern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.",
            "corpus_id": 233777216,
            "sentences": [
                {
                    "corpus_id": "233777216",
                    "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
                    "text": "However, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
                    "score": 0.6529731329615561,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 29410,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 134
                        },
                        {
                            "start": 137,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 533
                        },
                        {
                            "start": 534,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 956
                        },
                        {
                            "start": 959,
                            "end": 1077
                        },
                        {
                            "start": 1078,
                            "end": 1185
                        },
                        {
                            "start": 1186,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1861
                        },
                        {
                            "start": 1864,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2235
                        },
                        {
                            "start": 2236,
                            "end": 2349
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1620,
                            "end": 1624,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.873046875
                }
            ],
            "relevance_judgement": 0.873046875,
            "relevance_judgment_input_expanded": "# Title: On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective\n# Venue: IEEE Internet of Things Journal\n# Authors: Qihua Zhou, Zhihao Qu, Song Guo, Boyuan Luo, Jingcai Guo, Zhenda Xu, R. Akerkar\n## Abstract\nModern machine learning (ML) applications are often deployed in the cloud environment to exploit the computational power of clusters. However, this in-cloud computing scheme cannot satisfy the demands of emerging edge intelligence scenarios, including providing personalized models, protecting user privacy, adapting to real-time tasks, and saving resource cost. In order to conquer the limitations of conventional in-cloud computing, there comes the rise of on-device learning, which makes the end-to-end ML procedure totally on user devices, without unnecessary involvement of the cloud. In spite of the promising advantages of on-device learning, implementing a high-performance on-device learning system still faces with many severe challenges, such as insufficient user training data, backward propagation (BP) blocking, and limited peak processing speed. Observing the substantial improvement space in the implementation and acceleration of on-device learning systems, we intend to present a comprehensive analysis of the latest research progress and point out potential optimization directions from the system perspective. This survey presents a software and hardware synergy of on-device learning techniques, covering the scope of model-level neural network design, algorithm-level training optimization, and hardware-level instruction acceleration. We hope this survey could bring fruitful discussions and inspire the researchers to further promote the field of edge intelligence.\n## B. Knowledge Distillation\nHowever, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
            "reference_string": "[233777216 | Zhou et al. | 2021 | Citations: 52]"
        },
        {
            "title": "In-Network Flow Classification With Knowledge Distillation",
            "venue": "IEEE Access",
            "year": 2021,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2021.3100057",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2021.3100057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2021.3100057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145446632",
                    "name": "K. Lin"
                },
                {
                    "authorId": "2124901704",
                    "name": "Chen-yang Li"
                }
            ],
            "abstract": "Recent research has incorporated machine learning with software-defined networking to support intelligent traffic engineering. However, most frameworks only enable machine learning in remote controllers, which introduce significant signaling overhead and data forwarding costs. In this work, we present a new architecture called in-network inference (INI) to realize local learning in Neural Compute Stick (NCS), a portable device that can be connected to a programmable switch via a USB port. While NCS can flexibly extend the computing power of a switch, its limited capacity however cannot afford real-time inference for enormous traffic demands. To develop a practical local learning architecture, we design a two-phase learning framework that combines local learning with knowledge distillation and remote learning to achieve lightweight but accurate traffic classification. We further design an inference model deployment and adaptation algorithm to utilize multiple NCS devices equipped with different switches to share the inference workload of a network. Our testbed experiments show that the two-phase learning framework reduces the inference rejection rate by 46.5% and maintains the inference accuracy of 98.10%. The trace-driven simulations verify that the proposed adaptive model placement scheme considers load balancing and, hence, better utilizes the computing resources of NCS to serve dynamic inference requests.",
            "corpus_id": 237002008,
            "sentences": [
                {
                    "corpus_id": "237002008",
                    "title": "In-Network Flow Classification With Knowledge Distillation",
                    "text": "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model. \n\nTo enable knowledge distillation with temperature, the temperature of the soft label output by the teacher model should be the same with the temperature of the student model. Hence, the student model should now have two outputs, one using the standard softmax function (T = 1) for calculating the student loss and the other using the softmax function with the temperature T = \u03c4 . Then, the overall loss function can be expressed by \n\nwhere x is the input, W is the vector of the student model parameters, y is the ground truth label, H is the cross-entropy loss function and \u03c3 is the softmax function with temperature T . The coefficient \u03bb is used to determine the weight of the student loss and the distillation loss. By setting a large value of \u03bb, we emphasize more the knowledge transformation from the teacher model to the student model. The hyper-parameter T can be fine-tuned to adapt the learning efficiency from the teacher model.",
                    "score": 0.664025759239378,
                    "section_title": "2) SOFTMAX TEMPERATURE",
                    "char_start_offset": 15303,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 724
                        },
                        {
                            "start": 727,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 827
                        },
                        {
                            "start": 828,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1085
                        },
                        {
                            "start": 1088,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1519
                        },
                        {
                            "start": 1522,
                            "end": 1709
                        },
                        {
                            "start": 1710,
                            "end": 1806
                        },
                        {
                            "start": 1807,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                }
            ],
            "relevance_judgement": 0.86865234375,
            "relevance_judgment_input_expanded": "# Title: In-Network Flow Classification With Knowledge Distillation\n# Venue: IEEE Access\n# Authors: K. Lin, Chen-yang Li\n## Abstract\nRecent research has incorporated machine learning with software-defined networking to support intelligent traffic engineering. However, most frameworks only enable machine learning in remote controllers, which introduce significant signaling overhead and data forwarding costs. In this work, we present a new architecture called in-network inference (INI) to realize local learning in Neural Compute Stick (NCS), a portable device that can be connected to a programmable switch via a USB port. While NCS can flexibly extend the computing power of a switch, its limited capacity however cannot afford real-time inference for enormous traffic demands. To develop a practical local learning architecture, we design a two-phase learning framework that combines local learning with knowledge distillation and remote learning to achieve lightweight but accurate traffic classification. We further design an inference model deployment and adaptation algorithm to utilize multiple NCS devices equipped with different switches to share the inference workload of a network. Our testbed experiments show that the two-phase learning framework reduces the inference rejection rate by 46.5% and maintains the inference accuracy of 98.10%. The trace-driven simulations verify that the proposed adaptive model placement scheme considers load balancing and, hence, better utilizes the computing resources of NCS to serve dynamic inference requests.\n## 2) SOFTMAX TEMPERATURE\nDuring distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model. \n\nTo enable knowledge distillation with temperature, the temperature of the soft label output by the teacher model should be the same with the temperature of the student model. Hence, the student model should now have two outputs, one using the standard softmax function (T = 1) for calculating the student loss and the other using the softmax function with the temperature T = \u03c4 . Then, the overall loss function can be expressed by \n\nwhere x is the input, W is the vector of the student model parameters, y is the ground truth label, H is the cross-entropy loss function and \u03c3 is the softmax function with temperature T . The coefficient \u03bb is used to determine the weight of the student loss and the distillation loss. By setting a large value of \u03bb, we emphasize more the knowledge transformation from the teacher model to the student model. The hyper-parameter T can be fine-tuned to adapt the learning efficiency from the teacher model.",
            "reference_string": "[237002008 | Lin et al. | 2021 | Citations: 0]"
        },
        {
            "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15594, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354312339",
                    "name": "Tatsuhito Hasegawa"
                },
                {
                    "authorId": "2348382817",
                    "name": "Shunsuke Sakai"
                }
            ],
            "abstract": "In deep learning-based classification tasks, the softmax function's temperature parameter $T$ critically influences the output distribution and overall performance. This study presents a novel theoretical insight that the optimal temperature $T^*$ is uniquely determined by the dimensionality of the feature representations, thereby enabling training-free determination of $T^*$. Despite this theoretical grounding, empirical evidence reveals that $T^*$ fluctuates under practical conditions owing to variations in models, datasets, and other confounding factors. To address these influences, we propose and optimize a set of temperature determination coefficients that specify how $T^*$ should be adjusted based on the theoretical relationship to feature dimensionality. Additionally, we insert a batch normalization layer immediately before the output layer, effectively stabilizing the feature space. Building on these coefficients and a suite of large-scale experiments, we develop an empirical formula to estimate $T^*$ without additional training while also introducing a corrective scheme to refine $T^*$ based on the number of classes and task complexity. Our findings confirm that the derived temperature not only aligns with the proposed theoretical perspective but also generalizes effectively across diverse tasks, consistently enhancing classification performance and offering a practical, training-free solution for determining $T^*$.",
            "corpus_id": 277994315,
            "sentences": [
                {
                    "corpus_id": "277994315",
                    "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
                    "text": "Several studies have utilized the temperature of the softmax function, with knowledge distillation (KD) [5,13,14,15] and temperature scaling [6,16] serving as representative examples. Knowledge distillation is a method for transferring knowledge from a teacher model to a student model. This method aims to improve performance and reduce the number of model parameters. In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance. Liu et al. [13] proposed Meta KD, which optimizes T via meta-gradients computed on a validation set. Curriculum Temperature KD (CTKD) [14] introduces an adversarial learning scheme that directly learns the temperature parameter itself. Sun et al. [15] achieve a dynamic temperature by applying logit standardization independently to both the teacher and the student models. Furthermore, temperature scaling [6] reduces the model's prediction bias by correcting temperature parameters. Balanya et al. [16] utilized adaptive temperature in the temperature scaling. In contrastive learning, the temperature parameter is also adjusted to modulate the influence of negative samples [17]. \n\nIn addition to these approaches, Agarwala et al. [10] also highlighted the importance of tuning the temperature parameter in classification tasks. Furthermore, relaxed softmax [18] also proposes a tuning strategy for inverse temperature. In fields other than classification, temperature parameters have attracted attention in deep metric learning. Xu [19] proposed a heated-up strategy that involves training the model with increasing temperature. \n\nHowever, none of the above studies discusses the relationship between the dimensionality of the feature maps M and the optimal temperature T * . While these studies note the importance of tuning the temperature parameter, they do not sufficiently address the question of whether the optimal temperature parameter can be estimated without training the model. \n\nBuilding upon these general insights into temperature tuning, a study in sensor-based human activity recognition has explored the relationship between feature dimensionality and temperature [9]. Denoting the number of dimensions of the output of the feature extractor (e.g., ConvNets) as M , the experimental results indicate a potential relationship between the optimal temperature T * and M . However, there were several limitations: \n\n1.",
                    "score": 0.5496870777242786,
                    "section_title": "Temperature parameter",
                    "char_start_offset": 8052,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 286
                        },
                        {
                            "start": 287,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1191
                        },
                        {
                            "start": 1194,
                            "end": 1340
                        },
                        {
                            "start": 1341,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1641
                        },
                        {
                            "start": 1644,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 2001
                        },
                        {
                            "start": 2004,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2398
                        },
                        {
                            "start": 2399,
                            "end": 2439
                        },
                        {
                            "start": 2442,
                            "end": 2444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 104,
                            "end": 107,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 110,
                            "end": 113,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 113,
                            "end": 116,
                            "matchedPaperCorpusId": "268247468"
                        },
                        {
                            "start": 141,
                            "end": 144,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 144,
                            "end": 147,
                            "matchedPaperCorpusId": "251224012"
                        },
                        {
                            "start": 643,
                            "end": 647,
                            "matchedPaperCorpusId": "254069919"
                        },
                        {
                            "start": 756,
                            "end": 760,
                            "matchedPaperCorpusId": "268247468"
                        },
                        {
                            "start": 916,
                            "end": 919,
                            "matchedPaperCorpusId": "28671436"
                        },
                        {
                            "start": 1009,
                            "end": 1013,
                            "matchedPaperCorpusId": "251224012"
                        },
                        {
                            "start": 1186,
                            "end": 1190,
                            "matchedPaperCorpusId": "229297730"
                        },
                        {
                            "start": 1370,
                            "end": 1374,
                            "matchedPaperCorpusId": "53530110"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.83154296875
                }
            ],
            "relevance_judgement": 0.83154296875,
            "relevance_judgment_input_expanded": "# Title: Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification\n# Venue: arXiv.org\n# Authors: Tatsuhito Hasegawa, Shunsuke Sakai\n## Abstract\nIn deep learning-based classification tasks, the softmax function's temperature parameter $T$ critically influences the output distribution and overall performance. This study presents a novel theoretical insight that the optimal temperature $T^*$ is uniquely determined by the dimensionality of the feature representations, thereby enabling training-free determination of $T^*$. Despite this theoretical grounding, empirical evidence reveals that $T^*$ fluctuates under practical conditions owing to variations in models, datasets, and other confounding factors. To address these influences, we propose and optimize a set of temperature determination coefficients that specify how $T^*$ should be adjusted based on the theoretical relationship to feature dimensionality. Additionally, we insert a batch normalization layer immediately before the output layer, effectively stabilizing the feature space. Building on these coefficients and a suite of large-scale experiments, we develop an empirical formula to estimate $T^*$ without additional training while also introducing a corrective scheme to refine $T^*$ based on the number of classes and task complexity. Our findings confirm that the derived temperature not only aligns with the proposed theoretical perspective but also generalizes effectively across diverse tasks, consistently enhancing classification performance and offering a practical, training-free solution for determining $T^*$.\n## Temperature parameter\nSeveral studies have utilized the temperature of the softmax function, with knowledge distillation (KD) [5,13,14,15] and temperature scaling [6,16] serving as representative examples. Knowledge distillation is a method for transferring knowledge from a teacher model to a student model. This method aims to improve performance and reduce the number of model parameters. In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance. Liu et al. [13] proposed Meta KD, which optimizes T via meta-gradients computed on a validation set. Curriculum Temperature KD (CTKD) [14] introduces an adversarial learning scheme that directly learns the temperature parameter itself. Sun et al. [15] achieve a dynamic temperature by applying logit standardization independently to both the teacher and the student models. Furthermore, temperature scaling [6] reduces the model's prediction bias by correcting temperature parameters. Balanya et al. [16] utilized adaptive temperature in the temperature scaling. In contrastive learning, the temperature parameter is also adjusted to modulate the influence of negative samples [17]. \n\nIn addition to these approaches, Agarwala et al. [10] also highlighted the importance of tuning the temperature parameter in classification tasks. Furthermore, relaxed softmax [18] also proposes a tuning strategy for inverse temperature. In fields other than classification, temperature parameters have attracted attention in deep metric learning. Xu [19] proposed a heated-up strategy that involves training the model with increasing temperature. \n\nHowever, none of the above studies discusses the relationship between the dimensionality of the feature maps M and the optimal temperature T * . While these studies note the importance of tuning the temperature parameter, they do not sufficiently address the question of whether the optimal temperature parameter can be estimated without training the model. \n\nBuilding upon these general insights into temperature tuning, a study in sensor-based human activity recognition has explored the relationship between feature dimensionality and temperature [9]. Denoting the number of dimensions of the output of the feature extractor (e.g., ConvNets) as M , the experimental results indicate a potential relationship between the optimal temperature T * and M . However, there were several limitations: \n\n1.",
            "reference_string": "[277994315 | Hasegawa et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 20,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10422998.pdf",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3363088?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3363088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283215857",
                    "name": "Abdullah Al-Ahmadi"
                }
            ],
            "abstract": "This paper introduces an innovative deep learning-based approach for User Equipment positioning in massive MIMO systems, enhanced by Flying Reconfigurable Intelligent Surfaces. The study emphasizes the transformative impact of Massive MIMO technology, which substantially increases the capacity and efficiency of wireless networks through extensive antenna arrays. We present a knowledge distillation model where a comprehensive teacher model is trained on massive MIMO data to predict User Equipment positions, which in turn guides a more compact student model. This results in a system that not only retains high accuracy with a lower mean absolute error of 3.67m but also benefits from reduced complexity and resource requirements. Our findings reveal that the distilled student model achieves a significant reduction in computational load while maintaining precise positioning capabilities, showcasing the potential for practical deployment in future wireless communication systems.",
            "corpus_id": 267535483,
            "sentences": [
                {
                    "corpus_id": "267535483",
                    "title": "Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces",
                    "text": "where z i is the logit (raw output) of the model for class i. \n\nIn the context of knowledge distillation, the softmax function is modified by introducing the temperature parameter: \n\nIn higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better.",
                    "score": 0.6548672290863191,
                    "section_title": "A. TEACHER MODEL",
                    "char_start_offset": 25377,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 61
                        },
                        {
                            "start": 64,
                            "end": 180
                        },
                        {
                            "start": 183,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 996
                        },
                        {
                            "start": 997,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1221
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82568359375
                }
            ],
            "relevance_judgement": 0.82568359375,
            "relevance_judgment_input_expanded": "# Title: Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces\n# Venue: IEEE Access\n# Authors: Abdullah Al-Ahmadi\n## Abstract\nThis paper introduces an innovative deep learning-based approach for User Equipment positioning in massive MIMO systems, enhanced by Flying Reconfigurable Intelligent Surfaces. The study emphasizes the transformative impact of Massive MIMO technology, which substantially increases the capacity and efficiency of wireless networks through extensive antenna arrays. We present a knowledge distillation model where a comprehensive teacher model is trained on massive MIMO data to predict User Equipment positions, which in turn guides a more compact student model. This results in a system that not only retains high accuracy with a lower mean absolute error of 3.67m but also benefits from reduced complexity and resource requirements. Our findings reveal that the distilled student model achieves a significant reduction in computational load while maintaining precise positioning capabilities, showcasing the potential for practical deployment in future wireless communication systems.\n## A. TEACHER MODEL\nwhere z i is the logit (raw output) of the model for class i. \n\nIn the context of knowledge distillation, the softmax function is modified by introducing the temperature parameter: \n\nIn higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better.",
            "reference_string": "[267535483 | Al-Ahmadi | 2024 | Citations: 1]"
        },
        {
            "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 70,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.13440, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320013374",
                    "name": "Yingyue Li"
                },
                {
                    "authorId": "2060439659",
                    "name": "Bencheng Liao"
                },
                {
                    "authorId": "2313037989",
                    "name": "Wenyu Liu"
                },
                {
                    "authorId": "2266175736",
                    "name": "Xinggang Wang"
                }
            ],
            "abstract": "With the advancement of RNN models with linear complexity, the quadratic complexity challenge of transformers has the potential to be overcome. Notably, the emerging Mamba-2 has demonstrated competitive performance, bridging the gap between RNN models and transformers. However, due to sequential processing and vanishing gradients, RNN models struggle to capture long-range dependencies, limiting contextual understanding. This results in slow convergence, high resource demands, and poor performance on downstream understanding and complex reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2 layers. Leveraging the inherent relationship between attention and Mamba-2, we initialize Mamba-2 with corresponding attention weights to accelerate convergence. Subsequently, we employ a single-stage distillation process, using the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM, further enhancing convergence speed and performance. Furthermore, we investigate the impact of differential distillation loss within our training framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating competitive performance against the teacher model and existing VLMs while surpassing both Mamba-based VLMs and models of comparable parameter scales. Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher model while reducing GPU memory consumption by 27.5%, all without compromising performance. Code and models are released at http://github.com/hustvl/MaTVLM.",
            "corpus_id": 277104482,
            "sentences": [
                {
                    "corpus_id": "277104482",
                    "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
                    "text": "To further enhance the performance of the MaTVLM, we propose a knowledge distillation method that transfers knowledge from transformer layers to Mamba-2 layers. We use a pre-trained VLM as the teacher model and our MaTVLM as the student model. We will introduce the distillation strategies in the following. Probability Distribution Distillation First, our goal is to minimize the distance between probability distributions of the models, just the logits output by the models before applying the softmax function. This approach is widely adopted in knowledge distillation, as aligning the output distributions of the models allows the student model to gain a more nuanced understanding from the teacher model's prediction. To achieve this, we use the kullback-leibler (KL) divergence with a temperature scaling factor as the loss function. The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model. The loss function is defined as follows: \n\nThe softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively: \n\nwhere T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model. \n\nwhere T li (x) and S li (x) represent the outputs of the teacher model and the student model at layer l i , respectively. \n\nSequence Prediction Loss Finally, except of the distillation losses mentioned above, we also calculate the crossentropy loss between the output sequence prediction of the student model and the ground truth. This loss is used to guide the student model to learn the correct sequence prediction, which is crucial for the model to perform well on downstream tasks. The loss function is defined as: \n\nwhere y is the ground truth sequence, and \u0177s is the predicted sequence from the student model.",
                    "score": 0.5285917456665504,
                    "section_title": "Knowledge Distilling Transformers into Hybrid Models",
                    "char_start_offset": 11610,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1024
                        },
                        {
                            "start": 1025,
                            "end": 1065
                        },
                        {
                            "start": 1068,
                            "end": 1242
                        },
                        {
                            "start": 1245,
                            "end": 1458
                        },
                        {
                            "start": 1461,
                            "end": 1582
                        },
                        {
                            "start": 1585,
                            "end": 1791
                        },
                        {
                            "start": 1792,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 1979
                        },
                        {
                            "start": 1982,
                            "end": 2076
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.82421875
                }
            ],
            "relevance_judgement": 0.82421875,
            "relevance_judgment_input_expanded": "# Title: MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling\n# Venue: arXiv.org\n# Authors: Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang\n## Abstract\nWith the advancement of RNN models with linear complexity, the quadratic complexity challenge of transformers has the potential to be overcome. Notably, the emerging Mamba-2 has demonstrated competitive performance, bridging the gap between RNN models and transformers. However, due to sequential processing and vanishing gradients, RNN models struggle to capture long-range dependencies, limiting contextual understanding. This results in slow convergence, high resource demands, and poor performance on downstream understanding and complex reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2 layers. Leveraging the inherent relationship between attention and Mamba-2, we initialize Mamba-2 with corresponding attention weights to accelerate convergence. Subsequently, we employ a single-stage distillation process, using the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM, further enhancing convergence speed and performance. Furthermore, we investigate the impact of differential distillation loss within our training framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating competitive performance against the teacher model and existing VLMs while surpassing both Mamba-based VLMs and models of comparable parameter scales. Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher model while reducing GPU memory consumption by 27.5%, all without compromising performance. Code and models are released at http://github.com/hustvl/MaTVLM.\n## Knowledge Distilling Transformers into Hybrid Models\nTo further enhance the performance of the MaTVLM, we propose a knowledge distillation method that transfers knowledge from transformer layers to Mamba-2 layers. We use a pre-trained VLM as the teacher model and our MaTVLM as the student model. We will introduce the distillation strategies in the following. Probability Distribution Distillation First, our goal is to minimize the distance between probability distributions of the models, just the logits output by the models before applying the softmax function. This approach is widely adopted in knowledge distillation, as aligning the output distributions of the models allows the student model to gain a more nuanced understanding from the teacher model's prediction. To achieve this, we use the kullback-leibler (KL) divergence with a temperature scaling factor as the loss function. The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model. The loss function is defined as follows: \n\nThe softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively: \n\nwhere T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model. \n\nwhere T li (x) and S li (x) represent the outputs of the teacher model and the student model at layer l i , respectively. \n\nSequence Prediction Loss Finally, except of the distillation losses mentioned above, we also calculate the crossentropy loss between the output sequence prediction of the student model and the ground truth. This loss is used to guide the student model to learn the correct sequence prediction, which is crucial for the model to perform well on downstream tasks. The loss function is defined as: \n\nwhere y is the ground truth sequence, and \u0177s is the predicted sequence from the student model.",
            "reference_string": "[277104482 | Li et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "venue": "Fire",
            "year": 2023,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2571-6255/6/12/446/pdf?version=1700635738",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/fire6120446?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/fire6120446, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268047558",
                    "name": "Jinzhou Xie"
                },
                {
                    "authorId": "2268031861",
                    "name": "Hongmin Zhao"
                }
            ],
            "abstract": "This paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.",
            "corpus_id": 265384964,
            "sentences": [
                {
                    "corpus_id": "265384964",
                    "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                    "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
                    "score": 0.777654249706236,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 9617,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 819
                        },
                        {
                            "start": 820,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1315
                        },
                        {
                            "start": 1316,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1697
                        },
                        {
                            "start": 1698,
                            "end": 1761
                        },
                        {
                            "start": 1764,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2255
                        },
                        {
                            "start": 2256,
                            "end": 2403
                        },
                        {
                            "start": 2404,
                            "end": 2463
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8193359375
                },
                {
                    "corpus_id": "265384964",
                    "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                    "text": "This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization. \n\nIn 2015, Hinton et al. discovered the advantages of training the distilled model to not only produce the soft labels from the teacher but also the correct labels relying on the ground truth labels. Consequently, we compute the \"standard\" loss by comparing the predicted class probabilities of the student with the ground truth labels. This loss is named the \"student loss\". When calculating the class probabilities for the student loss, we employ T = 1. The comprehensive loss function, which integrates both the distillation and student losses, is determined as follows: \n\nHere, x represents the input, W represents the student model's parameters, y denotes the ground truth labels, H signifies the cross-entropy loss function, \u03c3 represents the softmax function characterized by the \"temperature\" T, and \u03b1 and \u03b2 are constants. The logits of the student and teacher are denoted as z s and z t , respectively. The general structure of knowledge distillation is depicted in Figure 2. \n\nHere,  represents the input,  represents the student model's parameters,  denotes the ground truth labels,  signifies the cross-entropy loss function,  represents the softmax function characterized by the \"temperature\" , and  and  are constants. The logits of the student and teacher are denoted as  and  , respectively. The general structure of knowledge distillation is depicted in Figure 2.",
                    "score": 0.5629285631782668,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 12021,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 59
                        },
                        {
                            "start": 60,
                            "end": 230
                        },
                        {
                            "start": 233,
                            "end": 430
                        },
                        {
                            "start": 431,
                            "end": 567
                        },
                        {
                            "start": 568,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 686
                        },
                        {
                            "start": 687,
                            "end": 804
                        },
                        {
                            "start": 807,
                            "end": 1060
                        },
                        {
                            "start": 1061,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1214
                        },
                        {
                            "start": 1217,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1610
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6328125
                }
            ],
            "relevance_judgement": 0.8193359375,
            "relevance_judgment_input_expanded": "# Title: Forest Fire Object Detection Analysis Based on Knowledge Distillation\n# Venue: Fire\n# Authors: Jinzhou Xie, Hongmin Zhao\n## Abstract\nThis paper investigates the application of the YOLOv7 object detection model combined with knowledge distillation techniques in forest fire detection. As an advanced object detection model, YOLOv7 boasts efficient real-time detection capabilities. However, its performance may be constrained in resource-limited environments. To address this challenge, this research proposes a novel approach: considering that deep neural networks undergo multi-layer mapping from the input to the output space, we define the knowledge propagation between layers by evaluating the dot product of features extracted from two different layers. To this end, we utilize the Flow of Solution Procedure (FSP) matrix based on the Gram matrix and redesign the distillation loss using the Pearson correlation coefficient, presenting a new knowledge distillation method termed ILKDG (Intermediate Layer Knowledge Distillation with Gram Matrix-based Feature Flow). Compared with the classical knowledge distillation algorithm, KD, ILKDG achieved a significant performance improvement on a self-created forest fire detection dataset. Specifically, without altering the student network\u2019s parameters or network layers, mAP@0.5 improved by 2.9%, and mAP@0.5:0.95 increased by 2.7%. These results indicate that the proposed ILKDG method effectively enhances the accuracy and performance of forest fire detection without introducing additional parameters. The ILKDG method, based on the Gram matrix and Pearson correlation coefficient, presents a novel knowledge distillation approach, providing a fresh avenue for future research. Researchers can further optimize and refine this method to achieve superior results in fire detection.\n## Knowledge Distillation\nKnowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"\n...\nThis kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization. \n\nIn 2015, Hinton et al. discovered the advantages of training the distilled model to not only produce the soft labels from the teacher but also the correct labels relying on the ground truth labels. Consequently, we compute the \"standard\" loss by comparing the predicted class probabilities of the student with the ground truth labels. This loss is named the \"student loss\". When calculating the class probabilities for the student loss, we employ T = 1. The comprehensive loss function, which integrates both the distillation and student losses, is determined as follows: \n\nHere, x represents the input, W represents the student model's parameters, y denotes the ground truth labels, H signifies the cross-entropy loss function, \u03c3 represents the softmax function characterized by the \"temperature\" T, and \u03b1 and \u03b2 are constants. The logits of the student and teacher are denoted as z s and z t , respectively. The general structure of knowledge distillation is depicted in Figure 2. \n\nHere,  represents the input,  represents the student model's parameters,  denotes the ground truth labels,  signifies the cross-entropy loss function,  represents the softmax function characterized by the \"temperature\" , and  and  are constants. The logits of the student and teacher are denoted as  and  , respectively. The general structure of knowledge distillation is depicted in Figure 2.",
            "reference_string": "[265384964 | Xie et al. | 2023 | Citations: 6]"
        },
        {
            "title": "Knowledge Migration Framework for Smart Contract Vulnerability Detection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.11175, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336141326",
                    "name": "Luqi Wang"
                },
                {
                    "authorId": "2335930082",
                    "name": "Wenbao Jiang"
                }
            ],
            "abstract": "As a cornerstone of blockchain technology in the 3.0 era, smart contracts play a pivotal role in the evolution of blockchain systems. In order to address the limitations of existing smart contract vulnerability detection models with regard to their generalisation capability, an AF-STip smart contract vulnerability detection framework incorporating efficient knowledge migration is proposed. AF-STip employs the teacher network as the main model and migrates the knowledge processed by the smart contract to the student model using a data-free knowledge distillation method. The student model utilises this knowledge to enhance its vulnerability detection capabilities. The approach markedly enhances the model's capacity for feature extraction and cross-class adaptation, while concurrently reducing computational overhead.In order to further enhance the extraction of vulnerability features, an adaptive fusion module is proposed in this paper, which aims to strengthen the interaction and fusion of feature information.The experimental results demonstrate that the STip model attains an average F1 value detection score of 91.16% for the four vulnerabilities without disclosing the original smart contract data. To validate the viability of the proposed lightweight migration approach, the student model is deployed in a migration learning task targeting a novel vulnerability type, resulting in an accuracy of 91.02% and an F1 score of 90.46%. To the best of our knowledge, AF-STip is the inaugural model to apply data-free knowledge migration to smart contract vulnerability detection. While markedly reducing the computational overhead, the method still demonstrates exceptional performance in detecting novel vulnerabilities.",
            "corpus_id": 274776589,
            "sentences": [
                {
                    "corpus_id": "274776589",
                    "title": "Knowledge Migration Framework for Smart Contract Vulnerability Detection",
                    "text": "The concept of Knowledge Distillation (KD) was initially proposed by Hinton et al [7]. The method entails transferring knowledge from a teacher model to a student model, thereby reducing the computational overhead while maintaining high performance. The core principle involves softening the Softmax output by a temperature parameter to enhance the similarity between categories and improve the generalisation ability of the student model. The underlying formula is q i = exp(z i /T) (Ej) exp(z j /T) , where z i represents the teacher model output and q i denotes the temperature-smoothed output. \n\nThe research methods employed in knowledge distillation can be broadly classified into three categories. The initial category encompasses temperature-based distillation methodologies. In a related contribution, Wei Y. [8] put forth a dynamic temperature adjustment mechanism that entails a reduction in temperature over the course of training steps, with the objective of optimising the student model. In a similar vein, Long J. [9] adopted a dual-temperature mechanism, utilising higher temperatures for those samples that are more challenging to classify and lower temperatures for those that are less so. The second category comprises feature alignment and self-distillation methods. The paper [10] employs mean squared error to align the probabilities of the teacher and student models, whereas the Born-Again model [11] utilises self-distillation for the transfer of knowledge. The paper [12] improves the performance of the student model by regularising feature norms. Similarly, Park et al. [13] put forth a proposal for the transfer of knowledge via the establishment of relationships between samples (such as distance or angle) with the objective of maintaining the diverse features of the teacher model. Nevertheless, in high-dimensional data spaces, this method is vulnerable to noise and may encounter difficulties in effectively capturing the intricate distribution of data. Furthermore, knowledge distillation has been extensively employed in a multitude of domains. The Cross-Task Distillation framework [14] is a multi-task learning method that enhances the performance of the student model in multiple tasks by sharing teacher knowledge across tasks. These knowledge distillation methods continue to rely on external labels or source training data, which constrains their applicability in scenarios with no or limited labelled data.",
                    "score": 0.5659285558981394,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 6320,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 86
                        },
                        {
                            "start": 87,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 597
                        },
                        {
                            "start": 600,
                            "end": 704
                        },
                        {
                            "start": 705,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1482
                        },
                        {
                            "start": 1483,
                            "end": 1574
                        },
                        {
                            "start": 1575,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2080
                        },
                        {
                            "start": 2081,
                            "end": 2267
                        },
                        {
                            "start": 2268,
                            "end": 2449
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1029,
                            "end": 1032,
                            "matchedPaperCorpusId": "270909666"
                        },
                        {
                            "start": 1420,
                            "end": 1424,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 1598,
                            "end": 1602,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 2119,
                            "end": 2123,
                            "matchedPaperCorpusId": "247011924"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81103515625
                }
            ],
            "relevance_judgement": 0.81103515625,
            "relevance_judgment_input_expanded": "# Title: Knowledge Migration Framework for Smart Contract Vulnerability Detection\n# Venue: arXiv.org\n# Authors: Luqi Wang, Wenbao Jiang\n## Abstract\nAs a cornerstone of blockchain technology in the 3.0 era, smart contracts play a pivotal role in the evolution of blockchain systems. In order to address the limitations of existing smart contract vulnerability detection models with regard to their generalisation capability, an AF-STip smart contract vulnerability detection framework incorporating efficient knowledge migration is proposed. AF-STip employs the teacher network as the main model and migrates the knowledge processed by the smart contract to the student model using a data-free knowledge distillation method. The student model utilises this knowledge to enhance its vulnerability detection capabilities. The approach markedly enhances the model's capacity for feature extraction and cross-class adaptation, while concurrently reducing computational overhead.In order to further enhance the extraction of vulnerability features, an adaptive fusion module is proposed in this paper, which aims to strengthen the interaction and fusion of feature information.The experimental results demonstrate that the STip model attains an average F1 value detection score of 91.16% for the four vulnerabilities without disclosing the original smart contract data. To validate the viability of the proposed lightweight migration approach, the student model is deployed in a migration learning task targeting a novel vulnerability type, resulting in an accuracy of 91.02% and an F1 score of 90.46%. To the best of our knowledge, AF-STip is the inaugural model to apply data-free knowledge migration to smart contract vulnerability detection. While markedly reducing the computational overhead, the method still demonstrates exceptional performance in detecting novel vulnerabilities.\n## Knowledge distillation\nThe concept of Knowledge Distillation (KD) was initially proposed by Hinton et al [7]. The method entails transferring knowledge from a teacher model to a student model, thereby reducing the computational overhead while maintaining high performance. The core principle involves softening the Softmax output by a temperature parameter to enhance the similarity between categories and improve the generalisation ability of the student model. The underlying formula is q i = exp(z i /T) (Ej) exp(z j /T) , where z i represents the teacher model output and q i denotes the temperature-smoothed output. \n\nThe research methods employed in knowledge distillation can be broadly classified into three categories. The initial category encompasses temperature-based distillation methodologies. In a related contribution, Wei Y. [8] put forth a dynamic temperature adjustment mechanism that entails a reduction in temperature over the course of training steps, with the objective of optimising the student model. In a similar vein, Long J. [9] adopted a dual-temperature mechanism, utilising higher temperatures for those samples that are more challenging to classify and lower temperatures for those that are less so. The second category comprises feature alignment and self-distillation methods. The paper [10] employs mean squared error to align the probabilities of the teacher and student models, whereas the Born-Again model [11] utilises self-distillation for the transfer of knowledge. The paper [12] improves the performance of the student model by regularising feature norms. Similarly, Park et al. [13] put forth a proposal for the transfer of knowledge via the establishment of relationships between samples (such as distance or angle) with the objective of maintaining the diverse features of the teacher model. Nevertheless, in high-dimensional data spaces, this method is vulnerable to noise and may encounter difficulties in effectively capturing the intricate distribution of data. Furthermore, knowledge distillation has been extensively employed in a multitude of domains. The Cross-Task Distillation framework [14] is a multi-task learning method that enhances the performance of the student model in multiple tasks by sharing teacher knowledge across tasks. These knowledge distillation methods continue to rely on external labels or source training data, which constrains their applicability in scenarios with no or limited labelled data.",
            "reference_string": "[274776589 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3452781",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2024.3452781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2024.3452781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152648598",
                    "name": "M. Sarikaya"
                },
                {
                    "authorId": "2146435812",
                    "name": "G\u00f6khan Ince"
                }
            ],
            "abstract": "Multimodal emotion recognition has the potential to impact various fields, including human-computer interaction, virtual reality, and emotional intelligence systems. This study introduces a comprehensive framework that enhances the accuracy and computational efficiency of emotion recognition by leveraging knowledge distillation and transfer learning, incorporating both unimodal and multimodal models. The framework also combines subject-specific and subject-independent models, achieving a balance between localization and generalization. Subject-independent models include EEG-based, non-EEG-based (i.e., electromyography, electrooculography, electrodermal activity, galvanic skin response, skin temperature, respiration, blood volume pulse, heart rate, and eye movements), and multimodal models trained on all training subjects, capturing a broader context. Subject-specific models, including EEG-based, non-EEG-based, and multimodal models, are trained on individual subjects to provide localized knowledge. The proposed framework then distills knowledge from these teacher models into a student model, utilizing six different distillation losses to combine both subject-independent and subject-specific insights. This approach makes the model subject-aware by using local patterns and modality-aware by incorporating unimodal data, enhancing the robustness and generalizability of emotion recognition systems to varied real-world scenarios. The framework was tested on two well-known datasets, SEED-V and DEAP, as well as an immersive three-Dimensional (3D) Virtual Reality (VR) dataset, GraffitiVR, which captures emotional and behavioral responses from individuals experiencing urban graffiti in a VR environment. This broader application provides insights into the effectiveness of emotion recognition models in both 2D and 3D settings, facilitating a wider range of assessment. Empirical results demonstrate that the proposed knowledge distillation-based model significantly elevates performance across all datasets when compared to traditional models. Specifically, the model demonstrated improvements ranging from 6.56% to 24.59% over unimodal models and from 1.56% to 4.11% over multimodal approaches across the SEED-V, DEAP, and GraffitiVR datasets. These results underscore the robustness and effectiveness of the proposed approach, suggesting that it significantly enhances emotion recognition processes across various environmental settings.",
            "corpus_id": 272341595,
            "sentences": [
                {
                    "corpus_id": "272341595",
                    "title": "Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation",
                    "text": "The knowledge distillation loss function defines the loss function used to transfer knowledge from the teacher models to the student model during training. The Kullback-Leibler divergence, denoted as KL, is a measure from information theory that quantifies how one probability distribution diverges from a second, expected probability distribution [44]. In the context of knowledge distillation, KL is used to calculate the loss between the softened predictions of student model (proposed multimodal model) and the teacher models (subject-independent and subjectspecific models). The distillation losses for each pair of teacher-student predictions are computed using the scaled softmax function with a temperature parameter T , and are then averaged to compute the final distillation loss [11]. The complete loss function, including the trade-off between student loss and distillation loss controlled by the hyperparameter \u03b1, is defined as follows: \n\n(1) \n\nwhere: \n\n\u2022 P k teacher and P student are the softmax predictions of the teacher and student models, respectively, for each set of models. \n\n\u2022 KL is the Kullback-Leibler divergence function applied to the softened predictions of the teacher and student models to compute the distillation loss. \n\n\u2022 T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training. \n\n\u2022 \u03b1 is a hyperparameter that balances the contribution of the student's loss and the distillation loss in the overall training loss. The distillation loss is used to update the final multimodal model by minimizing the difference between the teacher and student model predictions. By transferring knowledge from the teacher models to the student model, the final multimodal model can capture both subject-independent and subject-specific features, leading to improved performance and generalization.",
                    "score": 0.5852483558562601,
                    "section_title": "2) KNOWLEDGE DISTILLATION LOSS",
                    "char_start_offset": 31151,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 949
                        },
                        {
                            "start": 952,
                            "end": 955
                        },
                        {
                            "start": 958,
                            "end": 964
                        },
                        {
                            "start": 967,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1250
                        },
                        {
                            "start": 1253,
                            "end": 1445
                        },
                        {
                            "start": 1448,
                            "end": 1580
                        },
                        {
                            "start": 1581,
                            "end": 1727
                        },
                        {
                            "start": 1728,
                            "end": 1946
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 348,
                            "end": 352,
                            "matchedPaperCorpusId": "116908168"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80517578125
                }
            ],
            "relevance_judgement": 0.80517578125,
            "relevance_judgment_input_expanded": "# Title: Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation\n# Venue: IEEE Access\n# Authors: M. Sarikaya, G\u00f6khan Ince\n## Abstract\nMultimodal emotion recognition has the potential to impact various fields, including human-computer interaction, virtual reality, and emotional intelligence systems. This study introduces a comprehensive framework that enhances the accuracy and computational efficiency of emotion recognition by leveraging knowledge distillation and transfer learning, incorporating both unimodal and multimodal models. The framework also combines subject-specific and subject-independent models, achieving a balance between localization and generalization. Subject-independent models include EEG-based, non-EEG-based (i.e., electromyography, electrooculography, electrodermal activity, galvanic skin response, skin temperature, respiration, blood volume pulse, heart rate, and eye movements), and multimodal models trained on all training subjects, capturing a broader context. Subject-specific models, including EEG-based, non-EEG-based, and multimodal models, are trained on individual subjects to provide localized knowledge. The proposed framework then distills knowledge from these teacher models into a student model, utilizing six different distillation losses to combine both subject-independent and subject-specific insights. This approach makes the model subject-aware by using local patterns and modality-aware by incorporating unimodal data, enhancing the robustness and generalizability of emotion recognition systems to varied real-world scenarios. The framework was tested on two well-known datasets, SEED-V and DEAP, as well as an immersive three-Dimensional (3D) Virtual Reality (VR) dataset, GraffitiVR, which captures emotional and behavioral responses from individuals experiencing urban graffiti in a VR environment. This broader application provides insights into the effectiveness of emotion recognition models in both 2D and 3D settings, facilitating a wider range of assessment. Empirical results demonstrate that the proposed knowledge distillation-based model significantly elevates performance across all datasets when compared to traditional models. Specifically, the model demonstrated improvements ranging from 6.56% to 24.59% over unimodal models and from 1.56% to 4.11% over multimodal approaches across the SEED-V, DEAP, and GraffitiVR datasets. These results underscore the robustness and effectiveness of the proposed approach, suggesting that it significantly enhances emotion recognition processes across various environmental settings.\n## 2) KNOWLEDGE DISTILLATION LOSS\nThe knowledge distillation loss function defines the loss function used to transfer knowledge from the teacher models to the student model during training. The Kullback-Leibler divergence, denoted as KL, is a measure from information theory that quantifies how one probability distribution diverges from a second, expected probability distribution [44]. In the context of knowledge distillation, KL is used to calculate the loss between the softened predictions of student model (proposed multimodal model) and the teacher models (subject-independent and subjectspecific models). The distillation losses for each pair of teacher-student predictions are computed using the scaled softmax function with a temperature parameter T , and are then averaged to compute the final distillation loss [11]. The complete loss function, including the trade-off between student loss and distillation loss controlled by the hyperparameter \u03b1, is defined as follows: \n\n(1) \n\nwhere: \n\n\u2022 P k teacher and P student are the softmax predictions of the teacher and student models, respectively, for each set of models. \n\n\u2022 KL is the Kullback-Leibler divergence function applied to the softened predictions of the teacher and student models to compute the distillation loss. \n\n\u2022 T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training. \n\n\u2022 \u03b1 is a hyperparameter that balances the contribution of the student's loss and the distillation loss in the overall training loss. The distillation loss is used to update the final multimodal model by minimizing the difference between the teacher and student model predictions. By transferring knowledge from the teacher models to the student model, the final multimodal model can capture both subject-independent and subject-specific features, leading to improved performance and generalization.",
            "reference_string": "[272341595 | Sarikaya et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Synthetic Unknown Class Learning for Learning Unknowns",
            "venue": "Pattern Recognition",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2111.08062",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.08062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "88018822",
                    "name": "Jaeyeon Jang"
                }
            ],
            "abstract": null,
            "corpus_id": 244130170,
            "sentences": [
                {
                    "corpus_id": "244130170",
                    "title": "Synthetic Unknown Class Learning for Learning Unknowns",
                    "text": "Following the conventional DNN classifier training scheme, the teacher network (T ) is first trained with the aim of minimizing the cross-entropy loss function. DNN classifiers are usually overgeneralized, producing very high confidence scores even for unknowns during testing. Thus, we introduce an additional network, the student network, and apply a knowledge distillation technique [7] based on the pretrained teacher network to relax the posterior probabilities of the student network for known classes. Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature  \u03c4 in the softmax function as follows: \n\nand \n\nwhere l ik is the logit of sample x i for the k-th known class and l u ik is the logit for the k-th synthetic unknown class. The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function: \n\nwhere D is a known training sample set, |D| is the size of D, H is the cross-entropy function, and Q \u03c4 i and P \u03c4 i are temperature-scaled probability vectors for the teacher and student, respectively. \n\nUnlike original knowledge distillation [7], we do not consider the cross-entropy between the true labels and the nonscaled probability vector of the student because our purpose is to minimize overgeneralization while leaving room for learning unknowns. In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes. This chance is further amplified via the synthetic unknown class learning approach described in the next subsection.",
                    "score": 0.5336955927444893,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 12967,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 277
                        },
                        {
                            "start": 278,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 747
                        },
                        {
                            "start": 750,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1159
                        },
                        {
                            "start": 1162,
                            "end": 1362
                        },
                        {
                            "start": 1365,
                            "end": 1617
                        },
                        {
                            "start": 1618,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1933
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 386,
                            "end": 389,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1404,
                            "end": 1407,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79736328125
                }
            ],
            "relevance_judgement": 0.79736328125,
            "relevance_judgment_input_expanded": "# Title: Synthetic Unknown Class Learning for Learning Unknowns\n# Venue: Pattern Recognition\n# Authors: Jaeyeon Jang\n## Abstract\nNone\n## Knowledge distillation\nFollowing the conventional DNN classifier training scheme, the teacher network (T ) is first trained with the aim of minimizing the cross-entropy loss function. DNN classifiers are usually overgeneralized, producing very high confidence scores even for unknowns during testing. Thus, we introduce an additional network, the student network, and apply a knowledge distillation technique [7] based on the pretrained teacher network to relax the posterior probabilities of the student network for known classes. Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature  \u03c4 in the softmax function as follows: \n\nand \n\nwhere l ik is the logit of sample x i for the k-th known class and l u ik is the logit for the k-th synthetic unknown class. The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function: \n\nwhere D is a known training sample set, |D| is the size of D, H is the cross-entropy function, and Q \u03c4 i and P \u03c4 i are temperature-scaled probability vectors for the teacher and student, respectively. \n\nUnlike original knowledge distillation [7], we do not consider the cross-entropy between the true labels and the nonscaled probability vector of the student because our purpose is to minimize overgeneralization while leaving room for learning unknowns. In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes. This chance is further amplified via the synthetic unknown class learning approach described in the next subsection.",
            "reference_string": "[244130170 | Jang | 2021 | Citations: 3]"
        },
        {
            "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
            "venue": "IEEE International Conference on Computer Vision",
            "year": 2020,
            "reference_count": 56,
            "citation_count": 182,
            "influential_citation_count": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2006.12000",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.12000, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "65792157",
                    "name": "Kyungyul Kim"
                },
                {
                    "authorId": "152356074",
                    "name": "Byeongmoon Ji"
                },
                {
                    "authorId": "2054253051",
                    "name": "Doyoung Yoon"
                },
                {
                    "authorId": "1949871",
                    "name": "Sangheum Hwang"
                }
            ],
            "abstract": "The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model\u2019s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.",
            "corpus_id": 233714221,
            "sentences": [
                {
                    "corpus_id": "233714221",
                    "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
                    "text": "KD is a technique to transfer knowledge from one model (i.e., a teacher) to another (i.e., a student), usually from a larger model to a smaller one. The student learns from more informative sources, the predictive probabilities from the teacher, besides one-hot labels. Hence, it can attain a similar performance compared to the teacher although it is usually a much smaller model, and show comparable or even better performance when the student has the same capacity as the teacher [8,48].\n\nFor an input x and a K-dimensional one-hot target y, a model produces the logit vector z(x) = [z 1 (x), \u00b7 \u00b7 \u00b7 , z K (x)], and then outputs the predicted probabilities P (x) = [p 1 (x), \u00b7 \u00b7 \u00b7 , p K (x)] by a softmax function. Hinton et al. [17] suggest to utilize temperature scaling to soften these probabilities for better distillation:\n\nwhere \u03c4 denotes a temperature parameter. By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):\n\nTherefore, the existing methods that use the soft targets for regularization can be interpreted within the framework of knowledge distillation. For example, LS [25] is equivalent to distilling the knowledge from the teacher which produces uniformly distributed probabilities on any inputs.",
                    "score": 0.5944191254112605,
                    "section_title": "Knowledge Distillation as Softening Targets",
                    "char_start_offset": 8351,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 483,
                            "end": 486,
                            "matchedPaperCorpusId": "4110009"
                        },
                        {
                            "start": 486,
                            "end": 489,
                            "matchedPaperCorpusId": "219962714"
                        },
                        {
                            "start": 731,
                            "end": 735,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1403,
                            "end": 1407,
                            "matchedPaperCorpusId": "174802983"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79443359375
                }
            ],
            "relevance_judgement": 0.79443359375,
            "relevance_judgment_input_expanded": "# Title: Self-Knowledge Distillation with Progressive Refinement of Targets\n# Venue: IEEE International Conference on Computer Vision\n# Authors: Kyungyul Kim, Byeongmoon Ji, Doyoung Yoon, Sangheum Hwang\n## Abstract\nThe generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model\u2019s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.\n## Knowledge Distillation as Softening Targets\nKD is a technique to transfer knowledge from one model (i.e., a teacher) to another (i.e., a student), usually from a larger model to a smaller one. The student learns from more informative sources, the predictive probabilities from the teacher, besides one-hot labels. Hence, it can attain a similar performance compared to the teacher although it is usually a much smaller model, and show comparable or even better performance when the student has the same capacity as the teacher [8,48].\n\nFor an input x and a K-dimensional one-hot target y, a model produces the logit vector z(x) = [z 1 (x), \u00b7 \u00b7 \u00b7 , z K (x)], and then outputs the predicted probabilities P (x) = [p 1 (x), \u00b7 \u00b7 \u00b7 , p K (x)] by a softmax function. Hinton et al. [17] suggest to utilize temperature scaling to soften these probabilities for better distillation:\n\nwhere \u03c4 denotes a temperature parameter. By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):\n\nTherefore, the existing methods that use the soft targets for regularization can be interpreted within the framework of knowledge distillation. For example, LS [25] is equivalent to distilling the knowledge from the teacher which produces uniformly distributed probabilities on any inputs.",
            "reference_string": "[233714221 | Kim et al. | 2020 | Citations: 182]"
        },
        {
            "title": "Discriminative and Consistent Representation Distillation",
            "venue": "",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.11802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2196360101",
                    "name": "Nikolaos Giakoumoglou"
                },
                {
                    "authorId": "2292259667",
                    "name": "Tania Stathaki"
                }
            ],
            "abstract": "Knowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.",
            "corpus_id": 271218098,
            "sentences": [
                {
                    "corpus_id": "271218098",
                    "title": "Discriminative and Consistent Representation Distillation",
                    "text": "The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax. Similar to [33], our method uses temperature scaling but learns the optimal temperature during training for better knowledge transfer. \n\nLogit-based methods. Several methods have improved logit-based distillation through techniques like label decoupling [81], instance-specific label smoothing [73], probability reweighting [55], and normalizing logits before softmax and KL divergence [65]. Some works focus on dynamic temperature adjustment [42] and separate learning of response-based and feature-based distillation [79]. Additional improvements include transformations for better teacher-student alignment [80] and methods for knowledge transfer from stronger teachers [35]. Unlike existing methods such as [42] that use fixed temperature schedules and MLPs, our approach introduces truly learnable temperature parameters that adapt during training, making it more flexible and efficient. \n\nFeature-based methods. The work by [61] used intermediate feature hints to guide student learning, while [74] aligned attention maps between teacher and student. Several methods have explored structural relationships: [58] preserved feature space relationships, [45] ensured functional consistency, and [25] transferred class-level attention information. The method by [56] transferred mutual relations between data examples. Recent works have introduced cross-stage connection paths [12], direct reuse of teacher's classifier [8], and many-to-one representation matching [47]. Contrastive Representation Distillation (CRD) [66] used contrastive learning to maximize mutual information between representations but needed large memory buffers. Compared to [66], our approach uses efficient in-batch sampling and adds structural consistency, making it more practical. Our method also differs from previous approaches like [35,57,58,67] in how we define and preserve structural relationships. \n\nArchitecture-aware methods. Recent studies have examined how network architecture affects distillation success. Some works developed meta-learning for architecture search [19], training-free student architecture selection [20], and graph-based architecture adaptation [50]. Methods by [28] and [52] addressed distillation between different architectures through unified feature spaces and intermediate networks respectively.",
                    "score": 0.6266157525780078,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 4254,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 155
                        },
                        {
                            "start": 156,
                            "end": 290
                        },
                        {
                            "start": 293,
                            "end": 313
                        },
                        {
                            "start": 314,
                            "end": 547
                        },
                        {
                            "start": 548,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1048
                        },
                        {
                            "start": 1051,
                            "end": 1073
                        },
                        {
                            "start": 1074,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1476
                        },
                        {
                            "start": 1477,
                            "end": 1628
                        },
                        {
                            "start": 1629,
                            "end": 1793
                        },
                        {
                            "start": 1794,
                            "end": 1916
                        },
                        {
                            "start": 1917,
                            "end": 2040
                        },
                        {
                            "start": 2043,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2154
                        },
                        {
                            "start": 2155,
                            "end": 2316
                        },
                        {
                            "start": 2317,
                            "end": 2467
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1086,
                            "end": 1090,
                            "matchedPaperCorpusId": "2723173"
                        },
                        {
                            "start": 1156,
                            "end": 1160,
                            "matchedPaperCorpusId": "829159"
                        },
                        {
                            "start": 1269,
                            "end": 1273,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1420,
                            "end": 1424,
                            "matchedPaperCorpusId": "131765296"
                        },
                        {
                            "start": 1975,
                            "end": 1978,
                            "matchedPaperCorpusId": "52012952"
                        },
                        {
                            "start": 1978,
                            "end": 1981,
                            "matchedPaperCorpusId": "102483463"
                        },
                        {
                            "start": 1981,
                            "end": 1984,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7919921875
                }
            ],
            "relevance_judgement": 0.7919921875,
            "relevance_judgment_input_expanded": "# Title: Discriminative and Consistent Representation Distillation\n# Venue: \n# Authors: Nikolaos Giakoumoglou, Tania Stathaki\n## Abstract\nKnowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.\n## Knowledge Distillation\nThe original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax. Similar to [33], our method uses temperature scaling but learns the optimal temperature during training for better knowledge transfer. \n\nLogit-based methods. Several methods have improved logit-based distillation through techniques like label decoupling [81], instance-specific label smoothing [73], probability reweighting [55], and normalizing logits before softmax and KL divergence [65]. Some works focus on dynamic temperature adjustment [42] and separate learning of response-based and feature-based distillation [79]. Additional improvements include transformations for better teacher-student alignment [80] and methods for knowledge transfer from stronger teachers [35]. Unlike existing methods such as [42] that use fixed temperature schedules and MLPs, our approach introduces truly learnable temperature parameters that adapt during training, making it more flexible and efficient. \n\nFeature-based methods. The work by [61] used intermediate feature hints to guide student learning, while [74] aligned attention maps between teacher and student. Several methods have explored structural relationships: [58] preserved feature space relationships, [45] ensured functional consistency, and [25] transferred class-level attention information. The method by [56] transferred mutual relations between data examples. Recent works have introduced cross-stage connection paths [12], direct reuse of teacher's classifier [8], and many-to-one representation matching [47]. Contrastive Representation Distillation (CRD) [66] used contrastive learning to maximize mutual information between representations but needed large memory buffers. Compared to [66], our approach uses efficient in-batch sampling and adds structural consistency, making it more practical. Our method also differs from previous approaches like [35,57,58,67] in how we define and preserve structural relationships. \n\nArchitecture-aware methods. Recent studies have examined how network architecture affects distillation success. Some works developed meta-learning for architecture search [19], training-free student architecture selection [20], and graph-based architecture adaptation [50]. Methods by [28] and [52] addressed distillation between different architectures through unified feature spaces and intermediate networks respectively.",
            "reference_string": "[271218098 | Giakoumoglou et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.08038",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.08038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135847074",
                    "name": "Xiongtao Zhang"
                },
                {
                    "authorId": "2206403890",
                    "name": "Zezong Yin"
                },
                {
                    "authorId": "3247526",
                    "name": "Yunliang Jiang"
                },
                {
                    "authorId": "1390650781",
                    "name": "Yizhang Jiang"
                },
                {
                    "authorId": "2340356",
                    "name": "Da-Song Sun"
                },
                {
                    "authorId": "2189281",
                    "name": "Yong Liu"
                }
            ],
            "abstract": "High-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.",
            "corpus_id": 256900863,
            "sentences": [
                {
                    "corpus_id": "256900863",
                    "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                    "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
                    "score": 0.6916730103935106,
                    "section_title": "B. Knowledge Distillation",
                    "char_start_offset": 8754,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 617
                        },
                        {
                            "start": 620,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1583
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 23,
                            "end": 27,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 730,
                            "end": 734,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 759,
                            "end": 763,
                            "matchedPaperCorpusId": "233033712"
                        },
                        {
                            "start": 798,
                            "end": 802,
                            "matchedPaperCorpusId": "209405263"
                        },
                        {
                            "start": 959,
                            "end": 963,
                            "matchedPaperCorpusId": "247476179"
                        },
                        {
                            "start": 1188,
                            "end": 1192,
                            "matchedPaperCorpusId": "4110009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.783203125
                }
            ],
            "relevance_judgement": 0.783203125,
            "relevance_judgment_input_expanded": "# Title: Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK\n# Venue: arXiv.org\n# Authors: Xiongtao Zhang, Zezong Yin, Yunliang Jiang, Yizhang Jiang, Da-Song Sun, Yong Liu\n## Abstract\nHigh-order Takagi-Sugeno-Kang (TSK) fuzzy classifiers possess powerful classification performance yet have fewer fuzzy rules, but always be impaired by its exponential growth training time and poorer interpretability owing to High-order polynomial used in consequent part of fuzzy rule, while Low-order TSK fuzzy classifiers run quickly with high interpretability, however they usually require more fuzzy rules and perform relatively not very well. Address this issue, a novel TSK fuzzy classifier embeded with knowledge distillation in deep learning called HTSK-LLM-DKD is proposed in this study. HTSK-LLM-DKD achieves the following distinctive characteristics: 1) It takes High-order TSK classifier as teacher model and Low-order TSK fuzzy classifier as student model, and leverages the proposed LLM-DKD (Least Learning Machine based Decoupling Knowledge Distillation) to distill the fuzzy dark knowledge from High-order TSK fuzzy classifier to Low-order TSK fuzzy classifier, which resulting in Low-order TSK fuzzy classifier endowed with enhanced performance surpassing or at least comparable to High-order TSK classifier, as well as high interpretability; specifically 2) The Negative Euclidean distance between the output of teacher model and each class is employed to obtain the teacher logits, and then it compute teacher/student soft labels by the softmax function with distillating temperature parameter; 3) By reformulating the Kullback-Leibler divergence, it decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge, and transfers them to student model. The advantages of HTSK-LLM-DKD are verified on the benchmarking UCI datasets and a real dataset Cleveland heart disease, in terms of classification performance and model interpretability.\n## B. Knowledge Distillation\nKnowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "reference_string": "[256900863 | Zhang et al. | 2023 | Citations: 1]"
        },
        {
            "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm",
            "venue": "Scientific Reports",
            "year": 2024,
            "reference_count": 25,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41598-024-64289-w.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11637092, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305667661",
                    "name": "Xiaolian Liu"
                },
                {
                    "authorId": "2305658388",
                    "name": "Shaopeng Gong"
                },
                {
                    "authorId": "2305657615",
                    "name": "Xiangxu Hua"
                },
                {
                    "authorId": "2305821873",
                    "name": "Taotao Chen"
                },
                {
                    "authorId": "2306058720",
                    "name": "Chunjiang Zhao"
                }
            ],
            "abstract": "In the process of feeding the distilling bucket after vapor detection, the existing methods can only realize the lag detection after the steam overflow, and can not accurately detect the location of the steam, etc. At the same time, in order to effectively reduce the occupancy of the computational resources and improve the deployment performance, this study established infrared image dataset of fermented grains surface, and fused the YOLO v5n and the knowledge distillation and the model pruning algorithms, and an lightweight method YOLO v5ns-DP was proposed as as a model for detecting temperature changes in the surface layer of fermented grains during the process of feeding the distilling. The experimental results indicated that the improvement makes YOLOv5n improve its performance in all aspects. The number of parameters, GLOPs and model size of YOLO v5ns-DP have been reduced by 28.6%, 16.5%, and 26.4%, respectively, and the mAP has been improved by 0.6. Therefore, the algorithm is able to predict in advance and accurately detect the location of the liquor vapor, which effectively improves the precision and speed of the detection of the temperature of the surface fermented grains , and well completes the real-time detecting task.",
            "corpus_id": 270377282,
            "sentences": [
                {
                    "corpus_id": "270377282",
                    "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm",
                    "text": "However, negative labels also contain a great deal of information. Some negative labels may correspond to higher probabilities than others, and even contain more information than positive labels. In order to obtain more information from negative labels, knowledge distillation introduces a temperature variable, denoted by T. By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels. \n\nwhere q i is the \"softened\" probability vector, obtained by exponential operation and normalization; Z i indicates the logit value for the current category; j denotes the number of output nodes (number of categories); Z j represents the logit value for each category output by the full connectivity layer; T is the temperature parameter, and when T = 1, the function is the original Softmax function. The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,23 . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function. The second step is to perform Softmax computation on the logits output of the student network to get the hard predicted values. The hard predicted values were compared to the actual labels and the difference between them was measured using the student loss function. The two parts of the loss function are added together to get the total loss function, which is calculated as where V loss is the value of total loss function; V loss-SL is the value of the student loss function; V loss-KD is the value of distillation loss function; \u03b1 is the scaling factor, which is used to adjust the weights of the two loss functions. When \u03b1 is equal to zero, this corresponds to the network not being distilled and trained using only the student loss function.",
                    "score": 0.5539877391483787,
                    "section_title": "Fermented grains surface temperature detection based on YOLO v5n",
                    "char_start_offset": 12832,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 491
                        },
                        {
                            "start": 494,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1163
                        },
                        {
                            "start": 1164,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1296
                        },
                        {
                            "start": 1297,
                            "end": 1466
                        },
                        {
                            "start": 1467,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1753
                        },
                        {
                            "start": 1754,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2246
                        },
                        {
                            "start": 2247,
                            "end": 2373
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 989,
                            "end": 991,
                            "matchedPaperCorpusId": "264462123"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76171875
                }
            ],
            "relevance_judgement": 0.76171875,
            "relevance_judgment_input_expanded": "# Title: Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm\n# Venue: Scientific Reports\n# Authors: Xiaolian Liu, Shaopeng Gong, Xiangxu Hua, Taotao Chen, Chunjiang Zhao\n## Abstract\nIn the process of feeding the distilling bucket after vapor detection, the existing methods can only realize the lag detection after the steam overflow, and can not accurately detect the location of the steam, etc. At the same time, in order to effectively reduce the occupancy of the computational resources and improve the deployment performance, this study established infrared image dataset of fermented grains surface, and fused the YOLO v5n and the knowledge distillation and the model pruning algorithms, and an lightweight method YOLO v5ns-DP was proposed as as a model for detecting temperature changes in the surface layer of fermented grains during the process of feeding the distilling. The experimental results indicated that the improvement makes YOLOv5n improve its performance in all aspects. The number of parameters, GLOPs and model size of YOLO v5ns-DP have been reduced by 28.6%, 16.5%, and 26.4%, respectively, and the mAP has been improved by 0.6. Therefore, the algorithm is able to predict in advance and accurately detect the location of the liquor vapor, which effectively improves the precision and speed of the detection of the temperature of the surface fermented grains , and well completes the real-time detecting task.\n## Fermented grains surface temperature detection based on YOLO v5n\nHowever, negative labels also contain a great deal of information. Some negative labels may correspond to higher probabilities than others, and even contain more information than positive labels. In order to obtain more information from negative labels, knowledge distillation introduces a temperature variable, denoted by T. By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels. \n\nwhere q i is the \"softened\" probability vector, obtained by exponential operation and normalization; Z i indicates the logit value for the current category; j denotes the number of output nodes (number of categories); Z j represents the logit value for each category output by the full connectivity layer; T is the temperature parameter, and when T = 1, the function is the original Softmax function. The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,23 . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function. The second step is to perform Softmax computation on the logits output of the student network to get the hard predicted values. The hard predicted values were compared to the actual labels and the difference between them was measured using the student loss function. The two parts of the loss function are added together to get the total loss function, which is calculated as where V loss is the value of total loss function; V loss-SL is the value of the student loss function; V loss-KD is the value of distillation loss function; \u03b1 is the scaling factor, which is used to adjust the weights of the two loss functions. When \u03b1 is equal to zero, this corresponds to the network not being distilled and trained using only the student loss function.",
            "reference_string": "[270377282 | Liu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "AI Readiness in Healthcare through Storytelling XAI",
            "venue": "EXPLIMED@ECAI",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18725, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282540984",
                    "name": "Akshat Dubey"
                },
                {
                    "authorId": "2282577952",
                    "name": "Zewen Yang"
                },
                {
                    "authorId": "2282533557",
                    "name": "Georges Hattab"
                }
            ],
            "abstract": "Artificial Intelligence is rapidly advancing and radically impacting everyday life, driven by the increasing availability of computing power. Despite this trend, the adoption of AI in real-world healthcare is still limited. One of the main reasons is the trustworthiness of AI models and the potential hesitation of domain experts with model predictions. Explainable Artificial Intelligence (XAI) techniques aim to address these issues. However, explainability can mean different things to people with different backgrounds, expertise, and goals. To address the target audience with diverse needs, we develop storytelling XAI. In this research, we have developed an approach that combines multi-task distillation with interpretability techniques to enable audience-centric explainability. Using multi-task distillation allows the model to exploit the relationships between tasks, potentially improving interpretability as each task supports the other leading to an enhanced interpretability from the perspective of a domain expert. The distillation process allows us to extend this research to large deep models that are highly complex. We focus on both model-agnostic and model-specific methods of interpretability, supported by textual justification of the results in healthcare through our use case. Our methods increase the trust of both the domain experts and the machine learning experts to enable a responsible AI.",
            "corpus_id": 273549238,
            "sentences": [
                {
                    "corpus_id": "273549238",
                    "title": "AI Readiness in Healthcare through Storytelling XAI",
                    "text": "Knowledge distillation refers to the process of transferring knowledge from a comparatively large model to a smaller model without compromising on performance. The complex large model is called teacher and the smaller model is called student. The rationale behind model distillation is to train complex large models also called teacher for a specific task and then transfer the knowledge to a smaller model also called student using distillation loss (Equation: 1). The loss function employed in knowledge distillation to instruct the student model to imitate the teacher model's behavior is called the distillation loss. By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model. This allows the student model to capture the same high-level concepts and reasoning as the teacher but in a more transparent and interpretable form [15,16].",
                    "score": 0.8026488635511212,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 6175,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 878
                        },
                        {
                            "start": 881,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1301
                        },
                        {
                            "start": 1302,
                            "end": 1383
                        },
                        {
                            "start": 1384,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1682
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1678,
                            "end": 1681,
                            "matchedPaperCorpusId": "57189239"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75244140625
                }
            ],
            "relevance_judgement": 0.75244140625,
            "relevance_judgment_input_expanded": "# Title: AI Readiness in Healthcare through Storytelling XAI\n# Venue: EXPLIMED@ECAI\n# Authors: Akshat Dubey, Zewen Yang, Georges Hattab\n## Abstract\nArtificial Intelligence is rapidly advancing and radically impacting everyday life, driven by the increasing availability of computing power. Despite this trend, the adoption of AI in real-world healthcare is still limited. One of the main reasons is the trustworthiness of AI models and the potential hesitation of domain experts with model predictions. Explainable Artificial Intelligence (XAI) techniques aim to address these issues. However, explainability can mean different things to people with different backgrounds, expertise, and goals. To address the target audience with diverse needs, we develop storytelling XAI. In this research, we have developed an approach that combines multi-task distillation with interpretability techniques to enable audience-centric explainability. Using multi-task distillation allows the model to exploit the relationships between tasks, potentially improving interpretability as each task supports the other leading to an enhanced interpretability from the perspective of a domain expert. The distillation process allows us to extend this research to large deep models that are highly complex. We focus on both model-agnostic and model-specific methods of interpretability, supported by textual justification of the results in healthcare through our use case. Our methods increase the trust of both the domain experts and the machine learning experts to enable a responsible AI.\n## Knowledge Distillation\nKnowledge distillation refers to the process of transferring knowledge from a comparatively large model to a smaller model without compromising on performance. The complex large model is called teacher and the smaller model is called student. The rationale behind model distillation is to train complex large models also called teacher for a specific task and then transfer the knowledge to a smaller model also called student using distillation loss (Equation: 1). The loss function employed in knowledge distillation to instruct the student model to imitate the teacher model's behavior is called the distillation loss. By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model. This allows the student model to capture the same high-level concepts and reasoning as the teacher but in a more transparent and interpretable form [15,16].",
            "reference_string": "[273549238 | Dubey et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09030, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283014343",
                    "name": "Kazuhiro Matsuyama"
                },
                {
                    "authorId": "2312362922",
                    "name": "Usman Anjum"
                },
                {
                    "authorId": "2349642970",
                    "name": "Satoko Matsuyama"
                },
                {
                    "authorId": "2349646885",
                    "name": "Tetsuo Shoda"
                },
                {
                    "authorId": "2321910868",
                    "name": "Justin Zhan"
                }
            ],
            "abstract": "Knowledge distillation is a technique to imitate a performance that a deep learning model has, but reduce the size on another model. It applies the outputs of a model to train another model having comparable accuracy. These two distinct models are similar to the way information is delivered in human society, with one acting as the\"teacher\"and the other as the\"student\". Softmax plays a role in comparing logits generated by models with each other by converting probability distributions. It delivers the logits of a teacher to a student with compression through a parameter named temperature. Tuning this variable reinforces the distillation performance. Although only this parameter helps with the interaction of logits, it is not clear how temperatures promote information transfer. In this paper, we propose a novel approach to calculate the temperature. Our method only refers to the maximum logit generated by a teacher model, which reduces computational time against state-of-the-art methods. Our method shows a promising result in different student and teacher models on a standard benchmark dataset. Algorithms using temperature can obtain the improvement by plugging in this dynamic approach. Furthermore, the approximation of the distillation process converges to a correlation of logits by both models. This reinforces the previous argument that the distillation conveys the relevance of logits. We report that this approximating algorithm yields a higher temperature compared to the commonly used static values in testing.",
            "corpus_id": 276938088,
            "sentences": [
                {
                    "corpus_id": "276938088",
                    "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
                    "text": "Knowledge distillation is a technique to imitate a performance that a deep learning model has, but reduce the size on another model. It applies the outputs of a model to train another model having comparable accuracy. These two distinct models are similar to the way information is delivered in human society, with one acting as the\"teacher\"and the other as the\"student\". Softmax plays a role in comparing logits generated by models with each other by converting probability distributions. It delivers the logits of a teacher to a student with compression through a parameter named temperature. Tuning this variable reinforces the distillation performance. Although only this parameter helps with the interaction of logits, it is not clear how temperatures promote information transfer. In this paper, we propose a novel approach to calculate the temperature. Our method only refers to the maximum logit generated by a teacher model, which reduces computational time against state-of-the-art methods. Our method shows a promising result in different student and teacher models on a standard benchmark dataset. Algorithms using temperature can obtain the improvement by plugging in this dynamic approach. Furthermore, the approximation of the distillation process converges to a correlation of logits by both models. This reinforces the previous argument that the distillation conveys the relevance of logits. We report that this approximating algorithm yields a higher temperature compared to the commonly used static values in testing.",
                    "score": 0.5915096492892683,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73046875
                }
            ],
            "relevance_judgement": 0.73046875,
            "relevance_judgment_input_expanded": "# Title: Adaptive Temperature Based on Logits Correlation in Knowledge Distillation\n# Venue: arXiv.org\n# Authors: Kazuhiro Matsuyama, Usman Anjum, Satoko Matsuyama, Tetsuo Shoda, Justin Zhan\n## Abstract\nKnowledge distillation is a technique to imitate a performance that a deep learning model has, but reduce the size on another model. It applies the outputs of a model to train another model having comparable accuracy. These two distinct models are similar to the way information is delivered in human society, with one acting as the\"teacher\"and the other as the\"student\". Softmax plays a role in comparing logits generated by models with each other by converting probability distributions. It delivers the logits of a teacher to a student with compression through a parameter named temperature. Tuning this variable reinforces the distillation performance. Although only this parameter helps with the interaction of logits, it is not clear how temperatures promote information transfer. In this paper, we propose a novel approach to calculate the temperature. Our method only refers to the maximum logit generated by a teacher model, which reduces computational time against state-of-the-art methods. Our method shows a promising result in different student and teacher models on a standard benchmark dataset. Algorithms using temperature can obtain the improvement by plugging in this dynamic approach. Furthermore, the approximation of the distillation process converges to a correlation of logits by both models. This reinforces the previous argument that the distillation conveys the relevance of logits. We report that this approximating algorithm yields a higher temperature compared to the commonly used static values in testing.\n",
            "reference_string": "[276938088 | Matsuyama et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model",
            "venue": "Applied Soft Computing",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 4,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2406.03409",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03409, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283187602",
                    "name": "Jinyin Chen"
                },
                {
                    "authorId": "2289841303",
                    "name": "Xiao-Ming Zhao"
                },
                {
                    "authorId": "1796810",
                    "name": "Haibin Zheng"
                },
                {
                    "authorId": "2304969928",
                    "name": "Xiao Li"
                },
                {
                    "authorId": "2291571475",
                    "name": "Sheng Xiang"
                },
                {
                    "authorId": "2304823063",
                    "name": "Haifeng Guo"
                }
            ],
            "abstract": null,
            "corpus_id": 270257779,
            "sentences": [
                {
                    "corpus_id": "270257779",
                    "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model",
                    "text": "acquired by the teacher model to a smaller student model.Typically, teacher models are complex and powerful deep models, and student models can learn from both the output logits of the teacher model and the ground truth for knowledge transfer.While most neural networks [57,58] typically use the \"softmax\" output layer to generate category probabilities, the purpose of knowledge distillation is to make the softmax outputs of the student model and the teacher model similar enough.To achieve this, knowledge distillation introduces a softmax function with a temperature parameter, which can be defined as:\n\nwhere z i is the logits of the model, and T is the temperature factor.When T =1, q i is the standard softmax function.In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor.\n\nAccording to the above properties, when knowledge distillation is implemented, the logits output by the teacher model and the student model will be processed with a higher temperature factor to obtain a soft target.Let p T j and q T j denote the output soft target of the teacher model and the student model after being \"softened\" under the temperature T , N is the total number of labels, and let L denote a standard cross entropy loss which is used to measure the direct distribution difference between p T j and q T j .The loss function of the soft target is as follows:\n\nSince the teacher model also has a certain error rate, the use of ground truth can effectively reduce the possibility of errors being transmitted to the student model.c j is defined as the value of ground truth in the j-th class.The positive label takes '1' and the negative label takes '0'.The loss function of the hard target is as follows:\n\nCombining the loss of soft and hard target, the total object function of knowledge distillation:\n\nwhere \u03b1 is a hyperparameter balancing the two terms.",
                    "score": 0.6302388190913475,
                    "section_title": "Knowledge distillation is a compression technique designed to transfer knowledge or information",
                    "char_start_offset": 15843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 57
                        },
                        {
                            "start": 57,
                            "end": 243
                        },
                        {
                            "start": 243,
                            "end": 482
                        },
                        {
                            "start": 482,
                            "end": 606
                        },
                        {
                            "start": 608,
                            "end": 678
                        },
                        {
                            "start": 678,
                            "end": 726
                        },
                        {
                            "start": 726,
                            "end": 919
                        },
                        {
                            "start": 921,
                            "end": 1136
                        },
                        {
                            "start": 1136,
                            "end": 1443
                        },
                        {
                            "start": 1443,
                            "end": 1494
                        },
                        {
                            "start": 1496,
                            "end": 1663
                        },
                        {
                            "start": 1663,
                            "end": 1725
                        },
                        {
                            "start": 1725,
                            "end": 1787
                        },
                        {
                            "start": 1787,
                            "end": 1838
                        },
                        {
                            "start": 1840,
                            "end": 1936
                        },
                        {
                            "start": 1938,
                            "end": 1990
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 274,
                            "end": 277,
                            "matchedPaperCorpusId": "216263993"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7236328125
                }
            ],
            "relevance_judgement": 0.7236328125,
            "relevance_judgment_input_expanded": "# Title: Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model\n# Venue: Applied Soft Computing\n# Authors: Jinyin Chen, Xiao-Ming Zhao, Haibin Zheng, Xiao Li, Sheng Xiang, Haifeng Guo\n## Abstract\nNone\n## Knowledge distillation is a compression technique designed to transfer knowledge or information\nacquired by the teacher model to a smaller student model.Typically, teacher models are complex and powerful deep models, and student models can learn from both the output logits of the teacher model and the ground truth for knowledge transfer.While most neural networks [57,58] typically use the \"softmax\" output layer to generate category probabilities, the purpose of knowledge distillation is to make the softmax outputs of the student model and the teacher model similar enough.To achieve this, knowledge distillation introduces a softmax function with a temperature parameter, which can be defined as:\n\nwhere z i is the logits of the model, and T is the temperature factor.When T =1, q i is the standard softmax function.In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor.\n\nAccording to the above properties, when knowledge distillation is implemented, the logits output by the teacher model and the student model will be processed with a higher temperature factor to obtain a soft target.Let p T j and q T j denote the output soft target of the teacher model and the student model after being \"softened\" under the temperature T , N is the total number of labels, and let L denote a standard cross entropy loss which is used to measure the direct distribution difference between p T j and q T j .The loss function of the soft target is as follows:\n\nSince the teacher model also has a certain error rate, the use of ground truth can effectively reduce the possibility of errors being transmitted to the student model.c j is defined as the value of ground truth in the j-th class.The positive label takes '1' and the negative label takes '0'.The loss function of the hard target is as follows:\n\nCombining the loss of soft and hard target, the total object function of knowledge distillation:\n\nwhere \u03b1 is a hyperparameter balancing the two terms.",
            "reference_string": "[270257779 | Chen et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation",
            "venue": "Knowledge Discovery and Data Mining",
            "year": 2022,
            "reference_count": 33,
            "citation_count": 29,
            "influential_citation_count": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.13954",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13954, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064914349",
                    "name": "Bin Lu"
                },
                {
                    "authorId": "1693639",
                    "name": "Xiaoying Gan"
                },
                {
                    "authorId": "2145501350",
                    "name": "Lina Yang"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "1922573",
                    "name": "Luoyi Fu"
                },
                {
                    "authorId": "2107937507",
                    "name": "Xinbing Wang"
                }
            ],
            "abstract": "With the tremendous expansion of graphs data, node classification shows its great importance in many real-world applications. Existing graph neural network based methods mainly focus on classifying unlabeled nodes within fixed classes with abundant labeling. However, in many practical scenarios, graph evolves with emergence of new nodes and edges. Novel classes appear incrementally along with few labeling due to its newly emergence or lack of exploration. In this paper, we focus on this challenging but practical graph few-shot class-incremental learning (GFSCIL) problem and propose a novel method called Geometer. Instead of replacing and retraining the fully connected neural network classifier, Geometer predicts the label of a node by finding the nearest class prototype. Prototype is a vector representing a class in the metric space. With the pop-up of novel classes, Geometer learns and adjusts the attention-based prototypes by observing the geometric proximity, uniformity and separability. Teacher-student knowledge distillation and biased sampling are further introduced to mitigate catastrophic forgetting and unbalanced labeling problem respectively. Experimental results on four public datasets demonstrate that Geometer achieves a substantial improvement of 9.46% to 27.60% over state-of-the-art methods.",
            "corpus_id": 249151858,
            "sentences": [
                {
                    "corpus_id": "249151858",
                    "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation",
                    "text": "Due to the class-incremental nature of GFSCIL problem, Geometer applies the idea of teacher-student knowledge distillation to further mitigate \"forgetting old\" during finetuning. In contrast to the classic teacher-student knowledge distillation techniques [27][28][29] used to compress large model into lightweight model with better inference efficiency, we regard the model before streaming as the teacher model and new model as the student model. Knowledge distillation technique is used to transfer the classification ability of previous model, while preserving the interrelationships of the old classes in the metric space. Temperature-scaled softmax [12] is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model. Geometer proposes to calculate the KL-divergence of the softened logits to make the student model gain the experience of classifying old classes C  \u22121 from teacher model. The teacher-student knowledge distillation loss L  on -th session is calculated as \n\nwhere C \u22121 is the set of old classes of ( \u2212 1)-th streaming session and ",
                    "score": 0.639321471716715,
                    "section_title": "Teacher-Student Knowledge Distillation",
                    "char_start_offset": 16974,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 851
                        },
                        {
                            "start": 854,
                            "end": 887
                        },
                        {
                            "start": 888,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1311
                        },
                        {
                            "start": 1314,
                            "end": 1386
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 260,
                            "end": 264,
                            "matchedPaperCorpusId": "235446818"
                        },
                        {
                            "start": 264,
                            "end": 268,
                            "matchedPaperCorpusId": "221191366"
                        },
                        {
                            "start": 655,
                            "end": 659,
                            "matchedPaperCorpusId": "4853851"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.72265625
                }
            ],
            "relevance_judgement": 0.72265625,
            "relevance_judgment_input_expanded": "# Title: Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation\n# Venue: Knowledge Discovery and Data Mining\n# Authors: Bin Lu, Xiaoying Gan, Lina Yang, Weinan Zhang, Luoyi Fu, Xinbing Wang\n## Abstract\nWith the tremendous expansion of graphs data, node classification shows its great importance in many real-world applications. Existing graph neural network based methods mainly focus on classifying unlabeled nodes within fixed classes with abundant labeling. However, in many practical scenarios, graph evolves with emergence of new nodes and edges. Novel classes appear incrementally along with few labeling due to its newly emergence or lack of exploration. In this paper, we focus on this challenging but practical graph few-shot class-incremental learning (GFSCIL) problem and propose a novel method called Geometer. Instead of replacing and retraining the fully connected neural network classifier, Geometer predicts the label of a node by finding the nearest class prototype. Prototype is a vector representing a class in the metric space. With the pop-up of novel classes, Geometer learns and adjusts the attention-based prototypes by observing the geometric proximity, uniformity and separability. Teacher-student knowledge distillation and biased sampling are further introduced to mitigate catastrophic forgetting and unbalanced labeling problem respectively. Experimental results on four public datasets demonstrate that Geometer achieves a substantial improvement of 9.46% to 27.60% over state-of-the-art methods.\n## Teacher-Student Knowledge Distillation\nDue to the class-incremental nature of GFSCIL problem, Geometer applies the idea of teacher-student knowledge distillation to further mitigate \"forgetting old\" during finetuning. In contrast to the classic teacher-student knowledge distillation techniques [27][28][29] used to compress large model into lightweight model with better inference efficiency, we regard the model before streaming as the teacher model and new model as the student model. Knowledge distillation technique is used to transfer the classification ability of previous model, while preserving the interrelationships of the old classes in the metric space. Temperature-scaled softmax [12] is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model. Geometer proposes to calculate the KL-divergence of the softened logits to make the student model gain the experience of classifying old classes C  \u22121 from teacher model. The teacher-student knowledge distillation loss L  on -th session is calculated as \n\nwhere C \u22121 is the set of old classes of ( \u2212 1)-th streaming session and ",
            "reference_string": "[249151858 | Lu et al. | 2022 | Citations: 29]"
        },
        {
            "title": "Online Sensor Hallucination via Knowledge Distillation for Multimodal Image Classification",
            "venue": "arXiv.org",
            "year": 2019,
            "reference_count": 43,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.10559, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39703750",
                    "name": "S. Kumar"
                },
                {
                    "authorId": "49124777",
                    "name": "Biplab Banerjee"
                },
                {
                    "authorId": "144527832",
                    "name": "S. Chaudhuri"
                }
            ],
            "abstract": "We deal with the problem of information fusion driven satellite image/scene classification and propose a generic hallucination architecture considering that all the available sensor information are present during training while some of the image modalities may be absent while testing. It is well-known that different sensors are capable of capturing complementary information for a given geographical area and a classification module incorporating information from all the sources are expected to produce an improved performance as compared to considering only a subset of the modalities. However, the classical classifier systems inherently require all the features used to train the module to be present for the test instances as well, which may not always be possible for typical remote sensing applications (say, disaster management). As a remedy, we provide a robust solution in terms of a hallucination module that can approximate the missing modalities from the available ones during the decision-making stage. In order to ensure better knowledge transfer during modality hallucination, we explicitly incorporate concepts of knowledge distillation for the purpose of exploring the privileged (side) information in our framework and subsequently introduce an intuitive modular training approach. The proposed network is evaluated extensively on a large-scale corpus of PAN-MS image pairs (scene recognition) as well as on a benchmark hyperspectral image dataset (image classification) where we follow different experimental scenarios and find that the proposed hallucination based module indeed is capable of capturing the multi-source information, albeit the explicit absence of some of the sensor information, and aid in improved scene characterization.",
            "corpus_id": 201660247,
            "sentences": [
                {
                    "corpus_id": "201660247",
                    "title": "Online Sensor Hallucination via Knowledge Distillation for Multimodal Image Classification",
                    "text": "At the test time the student network is expected to perform better inference pertaining to the initial teacher guided training process. [40] recently proposed a graph-based distillation method to learn in presence of side information for action detection and action recognition tasks in videos. \n\nKnowledge distillation refers to the process of intelligently transferring the knowledge of an already trained, larger/complex learning module or an ensemble of several models to a rather smaller counterpart which is expected to be easier to comprehend and deploy. Knowledge distillation for machine learning models was first introduced by [41] while [42] presented a more general approach towards distillation within the scope of a feed-forward neural network and for the purpose of handwritten digit recognition. [41] considers the operations directly on the logits obtained from the softmax layer for learning the smaller model and minimize the squared difference between the logits from cumbersome and the smaller model. [42] scale the final logits by a temperature parameter to obtain soft thresholds and these are used to train the smaller model. They also show that matching the logits as done by [41], is a special case of the technique they propose. \n\nHowever, from the perspective of the distillation framework mentioned in [42], the knowledge gathered by a deep network is not interpreted merely as the network parameters (weights and biases) obtained during training but also includes the final logits generated by the network corresponding to the inputs. Precisely, [42] propose a much general methodology for distillation of knowledge from a larger teacher network to a drastically smaller student network by using a parameter termed as the temperature. Essentially, the output logits of teacher network are normalized with respect to the temperature parameter and subsequently the softmax operation is carried out on them in order to obtain a softer threshold which leads to an improved and more general knowledge transfer to the student network. The outputs of neural networks are typically class probabilities which are obtained by using the softmax function which converts the output logits z i , computed for each class into a probability, by comparing it to other logits. \n\nConsider a scenario where we have data from multiple modalities available during training, but while testing only one or a subset of these are available.",
                    "score": 0.5857243328843712,
                    "section_title": "Related work",
                    "char_start_offset": 12651,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 294
                        },
                        {
                            "start": 297,
                            "end": 561
                        },
                        {
                            "start": 562,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1149
                        },
                        {
                            "start": 1150,
                            "end": 1255
                        },
                        {
                            "start": 1258,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1764
                        },
                        {
                            "start": 1765,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2288
                        },
                        {
                            "start": 2291,
                            "end": 2444
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 136,
                            "end": 140,
                            "matchedPaperCorpusId": "51869487"
                        },
                        {
                            "start": 637,
                            "end": 641,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 812,
                            "end": 816,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1201,
                            "end": 1205,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71826171875
                }
            ],
            "relevance_judgement": 0.71826171875,
            "relevance_judgment_input_expanded": "# Title: Online Sensor Hallucination via Knowledge Distillation for Multimodal Image Classification\n# Venue: arXiv.org\n# Authors: S. Kumar, Biplab Banerjee, S. Chaudhuri\n## Abstract\nWe deal with the problem of information fusion driven satellite image/scene classification and propose a generic hallucination architecture considering that all the available sensor information are present during training while some of the image modalities may be absent while testing. It is well-known that different sensors are capable of capturing complementary information for a given geographical area and a classification module incorporating information from all the sources are expected to produce an improved performance as compared to considering only a subset of the modalities. However, the classical classifier systems inherently require all the features used to train the module to be present for the test instances as well, which may not always be possible for typical remote sensing applications (say, disaster management). As a remedy, we provide a robust solution in terms of a hallucination module that can approximate the missing modalities from the available ones during the decision-making stage. In order to ensure better knowledge transfer during modality hallucination, we explicitly incorporate concepts of knowledge distillation for the purpose of exploring the privileged (side) information in our framework and subsequently introduce an intuitive modular training approach. The proposed network is evaluated extensively on a large-scale corpus of PAN-MS image pairs (scene recognition) as well as on a benchmark hyperspectral image dataset (image classification) where we follow different experimental scenarios and find that the proposed hallucination based module indeed is capable of capturing the multi-source information, albeit the explicit absence of some of the sensor information, and aid in improved scene characterization.\n## Related work\nAt the test time the student network is expected to perform better inference pertaining to the initial teacher guided training process. [40] recently proposed a graph-based distillation method to learn in presence of side information for action detection and action recognition tasks in videos. \n\nKnowledge distillation refers to the process of intelligently transferring the knowledge of an already trained, larger/complex learning module or an ensemble of several models to a rather smaller counterpart which is expected to be easier to comprehend and deploy. Knowledge distillation for machine learning models was first introduced by [41] while [42] presented a more general approach towards distillation within the scope of a feed-forward neural network and for the purpose of handwritten digit recognition. [41] considers the operations directly on the logits obtained from the softmax layer for learning the smaller model and minimize the squared difference between the logits from cumbersome and the smaller model. [42] scale the final logits by a temperature parameter to obtain soft thresholds and these are used to train the smaller model. They also show that matching the logits as done by [41], is a special case of the technique they propose. \n\nHowever, from the perspective of the distillation framework mentioned in [42], the knowledge gathered by a deep network is not interpreted merely as the network parameters (weights and biases) obtained during training but also includes the final logits generated by the network corresponding to the inputs. Precisely, [42] propose a much general methodology for distillation of knowledge from a larger teacher network to a drastically smaller student network by using a parameter termed as the temperature. Essentially, the output logits of teacher network are normalized with respect to the temperature parameter and subsequently the softmax operation is carried out on them in order to obtain a softer threshold which leads to an improved and more general knowledge transfer to the student network. The outputs of neural networks are typically class probabilities which are obtained by using the softmax function which converts the output logits z i , computed for each class into a probability, by comparing it to other logits. \n\nConsider a scenario where we have data from multiple modalities available during training, but while testing only one or a subset of these are available.",
            "reference_string": "[201660247 | Kumar et al. | 2019 | Citations: 5]"
        },
        {
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294363351",
                    "name": "Sheikh Musa Kaleem"
                },
                {
                    "authorId": "2294363576",
                    "name": "Tufail Rouf"
                },
                {
                    "authorId": "2016381637",
                    "name": "Gousia Habib"
                },
                {
                    "authorId": "1468566373",
                    "name": "T. Saleem"
                },
                {
                    "authorId": "143632380",
                    "name": "B. Lall"
                }
            ],
            "abstract": "Deep learning techniques have been demonstrated to surpass preceding cutting-edge machine learning techniques in recent years, with computer vision being one of the most prominent examples. However, deep learning models suffer from significant drawbacks when deployed in resource-constrained environments due to their large model size and high complexity. Knowledge Distillation is one of the prominent solutions to overcome this challenge. This review paper examines the current state of research on knowledge distillation, a technique for compressing complex models into smaller and simpler ones. The paper provides an overview of the major principles and techniques associated with knowledge distillation and reviews the applications of knowledge distillation in the domain of computer vision. The review focuses on the benefits of knowledge distillation, as well as the problems that must be overcome to improve its effectiveness.",
            "corpus_id": 268820185,
            "sentences": [
                {
                    "corpus_id": "268820185",
                    "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
                    "text": "A powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
                    "score": 0.8337184633284824,
                    "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
                    "char_start_offset": 12659,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 186
                        },
                        {
                            "start": 186,
                            "end": 392
                        },
                        {
                            "start": 394,
                            "end": 563
                        },
                        {
                            "start": 563,
                            "end": 666
                        },
                        {
                            "start": 666,
                            "end": 822
                        },
                        {
                            "start": 822,
                            "end": 990
                        },
                        {
                            "start": 990,
                            "end": 1129
                        },
                        {
                            "start": 1129,
                            "end": 1284
                        },
                        {
                            "start": 1284,
                            "end": 1389
                        },
                        {
                            "start": 1391,
                            "end": 1604
                        },
                        {
                            "start": 1604,
                            "end": 1826
                        },
                        {
                            "start": 1828,
                            "end": 2050
                        },
                        {
                            "start": 2050,
                            "end": 2191
                        },
                        {
                            "start": 2191,
                            "end": 2298
                        },
                        {
                            "start": 2298,
                            "end": 2434
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1384,
                            "end": 1388,
                            "matchedPaperCorpusId": "219559263"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71728515625
                }
            ],
            "relevance_judgement": 0.71728515625,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Review of Knowledge Distillation in Computer Vision\n# Venue: arXiv.org\n# Authors: Sheikh Musa Kaleem, Tufail Rouf, Gousia Habib, T. Saleem, B. Lall\n## Abstract\nDeep learning techniques have been demonstrated to surpass preceding cutting-edge machine learning techniques in recent years, with computer vision being one of the most prominent examples. However, deep learning models suffer from significant drawbacks when deployed in resource-constrained environments due to their large model size and high complexity. Knowledge Distillation is one of the prominent solutions to overcome this challenge. This review paper examines the current state of research on knowledge distillation, a technique for compressing complex models into smaller and simpler ones. The paper provides an overview of the major principles and techniques associated with knowledge distillation and reviews the applications of knowledge distillation in the domain of computer vision. The review focuses on the benefits of knowledge distillation, as well as the problems that must be overcome to improve its effectiveness.\n## INTRODUCTION TO KNOWLEDGE DISTILLATION\nA powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
            "reference_string": "[268820185 | Kaleem et al. | 2024 | Citations: 13]"
        },
        {
            "title": "A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning",
            "venue": "Journal of Physics: Conference Series",
            "year": 2020,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1088/1742-6596/1607/1/012096",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1088/1742-6596/1607/1/012096?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1088/1742-6596/1607/1/012096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145253722",
                    "name": "Zihan Liu"
                },
                {
                    "authorId": "2110398805",
                    "name": "Zhi-yuan Shi"
                }
            ],
            "abstract": "In order to improve the prediction performance, the complex depth network model needs a large number of calculation parameters, and the negative impact is the fact that it greatly increases the calculation time and energy consumption. In this paper, a fusion algorithm of model pruning and joint multi-model knowledge distillation learning is proposed. By constructing an adaptive joint learning loss function including distillation, multi-model training is carried out. This method can replace the training process of model fine-tuning after pruning. In this paper, firstly, multi classification tasks are carried out on different model structures and data sets, and a complex network model with good effect is trained as a teacher\u2019s model. Then, the channel pruning process of random pruning degree is executed to generate multiple student models. Finally, the accuracy of multiple student models is improved by using this method. The experimental results show that this method effectively improves the accuracy of each model after pruning, and the experimental results achieve a suitable balance between model performance and accuracy.",
            "corpus_id": 225397480,
            "sentences": [
                {
                    "corpus_id": "225397480",
                    "title": "A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning",
                    "text": "Testing Process The neural network often obtains the network output through the softmax [17] layers at the end. By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  . In order to calculate the similarity between the output vector and the ideal vector in the neural network, cross entropy is usually describe the distance between the two probability distributions. KL divergence (relative entropy) refers to describe the difference between two probability distributions. \n\nIn knowledge distillation, cross entropy is a loss function commonly used with softmax in multiclassification problems: \n\nFor the two probability distributions P and Q, the relative entropy is defined as follows: \n\nThe combined neural network loss function formula is as follows: \n\nThe   ,  in the above formula is called  ,that is, the cross-entropy loss obtained by the soft targets generated by the teacher network, Cross Entropy ( ,  ) is called  , which is the cross-entropy loss obtained by using the hard targets in the training set. Use  to adjust the proportion of the two loss values. \n\nIn knowledge distillation, the teacher network only conducts one-way knowledge transfer to the student network during the learning process. In the method proposed in this paper, multiple networks are used for training at the same time. During the training process, each network not only accepts the supervision of the true value label, and the fixed probability estimation of the teacher network, but also refers to the dynamic probability estimation of other networks. This method can further improve the generalization ability. Through the process, feedback information is obtained from the learning state between the student networks to optimize the training process and achieve collaborative training. In each iteration, we calculate the predictions of different models and update the parameters of the network according to the predictions of other models.",
                    "score": 0.5362948524795161,
                    "section_title": "Training Process",
                    "char_start_offset": 5694,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 111
                        },
                        {
                            "start": 112,
                            "end": 208
                        },
                        {
                            "start": 209,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 375
                        },
                        {
                            "start": 378,
                            "end": 450
                        },
                        {
                            "start": 451,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1023
                        },
                        {
                            "start": 1026,
                            "end": 1145
                        },
                        {
                            "start": 1148,
                            "end": 1238
                        },
                        {
                            "start": 1241,
                            "end": 1305
                        },
                        {
                            "start": 1308,
                            "end": 1566
                        },
                        {
                            "start": 1567,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1858
                        },
                        {
                            "start": 1859,
                            "end": 2092
                        },
                        {
                            "start": 2093,
                            "end": 2152
                        },
                        {
                            "start": 2153,
                            "end": 2328
                        },
                        {
                            "start": 2329,
                            "end": 2483
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.712890625
                }
            ],
            "relevance_judgement": 0.712890625,
            "relevance_judgment_input_expanded": "# Title: A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning\n# Venue: Journal of Physics: Conference Series\n# Authors: Zihan Liu, Zhi-yuan Shi\n## Abstract\nIn order to improve the prediction performance, the complex depth network model needs a large number of calculation parameters, and the negative impact is the fact that it greatly increases the calculation time and energy consumption. In this paper, a fusion algorithm of model pruning and joint multi-model knowledge distillation learning is proposed. By constructing an adaptive joint learning loss function including distillation, multi-model training is carried out. This method can replace the training process of model fine-tuning after pruning. In this paper, firstly, multi classification tasks are carried out on different model structures and data sets, and a complex network model with good effect is trained as a teacher\u2019s model. Then, the channel pruning process of random pruning degree is executed to generate multiple student models. Finally, the accuracy of multiple student models is improved by using this method. The experimental results show that this method effectively improves the accuracy of each model after pruning, and the experimental results achieve a suitable balance between model performance and accuracy.\n## Training Process\nTesting Process The neural network often obtains the network output through the softmax [17] layers at the end. By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  . In order to calculate the similarity between the output vector and the ideal vector in the neural network, cross entropy is usually describe the distance between the two probability distributions. KL divergence (relative entropy) refers to describe the difference between two probability distributions. \n\nIn knowledge distillation, cross entropy is a loss function commonly used with softmax in multiclassification problems: \n\nFor the two probability distributions P and Q, the relative entropy is defined as follows: \n\nThe combined neural network loss function formula is as follows: \n\nThe   ,  in the above formula is called  ,that is, the cross-entropy loss obtained by the soft targets generated by the teacher network, Cross Entropy ( ,  ) is called  , which is the cross-entropy loss obtained by using the hard targets in the training set. Use  to adjust the proportion of the two loss values. \n\nIn knowledge distillation, the teacher network only conducts one-way knowledge transfer to the student network during the learning process. In the method proposed in this paper, multiple networks are used for training at the same time. During the training process, each network not only accepts the supervision of the true value label, and the fixed probability estimation of the teacher network, but also refers to the dynamic probability estimation of other networks. This method can further improve the generalization ability. Through the process, feedback information is obtained from the learning state between the student networks to optimize the training process and achieve collaborative training. In each iteration, we calculate the predictions of different models and update the parameters of the network according to the predictions of other models.",
            "reference_string": "[225397480 | Liu et al. | 2020 | Citations: 1]"
        },
        {
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "venue": "International Journal of Advanced Computer Science and Applications",
            "year": 2021,
            "reference_count": 41,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.14569/ijacsa.2021.0120309",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.14569/IJACSA.2021.0120309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.14569/IJACSA.2021.0120309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139643077",
                    "name": "Bhavesh Jaiswal"
                },
                {
                    "authorId": "32136431",
                    "name": "Nagendra Gajjar"
                }
            ],
            "abstract": "Deep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.",
            "corpus_id": 234336288,
            "sentences": [
                {
                    "corpus_id": "234336288",
                    "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                    "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
                    "score": 0.5908875132273588,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 8373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 57,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 153
                        },
                        {
                            "start": 156,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 691
                        },
                        {
                            "start": 692,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 864
                        },
                        {
                            "start": 867,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1606
                        },
                        {
                            "start": 1607,
                            "end": 1716
                        },
                        {
                            "start": 1717,
                            "end": 1788
                        },
                        {
                            "start": 1789,
                            "end": 1968
                        },
                        {
                            "start": 1969,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2128
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1077,
                            "end": 1081,
                            "matchedPaperCorpusId": "11536917"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70556640625
                }
            ],
            "relevance_judgement": 0.70556640625,
            "relevance_judgment_input_expanded": "# Title: Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification\n# Venue: International Journal of Advanced Computer Science and Applications\n# Authors: Bhavesh Jaiswal, Nagendra Gajjar\n## Abstract\nDeep learning frameworks have progressed beyond human recognition capabilities and, now it\u2019s the perfect opportunity to optimize them for implementation on the embedded platforms. The present deep learning architectures support learning capabilities, but they lack flexibility for applying learned knowledge on the tasks in other unfamiliar domains. This work tries to fill this gap with the deep neural network-based solution for object detection in unrelated domains with a focus on the reduced footprint of the developed model. Knowledge distillation provides efficient and effective teacher-student learning for a variety of different visual recognition tasks. A lightweight student network can be easily trained under the guidance of the high-capacity teacher networks. The teacher-student architecture implementation on binary classes shows a 20% improvement in accuracy within the same training iterations using the transfer learning approach. The scalability of the student model is tested with binary, ternary and multiclass and their performance is compared on basis of inference speed. The results show that the inference speed does not depend on the number of classes. For similar recognition accuracy, the inference speed of about 50 frames per second or 20ms per image. Thus, this approach can be generalized as per the application requirement with minimal changes, provided the dataset format compatibility.\n## C. Knowledge Distillation\nKnowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "reference_string": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]"
        },
        {
            "title": "Bringing AI To Edge: From Deep Learning's Perspective",
            "venue": "Neurocomputing",
            "year": 2020,
            "reference_count": 228,
            "citation_count": 123,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.14808",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.14808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115296404",
                    "name": "Di Liu"
                },
                {
                    "authorId": "2815982",
                    "name": "Hao Kong"
                },
                {
                    "authorId": "2041283357",
                    "name": "Xiangzhong Luo"
                },
                {
                    "authorId": "2248636",
                    "name": "Weichen Liu"
                },
                {
                    "authorId": "2062739126",
                    "name": "Ravi Subramaniam"
                }
            ],
            "abstract": null,
            "corpus_id": 227228186,
            "sentences": [
                {
                    "corpus_id": "227228186",
                    "title": "Bringing AI To Edge: From Deep Learning's Perspective",
                    "text": "Knowledge distillation is another technique to conduct model compression, where a more compact student model can learn the knowledge from a complicated and powerful teacher model. Bucila et al. [148] first propose the concept of knowledge distillation, and Hinton et al. [140] generalize knowledge distillation and apply it to DNNs. \n\nThe core idea of knowledge distillation is to train a compact model (student) with the assistant of a complicated, pre-trained model (teacher). During training, the student model exploit the conventional method to train the model and obtain a loss according to the one-hot class distribution, e.g., [0, 0, 1, 0], namely hard targets and at the same time the knowledge from the teacher model is distilled and transferred to the student model by calculating a new loss in which the target is the probability distribution of predicted class P from the teacher model, e.g., [0.1, 0.21, 0.6, 0.09], namely soft target. Nevertheless, the probability of the correct class dominates the probability distribution generated by the teacher network (e.g., [0.97, 0.1, 0.0, 0.2]), which significantly limits the knowledge transferring capability. To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.2]). Hence, we are able to formulate the softmax with temperature as follows: \n\nwhen T is set to 1, it is the original softmax function. Please note that we refer the softmax function with temperature T as \u03b4 T for simplicity. Therefore, we can formulate the overall loss function as: \n\nwhere F denotes the cross-entropy function. \u03b1 and \u03b2 are two balancing factors. z s and z t represent the output logits from student model and teacher model, respectively. y is the ground truth. An illustration about how knowledge distillation works is shown in Fig. 7. \n\nAfter [140], many efforts are made towards improving the performance of knowledge distillation.",
                    "score": 0.7048946471984068,
                    "section_title": "C. Knowledge Distillation",
                    "char_start_offset": 46895,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 179
                        },
                        {
                            "start": 180,
                            "end": 332
                        },
                        {
                            "start": 335,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1168
                        },
                        {
                            "start": 1169,
                            "end": 1317
                        },
                        {
                            "start": 1318,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1489
                        },
                        {
                            "start": 1492,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1637
                        },
                        {
                            "start": 1638,
                            "end": 1695
                        },
                        {
                            "start": 1698,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1868
                        },
                        {
                            "start": 1869,
                            "end": 1891
                        },
                        {
                            "start": 1892,
                            "end": 1966
                        },
                        {
                            "start": 1969,
                            "end": 2064
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 194,
                            "end": 199,
                            "matchedPaperCorpusId": "11253972"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70458984375
                }
            ],
            "relevance_judgement": 0.70458984375,
            "relevance_judgment_input_expanded": "# Title: Bringing AI To Edge: From Deep Learning's Perspective\n# Venue: Neurocomputing\n# Authors: Di Liu, Hao Kong, Xiangzhong Luo, Weichen Liu, Ravi Subramaniam\n## Abstract\nNone\n## C. Knowledge Distillation\nKnowledge distillation is another technique to conduct model compression, where a more compact student model can learn the knowledge from a complicated and powerful teacher model. Bucila et al. [148] first propose the concept of knowledge distillation, and Hinton et al. [140] generalize knowledge distillation and apply it to DNNs. \n\nThe core idea of knowledge distillation is to train a compact model (student) with the assistant of a complicated, pre-trained model (teacher). During training, the student model exploit the conventional method to train the model and obtain a loss according to the one-hot class distribution, e.g., [0, 0, 1, 0], namely hard targets and at the same time the knowledge from the teacher model is distilled and transferred to the student model by calculating a new loss in which the target is the probability distribution of predicted class P from the teacher model, e.g., [0.1, 0.21, 0.6, 0.09], namely soft target. Nevertheless, the probability of the correct class dominates the probability distribution generated by the teacher network (e.g., [0.97, 0.1, 0.0, 0.2]), which significantly limits the knowledge transferring capability. To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.2]). Hence, we are able to formulate the softmax with temperature as follows: \n\nwhen T is set to 1, it is the original softmax function. Please note that we refer the softmax function with temperature T as \u03b4 T for simplicity. Therefore, we can formulate the overall loss function as: \n\nwhere F denotes the cross-entropy function. \u03b1 and \u03b2 are two balancing factors. z s and z t represent the output logits from student model and teacher model, respectively. y is the ground truth. An illustration about how knowledge distillation works is shown in Fig. 7. \n\nAfter [140], many efforts are made towards improving the performance of knowledge distillation.",
            "reference_string": "[227228186 | Liu et al. | 2020 | Citations: 123]"
        },
        {
            "title": "HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks",
            "venue": "Neurocomputing",
            "year": 2022,
            "reference_count": 63,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2207.11887",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2207.11887, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153466123",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2062719264",
                    "name": "Tongya Zheng"
                },
                {
                    "authorId": "36851883",
                    "name": "Qinfen Hao"
                }
            ],
            "abstract": "Researchers have recently proposed plenty of heterogeneous graph neural networks (HGNNs) due to the ubiquity of heterogeneous graphs in both academic and industrial areas. Instead of pursuing a more powerful HGNN model, in this paper, we are interested in devising a versatile plug-and-play module, which accounts for distilling relational knowledge from pre-trained HGNNs. To the best of our knowledge, we are the first to propose a HIgh-order RElational (HIRE) knowledge distillation framework on heterogeneous graphs, which can significantly boost the prediction performance regardless of model architectures of HGNNs. Concretely, our HIRE framework initially performs first-order node-level knowledge distillation, which encodes the semantics of the teacher HGNN with its prediction logits. Meanwhile, the second-order relation-level knowledge distillation imitates the relational correlation between node embeddings of different types generated by the teacher HGNN. Extensive experiments on various popular HGNNs models and three real-world heterogeneous graphs demonstrate that our method obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",
            "corpus_id": 251040920,
            "sentences": [
                {
                    "corpus_id": "251040920",
                    "title": "HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks",
                    "text": "Aggregation Function RGCN [13] h \n\nnodes are imprecise. The node-level knowledge distillation is applied to transfer soft labels (p \u03c4 t i ) predicted by the teacher network to the student network, allowing students to learn smoother knowledge [17]. The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class. . Besides the teacher's soft labels, it can further boost the training performance with existing ground truth labels, as stated by Hinton et al. [17], which can significantly improve the performance of the student model. Accordingly, the NKD loss can be written as a weighted average of the cross-entropy loss and the distillation loss: \n\nwhere L CE = \u2212 N i y i log p s i is the basic cross-entropy loss, i denotes the i-th node, \u03b1 is the hyperparamemter for balancing the cross-entropy loss and the distillation loss. \n\nNevertheless, NKD cannot address the semantic relation modeling issue because it ignores the crucial correlation between different types of nodes in heterogeneous graphs, which can be solved in the next section.",
                    "score": 0.5605887099178611,
                    "section_title": "Models",
                    "char_start_offset": 13265,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 35,
                            "end": 55
                        },
                        {
                            "start": 56,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1322
                        },
                        {
                            "start": 1323,
                            "end": 1438
                        },
                        {
                            "start": 1441,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1834
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 26,
                            "end": 30,
                            "matchedPaperCorpusId": "5458500"
                        },
                        {
                            "start": 243,
                            "end": 247,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1247,
                            "end": 1251,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69921875
                }
            ],
            "relevance_judgement": 0.69921875,
            "relevance_judgment_input_expanded": "# Title: HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks\n# Venue: Neurocomputing\n# Authors: Jing Liu, Tongya Zheng, Qinfen Hao\n## Abstract\nResearchers have recently proposed plenty of heterogeneous graph neural networks (HGNNs) due to the ubiquity of heterogeneous graphs in both academic and industrial areas. Instead of pursuing a more powerful HGNN model, in this paper, we are interested in devising a versatile plug-and-play module, which accounts for distilling relational knowledge from pre-trained HGNNs. To the best of our knowledge, we are the first to propose a HIgh-order RElational (HIRE) knowledge distillation framework on heterogeneous graphs, which can significantly boost the prediction performance regardless of model architectures of HGNNs. Concretely, our HIRE framework initially performs first-order node-level knowledge distillation, which encodes the semantics of the teacher HGNN with its prediction logits. Meanwhile, the second-order relation-level knowledge distillation imitates the relational correlation between node embeddings of different types generated by the teacher HGNN. Extensive experiments on various popular HGNNs models and three real-world heterogeneous graphs demonstrate that our method obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.\n## Models\nAggregation Function RGCN [13] h \n\nnodes are imprecise. The node-level knowledge distillation is applied to transfer soft labels (p \u03c4 t i ) predicted by the teacher network to the student network, allowing students to learn smoother knowledge [17]. The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class. . Besides the teacher's soft labels, it can further boost the training performance with existing ground truth labels, as stated by Hinton et al. [17], which can significantly improve the performance of the student model. Accordingly, the NKD loss can be written as a weighted average of the cross-entropy loss and the distillation loss: \n\nwhere L CE = \u2212 N i y i log p s i is the basic cross-entropy loss, i denotes the i-th node, \u03b1 is the hyperparamemter for balancing the cross-entropy loss and the distillation loss. \n\nNevertheless, NKD cannot address the semantic relation modeling issue because it ignores the crucial correlation between different types of nodes in heterogeneous graphs, which can be solved in the next section.",
            "reference_string": "[251040920 | Liu et al. | 2022 | Citations: 8]"
        },
        {
            "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning",
            "venue": "Pattern Recognition",
            "year": 2019,
            "reference_count": 56,
            "citation_count": 32,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.05657",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1901.05657, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2145288039",
                    "name": "Lu Liu"
                },
                {
                    "authorId": "1726720",
                    "name": "R. Tan"
                }
            ],
            "abstract": null,
            "corpus_id": 234353710,
            "sentences": [
                {
                    "corpus_id": "234353710",
                    "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning",
                    "text": "Bengio et al. [14] first propose a progressive learning paradigm which organizes the training data from easiest to hardest. In active learning [38,39], similar indicators have been used to guide decisions about which data point to label next. In general, we all rely on some indicators to evaluate the training data. However, to our knowledge, applying this indicator in terms of the consistency regularization to SSL is new, and has not been explored before. \n\nKnowledge distillation. Hinton et al. [40] apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label. However, the method needs to set the value of temperature empirically, which is shared by all training samples. Our method can automatically define the temperature of each training sample according to its uncertainty, and use its own temperature to decide how much influence it has on training the student model.",
                    "score": 0.58406965573035,
                    "section_title": "Recently, Qiao et al. propose deep Co-Training",
                    "char_start_offset": 11142,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 459
                        },
                        {
                            "start": 462,
                            "end": 485
                        },
                        {
                            "start": 486,
                            "end": 707
                        },
                        {
                            "start": 708,
                            "end": 954
                        },
                        {
                            "start": 955,
                            "end": 1066
                        },
                        {
                            "start": 1067,
                            "end": 1267
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 14,
                            "end": 18,
                            "matchedPaperCorpusId": "873046"
                        },
                        {
                            "start": 147,
                            "end": 150,
                            "matchedPaperCorpusId": "127253432"
                        },
                        {
                            "start": 500,
                            "end": 504,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6943359375
                }
            ],
            "relevance_judgement": 0.6943359375,
            "relevance_judgment_input_expanded": "# Title: Certainty driven consistency loss on multi-teacher networks for semi-supervised learning\n# Venue: Pattern Recognition\n# Authors: Lu Liu, R. Tan\n## Abstract\nNone\n## Recently, Qiao et al. propose deep Co-Training\nBengio et al. [14] first propose a progressive learning paradigm which organizes the training data from easiest to hardest. In active learning [38,39], similar indicators have been used to guide decisions about which data point to label next. In general, we all rely on some indicators to evaluate the training data. However, to our knowledge, applying this indicator in terms of the consistency regularization to SSL is new, and has not been explored before. \n\nKnowledge distillation. Hinton et al. [40] apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label. However, the method needs to set the value of temperature empirically, which is shared by all training samples. Our method can automatically define the temperature of each training sample according to its uncertainty, and use its own temperature to decide how much influence it has on training the student model.",
            "reference_string": "[234353710 | Liu et al. | 2019 | Citations: 32]"
        },
        {
            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.07172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109519891",
                    "name": "Dongjin Lee"
                },
                {
                    "authorId": "120425550",
                    "name": "Seongsik Park"
                },
                {
                    "authorId": "2157129258",
                    "name": "Jongwan Kim"
                },
                {
                    "authorId": "2111845679",
                    "name": "Wuhyeong Doh"
                },
                {
                    "authorId": "2999019",
                    "name": "Sungroh Yoon"
                }
            ],
            "abstract": "Spiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods.",
            "corpus_id": 235421655,
            "sentences": [
                {
                    "corpus_id": "235421655",
                    "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
                    "text": "Efficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
                    "score": 0.5442193778387782,
                    "section_title": "Knowledge distillation",
                    "char_start_offset": 7213,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 270
                        },
                        {
                            "start": 271,
                            "end": 440
                        },
                        {
                            "start": 441,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1207
                        },
                        {
                            "start": 1208,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1525
                        },
                        {
                            "start": 1528,
                            "end": 1692
                        },
                        {
                            "start": 1693,
                            "end": 1816
                        },
                        {
                            "start": 1817,
                            "end": 1950
                        },
                        {
                            "start": 1951,
                            "end": 2070
                        },
                        {
                            "start": 2071,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2283
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 271,
                            "end": 274,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 1589,
                            "end": 1593,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1823,
                            "end": 1826,
                            "matchedPaperCorpusId": "198179476"
                        },
                        {
                            "start": 1826,
                            "end": 1829,
                            "matchedPaperCorpusId": "26071966"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68798828125
                }
            ],
            "relevance_judgement": 0.68798828125,
            "relevance_judgment_input_expanded": "# Title: Energy-efficient Knowledge Distillation for Spiking Neural Networks\n# Venue: arXiv.org\n# Authors: Dongjin Lee, Seongsik Park, Jongwan Kim, Wuhyeong Doh, Sungroh Yoon\n## Abstract\nSpiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods.\n## Knowledge distillation\nEfficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
            "reference_string": "[235421655 | Lee et al. | 2021 | Citations: 12]"
        },
        {
            "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2021,
            "reference_count": 39,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.302.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.01881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "8844876",
                    "name": "Woojeong Jin"
                },
                {
                    "authorId": "2095979",
                    "name": "Maziar Sanjabi"
                },
                {
                    "authorId": "35557488",
                    "name": "Shaoliang Nie"
                },
                {
                    "authorId": "48327785",
                    "name": "L Tan"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                }
            ],
            "abstract": "To reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large\"teacher\"model to a smaller\"student\"model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal information is challenging since different modalities present different types of information. In this paper, we perform a large-scale empirical study to investigate the importance and effects of each modality in knowledge distillation. Furthermore, we introduce a multimodal knowledge distillation framework, modality-specific distillation (MSD), to transfer knowledge from a teacher on multimodal tasks by learning the teacher's behavior within each modality. The idea aims at mimicking a teacher's modality-specific predictions by introducing auxiliary loss terms for each modality. Furthermore, because each modality has different saliency for predictions, we define saliency scores for each modality and investigate saliency-based weighting schemes for the auxiliary losses. We further study a weight learning approach to learn the optimal weights on these loss terms. In our empirical analysis, we examine the saliency of each modality in KD, demonstrate the effectiveness of the weighting scheme in MSD, and show that it achieves better performance than KD on four multimodal datasets.",
            "corpus_id": 239616535,
            "sentences": [
                {
                    "corpus_id": "239616535",
                    "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
                    "text": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions. Thus, the loss for the student is defined as:\n\nwhere L CE is a standard cross-entropy loss on hard labels, L KD is a distillation loss, which is a crossentropy loss on soft labels, and \u03bb \u2208 [0, 1] controls the balance between hard and soft targets. To be specific, knowledge distillation (Hinton et al., 2015) minimizes the Kullback-Leibler divergence between soft targets from a teacher and probabilities from a student. The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 . The distillation loss is defined as follows:\n\n, \u03c3 is a softmax function. The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels). Following Hinton et al. (2015), we scale the loss by \u03c4 2 in order to keep gradient magnitudes approximately constant when changing the temperature. We omit \u03c4 for brevity.\n\nLimitations. This KD can be applied to multimodal setups and student models in this distillation are directly trained to mimic a teacher's outputs. As a result, the student and teacher models may significantly differ in outputs with a single-modality input, i.e., modality-specific outputs, which may lead to inefficient distillation (Fig. 1). To better mimic the teacher's behaviors, we introduce a multimodal KD approach, modality-specific distillation, in the next section.",
                    "score": 0.5927799936249645,
                    "section_title": "Conventional Knowledge Distillation",
                    "char_start_offset": 4824,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6875
                }
            ],
            "relevance_judgement": 0.6875,
            "relevance_judgment_input_expanded": "# Title: MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, L Tan, Xiang Ren, Hamed Firooz\n## Abstract\nTo reduce a model size but retain performance, we often rely on knowledge distillation (KD) which transfers knowledge from a large\"teacher\"model to a smaller\"student\"model. However, KD on multimodal datasets such as vision-language tasks is relatively unexplored, and digesting multimodal information is challenging since different modalities present different types of information. In this paper, we perform a large-scale empirical study to investigate the importance and effects of each modality in knowledge distillation. Furthermore, we introduce a multimodal knowledge distillation framework, modality-specific distillation (MSD), to transfer knowledge from a teacher on multimodal tasks by learning the teacher's behavior within each modality. The idea aims at mimicking a teacher's modality-specific predictions by introducing auxiliary loss terms for each modality. Furthermore, because each modality has different saliency for predictions, we define saliency scores for each modality and investigate saliency-based weighting schemes for the auxiliary losses. We further study a weight learning approach to learn the optimal weights on these loss terms. In our empirical analysis, we examine the saliency of each modality in KD, demonstrate the effectiveness of the weighting scheme in MSD, and show that it achieves better performance than KD on four multimodal datasets.\n## Conventional Knowledge Distillation\nIn knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions. Thus, the loss for the student is defined as:\n\nwhere L CE is a standard cross-entropy loss on hard labels, L KD is a distillation loss, which is a crossentropy loss on soft labels, and \u03bb \u2208 [0, 1] controls the balance between hard and soft targets. To be specific, knowledge distillation (Hinton et al., 2015) minimizes the Kullback-Leibler divergence between soft targets from a teacher and probabilities from a student. The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 . The distillation loss is defined as follows:\n\n, \u03c3 is a softmax function. The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels). Following Hinton et al. (2015), we scale the loss by \u03c4 2 in order to keep gradient magnitudes approximately constant when changing the temperature. We omit \u03c4 for brevity.\n\nLimitations. This KD can be applied to multimodal setups and student models in this distillation are directly trained to mimic a teacher's outputs. As a result, the student and teacher models may significantly differ in outputs with a single-modality input, i.e., modality-specific outputs, which may lead to inefficient distillation (Fig. 1). To better mimic the teacher's behaviors, we introduce a multimodal KD approach, modality-specific distillation, in the next section.",
            "reference_string": "[239616535 | Jin et al. | 2021 | Citations: 6]"
        },
        {
            "title": "Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems",
            "venue": "IEEE Access",
            "year": 2020,
            "reference_count": 44,
            "citation_count": 31,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8948470/09162017.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3014922?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3014922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2941497",
                    "name": "Ingook Jang"
                },
                {
                    "authorId": "2118020583",
                    "name": "Hyunseok Kim"
                },
                {
                    "authorId": "2115475643",
                    "name": "Donghun Lee"
                },
                {
                    "authorId": "120595891",
                    "name": "Young-Sung Son"
                },
                {
                    "authorId": "4328349",
                    "name": "Seonghyun Kim"
                }
            ],
            "abstract": "Deep reinforcement learning (DRL) is a promising approach for developing control policies by learning how to perform tasks. Edge devices are required to control their actions by exploiting DRL to solve tasks autonomously in various applications such as smart manufacturing and autonomous driving. However, the resource limitations of edge devices make it unfeasible for them to train their policies from scratch. It is also impractical for such an edge device to use the policy with a large number of layers and parameters, which is pre-trained by a centralized cloud infrastructure with high computational power. In this paper, we propose a method, on-device DRL with distillation (OD3), to efficiently transfer distilled knowledge of how to behave for on-device DRL in resource-constrained edge computing systems. Our proposed method makes it possible to simultaneously perform knowledge transfer and policy model compression in a single training process on edge devices with considering their limited resource budgets. The novelty of our method is to apply a knowledge distillation approach to DRL based edge device control in integrated edge cloud environments. We analyze the performance of the proposed method by implementing it on a commercial embedded system-on-module equipped with limited hardware resources. The experimental results show that 1) edge policy training with the proposed method achieves near-cloud-performance in terms of average rewards, although the size of the edge policy network is significantly smaller compared to that of the cloud policy network and 2) the training time elapsed for edge policy training with our method is reduced significantly compared to edge policy training from scratch.",
            "corpus_id": 221284805,
            "sentences": [
                {
                    "corpus_id": "221284805",
                    "title": "Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems",
                    "text": "Knowledge distillation is a transfer learning approach to distill useful knowledge from a teacher model and transfer it to a student model. In a distillation process, the distilled knowledge can be transferred from a teacher network to a student network by utilizing softened targets parameterized by a temperature \u03c4 [31]. For a higher temperature \u03c4 > 1, teacher's outputs (e.g., logit vector) z i can be softened and converted into a probability distribution by passing them through a softmax function: \n\nIn the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space [11]. If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated. Update the edge policy network parameters \u03b8 S through a SGD algorithm 8 end",
                    "score": 0.7858972857938956,
                    "section_title": "C. KNOWLEDGE DISTILLATION",
                    "char_start_offset": 15384,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 139
                        },
                        {
                            "start": 140,
                            "end": 322
                        },
                        {
                            "start": 323,
                            "end": 503
                        },
                        {
                            "start": 506,
                            "end": 693
                        },
                        {
                            "start": 694,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1089
                        },
                        {
                            "start": 1090,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1415
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 918,
                            "end": 922,
                            "matchedPaperCorpusId": "1923568"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems\n# Venue: IEEE Access\n# Authors: Ingook Jang, Hyunseok Kim, Donghun Lee, Young-Sung Son, Seonghyun Kim\n## Abstract\nDeep reinforcement learning (DRL) is a promising approach for developing control policies by learning how to perform tasks. Edge devices are required to control their actions by exploiting DRL to solve tasks autonomously in various applications such as smart manufacturing and autonomous driving. However, the resource limitations of edge devices make it unfeasible for them to train their policies from scratch. It is also impractical for such an edge device to use the policy with a large number of layers and parameters, which is pre-trained by a centralized cloud infrastructure with high computational power. In this paper, we propose a method, on-device DRL with distillation (OD3), to efficiently transfer distilled knowledge of how to behave for on-device DRL in resource-constrained edge computing systems. Our proposed method makes it possible to simultaneously perform knowledge transfer and policy model compression in a single training process on edge devices with considering their limited resource budgets. The novelty of our method is to apply a knowledge distillation approach to DRL based edge device control in integrated edge cloud environments. We analyze the performance of the proposed method by implementing it on a commercial embedded system-on-module equipped with limited hardware resources. The experimental results show that 1) edge policy training with the proposed method achieves near-cloud-performance in terms of average rewards, although the size of the edge policy network is significantly smaller compared to that of the cloud policy network and 2) the training time elapsed for edge policy training with our method is reduced significantly compared to edge policy training from scratch.\n## C. KNOWLEDGE DISTILLATION\nKnowledge distillation is a transfer learning approach to distill useful knowledge from a teacher model and transfer it to a student model. In a distillation process, the distilled knowledge can be transferred from a teacher network to a student network by utilizing softened targets parameterized by a temperature \u03c4 [31]. For a higher temperature \u03c4 > 1, teacher's outputs (e.g., logit vector) z i can be softened and converted into a probability distribution by passing them through a softmax function: \n\nIn the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space [11]. If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated. Update the edge policy network parameters \u03b8 S through a SGD algorithm 8 end",
            "reference_string": "[221284805 | Jang et al. | 2020 | Citations: 31]"
        },
        {
            "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "year": 2020,
            "reference_count": 86,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2007.00051",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.00051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1842915",
                    "name": "H. Pouransari"
                },
                {
                    "authorId": "2577513",
                    "name": "Oncel Tuzel"
                }
            ],
            "abstract": "Knowledge distillation has been used to transfer knowledge learned by a sophisticated model (teacher) to a simpler model (student). This technique is widely used to compress model complexity. However, in most applications the compressed student model suffers from an accuracy gap with its teacher. We propose extracurricular learning, a novel knowledge distillation method, that bridges this gap by (1) modeling student and teacher output distributions; (2) sampling examples from an approximation to the underlying data distribution; and (3) matching student and teacher output distributions over this extended set including uncertain samples. We conduct rigorous evaluations on regression and classification tasks and show that compared to the standard knowledge distillation, extracurricular learning reduces the gap by 46% to 68%. This leads to major accuracy improvements compared to the empirical risk minimization-based training for various recent neural network architectures: 16% regression error reduction on the MPIIGaze dataset, +3.4% to +9.1% improvement in top-1 classification accuracy on the CIFAR100 dataset, and +2.9% top-1 improvement on the ImageNet dataset.",
            "corpus_id": 220280430,
            "sentences": [
                {
                    "corpus_id": "220280430",
                    "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution",
                    "text": "In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling). Results are reported in Table 10. Student is ResNet-18 and teacher is an ensemble of 8 ResNet-18 models trained with CutMix, having a top-1 accuracy of 84.6% on the CIFAR100 dataset. \n\nWe observe that XCL is not sensitive to temperature (Table 11). Note that finding an optimal T requires extensive hyper-parameter tuning. XCL does not require hyper-parameter tuning, and compared to the best KD with temperature scaling reduces the accuracy gap by 59%.",
                    "score": 0.5390094735753235,
                    "section_title": "Knowledge Distillation with Temperature Scaling",
                    "char_start_offset": 24958,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 275
                        },
                        {
                            "start": 276,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 729
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66650390625
                }
            ],
            "relevance_judgement": 0.66650390625,
            "relevance_judgment_input_expanded": "# Title: Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution\n# Venue: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\n# Authors: H. Pouransari, Oncel Tuzel\n## Abstract\nKnowledge distillation has been used to transfer knowledge learned by a sophisticated model (teacher) to a simpler model (student). This technique is widely used to compress model complexity. However, in most applications the compressed student model suffers from an accuracy gap with its teacher. We propose extracurricular learning, a novel knowledge distillation method, that bridges this gap by (1) modeling student and teacher output distributions; (2) sampling examples from an approximation to the underlying data distribution; and (3) matching student and teacher output distributions over this extended set including uncertain samples. We conduct rigorous evaluations on regression and classification tasks and show that compared to the standard knowledge distillation, extracurricular learning reduces the gap by 46% to 68%. This leads to major accuracy improvements compared to the empirical risk minimization-based training for various recent neural network architectures: 16% regression error reduction on the MPIIGaze dataset, +3.4% to +9.1% improvement in top-1 classification accuracy on the CIFAR100 dataset, and +2.9% top-1 improvement on the ImageNet dataset.\n## Knowledge Distillation with Temperature Scaling\nIn KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling). Results are reported in Table 10. Student is ResNet-18 and teacher is an ensemble of 8 ResNet-18 models trained with CutMix, having a top-1 accuracy of 84.6% on the CIFAR100 dataset. \n\nWe observe that XCL is not sensitive to temperature (Table 11). Note that finding an optimal T requires extensive hyper-parameter tuning. XCL does not require hyper-parameter tuning, and compared to the best KD with temperature scaling reduces the accuracy gap by 59%.",
            "reference_string": "[220280430 | Pouransari et al. | 2020 | Citations: 5]"
        },
        {
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "venue": "IEEE Transactions on Communications",
            "year": 2022,
            "reference_count": 63,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.17113",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.17113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2172485573",
                    "name": "Yiming Cui"
                },
                {
                    "authorId": "47093519",
                    "name": "Jiajia Guo"
                },
                {
                    "authorId": "2113999930",
                    "name": "Zheng Cao"
                },
                {
                    "authorId": "120710335",
                    "name": "Huaze Tang"
                },
                {
                    "authorId": "2257212132",
                    "name": "Chao-Kai Wen"
                },
                {
                    "authorId": "2227268421",
                    "name": "Shi Jin"
                },
                {
                    "authorId": "2288090155",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2240356164",
                    "name": "Xiaolin Hou"
                }
            ],
            "abstract": "Deep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.",
            "corpus_id": 260447668,
            "sentences": [
                {
                    "corpus_id": "260447668",
                    "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                    "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
                    "score": 0.6136532508196916,
                    "section_title": "B. Key Idea and Training Framework",
                    "char_start_offset": 18765,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 187
                        },
                        {
                            "start": 190,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 524
                        },
                        {
                            "start": 525,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 897
                        },
                        {
                            "start": 898,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1468
                        },
                        {
                            "start": 1469,
                            "end": 1543
                        },
                        {
                            "start": 1546,
                            "end": 1711
                        },
                        {
                            "start": 1712,
                            "end": 1912
                        },
                        {
                            "start": 1913,
                            "end": 2061
                        },
                        {
                            "start": 2062,
                            "end": 2175
                        },
                        {
                            "start": 2178,
                            "end": 2202
                        },
                        {
                            "start": 2203,
                            "end": 2281
                        },
                        {
                            "start": 2282,
                            "end": 2414
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66455078125
                }
            ],
            "relevance_judgement": 0.66455078125,
            "relevance_judgment_input_expanded": "# Title: Lightweight Neural Network With Knowledge Distillation for CSI Feedback\n# Venue: IEEE Transactions on Communications\n# Authors: Yiming Cui, Jiajia Guo, Zheng Cao, Huaze Tang, Chao-Kai Wen, Shi Jin, Xin Wang, Xiaolin Hou\n## Abstract\nDeep learning has shown promise in enhancing channel state information (CSI) feedback. However, many studies indicate that better feedback performance often accompanies higher computational complexity. Pursuing better performance-complexity tradeoffs is crucial to facilitate practical deployment, especially on computation-limited devices, which may have to use lightweight autoencoder with unfavorable performance. To achieve this goal, this paper introduces knowledge distillation (KD) to achieve better tradeoffs, where knowledge from a complicated teacher autoencoder is transferred to a lightweight student autoencoder for performance improvement. Specifically, two methods are proposed for implementation. Firstly, an autoencoder KD-based method is introduced by training a student autoencoder to mimic the reconstructed CSI of a pretrained teacher autoencoder. Secondly, an encoder KD-based method is proposed to reduce training overhead by performing KD only on the student encoder. Additionally, a variant of encoder KD is introduced to protect user equipment and base station vendor intellectual property. Numerical simulations demonstrate that the proposed methods can significantly improve the student autoencoder\u2019s performance, while reducing the number of floating point operations and inference time to 3.05%\u20135.28% and 13.80%\u201314.76% of the teacher network, respectively. Furthermore, the variant encoder KD method effectively enhances the student autoencoder\u2019s generalization capability across different scenarios, environments, and bandwidths.\n## B. Key Idea and Training Framework\nIn this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "reference_string": "[260447668 | Cui et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Improving Route Choice Models by Incorporating Contextual Factors via Knowledge Distillation",
            "venue": "IEEE International Joint Conference on Neural Network",
            "year": 2019,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1903.11253",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1903.11253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1688015",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "1807340",
                    "name": "S. Mukhopadhyay"
                },
                {
                    "authorId": "143861822",
                    "name": "Yimin Zhu"
                },
                {
                    "authorId": "88738552",
                    "name": "Ravindra Gudishala"
                },
                {
                    "authorId": "66590789",
                    "name": "Sanaz Saeidi"
                },
                {
                    "authorId": "88739416",
                    "name": "Alimire Nabijiang"
                }
            ],
            "abstract": "Route Choice Models predict the route choices of travelers traversing an urban area. Most of the route choice models link route characteristics of alternative routes to those chosen by the drivers. The models play an important role in prediction of traffic levels on different routes and thus assist in development of efficient traffic management strategies that result in minimizing traffic delay and maximizing effective utilization of transport system. High fidelity route choice models are required to predict traffic levels with higher accuracy. Existing route choice models do not take into account dynamic contextual conditions such as the occurrence of an accident, the socio-cultural and economic background of drivers, other human behaviors, the dynamic personal risk level, etc. As a result, they can only make predictions at an aggregate level and for a fixed set of contextual factors. For higher fidelity, it is highly desirable to use a model that captures significance of subjective or contextual factors in route choice. This paper presents a novel approach for developing high-fidelity route choice models with increased predictive power by augmenting existing aggregate level baseline models with information on drivers\u2019 responses to contextual factors obtained from Stated Choice Experiments carried out in an Immersive Virtual Environment through the use of knowledge distillation.",
            "corpus_id": 85528498,
            "sentences": [
                {
                    "corpus_id": "85528498",
                    "title": "Improving Route Choice Models by Incorporating Contextual Factors via Knowledge Distillation",
                    "text": "Knowledge distillation, also known as model compression, aims to learn a small or shallow neural network (normally called the student model, denoted as S(\u00b7)) with limited training examples and computational power by transferring the generalization ability from a large well-trained neural network (called the teacher model, denoted as T (\u00b7)), as shown in Fig.2 and Fig.4. During training, the student model will be guided by the teacher. The student attempts to match its softened softmax outputs with that of the teacher, and its hard softmax outputs with the ground truth. Given the output z i before the last layer of a neural network, usually called logits, the softmax transforms z i to a probability P i using the following equation.\n\nNormal softmax tends to set the probability for one class to one and that for the rest to zero. This makes it hard to distill hidden knowledge to the student with the teacher as the source. To improve the generalization ability of student model and efficiently use the hidden knowledge, Hinton et. al. [27] proposed high temperature softmax function in lieu of using the normal softmax (temperature=1 in this case). Then the probability P i is given by the following equation.\n\nLet the input features be denoted by x. Let the output from the teacher model T (\u00b7) with parameters \u03b8 t be T (x; \u03b8 t ). Let the output from the student model S(\u00b7) with parameters \u03b8 s be S(x; \u03b8 s ). Knowledge distillation from the teacher to the student is achieved by minimizing the distillation loss mathcalL d , between the networks, given by the following equation, L d (X, Y ) =\u03b1 n L 1 (y n , S(x n ; \u03b8 s )) + \u03b2 n L 2 (S(x n ; \u03b8 s ), T (x n ; \u03b8 t )) where X is a batch of training data that has n training examples, Y is the vector of corresponding labels for the training data X. The distillation loss L d comprises of two losses, L 1 and L 2 (both cross entropy loss functions), which correspond to supervised loss constrained by the ground truth data and softened loss constrained by the teacher model, respectively. The constants \u03b1 and \u03b2",
                    "score": 0.6200372993183216,
                    "section_title": "D. Knowledge Distillation",
                    "char_start_offset": 11507,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6630859375
                }
            ],
            "relevance_judgement": 0.6630859375,
            "relevance_judgment_input_expanded": "# Title: Improving Route Choice Models by Incorporating Contextual Factors via Knowledge Distillation\n# Venue: IEEE International Joint Conference on Neural Network\n# Authors: Qun Liu, S. Mukhopadhyay, Yimin Zhu, Ravindra Gudishala, Sanaz Saeidi, Alimire Nabijiang\n## Abstract\nRoute Choice Models predict the route choices of travelers traversing an urban area. Most of the route choice models link route characteristics of alternative routes to those chosen by the drivers. The models play an important role in prediction of traffic levels on different routes and thus assist in development of efficient traffic management strategies that result in minimizing traffic delay and maximizing effective utilization of transport system. High fidelity route choice models are required to predict traffic levels with higher accuracy. Existing route choice models do not take into account dynamic contextual conditions such as the occurrence of an accident, the socio-cultural and economic background of drivers, other human behaviors, the dynamic personal risk level, etc. As a result, they can only make predictions at an aggregate level and for a fixed set of contextual factors. For higher fidelity, it is highly desirable to use a model that captures significance of subjective or contextual factors in route choice. This paper presents a novel approach for developing high-fidelity route choice models with increased predictive power by augmenting existing aggregate level baseline models with information on drivers\u2019 responses to contextual factors obtained from Stated Choice Experiments carried out in an Immersive Virtual Environment through the use of knowledge distillation.\n## D. Knowledge Distillation\nKnowledge distillation, also known as model compression, aims to learn a small or shallow neural network (normally called the student model, denoted as S(\u00b7)) with limited training examples and computational power by transferring the generalization ability from a large well-trained neural network (called the teacher model, denoted as T (\u00b7)), as shown in Fig.2 and Fig.4. During training, the student model will be guided by the teacher. The student attempts to match its softened softmax outputs with that of the teacher, and its hard softmax outputs with the ground truth. Given the output z i before the last layer of a neural network, usually called logits, the softmax transforms z i to a probability P i using the following equation.\n\nNormal softmax tends to set the probability for one class to one and that for the rest to zero. This makes it hard to distill hidden knowledge to the student with the teacher as the source. To improve the generalization ability of student model and efficiently use the hidden knowledge, Hinton et. al. [27] proposed high temperature softmax function in lieu of using the normal softmax (temperature=1 in this case). Then the probability P i is given by the following equation.\n\nLet the input features be denoted by x. Let the output from the teacher model T (\u00b7) with parameters \u03b8 t be T (x; \u03b8 t ). Let the output from the student model S(\u00b7) with parameters \u03b8 s be S(x; \u03b8 s ). Knowledge distillation from the teacher to the student is achieved by minimizing the distillation loss mathcalL d , between the networks, given by the following equation, L d (X, Y ) =\u03b1 n L 1 (y n , S(x n ; \u03b8 s )) + \u03b2 n L 2 (S(x n ; \u03b8 s ), T (x n ; \u03b8 t )) where X is a batch of training data that has n training examples, Y is the vector of corresponding labels for the training data X. The distillation loss L d comprises of two losses, L 1 and L 2 (both cross entropy loss functions), which correspond to supervised loss constrained by the ground truth data and softened loss constrained by the teacher model, respectively. The constants \u03b1 and \u03b2",
            "reference_string": "[85528498 | Liu et al. | 2019 | Citations: 7]"
        },
        {
            "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing",
            "venue": "Electronics",
            "year": 2022,
            "reference_count": 35,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2079-9292/11/6/899/pdf?version=1647254459",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics11060899?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics11060899, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121943561",
                    "name": "Seokju Oh"
                },
                {
                    "authorId": "2145183886",
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "2109513069",
                    "name": "Chae-Suk Lee"
                },
                {
                    "authorId": "3453343",
                    "name": "Jongpil Jeong"
                }
            ],
            "abstract": "Along with the fourth industrial revolution, smart factories are receiving a great deal of attention. Large volumes of real-time data that are generated at high rates, especially in industries, are becoming increasingly important. Accordingly, the Industrial Internet of Things (IIoT), which connects, controls, and communicates with heterogeneous devices, is important to industrial sites and is now indispensable. To ensure the fairness and quality of the IIoT with limited network resources, the network connection of the IIoT needs to be constructed more intelligently. Many studies are being conducted on the efficient use of the resources that are imposed on IIoT devices. Therefore, in this paper, we propose a collaboration optimization method for heterogeneous devices that is based on cloud\u2013fog\u2013edge architecture. First, this paper proposes a knowledge distillation-based algorithm that can collaborate on cloud\u2013fog\u2013edge computing on the basis of distributed control. Second, to compensate for the shortcomings of knowledge distillation, we propose a framework for combining a soft-label-based alarm level. Finally, the method that is proposed in this paper was verified through several experiments, and it is shown that this method can effectively shorten the response time and solve the problems of existing IIoT networks, and that it can be efficiently applied to heterogeneous devices.",
            "corpus_id": 247486748,
            "sentences": [
                {
                    "corpus_id": "247486748",
                    "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing",
                    "text": "\"Knowledge distillation\" is a concept that first appeared in 2014 and that was an idea that was proposed by Geoffrey Hinton [30]. \"Knowledge distillation\" refers to a method of transferring knowledge from a large pretrained model (teacher model) to a small model to be used in practice (student model), in terms of the model distribution. In a deep learning model, the more parameters and the more computation, the better the feature extraction, and, accordingly, the performance of the model improves. However, since it uses a lot of memory and requires a lot of computing power, the efficiency is reduced in actual use cases. In other words, knowledge distillation is a method that is used to improve the performance of a small model by transferring the knowledge of a large model to a small model in the learning process, without changing the structure of the small model, so that the small model can perform as well as the large model. The architecture of the knowledge distillation model is shown in Figure 1. Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model. In classification tasks, neural networks generally use a Softmax output layer to transform the output into a probability, whereas knowledge distillation uses a smooth refinement of the Softmax. The Softmax equation of knowledge distillation is as follows: Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model.",
                    "score": 0.565325663008369,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 8710,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 338
                        },
                        {
                            "start": 339,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1014
                        },
                        {
                            "start": 1015,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1342
                        },
                        {
                            "start": 1343,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1738
                        },
                        {
                            "start": 1739,
                            "end": 1926
                        },
                        {
                            "start": 1927,
                            "end": 2128
                        },
                        {
                            "start": 2129,
                            "end": 2330
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 124,
                            "end": 128,
                            "matchedPaperCorpusId": "7200347"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6630859375
                }
            ],
            "relevance_judgement": 0.6630859375,
            "relevance_judgment_input_expanded": "# Title: Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing\n# Venue: Electronics\n# Authors: Seokju Oh, Donghyun Kim, Chae-Suk Lee, Jongpil Jeong\n## Abstract\nAlong with the fourth industrial revolution, smart factories are receiving a great deal of attention. Large volumes of real-time data that are generated at high rates, especially in industries, are becoming increasingly important. Accordingly, the Industrial Internet of Things (IIoT), which connects, controls, and communicates with heterogeneous devices, is important to industrial sites and is now indispensable. To ensure the fairness and quality of the IIoT with limited network resources, the network connection of the IIoT needs to be constructed more intelligently. Many studies are being conducted on the efficient use of the resources that are imposed on IIoT devices. Therefore, in this paper, we propose a collaboration optimization method for heterogeneous devices that is based on cloud\u2013fog\u2013edge architecture. First, this paper proposes a knowledge distillation-based algorithm that can collaborate on cloud\u2013fog\u2013edge computing on the basis of distributed control. Second, to compensate for the shortcomings of knowledge distillation, we propose a framework for combining a soft-label-based alarm level. Finally, the method that is proposed in this paper was verified through several experiments, and it is shown that this method can effectively shorten the response time and solve the problems of existing IIoT networks, and that it can be efficiently applied to heterogeneous devices.\n## Knowledge Distillation\n\"Knowledge distillation\" is a concept that first appeared in 2014 and that was an idea that was proposed by Geoffrey Hinton [30]. \"Knowledge distillation\" refers to a method of transferring knowledge from a large pretrained model (teacher model) to a small model to be used in practice (student model), in terms of the model distribution. In a deep learning model, the more parameters and the more computation, the better the feature extraction, and, accordingly, the performance of the model improves. However, since it uses a lot of memory and requires a lot of computing power, the efficiency is reduced in actual use cases. In other words, knowledge distillation is a method that is used to improve the performance of a small model by transferring the knowledge of a large model to a small model in the learning process, without changing the structure of the small model, so that the small model can perform as well as the large model. The architecture of the knowledge distillation model is shown in Figure 1. Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model. In classification tasks, neural networks generally use a Softmax output layer to transform the output into a probability, whereas knowledge distillation uses a smooth refinement of the Softmax. The Softmax equation of knowledge distillation is as follows: Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model.",
            "reference_string": "[247486748 | Oh et al. | 2022 | Citations: 1]"
        },
        {
            "title": "Crowd counting at the edge using weighted knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2025,
            "reference_count": 65,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41598-025-90750-5",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11978936, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115771635",
                    "name": "Muhammad Asif Khan"
                },
                {
                    "authorId": "2330247142",
                    "name": "Hamid Menouar"
                },
                {
                    "authorId": "2311725227",
                    "name": "Ridha Hamila"
                },
                {
                    "authorId": "2354319497",
                    "name": "Adnan Abu-Dayya"
                }
            ],
            "abstract": "Visual crowd counting has gained serious attention during the last couple of years. The consistent contributions to this topic have now solved several inherited challenges such as scale variations, occlusions, and cross-scene applications. However, these works attempt to improve accuracy and often ignore model size and computational complexity. Several practical applications employ resource-limited stand-alone devices like drones to run crowd models and require real-time inference. Though there have been some good efforts to develop lightweight shallow crowd models offering fast inference time, the relevant literature dedicated to lightweight crowd counting is limited. One possible reason is that lightweight deep-learning models suffer from accuracy degradation in complex scenes due to limited generalization capabilities. This paper addresses this important problem by proposing knowledge distillation to improve the learning capability of lightweight crowd models. Knowledge distillation enables lightweight models to emulate deeper models by distilling the knowledge learned by the deeper model during the training process. The paper presents a detailed experimental analysis with three lightweight crowd models over six benchmark datasets. The results report a clear significance of the proposed method supported by several ablation studies.",
            "corpus_id": 277632659,
            "sentences": [
                {
                    "corpus_id": "277632659",
                    "title": "Crowd counting at the edge using weighted knowledge distillation",
                    "text": "Knowledge distillation (KD) in neural networks was proposed by Hinton et al. in 5 which is inspired by the work 48 demonstrating that \"knowledge learned by a large ensemble of models can be transferred to a single model\". Hinton et al. 5 introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case. However, the method was limited to supervised learning. To extend the KD concept to any type of deep learning model, Romero et al. 6 proposed feature-based KD to learn the intermediate representation (activation maps). They proposed to match the intermediate layer outputs of the teacher and student models to guide the student model. Several works extended this model to propose different methods to match feature activation maps such as using attention maps 49 , neuron selectivity 50 , and factors 51 . Similar to the response-based KD in which the knowledge is distilled from the output layer, in feature-based KD the knowledge is distilled from a single intermediate layer. \n\nIn contrast, 7 proposed relations-based KD in which a Gram matrix is calculated as an inner product between the feature maps of two layers. The model has been further extended by 52 to learn from more than one teacher. \n\nAuthors in 53 highlighted that the transfer of knowledge is impacted by the difference in the capacity of the teacher and student models. Knowledge distillation from multiple teacher networks is proposed in 54 .",
                    "score": 0.5801159363881268,
                    "section_title": "Related work",
                    "char_start_offset": 9297,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 628
                        },
                        {
                            "start": 629,
                            "end": 850
                        },
                        {
                            "start": 851,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1031
                        },
                        {
                            "start": 1032,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1481
                        },
                        {
                            "start": 1482,
                            "end": 1654
                        },
                        {
                            "start": 1657,
                            "end": 1796
                        },
                        {
                            "start": 1797,
                            "end": 1875
                        },
                        {
                            "start": 1878,
                            "end": 2015
                        },
                        {
                            "start": 2016,
                            "end": 2089
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 80,
                            "end": 81,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 112,
                            "end": 114,
                            "matchedPaperCorpusId": "11253972"
                        },
                        {
                            "start": 236,
                            "end": 237,
                            "matchedPaperCorpusId": "7200347"
                        },
                        {
                            "start": 1477,
                            "end": 1479,
                            "matchedPaperCorpusId": "52012952"
                        },
                        {
                            "start": 1670,
                            "end": 1671,
                            "matchedPaperCorpusId": "206596723"
                        },
                        {
                            "start": 1836,
                            "end": 1838,
                            "matchedPaperCorpusId": "13745134"
                        },
                        {
                            "start": 1889,
                            "end": 1891,
                            "matchedPaperCorpusId": "212908749"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.658203125
                }
            ],
            "relevance_judgement": 0.658203125,
            "relevance_judgment_input_expanded": "# Title: Crowd counting at the edge using weighted knowledge distillation\n# Venue: Scientific Reports\n# Authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila, Adnan Abu-Dayya\n## Abstract\nVisual crowd counting has gained serious attention during the last couple of years. The consistent contributions to this topic have now solved several inherited challenges such as scale variations, occlusions, and cross-scene applications. However, these works attempt to improve accuracy and often ignore model size and computational complexity. Several practical applications employ resource-limited stand-alone devices like drones to run crowd models and require real-time inference. Though there have been some good efforts to develop lightweight shallow crowd models offering fast inference time, the relevant literature dedicated to lightweight crowd counting is limited. One possible reason is that lightweight deep-learning models suffer from accuracy degradation in complex scenes due to limited generalization capabilities. This paper addresses this important problem by proposing knowledge distillation to improve the learning capability of lightweight crowd models. Knowledge distillation enables lightweight models to emulate deeper models by distilling the knowledge learned by the deeper model during the training process. The paper presents a detailed experimental analysis with three lightweight crowd models over six benchmark datasets. The results report a clear significance of the proposed method supported by several ablation studies.\n## Related work\nKnowledge distillation (KD) in neural networks was proposed by Hinton et al. in 5 which is inspired by the work 48 demonstrating that \"knowledge learned by a large ensemble of models can be transferred to a single model\". Hinton et al. 5 introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case. However, the method was limited to supervised learning. To extend the KD concept to any type of deep learning model, Romero et al. 6 proposed feature-based KD to learn the intermediate representation (activation maps). They proposed to match the intermediate layer outputs of the teacher and student models to guide the student model. Several works extended this model to propose different methods to match feature activation maps such as using attention maps 49 , neuron selectivity 50 , and factors 51 . Similar to the response-based KD in which the knowledge is distilled from the output layer, in feature-based KD the knowledge is distilled from a single intermediate layer. \n\nIn contrast, 7 proposed relations-based KD in which a Gram matrix is calculated as an inner product between the feature maps of two layers. The model has been further extended by 52 to learn from more than one teacher. \n\nAuthors in 53 highlighted that the transfer of knowledge is impacted by the difference in the capacity of the teacher and student models. Knowledge distillation from multiple teacher networks is proposed in 54 .",
            "reference_string": "[277632659 | Khan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 18,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18538, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2342923579",
                    "name": "Amna Murtada"
                },
                {
                    "authorId": "88856585",
                    "name": "Omnia Abdelrhman"
                },
                {
                    "authorId": "30665494",
                    "name": "T. Attia"
                }
            ],
            "abstract": "Facial Emotion Recognition has emerged as increasingly pivotal in the domain of User Experience, notably within modern usability testing, as it facilitates a deeper comprehension of user satisfaction and engagement. This study aims to extend the ResEmoteNet model by employing a knowledge distillation framework to develop Mini-ResEmoteNet models - lightweight student models - tailored for usability testing. Experiments were conducted on the FER2013 and RAF-DB datasets to assess the efficacy of three student model architectures: Student Model A, Student Model B, and Student Model C. Their development involves reducing the number of feature channels in each layer of the teacher model by approximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the FER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%, marking a 0.21% absolute improvement over EmoNeXt. Moreover, the results exhibit absolute improvements in terms of inference speed and memory usage during inference compared to the ResEmoteNet model. The findings indicate that the proposed methods surpass other state-of-the-art approaches.",
            "corpus_id": 275993890,
            "sentences": [
                {
                    "corpus_id": "275993890",
                    "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design",
                    "text": "Employing ResEmoteNet as a teacher model within the framework of knowledge distillation, the methodology prioritizes the extraction of hard labels and soft predictions from the teacher model, culminating in a composite of distillation loss functions. The input data is subsequently propagated through the student model for processing. Fig. 2 illustrates the proposed method. \n\nIn the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training. Where  , is the \"softened\" probability distribution of the teacher model and  , is the \"softened\" probability distribution of the student model. In the context of Hard Label Learning, the Cross-entropy loss is employed to guarantee that the student model sustains its accuracy concerning the primary task while preserving direct supervision through ground truth labels; this loss can be calculated between the predictions of the student model and the ground truth labels. \n\nWhere C is the number of classes,  , is the ground truth label for class c (1 for the correct class, 0 otherwise), ` , is the predicted probability (softmax output) for class c for sample i. Then the total loss function is a weighted combination of two components:  =  +  \u00d7  2 \u00d7  (6) Where \u03b1 is the weight for hard label loss, \u03b2 is the weight for distillation loss, T is the temperature parameter, CrossEntropyLoss is Cross-entropy loss with hard labels and KLDivLoss is KL divergence loss between softened predictions.",
                    "score": 0.5707056986401122,
                    "section_title": "B. Our Knowledge Distillation Method:",
                    "char_start_offset": 6372,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 250
                        },
                        {
                            "start": 251,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 374
                        },
                        {
                            "start": 377,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1022
                        },
                        {
                            "start": 1023,
                            "end": 1167
                        },
                        {
                            "start": 1168,
                            "end": 1494
                        },
                        {
                            "start": 1497,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 2016
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.654296875
                }
            ],
            "relevance_judgement": 0.654296875,
            "relevance_judgment_input_expanded": "# Title: Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design\n# Venue: arXiv.org\n# Authors: Amna Murtada, Omnia Abdelrhman, T. Attia\n## Abstract\nFacial Emotion Recognition has emerged as increasingly pivotal in the domain of User Experience, notably within modern usability testing, as it facilitates a deeper comprehension of user satisfaction and engagement. This study aims to extend the ResEmoteNet model by employing a knowledge distillation framework to develop Mini-ResEmoteNet models - lightweight student models - tailored for usability testing. Experiments were conducted on the FER2013 and RAF-DB datasets to assess the efficacy of three student model architectures: Student Model A, Student Model B, and Student Model C. Their development involves reducing the number of feature channels in each layer of the teacher model by approximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the FER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%, marking a 0.21% absolute improvement over EmoNeXt. Moreover, the results exhibit absolute improvements in terms of inference speed and memory usage during inference compared to the ResEmoteNet model. The findings indicate that the proposed methods surpass other state-of-the-art approaches.\n## B. Our Knowledge Distillation Method:\nEmploying ResEmoteNet as a teacher model within the framework of knowledge distillation, the methodology prioritizes the extraction of hard labels and soft predictions from the teacher model, culminating in a composite of distillation loss functions. The input data is subsequently propagated through the student model for processing. Fig. 2 illustrates the proposed method. \n\nIn the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training. Where  , is the \"softened\" probability distribution of the teacher model and  , is the \"softened\" probability distribution of the student model. In the context of Hard Label Learning, the Cross-entropy loss is employed to guarantee that the student model sustains its accuracy concerning the primary task while preserving direct supervision through ground truth labels; this loss can be calculated between the predictions of the student model and the ground truth labels. \n\nWhere C is the number of classes,  , is the ground truth label for class c (1 for the correct class, 0 otherwise), ` , is the predicted probability (softmax output) for class c for sample i. Then the total loss function is a weighted combination of two components:  =  +  \u00d7  2 \u00d7  (6) Where \u03b1 is the weight for hard label loss, \u03b2 is the weight for distillation loss, T is the temperature parameter, CrossEntropyLoss is Cross-entropy loss with hard labels and KLDivLoss is KL divergence loss between softened predictions.",
            "reference_string": "[275993890 | Murtada et al. | 2025 | Citations: 1]"
        },
        {
            "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/s24227322",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11598830, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311724069",
                    "name": "Hamada Rizk"
                },
                {
                    "authorId": "2677103",
                    "name": "Ahmed M. Elmogy"
                },
                {
                    "authorId": "145195785",
                    "name": "Mohamed Rihan"
                },
                {
                    "authorId": "2171271984",
                    "name": "Hirozumi Yamaguchi"
                }
            ],
            "abstract": "User location has emerged as a pivotal factor in human-centered environments, driving applications like tracking, navigation, healthcare, and emergency response that align with Sustainable Development Goals (SDGs). However, accurate indoor localization remains challenging due to the limitations of GPS in indoor settings, where signal interference and reflections disrupt satellite connections. While Received Signal Strength Indicator (RSSI) methods are commonly employed, they are affected by environmental noise, multipath fading, and signal interference. Round-Trip Time (RTT)-based localization techniques provide a more resilient alternative but are not universally supported across access points due to infrastructure limitations. To address these challenges, we introduce DistilLoc: a cross-knowledge distillation framework that transfers knowledge from an RTT-based teacher model to an RSSI-based student model. By applying a teacher\u2013student architecture, where the RTT model (teacher) trains the RSSI model (student), DistilLoc enhances RSSI-based localization with the accuracy and robustness of RTT without requiring RTT data during deployment. At the core of DistilLoc, the FNet architecture is employed for its computational efficiency and capacity to capture complex relationships among RSSI signals from multiple access points. This enables the student model to learn a robust mapping from RSSI measurements to precise location estimates, reducing computational demands while improving scalability. Evaluation in two cluttered indoor environments of varying sizes using Android devices and Google WiFi access points, DistilLoc achieved sub-meter localization accuracy, with median errors of 0.42 m and 0.32 m, respectively, demonstrating improvements of 267% over conventional RSSI methods and 496% over multilateration-based approaches. These results validate DistilLoc as a scalable, accurate solution for indoor localization, enabling intelligent, resource-efficient urban environments that contribute to SDG 9 (Industry, Innovation, and Infrastructure) and SDG 11 (Sustainable Cities and Communities).",
            "corpus_id": 274269913,
            "sentences": [
                {
                    "corpus_id": "274269913",
                    "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation",
                    "text": "Knowledge distillation (KD) is a powerful technique for transferring knowledge from a larger, more complex model (the teacher) to a smaller, more efficient model (the student). The goal is to retain the performance benefits of the larger model while reducing the computational complexity, enabling deployment in resource-constrained environments. \n\nIn the knowledge distillation framework, let x \u2208 R d denote the input data, and let y \u2208 {1, 2, . . . , C} represent the corresponding true labels, where C is the number of classes. The teacher model is a pre-trained model with parameters \u03b8 T , and it outputs a set of logits z T = f T (x; \u03b8 T ). These logits are then passed through a softmax function to obtain the predicted probability distribution: \n\nSimilarly, the student model, with parameters \u03b8 S , produces its own set of logits z S = f S (x; \u03b8 S ) and corresponding probability distribution: \n\nThe central idea in knowledge distillation is to train the student model such that its output distribution p S closely matches the teacher's output distribution p T , while also ensuring that the student performs well on the actual classification task. \n\nA key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from. \n\nThe loss function used to train the student model is a weighted combination of two components: the supervised loss and the distillation loss. The supervised cross-entropy loss ensures that the student model performs well on the actual classification task by minimizing the cross-entropy between the true labels, denoted as y, and the student's predictions, p S . This loss is calculated as follows: \n\nwhere y i represents the one-hot encoded true label. In addition to this, the distillation loss encourages the student model to mimic the softened output distribution of the teacher model.",
                    "score": 0.5432051829267694,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 14052,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 177,
                            "end": 346
                        },
                        {
                            "start": 349,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 529
                        },
                        {
                            "start": 530,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 750
                        },
                        {
                            "start": 753,
                            "end": 899
                        },
                        {
                            "start": 902,
                            "end": 1154
                        },
                        {
                            "start": 1157,
                            "end": 1327
                        },
                        {
                            "start": 1328,
                            "end": 1401
                        },
                        {
                            "start": 1404,
                            "end": 1471
                        },
                        {
                            "start": 1472,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1649
                        },
                        {
                            "start": 1650,
                            "end": 1781
                        },
                        {
                            "start": 1784,
                            "end": 1925
                        },
                        {
                            "start": 1926,
                            "end": 2146
                        },
                        {
                            "start": 2147,
                            "end": 2182
                        },
                        {
                            "start": 2185,
                            "end": 2237
                        },
                        {
                            "start": 2238,
                            "end": 2373
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6533203125
                }
            ],
            "relevance_judgement": 0.6533203125,
            "relevance_judgment_input_expanded": "# Title: A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation\n# Venue: Italian National Conference on Sensors\n# Authors: Hamada Rizk, Ahmed M. Elmogy, Mohamed Rihan, Hirozumi Yamaguchi\n## Abstract\nUser location has emerged as a pivotal factor in human-centered environments, driving applications like tracking, navigation, healthcare, and emergency response that align with Sustainable Development Goals (SDGs). However, accurate indoor localization remains challenging due to the limitations of GPS in indoor settings, where signal interference and reflections disrupt satellite connections. While Received Signal Strength Indicator (RSSI) methods are commonly employed, they are affected by environmental noise, multipath fading, and signal interference. Round-Trip Time (RTT)-based localization techniques provide a more resilient alternative but are not universally supported across access points due to infrastructure limitations. To address these challenges, we introduce DistilLoc: a cross-knowledge distillation framework that transfers knowledge from an RTT-based teacher model to an RSSI-based student model. By applying a teacher\u2013student architecture, where the RTT model (teacher) trains the RSSI model (student), DistilLoc enhances RSSI-based localization with the accuracy and robustness of RTT without requiring RTT data during deployment. At the core of DistilLoc, the FNet architecture is employed for its computational efficiency and capacity to capture complex relationships among RSSI signals from multiple access points. This enables the student model to learn a robust mapping from RSSI measurements to precise location estimates, reducing computational demands while improving scalability. Evaluation in two cluttered indoor environments of varying sizes using Android devices and Google WiFi access points, DistilLoc achieved sub-meter localization accuracy, with median errors of 0.42 m and 0.32 m, respectively, demonstrating improvements of 267% over conventional RSSI methods and 496% over multilateration-based approaches. These results validate DistilLoc as a scalable, accurate solution for indoor localization, enabling intelligent, resource-efficient urban environments that contribute to SDG 9 (Industry, Innovation, and Infrastructure) and SDG 11 (Sustainable Cities and Communities).\n## Knowledge Distillation\nKnowledge distillation (KD) is a powerful technique for transferring knowledge from a larger, more complex model (the teacher) to a smaller, more efficient model (the student). The goal is to retain the performance benefits of the larger model while reducing the computational complexity, enabling deployment in resource-constrained environments. \n\nIn the knowledge distillation framework, let x \u2208 R d denote the input data, and let y \u2208 {1, 2, . . . , C} represent the corresponding true labels, where C is the number of classes. The teacher model is a pre-trained model with parameters \u03b8 T , and it outputs a set of logits z T = f T (x; \u03b8 T ). These logits are then passed through a softmax function to obtain the predicted probability distribution: \n\nSimilarly, the student model, with parameters \u03b8 S , produces its own set of logits z S = f S (x; \u03b8 S ) and corresponding probability distribution: \n\nThe central idea in knowledge distillation is to train the student model such that its output distribution p S closely matches the teacher's output distribution p T , while also ensuring that the student performs well on the actual classification task. \n\nA key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from. \n\nThe loss function used to train the student model is a weighted combination of two components: the supervised loss and the distillation loss. The supervised cross-entropy loss ensures that the student model performs well on the actual classification task by minimizing the cross-entropy between the true labels, denoted as y, and the student's predictions, p S . This loss is calculated as follows: \n\nwhere y i represents the one-hot encoded true label. In addition to this, the distillation loss encourages the student model to mimic the softened output distribution of the teacher model.",
            "reference_string": "[274269913 | Rizk et al. | 2024 | Citations: 2]"
        },
        {
            "title": "A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition",
            "venue": "Journal of Imaging",
            "year": 2023,
            "reference_count": 78,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2313-433X/9/4/82/pdf?version=1681454532",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC10142350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1491632645",
                    "name": "Hayat Ullah"
                },
                {
                    "authorId": "1748235",
                    "name": "Arslan Munir"
                }
            ],
            "abstract": "Human action recognition has been actively explored over the past two decades to further advancements in video analytics domain. Numerous research studies have been conducted to investigate the complex sequential patterns of human actions in video streams. In this paper, we propose a knowledge distillation framework, which distills spatio-temporal knowledge from a large teacher model to a lightweight student model using an offline knowledge distillation technique. The proposed offline knowledge distillation framework takes two models: a large pre-trained 3DCNN (three-dimensional convolutional neural network) teacher model and a lightweight 3DCNN student model (i.e., the teacher model is pre-trained on the same dataset on which the student model is to be trained on). During offline knowledge distillation training, the distillation algorithm trains only the student model to help enable the student model to achieve the same level of prediction accuracy as the teacher model. To evaluate the performance of the proposed method, we conduct extensive experiments on four benchmark human action datasets. The obtained quantitative results verify the efficiency and robustness of the proposed method over the state-of-the-art human action recognition methods by obtaining up to 35% improvement in accuracy over existing methods. Furthermore, we evaluate the inference time of the proposed method and compare the obtained results with the inference time of the state-of-the-art methods. Experimental results reveal that the proposed method attains an improvement of up to 50\u00d7 in terms of frames per seconds (FPS) over the state-of-the-art methods. The short inference time and high accuracy make our proposed framework suitable for human activity recognition in real-time applications.",
            "corpus_id": 258156357,
            "sentences": [
                {
                    "corpus_id": "258156357",
                    "title": "A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition",
                    "text": "Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy. The student loss as given in Equation (3) computes the difference between the student's prediction and the ground truth based on which the student model then generates the output class probabilities using standard softmax function as given in Equation ( 4). The reason for using standard softmax function instead of the normalized softmax function is that the student loss computes the difference between the predictions made by the student and the ground truth. \n\nHere in Equation ( 3), the variable h represents the final predictions for a specific class (derived from the student probability vector) and the variable y denotes the ground truth. The final loss function is the weighted sum of the distillation loss and the student loss as given in Equation ( 5) \n\nwhere Student_Loss and Distillation_Loss denote the student loss and the mutual distillation loss of the teacher and the student model, respectively, and the variable \u03b1 is the weight factor of the final loss that defines contribution of the student and the distillation loss in the final loss.",
                    "score": 0.5501498647227242,
                    "section_title": "Spatio-Temporal Knowledge Distillation from Teacher to Student Model",
                    "char_start_offset": 31830,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 891
                        },
                        {
                            "start": 892,
                            "end": 1006
                        },
                        {
                            "start": 1007,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1469
                        },
                        {
                            "start": 1472,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1770
                        },
                        {
                            "start": 1773,
                            "end": 2066
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6474609375
                }
            ],
            "relevance_judgement": 0.6474609375,
            "relevance_judgment_input_expanded": "# Title: A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition\n# Venue: Journal of Imaging\n# Authors: Hayat Ullah, Arslan Munir\n## Abstract\nHuman action recognition has been actively explored over the past two decades to further advancements in video analytics domain. Numerous research studies have been conducted to investigate the complex sequential patterns of human actions in video streams. In this paper, we propose a knowledge distillation framework, which distills spatio-temporal knowledge from a large teacher model to a lightweight student model using an offline knowledge distillation technique. The proposed offline knowledge distillation framework takes two models: a large pre-trained 3DCNN (three-dimensional convolutional neural network) teacher model and a lightweight 3DCNN student model (i.e., the teacher model is pre-trained on the same dataset on which the student model is to be trained on). During offline knowledge distillation training, the distillation algorithm trains only the student model to help enable the student model to achieve the same level of prediction accuracy as the teacher model. To evaluate the performance of the proposed method, we conduct extensive experiments on four benchmark human action datasets. The obtained quantitative results verify the efficiency and robustness of the proposed method over the state-of-the-art human action recognition methods by obtaining up to 35% improvement in accuracy over existing methods. Furthermore, we evaluate the inference time of the proposed method and compare the obtained results with the inference time of the state-of-the-art methods. Experimental results reveal that the proposed method attains an improvement of up to 50\u00d7 in terms of frames per seconds (FPS) over the state-of-the-art methods. The short inference time and high accuracy make our proposed framework suitable for human activity recognition in real-time applications.\n## Spatio-Temporal Knowledge Distillation from Teacher to Student Model\nHere, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy. The student loss as given in Equation (3) computes the difference between the student's prediction and the ground truth based on which the student model then generates the output class probabilities using standard softmax function as given in Equation ( 4). The reason for using standard softmax function instead of the normalized softmax function is that the student loss computes the difference between the predictions made by the student and the ground truth. \n\nHere in Equation ( 3), the variable h represents the final predictions for a specific class (derived from the student probability vector) and the variable y denotes the ground truth. The final loss function is the weighted sum of the distillation loss and the student loss as given in Equation ( 5) \n\nwhere Student_Loss and Distillation_Loss denote the student loss and the mutual distillation loss of the teacher and the student model, respectively, and the variable \u03b1 is the weight factor of the final loss that defines contribution of the student and the distillation loss in the final loss.",
            "reference_string": "[258156357 | Ullah et al. | 2023 | Citations: 3]"
        },
        {
            "title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 42,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.03555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35369096",
                    "name": "Roy Henha Eyono"
                },
                {
                    "authorId": "2264326",
                    "name": "Fabio Maria Carlucci"
                },
                {
                    "authorId": "27245978",
                    "name": "P. Esperan\u00e7a"
                },
                {
                    "authorId": "88739736",
                    "name": "Binxin Ru"
                },
                {
                    "authorId": "73007356",
                    "name": "Phillip Torr"
                }
            ],
            "abstract": "State-of-the-art results in deep learning have been improving steadily, in good part due to the use of larger models. However, widespread use is constrained by device hardware limitations, resulting in a substantial performance gap between state-of-the-art models and those that can be effectively deployed on small devices. While Knowledge Distillation (KD) theoretically enables small student models to emulate larger teacher models, in practice selecting a good student architecture requires considerable human expertise. Neural Architecture Search (NAS) appears as a natural solution to this problem but most approaches can be inefficient, as most of the computation is spent comparing architectures sampled from the same distribution, with negligible differences in performance. In this paper, we propose to instead search for a family of student architectures sharing the property of being good at learning from a given teacher. Our approach AutoKD, powered by Bayesian Optimization, explores a flexible graph-based search space, enabling us to automatically learn the optimal student architecture distribution and KD parameters, while being 20x more sample efficient compared to existing state-of-the-art. We evaluate our method on 3 datasets; on large images specifically, we reach the teacher performance while using 3x less memory and 10x less parameters. Finally, while AutoKD uses the traditional KD loss, it outperforms more advanced KD variants using hand-designed students.",
            "corpus_id": 243832889,
            "sentences": [
                {
                    "corpus_id": "243832889",
                    "title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family",
                    "text": "Knowledge Distillation (KD; Hinton et al., 2015b) is a method to transfer, or distill, knowledge from one model to another-usually from a large model to small one-such that the small student model learns to emulate the performance of the large teacher model. KD can be formalized as minimizing the objective function: \n\nwhere l is the loss function that measures the difference in performance between the teacher f T and the student f S , and x i is the ith input. The conventional loss function l used in practice is a linear combination of the traditional cross entropy loss L CE and the Kullback-Leibler divergence L KL of the pre-softmax outputs for f T and f S : \n\nwhere \u03c3 is the softmax function \u03c3(x) = 1/(1 + exp(\u2212x)), and \u03c4 is the softmax temperature. Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1. \n\nThe parameter \u03b1 represents the weight trade-off between the KL loss and the cross entropy loss L CE . end while 11: end for 12: Obtain the best performing configuration \u03b8 * for the student network generator. 13: Sample k architectures from G(\u03b8 * ), train them to completion, and obtain test performance. \n\nThe L KD loss is characterized by the hyper-parameters: \u03b1 and \u03c4 ; popular choices are \u03c4 \u2208 {3, 4, 5} and \u03b1 = 0.9 (Huang & Wang, 2017;Zagoruyko & Komodakis, 2016;Zhu et al., 2018). Numerous other methods (Polino et al., 2018;Huang & Wang, 2017;Tung & Mori, 2019) can be formulated as a form of Equation ( 2), but in this paper we use the conventional loss function l. \n\nTraditionally in KD, both the teacher and the student network have predefined architectures. In contrast, AutoKD defines a search space of student network architectures and finds the optimal student by leveraging neural architecture search, as detailed below.",
                    "score": 0.5444693937194403,
                    "section_title": "KNOWLEDGE DISTILLATION",
                    "char_start_offset": 8368,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 667
                        },
                        {
                            "start": 670,
                            "end": 759
                        },
                        {
                            "start": 760,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1067
                        },
                        {
                            "start": 1068,
                            "end": 1163
                        },
                        {
                            "start": 1166,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1531
                        },
                        {
                            "start": 1534,
                            "end": 1626
                        },
                        {
                            "start": 1627,
                            "end": 1793
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1326,
                            "end": 1343,
                            "matchedPaperCorpusId": "48352434"
                        },
                        {
                            "start": 1408,
                            "end": 1425,
                            "matchedPaperCorpusId": "198179476"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.64404296875
                }
            ],
            "relevance_judgement": 0.64404296875,
            "relevance_judgment_input_expanded": "# Title: AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family\n# Venue: arXiv.org\n# Authors: Roy Henha Eyono, Fabio Maria Carlucci, P. Esperan\u00e7a, Binxin Ru, Phillip Torr\n## Abstract\nState-of-the-art results in deep learning have been improving steadily, in good part due to the use of larger models. However, widespread use is constrained by device hardware limitations, resulting in a substantial performance gap between state-of-the-art models and those that can be effectively deployed on small devices. While Knowledge Distillation (KD) theoretically enables small student models to emulate larger teacher models, in practice selecting a good student architecture requires considerable human expertise. Neural Architecture Search (NAS) appears as a natural solution to this problem but most approaches can be inefficient, as most of the computation is spent comparing architectures sampled from the same distribution, with negligible differences in performance. In this paper, we propose to instead search for a family of student architectures sharing the property of being good at learning from a given teacher. Our approach AutoKD, powered by Bayesian Optimization, explores a flexible graph-based search space, enabling us to automatically learn the optimal student architecture distribution and KD parameters, while being 20x more sample efficient compared to existing state-of-the-art. We evaluate our method on 3 datasets; on large images specifically, we reach the teacher performance while using 3x less memory and 10x less parameters. Finally, while AutoKD uses the traditional KD loss, it outperforms more advanced KD variants using hand-designed students.\n## KNOWLEDGE DISTILLATION\nKnowledge Distillation (KD; Hinton et al., 2015b) is a method to transfer, or distill, knowledge from one model to another-usually from a large model to small one-such that the small student model learns to emulate the performance of the large teacher model. KD can be formalized as minimizing the objective function: \n\nwhere l is the loss function that measures the difference in performance between the teacher f T and the student f S , and x i is the ith input. The conventional loss function l used in practice is a linear combination of the traditional cross entropy loss L CE and the Kullback-Leibler divergence L KL of the pre-softmax outputs for f T and f S : \n\nwhere \u03c3 is the softmax function \u03c3(x) = 1/(1 + exp(\u2212x)), and \u03c4 is the softmax temperature. Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1. \n\nThe parameter \u03b1 represents the weight trade-off between the KL loss and the cross entropy loss L CE . end while 11: end for 12: Obtain the best performing configuration \u03b8 * for the student network generator. 13: Sample k architectures from G(\u03b8 * ), train them to completion, and obtain test performance. \n\nThe L KD loss is characterized by the hyper-parameters: \u03b1 and \u03c4 ; popular choices are \u03c4 \u2208 {3, 4, 5} and \u03b1 = 0.9 (Huang & Wang, 2017;Zagoruyko & Komodakis, 2016;Zhu et al., 2018). Numerous other methods (Polino et al., 2018;Huang & Wang, 2017;Tung & Mori, 2019) can be formulated as a form of Equation ( 2), but in this paper we use the conventional loss function l. \n\nTraditionally in KD, both the teacher and the student network have predefined architectures. In contrast, AutoKD defines a search space of student network architectures and finds the optimal student by leveraging neural architecture search, as detailed below.",
            "reference_string": "[243832889 | Eyono et al. | 2021 | Citations: 3]"
        },
        {
            "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 15,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.17574, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281945399",
                    "name": "Tokiniaina Raharison Ralambomihanta"
                },
                {
                    "authorId": "2281945133",
                    "name": "Shahrad Mohammadzadeh"
                },
                {
                    "authorId": "2282084764",
                    "name": "Mohammad Sami Nur Islam"
                },
                {
                    "authorId": "2281945352",
                    "name": "Wassim Jabbour"
                },
                {
                    "authorId": "2282098962",
                    "name": "Laurence Liang"
                }
            ],
            "abstract": "The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.",
            "corpus_id": 267335043,
            "sentences": [
                {
                    "corpus_id": "267335043",
                    "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
                    "text": "Knowledge distillation in neural networks (Hinton et al., 2015) involves transferring information from a larger, more complex model to a smaller one while minimizing information loss. This method extends to both compressing a single larger model and consolidating insights from multiple models (ensemble) into a singular one. \n\nDistillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning. \n\nThe training involves a weighted average of two objective functions: the first part is the Kullback-Leibler divergence with the soft targets (at higher temperature). The second part is the cross entropy loss with correct labels (at temperature 1). \n\nThis methodology allows the distilled model to effectively learn from both the nuanced information present in the soft targets generated by the larger model and the precise ground truth labels, resulting in a more compact yet knowledgeable model. \n\nOne notable example of distillation in LLMs is the Distil-BERT model: DistilBERT is 40% smaller than its parent model BERT, 60% faster than its parent model, and yet retains 97% of BERT's language capabilities. (Sanh et al., 2020) 2.4. Progressive Knowledge Transfer. \n\nWhen distillation is implemented on large models, there is a risk that knowledge transfer is not optimally passed on from the teacher model to the student model due to differences between the architectures of the teacher and student models. One approach to maximize knowledge transfer is progressive knowledge transfer: the student model is first trained only on the inputs and outputs of the first encoder block, and the student model then subsequently trains the output of the next encoder block while freezing the previous trained blocks. (Sun et al., 2020) In our case, encoder blocks are replaced by decoders as the architecture is autoregressive. (Fig. 2) \n\n3. Methods",
                    "score": 0.6233808568676624,
                    "section_title": "Distillation",
                    "char_start_offset": 6794,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 325
                        },
                        {
                            "start": 328,
                            "end": 443
                        },
                        {
                            "start": 444,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 707
                        },
                        {
                            "start": 710,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 957
                        },
                        {
                            "start": 960,
                            "end": 1206
                        },
                        {
                            "start": 1209,
                            "end": 1419
                        },
                        {
                            "start": 1420,
                            "end": 1444
                        },
                        {
                            "start": 1445,
                            "end": 1476
                        },
                        {
                            "start": 1479,
                            "end": 1719
                        },
                        {
                            "start": 1720,
                            "end": 2020
                        },
                        {
                            "start": 2021,
                            "end": 2131
                        },
                        {
                            "start": 2132,
                            "end": 2140
                        },
                        {
                            "start": 2143,
                            "end": 2153
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6435546875
                }
            ],
            "relevance_judgement": 0.6435546875,
            "relevance_judgment_input_expanded": "# Title: Scavenging Hyena: Distilling Transformers into Long Convolution Models\n# Venue: arXiv.org\n# Authors: Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, Laurence Liang\n## Abstract\nThe rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.\n## Distillation\nKnowledge distillation in neural networks (Hinton et al., 2015) involves transferring information from a larger, more complex model to a smaller one while minimizing information loss. This method extends to both compressing a single larger model and consolidating insights from multiple models (ensemble) into a singular one. \n\nDistillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning. \n\nThe training involves a weighted average of two objective functions: the first part is the Kullback-Leibler divergence with the soft targets (at higher temperature). The second part is the cross entropy loss with correct labels (at temperature 1). \n\nThis methodology allows the distilled model to effectively learn from both the nuanced information present in the soft targets generated by the larger model and the precise ground truth labels, resulting in a more compact yet knowledgeable model. \n\nOne notable example of distillation in LLMs is the Distil-BERT model: DistilBERT is 40% smaller than its parent model BERT, 60% faster than its parent model, and yet retains 97% of BERT's language capabilities. (Sanh et al., 2020) 2.4. Progressive Knowledge Transfer. \n\nWhen distillation is implemented on large models, there is a risk that knowledge transfer is not optimally passed on from the teacher model to the student model due to differences between the architectures of the teacher and student models. One approach to maximize knowledge transfer is progressive knowledge transfer: the student model is first trained only on the inputs and outputs of the first encoder block, and the student model then subsequently trains the output of the next encoder block while freezing the previous trained blocks. (Sun et al., 2020) In our case, encoder blocks are replaced by decoders as the architecture is autoregressive. (Fig. 2) \n\n3. Methods",
            "reference_string": "[267335043 | Ralambomihanta et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.05449, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301005855",
                    "name": "Gang Hu"
                },
                {
                    "authorId": "2300397509",
                    "name": "Ming Gu"
                }
            ],
            "abstract": "Investment portfolios, central to finance, balance potential returns and risks. This paper introduces a hybrid approach combining Markowitz's portfolio theory with reinforcement learning, utilizing knowledge distillation for training agents. In particular, our proposed method, called KDD (Knowledge Distillation DDPG), consist of two training stages: supervised and reinforcement learning stages. The trained agents optimize portfolio assembly. A comparative analysis against standard financial models and AI frameworks, using metrics like returns, the Sharpe ratio, and nine evaluation indices, reveals our model's superiority. It notably achieves the highest yield and Sharpe ratio of 2.03, ensuring top profitability with the lowest risk in comparable return scenarios.",
            "corpus_id": 269635406,
            "sentences": [
                {
                    "corpus_id": "269635406",
                    "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management",
                    "text": "Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution.\n\nThe student model learns to match both true labels and softened probabilities from the teacher.The objective function combines cross-entropy loss with respect to true labels and an additional term for the softened probabilities:\n\nwhere H is cross-entropy, y are true labels, \u03c3 is the softmax function, and \u03bb balances the two terms.Knowledge distillation's efficiency is practical for scenarios requiring lightweight models, such as mobile or embedded systems.It enables complex model emulation with remarkable efficiency, broadening access to advanced machine learning for various applications, including financial investments.",
                    "score": 0.5970264166437512,
                    "section_title": "Knowledge Distillation",
                    "char_start_offset": 10828,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 211,
                            "end": 263
                        },
                        {
                            "start": 265,
                            "end": 360
                        },
                        {
                            "start": 360,
                            "end": 493
                        },
                        {
                            "start": 495,
                            "end": 596
                        },
                        {
                            "start": 596,
                            "end": 724
                        },
                        {
                            "start": 724,
                            "end": 892
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6298828125
                }
            ],
            "relevance_judgement": 0.6298828125,
            "relevance_judgment_input_expanded": "# Title: Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management\n# Venue: arXiv.org\n# Authors: Gang Hu, Ming Gu\n## Abstract\nInvestment portfolios, central to finance, balance potential returns and risks. This paper introduces a hybrid approach combining Markowitz's portfolio theory with reinforcement learning, utilizing knowledge distillation for training agents. In particular, our proposed method, called KDD (Knowledge Distillation DDPG), consist of two training stages: supervised and reinforcement learning stages. The trained agents optimize portfolio assembly. A comparative analysis against standard financial models and AI frameworks, using metrics like returns, the Sharpe ratio, and nine evaluation indices, reveals our model's superiority. It notably achieves the highest yield and Sharpe ratio of 2.03, ensuring top profitability with the lowest risk in comparable return scenarios.\n## Knowledge Distillation\nKnowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution.\n\nThe student model learns to match both true labels and softened probabilities from the teacher.The objective function combines cross-entropy loss with respect to true labels and an additional term for the softened probabilities:\n\nwhere H is cross-entropy, y are true labels, \u03c3 is the softmax function, and \u03bb balances the two terms.Knowledge distillation's efficiency is practical for scenarios requiring lightweight models, such as mobile or embedded systems.It enables complex model emulation with remarkable efficiency, broadening access to advanced machine learning for various applications, including financial investments.",
            "reference_string": "[269635406 | Hu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
            "venue": "European Conference on Computer Vision",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.18565, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2180087917",
                    "name": "Yaomin Huang"
                },
                {
                    "authorId": "2257133494",
                    "name": "Zaoming Yan"
                },
                {
                    "authorId": "2242115637",
                    "name": "Chaomin Shen"
                },
                {
                    "authorId": "152786529",
                    "name": "Faming Fang"
                },
                {
                    "authorId": "2323414943",
                    "name": "Guixu Zhang"
                }
            ],
            "abstract": "Knowledge distillation (KD), known for its ability to transfer knowledge from a cumbersome network (teacher) to a lightweight one (student) without altering the architecture, has been garnering increasing attention. Two primary categories emerge within KD methods: feature-based, focusing on intermediate layers' features, and logits-based, targeting the final layer's logits. This paper introduces a novel perspective by leveraging diverse knowledge sources within a unified KD framework. Specifically, we aggregate features from intermediate layers into a comprehensive representation, effectively gathering semantic information from different stages and scales. Subsequently, we predict the distribution parameters from this representation. These steps transform knowledge from the intermediate layers into corresponding distributive forms, thereby allowing for knowledge distillation through a unified distribution constraint at different stages of the network, ensuring the comprehensiveness and coherence of knowledge transfer. Numerous experiments were conducted to validate the effectiveness of the proposed method.",
            "corpus_id": 272969060,
            "sentences": [
                {
                    "corpus_id": "272969060",
                    "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
                    "text": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
                    "score": 0.5514709252116656,
                    "section_title": "Preliminaries",
                    "char_start_offset": 8573,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 356
                        },
                        {
                            "start": 357,
                            "end": 415
                        },
                        {
                            "start": 418,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 623
                        },
                        {
                            "start": 626,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 789
                        },
                        {
                            "start": 792,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1565
                        },
                        {
                            "start": 1566,
                            "end": 1803
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.625
                }
            ],
            "relevance_judgement": 0.625,
            "relevance_judgment_input_expanded": "# Title: Harmonizing knowledge Transfer in Neural Network with Unified Distillation\n# Venue: European Conference on Computer Vision\n# Authors: Yaomin Huang, Zaoming Yan, Chaomin Shen, Faming Fang, Guixu Zhang\n## Abstract\nKnowledge distillation (KD), known for its ability to transfer knowledge from a cumbersome network (teacher) to a lightweight one (student) without altering the architecture, has been garnering increasing attention. Two primary categories emerge within KD methods: feature-based, focusing on intermediate layers' features, and logits-based, targeting the final layer's logits. This paper introduces a novel perspective by leveraging diverse knowledge sources within a unified KD framework. Specifically, we aggregate features from intermediate layers into a comprehensive representation, effectively gathering semantic information from different stages and scales. Subsequently, we predict the distribution parameters from this representation. These steps transform knowledge from the intermediate layers into corresponding distributive forms, thereby allowing for knowledge distillation through a unified distribution constraint at different stages of the network, ensuring the comprehensiveness and coherence of knowledge transfer. Numerous experiments were conducted to validate the effectiveness of the proposed method.\n## Preliminaries\nLogits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
            "reference_string": "[272969060 | Huang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 55,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/09991956.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2022.3230596?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2022.3230596, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2164150522",
                    "name": "Saddam Al Amin"
                },
                {
                    "authorId": "1806836",
                    "name": "Md. Saddam Hossain Mukta"
                },
                {
                    "authorId": "2197620015",
                    "name": "Md. Sezan Mahmud Saikat"
                },
                {
                    "authorId": "2197241772",
                    "name": "Md. Ismail Hossain"
                },
                {
                    "authorId": "7484275",
                    "name": "Md. Adnanul Islam"
                },
                {
                    "authorId": "2115615362",
                    "name": "Mohiuddin Ahmed"
                },
                {
                    "authorId": "25447315",
                    "name": "S. Azam"
                }
            ],
            "abstract": "The opioid crisis has led to an increased number of drug overdoses in recent years. Several approaches have been established to predict opioid prescription by health practitioners. However, due to the complex nature of the problem, the accuracy of such methods is not yet satisfactory. Dependable and reliable classification of opioid dependent patients from well-grounded data sources is essential. Majority of the previous studies do not focus on the users\u2019 mental health association for opioid intake classification. These studies do not also employ the latest deep learning based techniques such as attention and knowledge distillation mechanism to find better insights. This paper investigates the opioid classification problem by using machine learning and deep learning based techniques. We used structured and unstructured data from the MIMIC-III database to identify intentional and unintentional intake of opioid drugs. We selected 455 patient instances and used traditional machine learning and deep learning to predict intentional and accidental users. We obtained 95% and 64% test accuracy to predict the intentional and accidental users from the structured and unstructured datasets, respectively. We also achieve a distilled knowledge based test accuracy of 76.44% from the integrated above two models. Our research includes an ablation analysis and new insights related to opioid patients are extracted.",
            "corpus_id": 254910794,
            "sentences": [
                {
                    "corpus_id": "254910794",
                    "title": "Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation",
                    "text": "When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results. \n\nFormally, Let (x,Y) in D us where x is an eventnote and Y is the corresponding label. Now our student model M S , given an input x will output logits L S which can be shown as L S = M S (x). These logit values are softened by using the temperature \u03c4 and used in the softmax function \u03c3 to get the soft probabilities denoted by \u0176S\u03c4 = \u03c3 (L S /\u03c4 ). On the other hand, Y S denotes the hard probabilities in Y S = \u03c3 (L S ) to be used by the CE (cross entropy) loss. \n\nThe teacher model M T , outputs the score for each inputs from dataset D sn . Assuming, (I,Y) in D us where i is the set of structured features, gathered from the dataset D s and Y is the corresponding label that is identical to the label of dataset D us . Therefore, The score provided by the model, can be denoted as L T i = M T (I i ) and the hard probability distribution for each input can be shown as Y T i == \u03c3 (L T i /\u03c4 ), the soften probability would be then,in \u0176T \u03c4 = \u03c3 (L T /\u03c4 ). The final loss function now can be derived as Equation 12: \n\nIV. RESULTS \n\nSeveral classification algorithms and techniques have been used to classify the opioid patients from both structured and unstructured datasets.",
                    "score": 0.5670786799050526,
                    "section_title": "G. COMBINING MODELS BY KNOWLEDGE DISTILLATION",
                    "char_start_offset": 33784,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 48
                        },
                        {
                            "start": 49,
                            "end": 197
                        },
                        {
                            "start": 198,
                            "end": 242
                        },
                        {
                            "start": 243,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 546
                        },
                        {
                            "start": 547,
                            "end": 653
                        },
                        {
                            "start": 656,
                            "end": 741
                        },
                        {
                            "start": 742,
                            "end": 846
                        },
                        {
                            "start": 847,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1115
                        },
                        {
                            "start": 1118,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1374
                        },
                        {
                            "start": 1375,
                            "end": 1608
                        },
                        {
                            "start": 1609,
                            "end": 1667
                        },
                        {
                            "start": 1670,
                            "end": 1681
                        },
                        {
                            "start": 1684,
                            "end": 1827
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61328125
                }
            ],
            "relevance_judgement": 0.61328125,
            "relevance_judgment_input_expanded": "# Title: Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation\n# Venue: IEEE Access\n# Authors: Saddam Al Amin, Md. Saddam Hossain Mukta, Md. Sezan Mahmud Saikat, Md. Ismail Hossain, Md. Adnanul Islam, Mohiuddin Ahmed, S. Azam\n## Abstract\nThe opioid crisis has led to an increased number of drug overdoses in recent years. Several approaches have been established to predict opioid prescription by health practitioners. However, due to the complex nature of the problem, the accuracy of such methods is not yet satisfactory. Dependable and reliable classification of opioid dependent patients from well-grounded data sources is essential. Majority of the previous studies do not focus on the users\u2019 mental health association for opioid intake classification. These studies do not also employ the latest deep learning based techniques such as attention and knowledge distillation mechanism to find better insights. This paper investigates the opioid classification problem by using machine learning and deep learning based techniques. We used structured and unstructured data from the MIMIC-III database to identify intentional and unintentional intake of opioid drugs. We selected 455 patient instances and used traditional machine learning and deep learning to predict intentional and accidental users. We obtained 95% and 64% test accuracy to predict the intentional and accidental users from the structured and unstructured datasets, respectively. We also achieve a distilled knowledge based test accuracy of 76.44% from the integrated above two models. Our research includes an ablation analysis and new insights related to opioid patients are extracted.\n## G. COMBINING MODELS BY KNOWLEDGE DISTILLATION\nWhen \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results. \n\nFormally, Let (x,Y) in D us where x is an eventnote and Y is the corresponding label. Now our student model M S , given an input x will output logits L S which can be shown as L S = M S (x). These logit values are softened by using the temperature \u03c4 and used in the softmax function \u03c3 to get the soft probabilities denoted by \u0176S\u03c4 = \u03c3 (L S /\u03c4 ). On the other hand, Y S denotes the hard probabilities in Y S = \u03c3 (L S ) to be used by the CE (cross entropy) loss. \n\nThe teacher model M T , outputs the score for each inputs from dataset D sn . Assuming, (I,Y) in D us where i is the set of structured features, gathered from the dataset D s and Y is the corresponding label that is identical to the label of dataset D us . Therefore, The score provided by the model, can be denoted as L T i = M T (I i ) and the hard probability distribution for each input can be shown as Y T i == \u03c3 (L T i /\u03c4 ), the soften probability would be then,in \u0176T \u03c4 = \u03c3 (L T /\u03c4 ). The final loss function now can be derived as Equation 12: \n\nIV. RESULTS \n\nSeveral classification algorithms and techniques have been used to classify the opioid patients from both structured and unstructured datasets.",
            "reference_string": "[254910794 | Amin et al. | 2023 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "268820185",
            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
            "text": "A powerful machine learning approach called knowledge distillation enables the transfer of knowledge from a large and complex model to a more compact and computationally efficient model.The idea of knowledge distillation was first introduced by Bucila et al. [12] in 2006, and since then it has gained significant attention from researchers and practitioners in the field of machine learning.\n\nTraining a smaller model, known as a student model, to do the same tasks as the bigger model, known as a teacher model, is the fundamental aim of knowledge distillation.To achieve cutting-edge performance, the teacher model is often trained on a specific task and dataset.On the other hand, the student model is smaller and more computationally effective, making it suitable for deployment on devices with constrained resources.Transferring knowledge from the teacher model to the student model allows us to improve the performance of the student model without appreciably raising its complexity.The student model is trained using the same task and dataset as the teacher model throughout the knowledge distillation process (figure 2).The student model is trained according to the teacher model's predictions, which provide the student model additional knowledge and help it perform better.Knowledge transfer techniques include soft target training, attention transfer, and feature mimicry [52].\n\nFig 2 : The generic teacher-student framework for knowledge distillation [50] In the distillation loss, the lightweight student model duplicates the output produced by the teacher model by using the loss function.The weighted total contains the cross-entropy loss between the student's output and the real labels, as well as the cross-entropy loss between the student's output and the teacher's output (both after temperature scaling).\n\nwhere   is the student model output,  \u210e is the teacher model output, t is the true labels,  is a hyperparameter that regulates the weight given to each term, and T is the temperature parameter used for temperature scaling.The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation.This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function.The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
            "score": 0.8337184633284824,
            "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
            "char_start_offset": 12659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 392
                },
                {
                    "start": 394,
                    "end": 563
                },
                {
                    "start": 563,
                    "end": 666
                },
                {
                    "start": 666,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1389
                },
                {
                    "start": 1391,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2191
                },
                {
                    "start": 2191,
                    "end": 2298
                },
                {
                    "start": 2298,
                    "end": 2434
                }
            ],
            "ref_mentions": [
                {
                    "start": 1384,
                    "end": 1388,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "The softmax loss, also known as the cross-entropy loss between the teacher's softened outputs and the student's predictions, plays a pivotal role in knowledge distillation. We handle this loss calculation as follows: \n\nThe softmax function with temperature scaling is applied to both the teacher's and student's logits, generating softened probability distributions. \n\nThe cross-entropy loss is then computed between these two distributions. This loss quantifies the difference between the teacher's guidance and the student's current understanding. \n\n(5) \n\nThis cross-entropy loss is combined with the traditional loss function to form the final loss function for training the student model. This combined loss function guides the student to not only mimic the teacher's output distribution but also to align correctly with the ground truth. \n\nThe choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer. \n\nEquations: Softmax function with temperature scaling: \n\nwhere z i is the logit for class i and T is the temperature.",
            "score": 0.8037073820339646,
            "section_title": "Softmax loss calculation:",
            "char_start_offset": 23189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "273549238",
            "title": "AI Readiness in Healthcare through Storytelling XAI",
            "text": "Knowledge distillation refers to the process of transferring knowledge from a comparatively large model to a smaller model without compromising on performance. The complex large model is called teacher and the smaller model is called student. The rationale behind model distillation is to train complex large models also called teacher for a specific task and then transfer the knowledge to a smaller model also called student using distillation loss (Equation: 1). The loss function employed in knowledge distillation to instruct the student model to imitate the teacher model's behavior is called the distillation loss. By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model. This allows the student model to capture the same high-level concepts and reasoning as the teacher but in a more transparent and interpretable form [15,16].",
            "score": 0.8026488635511212,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1682
                }
            ],
            "ref_mentions": [
                {
                    "start": 1678,
                    "end": 1681,
                    "matchedPaperCorpusId": "57189239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "221284805",
            "title": "Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems",
            "text": "Knowledge distillation is a transfer learning approach to distill useful knowledge from a teacher model and transfer it to a student model. In a distillation process, the distilled knowledge can be transferred from a teacher network to a student network by utilizing softened targets parameterized by a temperature \u03c4 [31]. For a higher temperature \u03c4 > 1, teacher's outputs (e.g., logit vector) z i can be softened and converted into a probability distribution by passing them through a softmax function: \n\nIn the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space [11]. If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated. Update the edge policy network parameters \u03b8 S through a SGD algorithm 8 end",
            "score": 0.7858972857938956,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 15384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1415
                }
            ],
            "ref_mentions": [
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "1923568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "Knowledge distillation [34] involves the process of transferring knowledge from a sizable, intricate model (teacher model) to a more compact, effective model (student model) [35]. It involves training the student network to mimic the teacher network's output and emulate its internal representations or decision-making process. This technique is used to enhance the performance of smaller models, making them approximate the behavior of larger models while reducing computational costs and memory requirements. In the process of knowledge distillation, logits are used as a basis for comparing the outputs of the student model with those of the teacher model. By examining logits, the model can measure the certainty or confidence of its predictions before applying the softmax function to obtain the final probability. The standard cross-entropy loss depends on the predicted probabilities and ground truth labels. Additionally, the loss function is extended to include the standard cross-entropy loss between the predictions of the student model and the ground truth labels, as well as an additional loss term that measures the discrepancy between the softened probabilities (obtained through a higher temperature softmax) of the teacher model's predictions and the corresponding predictions of the student model. This additional term ensures that the student model not only learns to predict the correct labels but also aims to replicate the softened outputs of the teacher model, effectively transferring its knowledge to the student model. By jointly minimizing these two loss terms, the student model can learn to generalize better and imitate the behavior of the more complex teacher model. The calculation of the probability for the class is as follows: \n\nHere, T represents the \"temperature\" of knowledge distillation. When T = 1, it corresponds to a normalized exponential function. With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\"",
            "score": 0.777654249706236,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2255
                },
                {
                    "start": 2256,
                    "end": 2403
                },
                {
                    "start": 2404,
                    "end": 2463
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8193359375
        },
        {
            "corpus_id": "274436184",
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "text": "In this subsection, we introduce the concept of learning node-specific temperatures to enhance knowledge transfer from teacher to student models. While dual-teacher distillation and graphlevel distillation ensure that teacher models encode fair and information-rich representations, a critical remaining challenge is ensuring effective transfer of this knowledge to the student model, thereby improving performance. \n\nLearning node-specific temperatures addresses the limitations inherent in the temperature mechanism used in traditional distillation methods. In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge. However, using a fixed temperature treats the softening of all teacher logits equally, ignoring variations in node representations. This uniform approach fails to provide precise guidance for individual nodes, which can hinder knowledge transfer. Moreover, existing research has shown that temperature scaling can improve model performance (Z. Zhang and Sabuncu, 2020). \n\nTo confront these issues, we propose an entropy-based approach to learn node-specific temperatures. This approach assigns a unique temperature \u03c4i to each node, controlling the degree of softening individually. The softmax function with node-specific temperatures is defined as: \n\nwhere the temperature parameter \u03c4i is the specific temperature of the i-th node. Obviously, \u03c4i determines the softening direction of each node. The temperature of the node is related to the teacher's confidence for each respective node (Zhichun Guo et al., 2023). Therefore, we can judge the teacher's confidence for each node through the entropy of the teacher's logits. The lower the entropy value, the higher the teacher's confidence for this node. Specifically, we calculate the node-specific temperature through the probability distribution of the teacher and the confidence of the teacher, defined as follows:",
            "score": 0.7337937035485411,
            "section_title": "Learning node-specific temperatures",
            "char_start_offset": 17591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2473
                }
            ],
            "ref_mentions": [
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88623046875
        },
        {
            "corpus_id": "247628251",
            "title": "Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator",
            "text": "The knowledge distillation was proposed by hinton et al [9]. The output of the pretrained teacher model is scaled by softmax function with temperature and used as a soft label to train the student model. The student model learns inter-class similarity(dark knowledge) from the soft label of the teacher model and achieves performance similar to the teacher model despite being shallower than the teacher model. Zhang et al. [24] also improved performance through a self-distillation technique that configured the structure of the teacher model and the student model equally. Many teams used the knowledge distillation model at the 2021 ABAW and were also on the leaderboard. In particular, Deng et al. [7] made it possible to train deeper dark knowledge using the knowledge distillation technique, the ensemble technique, and the generation technique in which a trained student model becomes a teacher model and trains a new student model.",
            "score": 0.711975077768813,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 3996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 939
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "159041406"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "236987020",
            "title": "Learning from Matured Dumb Teacher for Fine Generalization",
            "text": "Knowledge Distillation (KD) is a method of transferring information from a larger and cumbersome teacher model to a new student model, which induces good performance even that student is a small and shallow model [14]- [16]. The main factor of this effect is to transfer dark knowledge of the teacher as a smoothed soft label. [8], [17] To effectively use the dark knowledge, the softmax function is modified as follows: \n\nwhere T is temperature, and N is the number of classes. The new loss function that uses both hard and soft labels is defined as \n\nwhere L CE is the cross-entropy loss, y t and y s are the output probability of the teacher and student, and y gt is the ground truth.",
            "score": 0.7113607768035329,
            "section_title": "II. RELATED WORK A. Knowledge Distillation",
            "char_start_offset": 3835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 420
                },
                {
                    "start": 423,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 687
                }
            ],
            "ref_mentions": [
                {
                    "start": 219,
                    "end": 223,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 332,
                    "end": 336,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4169921875
        },
        {
            "corpus_id": "235186864",
            "title": "KnowSR: Knowledge Sharing among Homogeneous Agents in Multi-agent Reinforcement Learning",
            "text": "As mentioned earlier, KD has achieved great success in the field of computer vision though extracting dark knowledge. The key to extracting dark knowledge is to get the soft probability output of trained teachers with distillation. Here, we let a t be the input log of the final softmax layer of the teacher network, where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] using the following softmax function: q i = e a i \u03a3je a j where i represents the i-th neuron. [3] proposed softening the teacher probabilities with temperature T to extract more knowledge: \n\nCompared to true labels from datasets, the soft output of the teacher provides more information. Based on the same input x, the teacher and student networks produce probabilities q t (x) and q s (x) with Equation ( 4) respectively. The gap between q t (x) and q s (x) is usually penalised by the Kullback-Leibler (KL) divergence (Equation ( 6)): \n\n(5) \n\nwhere P(x) and Q(x) are two probability distributions of the random variable x. KD inspires us to believe that minimizing the gap between agent models through the skill of distillation is the essence of knowledge sharing. We then draw upon the KD thought in our MARL research to share knowledge and verify the feasibility of KnowSR in Section 3.3.",
            "score": 0.7064110048639755,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "227228186",
            "title": "Bringing AI To Edge: From Deep Learning's Perspective",
            "text": "Knowledge distillation is another technique to conduct model compression, where a more compact student model can learn the knowledge from a complicated and powerful teacher model. Bucila et al. [148] first propose the concept of knowledge distillation, and Hinton et al. [140] generalize knowledge distillation and apply it to DNNs. \n\nThe core idea of knowledge distillation is to train a compact model (student) with the assistant of a complicated, pre-trained model (teacher). During training, the student model exploit the conventional method to train the model and obtain a loss according to the one-hot class distribution, e.g., [0, 0, 1, 0], namely hard targets and at the same time the knowledge from the teacher model is distilled and transferred to the student model by calculating a new loss in which the target is the probability distribution of predicted class P from the teacher model, e.g., [0.1, 0.21, 0.6, 0.09], namely soft target. Nevertheless, the probability of the correct class dominates the probability distribution generated by the teacher network (e.g., [0.97, 0.1, 0.0, 0.2]), which significantly limits the knowledge transferring capability. To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.2]). Hence, we are able to formulate the softmax with temperature as follows: \n\nwhen T is set to 1, it is the original softmax function. Please note that we refer the softmax function with temperature T as \u03b4 T for simplicity. Therefore, we can formulate the overall loss function as: \n\nwhere F denotes the cross-entropy function. \u03b1 and \u03b2 are two balancing factors. z s and z t represent the output logits from student model and teacher model, respectively. y is the ground truth. An illustration about how knowledge distillation works is shown in Fig. 7. \n\nAfter [140], many efforts are made towards improving the performance of knowledge distillation.",
            "score": 0.7048946471984068,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 46895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 199,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70458984375
        },
        {
            "corpus_id": "256900863",
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "text": "Knowledge distillation [14] transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1. \n\nThere are many strategies to construct the loss function in knowledge distillation, such as the KL divergence [14], the mean squared error [37] and the Jensen-Shannon divergence [38], etc. Traditional knowledge distillation transfers dark knowledge in a highly coupled way, which limits the flexibility for knowledge transfer. Zhao et al. [32] pointed out that dark knowledge can be decoupled into target class knowledge and non-target class knowledge and to be transfered to student model in a more flexible way by reconstructing the KL divergence. Furlanello et al. [33] demonstrated that the decoupled dark knowledge of teacher model can guide student model to have stronger generalization ability than that of teacher model. In this paper, we attempt to distill fuzzy dark knowledge from High-order TSK fuzzy classifier, and propose a novel born-again TSK fuzzy classifier endowed with the powerful classification performance as well as high interpretability.",
            "score": 0.6916730103935106,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 8754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1583
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "233033712"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "209405263"
                },
                {
                    "start": 959,
                    "end": 963,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1188,
                    "end": 1192,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "255546282",
            "title": "Designing an improved deep learning-based model for COVID-19 recognition in chest X-ray images: a knowledge distillation approach",
            "text": "To distill knowledge from the teacher to the student, a weighted average (mean) is necessary. The initial objective function is Cross-Entropy with soft targets, which is calculated through the softmax function in the smaller network using a higher temperature. Soft targets are generated by a larger architecture or network. The second objective function is Cross-Entropy with valid labels, which utilizes the softmax output from the student model with a temperature of zero. \n\nThe teacher network and the student network start receiving training data in parallel. The teacher model incorporates a softmax function with temperature in its output. In contrast, the student model generates two distinct outputs. The first output is softmax with temperature, while the second output consists of the standard softmax. The purpose of the student model is to produce softened probabilities, which correspond to the output of the teacher model. The loss of knowledge distillation is calculated using the following formula: 2 ( . ) \n\n(1 ) ( . ) \n\nHere , p and q denote probabilities generated by the student and teacher networks in a specific temperature (T), respectively, and KL denotes the Kullback-Leibler divergence, that measures the level of distinction between two probabilistic distributions. The Cross-Entropy of the student model with T=1 is (LWs.x). \n\nAccording to [38], \uf061 and T are hyperparameters where the greater the value of \uf061 , the better the learning experience for the student model. \n\nDuring the distillation phase, back-propagation should only be performed in the student network, as the teacher network has already fine-tuned its parameters. The transfer of knowledge from the teacher to the student model takes place throughout the distillation procedure. It is worth noting that the student model can be trained at a faster rate compared to the teacher model. For further details on the distillation procedure, please refer to [39]. The knowledge distillation procedure is illustrated in Figure 2.",
            "score": 0.690814574414431,
            "section_title": "Hinton et al. pioneered Knowledge Distillation in",
            "char_start_offset": 18494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 1944,
                    "end": 1948,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "264555654",
            "title": "Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation",
            "text": "The core idea behind Knowledge Distillation (KD) (Hinton et al., 2015) is transferring knowledge of a high-capacity teacher network to a relatively smaller student model. In knowledge distillation, a student network is guided by not only a one-hot encoded ground-truth but also a soft target mapped by the teacher network (probability distribution). This is known to transfer a class-wise relation mapped by a teacher is commonly termed the dark knowledge. Given a data sample from a joint distribution (x, y) \u2208 X \u00d7 Y, a student model is optimized by combining two cross-entropy terms. \n\nwhere |Y | and \u0177k denote the number of classes and a k-th target label (one-hot encoded) respectively. \u03b3, and P denote a balancing parameter, and a probability distribution scaled with a temperature. \u03b8 and \u03d5 are parameters of a student and teacher network respectively.",
            "score": 0.6883479492964902,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 349
                },
                {
                    "start": 350,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 857
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036346435546875
        },
        {
            "corpus_id": "252596141",
            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
            "text": "Knowledge Distillation (KD) Hinton et al. (2015) employs a technique to transfer the learned knowledge from a pre-trained teacher model to another model with less or similar complexity (student model). The model transfer process is implemented in two steps. In the first step, a surrogate output probability function, namely a temperature-softmax function, is utilized. By adding a temperature scaling variable T , the conventional softmax function then becomes: \n\nwhere z j is the output set corresponding to class j of the given DNN with batch of input data X. \n\nThe subscript l denotes the index of softmax output which corresponds to the prediction on class l of the DNN. The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T , the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input z j is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective. \n\nIn the second step, to help implement the transfer of knowledge from teacher to student, the authors in Hinton et al. (2015) presented a new loss function, called distillation loss function (also known as soft-loss function). This loss function comprises two terms: the intrinsic entropy function of the teacher and the cross-entropy function between the teacher and the student's outputs, which can be expressed as follows: \n\nThe purpose of this function is to compare the output distribution between teacher p(X) and student model q(X). In terms of information theory, this measurement shows the under-performance of the distribution set created by student, when the output distribution set of the teacher is taken as the sample distribution set, with precision of 100%. By minimizing this function, we reduce the functional difference between the two deep networks. As a result, the student model tends to make it's output distribution become more identical to the teacher model's behavior.",
            "score": 0.687327440469715,
            "section_title": "A.2 KNOWLEDGE DISTILLATION",
            "char_start_offset": 25127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "236272577",
            "title": "Modeling microscopic and macroscopic information diffusion for rumor detection",
            "text": "To further improve the model performance on rumor detection task, inspired by knowledge distillation technique 43 -which involves capturing the \"dark knowledge\" from a teacher model to guide the learning of a student network, has emerged as an essential technique for model improving. We first train a teacher model via Algorithm 2, and then transfer the knowledge from the teacher model to a student model, here in our work, the student model has the same model architecture as the teacher model (self-distillation 44,45 ). Before introducing the concrete training procedure of MMRD with knowledge distillation, we first give the definition of the softmax with temperature: \n\nwhere \u03c4 is a temperature that is normally set to 1, using a higher value for temperature \u03c4 to produce a softer probability distribution over the class, which brings the advantage that the information carried by the negative label will be relatively amplified, and the model training will pay more attention to the negative label. \n\nThe concrete training procedure of the knowledge distillation is listed in Algorithm 4.6, and Figure 1C gives a visualization of Algorithm 4.6. The objective function of the knowledge distillation is a weighted average of two different objective functions. The first loss function is the cross-entropy with the soft targets and it is computed using the same high temperature \u03c4 t = in the softmax of the student model as was used for generating the soft targets from the teacher model. is soft output from student model. The second loss function is the cross-entropy with the ground truth. This is computed using exactly the same logits in softmax of the student model but at a temperature of 1. \n\nwhere y i is the ground truth and y \u03c4 H \u02c6= softmax(FC( ), = 1) \n\nis the hard output of student model. Finally, the objective function of knowledge distillation is: \n\nwhere \u03b2 is the balance weight, which always been a considerably lower value since the am- plitude of the gradients produced by the scale of the soft output as \u03c4 1 2 \u2215 . This ensures that the relative contributions of the hard and soft targets remain roughly unchanged. 43 gorithm 3. Training procedure of MMRD with knowledge distillation. \n\n\uf047 \uf050 , the max-order number K , temperature \u03c4 . \n\nInput: Student-optimized parameters \u0398. We now present the findings from our experimental evaluations.",
            "score": 0.6867697542560321,
            "section_title": "| Rumor detection with knowledge distilling",
            "char_start_offset": 20002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1703
                },
                {
                    "start": 1706,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2210
                },
                {
                    "start": 2213,
                    "end": 2259
                },
                {
                    "start": 2262,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2363
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 113,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "199405591"
                },
                {
                    "start": 519,
                    "end": 521,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 2141,
                    "end": 2143,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460693359375
        },
        {
            "corpus_id": "247486748",
            "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing",
            "text": "The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model. In classification tasks, neural networks generally use a Softmax output layer to transform the output into a probability, whereas knowledge distillation uses a smooth refinement of the Softmax. The Softmax equation of knowledge distillation is as follows: \n\nZ i is the predicted value of each class, and T is the parameter that is introduced by the knowledge distillation. Higher values of T produce a smoother probability distribution.",
            "score": 0.677076403016169,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "253655030",
            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
            "text": "Knowledge distillation [3] was proposed as an effective and elegant compression technique to derive a lighter and faster network (student) from a more complex one (teacher), by penalizing the difference between their logits. Later, this mechanism was formalized by [2] to distill the \"dark-knowledge\" from the teacher to the student. The authors noticed that a powerful and confident teacher does not bring more knowledge than ground truth data, as its prediction tends to be a narrow probability distribution with a single peak for the ground truth class. To alleviate this issue, the teacher's logits are \"softened\" by a temperature scaling factor of the softmax activation. In such a manner, the lightweight network can infer what other classes were found similar by the teacher network. More formally, this can be expressed as \n\nwhere \u03c4 is the temperature of the softmax function (equation from [2]). When \u03c4 is greater than 1, the small probabilities of the softmax function are increased and the output is \"softened\". \n\nIn the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions. Other methods proposed using the root-mean-square error loss [9] or distilling from hardlabels [10].",
            "score": 0.6757643238272186,
            "section_title": "Related Work",
            "char_start_offset": 3724,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 26,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1291,
                    "end": 1295,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "267759996",
            "title": "FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework",
            "text": "Subsequently, the student model distills the knowledge from the teacher model by matching its predicted logits with those of the teacher model, described as follows: \n\nwhere (\u2022, \u2022) denotes the Kullback-Leibler divergence, which is applied to measure the discrepancy between the distribution of the predicted logits from teacher and student models. softmax(\u2022) is the softmax function, i.e., softmax(  /) = exp(  / ) \n\n exp(  / ) , and  is the temperature factor that controls the smoothness of the distillation.",
            "score": 0.6752696583492067,
            "section_title": "Self-boosted Graph Knowledge Distillation",
            "char_start_offset": 17654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 510
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "The Kullback-Liebler loss provides additional gradient information to the distillation loss that transfers a teacher's knowledge to a student. Properly softening the hyper-parameter Temperature, \u03c4, is required to produce the softening effect that introduces dark knowledge for the",
            "score": 0.6725881443037449,
            "section_title": "Issues with knowledge distillation",
            "char_start_offset": 14882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017913818359375
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Neural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit, z i , computed for each class into a probability, q i , by comparing z i with the other logits. \n\nwhere T is a temperature that is normally set to 1. As T is set to a higher value, the probability distribution over classes becomes softer. In the most basic form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set, which is produced by using a cumbersome model with a high softmax temperature. There is theoretical study on how the dark knowledge helps the student to learn finer structure from the big model, the teacher model. The low temperature models are usually good at hard predictions and we lose the dark knowledge. That's why, the main objective of distillation is to transfer the dark knowledge from teacher to student.",
            "score": 0.6686389494935696,
            "section_title": "A. Distillation Theory",
            "char_start_offset": 13849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52734375
        },
        {
            "corpus_id": "237002008",
            "title": "In-Network Flow Classification With Knowledge Distillation",
            "text": "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model. \n\nTo enable knowledge distillation with temperature, the temperature of the soft label output by the teacher model should be the same with the temperature of the student model. Hence, the student model should now have two outputs, one using the standard softmax function (T = 1) for calculating the student loss and the other using the softmax function with the temperature T = \u03c4 . Then, the overall loss function can be expressed by \n\nwhere x is the input, W is the vector of the student model parameters, y is the ground truth label, H is the cross-entropy loss function and \u03c3 is the softmax function with temperature T . The coefficient \u03bb is used to determine the weight of the student loss and the distillation loss. By setting a large value of \u03bb, we emphasize more the knowledge transformation from the teacher model to the student model. The hyper-parameter T can be fine-tuned to adapt the learning efficiency from the teacher model.",
            "score": 0.664025759239378,
            "section_title": "2) SOFTMAX TEMPERATURE",
            "char_start_offset": 15303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "274436047",
            "title": "Streamlined Federated Unlearning: Unite as One to Be Highly Efficient",
            "text": "The concept of the teacher-student model originated from knowledge distillation and is widely used in FL [41], [42]. The principle of a general teacher-student model is illustrated below: \n\n1) Step 1: Knowledge Extraction: In the initial phase of the training process, the knowledge base is extracted from the teacher model. Consider a scenario where the teacher model has an output q, and the student model has an output p. These outputs are typically represented as probability distributions. The raw output of each model is converted into probabilities using the softmax function: \n\nThe variables Z t and Z s represent the raw outputs of the teacher and student models, respectively. T denotes a hyperparameter known as the distillation temperature, which is used to control the extent of smoothing applied to the probability distribution. Since the objective of this study is to achieve unlearning rather than distillation, the distillation temperature T is set to a constant value of 1.",
            "score": 0.6604531347165636,
            "section_title": "A. Teacher-student Model",
            "char_start_offset": 15114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 991
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 109,
                    "matchedPaperCorpusId": "262054040"
                },
                {
                    "start": 111,
                    "end": 115,
                    "matchedPaperCorpusId": "254089825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "259274976",
            "title": "Mitigating Accuracy-Robustness Trade-Off via Balanced Multi-Teacher Adversarial Distillation",
            "text": "Knowledge distillation can transfer the performance of other models to the target model [20]. Extensive research has been widely studied in recent years [23], [25], [52]. \n\nKnowledge distillation can briefly be formulated as the following optimization: \n\nwhere S represents the student model with weight \u03b8 S , T represents the teacher model. KL is Kullback-Leibler divergence loss, CE represents the cross-entropy loss. \u03c4 is a temperature constant combined with softmax operation, \u03b1 is a weight hyper-parameter. \n\nThe original knowledge distillation [20] attempts to improve the student's performance on clean examples, and pre-define a high temperature until the teacher produces suitably soft labels, while the same high temperature is applied to train the student to match these soft labels, which allows the student to better acquire knowledge from a single type of teacher. Our method (with our Entropy-Based Balance Algorithm) is to balance the student's performance between accuracy and robustness, and we adaptively adjust the temperature for different types of teachers to balance their knowledge scales, which allows the student to equally acquire knowledge from different types of teachers. \n\nThe temperature \u03c4 is artificially adjusted as a hyperparameter in previous work. Recently, [26], [28] propose to adjust \u03c4 automatically based on the designed learnable sub-network. These methods need to train the sub-network according to the feedback of the student model, which adds additional training overhead. Our method automatically adjusts the temperature only based on multiple teachers' knowledge scales and has almost no computational cost. Some studies also exist on multi-teacher knowledge distillation, and are designed from different views: including response-based knowledge [23], [52], feature-based knowledge [29], [57], and relation-based knowledge [48], [51]",
            "score": 0.6584013865031657,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1201
                },
                {
                    "start": 1204,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "216265175"
                },
                {
                    "start": 165,
                    "end": 169,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "216265175"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "228376532"
                },
                {
                    "start": 1830,
                    "end": 1834,
                    "matchedPaperCorpusId": "224818016"
                },
                {
                    "start": 1836,
                    "end": 1840,
                    "matchedPaperCorpusId": "198179767"
                },
                {
                    "start": 1877,
                    "end": 1881,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3408203125
        },
        {
            "corpus_id": "227228204",
            "title": "A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models",
            "text": "Despite the recent advances of knowledge distillation technique, a clear understanding of where knowledge resides in a deep neural network and an optimal method for capturing knowledge from teacher and transferring it to student remains an open question. \n\nIn recent advances of knowledge distillation, many forms of knowledge have been defined (Jiaxi Tang et al., 2020 [18]) based on the teacher-student learning paradigm and have shown dramatic success and were analyzed empirically: \n\n\u2022 Layer activation [7] \u2022 Auxiliary information [23] \u2022 Jacobian matrix of the model parameters [72], [73] \u2022 Gram matrix derived from pairs of layers [74] \u2022 Activation boundary [34] Distillation loss for knowledge distillation training is a key factor which is used to penalize the student to transfer this Knowledge from the Teacher to the Student. \n\nFahad Sarfraz et al. [75] presented broad categorization of a diverse set of knowledge distillation methods which differ from each other with respect to how knowledge is defined and transferred from the teacher. Borrowing from their categorization, we cite two groups below to demonstrate how to capture the knowledge from teacher. \n\na) Response Distillation uses only the outputs of a Teacher to train the student to mimic it. C. Bucilu\u01ce et al. [4] proposed to use the logits of a teacher network as target for the student and to minimize the squared difference. Hinton et al. [6] proposed to minimize the KL divergence between the smoother output probabilities. In the original formulation, Hinton et al. [6] introduced a knowledge distillation compression framework and proposed mimicking the softened softmax output of the teacher using a temperature parameter. It raised the temperature of the final softmax function and minimize the Kullback-Leibler (KL) divergence between the smoother output probabilities. This softened output transfers more important information which is called dark knowledge compared to the hard output. When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate.",
            "score": 0.6565143849529327,
            "section_title": "A. Distilled Knowledge and Loss",
            "char_start_offset": 4459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 257,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 582,
                    "end": 586,
                    "matchedPaperCorpusId": "21596346"
                },
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1416,
                    "end": 1419,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51708984375
        },
        {
            "corpus_id": "267535483",
            "title": "Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces",
            "text": "where z i is the logit (raw output) of the model for class i. \n\nIn the context of knowledge distillation, the softmax function is modified by introducing the temperature parameter: \n\nIn higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better.",
            "score": 0.6548672290863191,
            "section_title": "A. TEACHER MODEL",
            "char_start_offset": 25377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 64,
                    "end": 180
                },
                {
                    "start": 183,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1221
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "276538080",
            "title": "Tiny Language Models for Automation and Control: Overview, Potential Applications, and Future Research Directions",
            "text": "Knowledge distillation is a technique that transfers knowledge from a larger, pretrained teacher model to a smaller student model [49][50][51][52]. The student model is trained to mimic the teacher's behavior, matching its predictions to achieve similar performance with fewer parameters. \n\nThe distillation loss combines task-specific loss L task and the knowledge distillation loss L distill , which measures the KL-divergence between the teacher's and student's softened probability distributions. The total loss is given by: \n\nwhere P t = softmax(z t /T) and P s = softmax(z s /T) are the probability distributions from the teacher and student models, respectively, T is the temperature parameter that controls the softness of the output logits, and \u03b1 is a balancing factor between the task and distillation loss. Algorithm 1 demonstrates the process of knowledge distillation, which involves training the student model by combining the task-specific loss and the distillation loss. The algorithm begins with the teacher and student models' forward passes to calculate logits and softened probabilities. It then computes the total loss using both components and iteratively updates the student model's weights to minimize the combined loss. This iterative process ensures that the student model captures the teacher's knowledge while maintaining computational efficiency. Forward pass M t and M s to obtain logits z t and z s 4:",
            "score": 0.6542556422687432,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1432
                }
            ],
            "ref_mentions": [
                {
                    "start": 138,
                    "end": 142,
                    "matchedPaperCorpusId": "259164722"
                },
                {
                    "start": 142,
                    "end": 146,
                    "matchedPaperCorpusId": "203642130"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1153564453125
        },
        {
            "corpus_id": "233777216",
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "text": "However, the key difference is that knowledge distillation requires a pre-defined student model and needs to modify its loss function. \n\n1) Combination of Loss Functions: Although the basic idea is to extract knowledge from the teacher model, we cannot simply employ its vanilla loss function. As the teach model is well trained with high prediction accuracy, the output class corresponding to the ground-truth label dominates in the classification distribution, while other classes are approximately to zero after the softmax block. In this case, even employing the teacher's loss function cannot bring extra useful information to the student, over the vanilla loss function based on the groundtruth labels. An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge [83]. The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student. Therefore, the core modification of the student's loss function (i.e., overall loss) can be formulated as: \n\nwhere x, y, w and \u03c3 represent the input data, ground-truth label, model parameters and the softmax block, respectively. Besides, z s and z d are the logits of the student and teacher, respectively. Note that the overall loss function L contains two parts: the student loss L s and distillation loss the L d , under the control of the coefficient hyper-parameters \u03b1 and \u03b2. We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions.",
            "score": 0.6529731329615561,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 29410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 1620,
                    "end": 1624,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "257308816",
            "title": "A Survey on Deep-Learning-Based Real-Time SAR Ship Detection",
            "text": "Hinton put forward the concept of knowledge distillation for the first time in distilling the knowledge in a neural network. And he introduced the soft targets of teacher to induce the training of students' network. The knowledge distillation is classified into three categories, they are logits transfer, teacher assistant, and domain adaptation [109]. \n\nGenerally speaking, the teacher model has strong ability and performance, while the student model is compact. The knowledge distillation methods transfer the generalization ability of the teacher model to the compact student model to improve its performance with less complexity. The basic idea of knowledge distillation is to transfer the dark knowledge in the complex teacher model to the simple student model. These methods match or outperform the teacher's performance, while requiring notably fewer parameters and multiplications [110], [111], [112], as shown in Fig. 7. \n\nThe parameter T represents temperature. Generally, T is 1. When T is larger, a softer probability distribution will be obtained. There are two loss functions. The first loss function requires that the student model and the teacher model use the same T when calculating the softmax layer. The second loss function requires the student model T to be taken as 1, and the loss function is the weighted average of the two objective functions. Soft prediction carries more and more useful information than hard prediction. The knowledge distillation can get a lightweight CNN model with high accuracy [113]. \n\nThe softmax function is formulated as follows:",
            "score": 0.6527641183421158,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 14030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1584
                }
            ],
            "ref_mentions": [
                {
                    "start": 891,
                    "end": 896,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 898,
                    "end": 903,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 905,
                    "end": 910,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1529,
                    "end": 1534,
                    "matchedPaperCorpusId": "102483463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "The classical KD method uses the Kullback-Leibler scatter to minimize the difference between the output probabilities of teachers and students. However, recent studies have shown that more accurate teachers do not necessarily teach better, and the reasons for this remain unknown. In the following, a new asymmetric temperature scaling (ATS) method is proposed to improve the clarity of teacher-provided error class probabilities and make large teachers teach better. There are various approaches to knowledge distillation, which include transferring knowledge from complex teachers to smaller students through a combination of cross-entropy and distillation loss. Some studies have examined the dependence of knowledge transfer on student and teacher architecture and found that larger models are not always better teachers. Other work has focused on understanding the advantages of knowledge distillation from a principles-based perspective. The paper also explains the notation used, such as softmax functions, logarithms, and probabilities. In addition, a particular phenomenon where disadvantaged students are unable to fully imitate good teachers due to a mismatch of abilities is discussed and explained in detail. Knowledge distillation can be decomposed into three parts, namely, correct guidance, smooth regularization, and category discrimination. These three terms are measured quantitatively using the target category probability, the mean of the error category probability, and the variance of the error category probability. An appropriate temperature scaling method that incorporates the validity of these three terms simultaneously is presented here. Category defensibility is considered to be the basis of KD. A theoretical analysis of the application of temperature scaling to knowledge distillation is presented below. The softening probability vector used in KD is analyzed, showing that an increase in temperature leads to a more uniform probability distribution, and the concepts of intrinsic variance (IV) and derived variance (DV) are also introduced to analyze the class-discriminable terms. And further explains why larger teachers cannot teach better, showing that the effectiveness of KD is more related to differences between error classes than to all classes. A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable.",
            "score": 0.6496534893795064,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 15480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "The classical KD method uses the Kullback-Leibler scatter to minimize the difference between the output probabilities of teachers and students. However, recent studies have shown that more accurate teachers do not necessarily teach better, and the reasons for this remain unknown. In the following, a new asymmetric temperature scaling (ATS) method is proposed to improve the clarity of teacher-provided error class probabilities and make large teachers teach better. There are various approaches to knowledge distillation, which include transferring knowledge from complex teachers to smaller students through a combination of cross-entropy and distillation loss. Some studies have examined the dependence of knowledge transfer on student and teacher architecture and found that larger models are not always better teachers. Other work has focused on understanding the advantages of knowledge distillation from a principles-based perspective. The paper also explains the notation used, such as softmax functions, logarithms, and probabilities. In addition, a particular phenomenon where disadvantaged students are unable to fully imitate good teachers due to a mismatch of abilities is discussed and explained in detail. Knowledge distillation can be decomposed into three parts, namely, correct guidance, smooth regularization, and category discrimination. These three terms are measured quantitatively using the target category probability, the mean of the error category probability, and the variance of the error category probability. An appropriate temperature scaling method that incorporates the validity of these three terms simultaneously is presented here. Category defensibility is considered to be the basis of KD. A theoretical analysis of the application of temperature scaling to knowledge distillation is presented below. The softening probability vector used in KD is analyzed, showing that an increase in temperature leads to a more uniform probability distribution, and the concepts of intrinsic variance (IV) and derived variance (DV) are also introduced to analyze the class-discriminable terms. And further explains why larger teachers cannot teach better, showing that the effectiveness of KD is more related to differences between error classes than to all classes. A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable.",
            "score": 0.6496024189126799,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 15531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2117
                },
                {
                    "start": 2118,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2463
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153076171875
        },
        {
            "corpus_id": "232404302",
            "title": "KnowRU: Knowledge Reuse via Knowledge Distillation in Multi-Agent Reinforcement Learning",
            "text": "As mentioned above, because the teacher model provides more useful information for the student model, KD has achieved success in the field of computer vision. The soft probabilities output of trained teachers is the key of distillation. Let a t be the input logits of final softmax layer of teacher network where a t = [a 1 , a 2 , ...., a j ]. The logits are converted into probabilities q t = [q 1 , q 2 , ...., q j ] with softmax function: \n\n\u03a3j e a j . In order to extract more information compared with true labels, [7] proposes to soften the teacher probabilities with temperature T : \n\nIn KD, such dark knowledge from soft output of teacher provides more information than true labels. Based on the same image input x, the teacher network and student network produce probability q t (x) and q s (x) with Equation (6). The gap between q t (x) and q s (x) is usually penalized by Kullback-Leibler divergence (Equation ( 8)) or cross-entropy loss : \n\nwhere P(x) and Q(x) are two probability distributions on random variable x. \n\nTemperature T in Equation ( 7) also aims to soften the output of the teacher network. Then the student network could reuse knowledge the teacher network by the back propagation of L KD . Knowledge distillation inspires us that minimizing the gap between previous agents and current agents with the skill of distillation is the essence of knowledge reusing. We then draw upon such KD thought in our MARL research to reuse knowledge and verify the feasibility of KnowRU in section 3.3.",
            "score": 0.6494410891486814,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1514
                }
            ],
            "ref_mentions": [
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "33870153"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3505859375
        },
        {
            "corpus_id": "276725399",
            "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
            "text": "The softmax function has been a longstanding component of neural networks, usually used to normalize a vector of real values into probabilities. Modulating the temperature scaling factor within the softmax function allows for reshaping the probability distribution. This section provides a concise overview of the application of temperature scaling in various computational tasks. \n\nKnowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model. Note that despite various temperatures used during training, it is set to 1 when the model is deployed. \n\nModel Confidence Calibration usually utilizes temperature scaling to address the over-confident issue in deep learning [7], [15], [19]. It centers on estimating predictive uncertainty to match its expected accuracy [13], [14]. Despite multiple generic calibration methods being proposed, temperature scaling proposed by [7] remains a baseline method for being simple, effective and able to apply to various cases without major expense. The motivation behind temperature scaling is simple, since the goal is to control the network's confidence to match its accuracy, applying temperature to the softmax function that can directly modify the probability distribution seems a perfect fit for the problem. During training, a validation set is needed to find the ideal temperature parameter for the network, and the same temperature is used when deployed. \n\nContrastive Learning is one paradigm for unsupervised learning [20], [26]. To achieve a powerful feature encoder, it utilizes contrastive loss to pull similar samples close and push negative pairs away in the latent space.",
            "score": 0.6489362584307459,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 3806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2130
                },
                {
                    "start": 2133,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 1399,
                    "end": 1402,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1404,
                    "end": 1408,
                    "matchedPaperCorpusId": "6294674"
                },
                {
                    "start": 1410,
                    "end": 1414,
                    "matchedPaperCorpusId": "235435823"
                },
                {
                    "start": 1495,
                    "end": 1499,
                    "matchedPaperCorpusId": "202773833"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "49314079"
                },
                {
                    "start": 1600,
                    "end": 1603,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 2202,
                    "end": 2206,
                    "matchedPaperCorpusId": "4591284"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "226282114",
            "title": "Knowledge Distillation for Singing Voice Detection",
            "text": "The basic idea behind knowledge distillation is to have the student network trained with not only the information provided by true labels (also called hard targets) but also by using soft targets produced by the teacher network (also referred to as cumbersome model) as a regularizer. This helps the student network learn to mimic the teacher's behaviour. In accordance with [10], a scaling hyperparameter referred to as temperature is used to regulate the softness of the targets from the teacher network. \n\nThe generalized formula for softmax is given by: \n\nwhere si and pi are the logit produced for the i th class and the corresponding class probability respectively and \u03c4 is the softness regulating temperature. \n\nA KL-divergence loss is taken between the softmax probabilities of the teacher and the student, represented as q and p respectively, raised to temperature \u03c4 (between 2-20 in our experiments) and is given by: \n\nThe combined loss is given by:. \n\nwhere LCE is the cross-entropy loss between the student's softmax probabilities (at \u03c4 = 1) and the correct labels (hard targets) and \u03bb is a second hyperparameter controlling the trade-off between the 2 losses. Generally, better results are obtained by keeping \u03bb close to 1, i.e., weight on LKD higher.",
            "score": 0.6443235389328781,
            "section_title": "Basics of Knowledge Distillation",
            "char_start_offset": 2244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 506
                },
                {
                    "start": 509,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "233777216",
            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
            "text": "We use Fig. 4 to better illustrate the workflow of knowledge distillation with these two kinds of loss functions. The student loss captures the distance between predicted class and the groundtruth labels marked in the dataset, under the configuration of T = 1. Meanwhile, the distillation loss measures the difference between student's loss and the teacher's loss, which can be used to reflect the dark knowledge learned from the teacher and the temperature inside is T = t. \n\n2) Tuning of Hyper-parameters: It is worth noting that the configuration of the hyper-parameters of \u03b1, \u03b2 and t is an errorand-trial procedure, which significantly impacts the distillation efficiency. In the implementation of existing methods [84,85], the temperature value t is often set as t \u2208 [1,20] . Empirically, a lower temperature makes sense when the student model is much smaller over the size of teacher model. The core reason of this setting is that a higher value of the temperature will bring too much information from the distillation loss, which may easily exceed the learning capacity of the student model [86]. However, as the true learning capacity of a student model is hard to measure in prior, the setting of temperature t still requires future research to obtain a more intelligent and efficient controlling. Meanwhile, the values of \u03b1 and \u03b2 also require careful setting because these hyper-parameters reflect the learning weight between the student loss from the ground-truth labels and the distillation loss from the teacher's experience. A general setting is to make \u03b1 + \u03b2 = 1 and \u03b1 is often smaller than \u03b2 [28]. Some recent researchers also focus on how to adjust these hyper-parameters in a more flexible manner [87]. \n\n3) Usage of Model Training: On top of the tuning of loss function and coefficient hyper-parameters, we can easily generate a light-weight student model that holds similar performance of the complex teacher model. More precisely, at the beginning of each iteration, the data samples will input to both student and teacher models. After the forwarding propagation to calculate the output prediction, the student model can adjust its loss function by introducing the teacher's loss. This pattern ensures that the student can learn from ground-truth labels and teacher's experience.",
            "score": 0.6425264681916791,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 31646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2301
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 772,
                    "end": 775,
                    "matchedPaperCorpusId": "206594738"
                },
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "52920837"
                },
                {
                    "start": 1098,
                    "end": 1102,
                    "matchedPaperCorpusId": "829159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05645751953125
        },
        {
            "corpus_id": "159041346",
            "title": "Zero-Shot Knowledge Distillation in Deep Networks",
            "text": "Knowledge Distillation (Hinton et al., 2015) enables to transfer the complex mapping functions learned by cumbersome models to relatively simpler models. The cumbersome model can be an ensemble of multiple large models or a single model with large capacity and strong regualrizers such as Dropout (Srivastava et al., 2014), BatchNorm (Ioffe & Szegedy, 2015), etc. Typically the complex and small models are referred to as Teacher (T) and Student (S) models respectively. Generally the Teacher models deliver excellent performance, but they can be huge and computationally expensive. Hence, these models can not be deployed in The latent information hidden in the confidences assigned by the Teacher to the incorrect categories, referred to as 'dark knowledge' is transferred to the Student via the distillation process. It is this knowledge that helps the Teacher to generalize better and transfers to the Student via matching their soft-labels (output of the soft-max layer) instead of the one-hot vector encoded labels. Matching the softlabels produced by the Teacher is the natural way to transfer its generalization ability. For performing the knowledge distillation, one can use the training data from the target distribution or an arbitrary data. Typically, the data used to perform the distillation is called 'Transfer set'. In order to maximize the information provided per sample, we can make the soft targets to have a high entropy (non-peaky). This is generally achieved by using a high temperature at the softmax layer (Hinton et al., 2015). Also, because of non-peaky soft-labels, the training gradients computed on the loss will have less variance and enable to use higher learning rates leading to quick convergence. \n\nThe existing approaches use natural data either from the target data distribution or a different transfer set to perform the distillation. It is found by (Hinton et al., 2015) that using original training data performs relatively better. They also suggest to have an additional term in the objective for the Student to predict correct labels on the training data along with matching the soft-labels from the Teacher (as shown in eq. ( 1)). However, accessing the samples over which the Teacher had been trained may not always be feasible.",
            "score": 0.6394450498585347,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1731
                },
                {
                    "start": 1734,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 322,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 334,
                    "end": 357,
                    "matchedPaperCorpusId": "5808102"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "249151858",
            "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation",
            "text": "Due to the class-incremental nature of GFSCIL problem, Geometer applies the idea of teacher-student knowledge distillation to further mitigate \"forgetting old\" during finetuning. In contrast to the classic teacher-student knowledge distillation techniques [27][28][29] used to compress large model into lightweight model with better inference efficiency, we regard the model before streaming as the teacher model and new model as the student model. Knowledge distillation technique is used to transfer the classification ability of previous model, while preserving the interrelationships of the old classes in the metric space. Temperature-scaled softmax [12] is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model. Geometer proposes to calculate the KL-divergence of the softened logits to make the student model gain the experience of classifying old classes C  \u22121 from teacher model. The teacher-student knowledge distillation loss L  on -th session is calculated as \n\nwhere C \u22121 is the set of old classes of ( \u2212 1)-th streaming session and ",
            "score": 0.639321471716715,
            "section_title": "Teacher-Student Knowledge Distillation",
            "char_start_offset": 16974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1386
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "235446818"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "221191366"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72265625
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": "Knowledge distillation is a way to transfer knowledge from large ensembles of strong models to smaller/faster networks. Knowledge distillation consists of two different networks, called the teacher network and the student network. Teacher networks are generally large ensembles of networks or a single neural network larger than student networks. The teacher network, generally pre-trained in vanilla form, takes in data and outputs logits, which the student network tries to mimic. Logits are the values we get as an output of the neural networks. Logits are generally passed through normalization layers such as Softmax. The softmax layer converts the real values to a distribution of probabilities for K classes that add up to 1. The loss function compares the output of both teacher and student models and provides a loss value used to iterate and train the Student through Backpropagation [37]. The architecture of vanilla knowledge distillation (or the teacher-student framework) is shown in Figure 1 The student's output is compared with the teacher's output instead of the ground truth that generally occurs during transfer learning. The student network is then optimized and trained with the signal from the loss function. The teacher network is frozen and is not trained during this process. This figure is adapted from [18].\n\nDuring training, a batch of images(x) is passed through the network to produce a vector of raw outputs(z(x)). If there are K classes from which the model has to choose, the dimension of the raw output is also K. To give the probabilities for each class, the raw outputs are passed through normalization layers like Softmax. For an input x and the raw output vector z(x) = [z 1 (x), z 2 (x)...z K (x)] where K is the number of classes, i th output from the Softmax function is:\n\nwhere j ranges from 0 to K and i is an instance of j.\n\nFurthermore, as proposed in a seminal paper [14], we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of",
            "score": 0.633540842221183,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 24585,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "121072881"
                },
                {
                    "start": 1330,
                    "end": 1334,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1914,
                    "end": 1918,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80712890625
        },
        {
            "corpus_id": "273113737",
            "title": "SwAV transfer learning and knowledge distillation on chest X-ray classification",
            "text": "KD was a technique for knowledge transfer that facilitated a more compact model in learning from a bigger model [10]. This technique involved the student model to replicate the behavior and predictions of the teacher model [33]. Figure 1C illustrated how this technique was performed. \n\nThe teacher model, which already possessed knowledge, was trained alongside the student model on the input data. Both models were trained up to the last fully connected layer to produce raw logits. These raw logits were processed through the softmax activation function, which included a temperature parameter. The formula for the softmax temperature was shown below: \n\nwhere   () is the model that produce logits for class  and  is the temperature parameter that produced a softer probability distribution over classes. The result of this process was soft labels and soft predictions. On the other hand, the student model also performed a standard softmax activation function with a temperature of 1 to produce standard soft predictions. \n\nwhere   () is the probability distribution over the classes predicted by the teacher model for a particular input ,   () is the probability distribution over the classes predicted by the student model for the same input , and  is a weighting factor that balances the contributions of the Student and Distillation Loss. This Total Loss was used to update the weights of the student model, enabling it to acquire knowledge from the teacher model. During this process, the teacher model's weights remained frozen, preventing further learning. \n\nThis study utilized the KD technique to transfer insight from the bigger model to the smaller model, resulting in an improvement in the performance of the student model. This method not only preserved the performance of the larger model but also lowered computational demands, making it ideal for use in environments with limited resources.",
            "score": 0.6304633505233637,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 11494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1910
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "270257779",
            "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model",
            "text": "acquired by the teacher model to a smaller student model.Typically, teacher models are complex and powerful deep models, and student models can learn from both the output logits of the teacher model and the ground truth for knowledge transfer.While most neural networks [57,58] typically use the \"softmax\" output layer to generate category probabilities, the purpose of knowledge distillation is to make the softmax outputs of the student model and the teacher model similar enough.To achieve this, knowledge distillation introduces a softmax function with a temperature parameter, which can be defined as:\n\nwhere z i is the logits of the model, and T is the temperature factor.When T =1, q i is the standard softmax function.In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor.\n\nAccording to the above properties, when knowledge distillation is implemented, the logits output by the teacher model and the student model will be processed with a higher temperature factor to obtain a soft target.Let p T j and q T j denote the output soft target of the teacher model and the student model after being \"softened\" under the temperature T , N is the total number of labels, and let L denote a standard cross entropy loss which is used to measure the direct distribution difference between p T j and q T j .The loss function of the soft target is as follows:\n\nSince the teacher model also has a certain error rate, the use of ground truth can effectively reduce the possibility of errors being transmitted to the student model.c j is defined as the value of ground truth in the j-th class.The positive label takes '1' and the negative label takes '0'.The loss function of the hard target is as follows:\n\nCombining the loss of soft and hard target, the total object function of knowledge distillation:\n\nwhere \u03b1 is a hyperparameter balancing the two terms.",
            "score": 0.6302388190913475,
            "section_title": "Knowledge distillation is a compression technique designed to transfer knowledge or information",
            "char_start_offset": 15843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 57,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 606
                },
                {
                    "start": 608,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 919
                },
                {
                    "start": 921,
                    "end": 1136
                },
                {
                    "start": 1136,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1494
                },
                {
                    "start": 1496,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1725
                },
                {
                    "start": 1725,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1838
                },
                {
                    "start": 1840,
                    "end": 1936
                },
                {
                    "start": 1938,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "216263993"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7236328125
        },
        {
            "corpus_id": "247792910",
            "title": "Rainbow Keywords: Efficient Incremental Learning for Online Spoken Keyword Spotting",
            "text": "Recent studies [28,29,30] show that Knowledge Distillation (KD) is effective for transferring knowledge between teacherstudent models. Inspired by such theory, we consider the model of task \u03c4t\u22121 as the teacher model and the model of task \u03c4t as the student model. We propose a knowledge distillation loss to preserve the prior knowledge from the teacher for the student model to avoid catastrophic forgetting, which is formulated as: \n\nwhere o t\u22121 (x) and o t (x) denote the output logits of the teacher model and student model, respectively. N t is all keywords up to the task \u03c4t. \u03c3 is the knowledge distillation softmax function parameterized by the temperature T . The temperature T is the experiential hyper-parameters of knowledge distillation set as 2.0. As a result, we aim to minimize the total loss of all keywords N t up to the current task \u03c4t formulated as: \n\nWhere LCE is the cross-entropy loss defined in Eq.1, and LKD is the knowledge distillation loss defined above. \u03bb is the experiential hyper-parameters defined as 1 \u2212 N t\u22121 N t .",
            "score": 0.6278610701229193,
            "section_title": "Knowledge Distillation Loss",
            "char_start_offset": 5131,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1046
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 22,
                    "matchedPaperCorpusId": "173187918"
                },
                {
                    "start": 22,
                    "end": 25,
                    "matchedPaperCorpusId": "206596260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1309814453125
        },
        {
            "corpus_id": "271218098",
            "title": "Discriminative and Consistent Representation Distillation",
            "text": "The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax. Similar to [33], our method uses temperature scaling but learns the optimal temperature during training for better knowledge transfer. \n\nLogit-based methods. Several methods have improved logit-based distillation through techniques like label decoupling [81], instance-specific label smoothing [73], probability reweighting [55], and normalizing logits before softmax and KL divergence [65]. Some works focus on dynamic temperature adjustment [42] and separate learning of response-based and feature-based distillation [79]. Additional improvements include transformations for better teacher-student alignment [80] and methods for knowledge transfer from stronger teachers [35]. Unlike existing methods such as [42] that use fixed temperature schedules and MLPs, our approach introduces truly learnable temperature parameters that adapt during training, making it more flexible and efficient. \n\nFeature-based methods. The work by [61] used intermediate feature hints to guide student learning, while [74] aligned attention maps between teacher and student. Several methods have explored structural relationships: [58] preserved feature space relationships, [45] ensured functional consistency, and [25] transferred class-level attention information. The method by [56] transferred mutual relations between data examples. Recent works have introduced cross-stage connection paths [12], direct reuse of teacher's classifier [8], and many-to-one representation matching [47]. Contrastive Representation Distillation (CRD) [66] used contrastive learning to maximize mutual information between representations but needed large memory buffers. Compared to [66], our approach uses efficient in-batch sampling and adds structural consistency, making it more practical. Our method also differs from previous approaches like [35,57,58,67] in how we define and preserve structural relationships. \n\nArchitecture-aware methods. Recent studies have examined how network architecture affects distillation success. Some works developed meta-learning for architecture search [19], training-free student architecture selection [20], and graph-based architecture adaptation [50]. Methods by [28] and [52] addressed distillation between different architectures through unified feature spaces and intermediate networks respectively.",
            "score": 0.6266157525780078,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2316
                },
                {
                    "start": 2317,
                    "end": 2467
                }
            ],
            "ref_mentions": [
                {
                    "start": 1086,
                    "end": 1090,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1269,
                    "end": 1273,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1975,
                    "end": 1978,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1978,
                    "end": 1981,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1981,
                    "end": 1984,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7919921875
        },
        {
            "corpus_id": "276938088",
            "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
            "text": "Knowledge distillation [8] applies the outputs of a model to train another model having comparable accuracy. These two distinct models imitate transferring information in human society, calling one the teacher and the other the student. This derives from that a student model refers the output of the teacher model to obtain a better performance. \n\nFollowing the different type of the output used during the transfer, there are two approaches as logit-based [8, 13,19,24,25] and feature-based [3,4,7,16,18,20]. Logit-based approach only transfers the logits to a student model. For instance, KD loss [8] utilizes Kullback-Leibler divergence (KL divergence), comparing the logits generated by both teacher and student models. The student is able to generate the similar output of the teacher by minimizing the loss function. This comparison occurs with compressing logits through a temperature. The parameter makes the probability distribution smooth after applying softmax into the compressed logits and emphasize the difference between models. \n\nThe temperature parameter controls how much attention students pay to negative logits compared to the average [2,8,10]. When the temperature is very high, the logits distribution becomes uniform, and the between the compressed logits of the two models converge. In contrast, the low temperature emphasizes the difference between logits. The optimal temperature has been determined by trial and error, but recently there has been a shift towards dynamic control. Liu et al. [15] propose a method to update a temperature by meta-learning. Zheng et al. [13] develop a curriculum-based method that gradually changes the distillation from simple to difficult knowledge. The combination of the adversarial learning framework and the temperature increase schedule enabled temperature changes that matched the improvement in the accuracy of the student model. Kaixiang et al. [25], was inspired by R\u00e9nyi Entropy and redefined temperature by only providing a temperature setting to the teacher model. The temperature is substituted by a power transformation of probabilities. This makes the probability distribution of the student model, obtaining a smooth distribution. They propose the samplewise implementation with weighted values. These methods reported improved performance over the conventional global temperature setting, suggesting that dynamic temperature settings promote knowledge distillation.",
            "score": 0.6254341388624529,
            "section_title": "Related Work",
            "char_start_offset": 3014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2444
                }
            ],
            "ref_mentions": [
                {
                    "start": 462,
                    "end": 465,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 465,
                    "end": 468,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 471,
                    "end": 474,
                    "matchedPaperCorpusId": "267751335"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 496,
                    "end": 498,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1157,
                    "end": 1160,
                    "matchedPaperCorpusId": "250113551"
                },
                {
                    "start": 1597,
                    "end": 1601,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1915,
                    "end": 1919,
                    "matchedPaperCorpusId": "267751335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1063232421875
        },
        {
            "corpus_id": "267335043",
            "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
            "text": "Knowledge distillation in neural networks (Hinton et al., 2015) involves transferring information from a larger, more complex model to a smaller one while minimizing information loss. This method extends to both compressing a single larger model and consolidating insights from multiple models (ensemble) into a singular one. \n\nDistillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning. \n\nThe training involves a weighted average of two objective functions: the first part is the Kullback-Leibler divergence with the soft targets (at higher temperature). The second part is the cross entropy loss with correct labels (at temperature 1). \n\nThis methodology allows the distilled model to effectively learn from both the nuanced information present in the soft targets generated by the larger model and the precise ground truth labels, resulting in a more compact yet knowledgeable model. \n\nOne notable example of distillation in LLMs is the Distil-BERT model: DistilBERT is 40% smaller than its parent model BERT, 60% faster than its parent model, and yet retains 97% of BERT's language capabilities. (Sanh et al., 2020) 2.4. Progressive Knowledge Transfer. \n\nWhen distillation is implemented on large models, there is a risk that knowledge transfer is not optimally passed on from the teacher model to the student model due to differences between the architectures of the teacher and student models. One approach to maximize knowledge transfer is progressive knowledge transfer: the student model is first trained only on the inputs and outputs of the first encoder block, and the student model then subsequently trains the output of the next encoder block while freezing the previous trained blocks. (Sun et al., 2020) In our case, encoder blocks are replaced by decoders as the architecture is autoregressive. (Fig. 2) \n\n3. Methods",
            "score": 0.6233808568676624,
            "section_title": "Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2140
                },
                {
                    "start": 2143,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "269664643",
            "title": "Few-Shot Federated Learning: A Federated Learning Model for Small-Sample Scenarios",
            "text": "The concept of knowledge distillation was first introduced by Hinton et al. in [22], comprising a system of teacher and student models [23][24][25][26]. The so-called teacher model refers to a large, structurally complex, and redundantly trained network that acts as the \"teacher\" during training, instructing a simpler, under-trained \"student\" model to achieve a performance as close as possible to that of the teacher model. This process is realized through the use of soft and hard labels along with corresponding loss functions. Specifically, it begins by calculating the soft output of the teacher model, which is the raw prediction values, or logits, from the last fully connected layer of the neural network-values that have not been activated by the softmax function. Using the probability distribution after applying the softmax function would limit the knowledge transferred to the student model and might even have a negative effect by amplifying errors. These prediction values represent the teacher model's confidence level for each class. \n\nHere, z T i denotes the logit output of the teacher model for the ith class, and T is the temperature parameter that controls the smoothness of the output probability distribution. It is evident that a smaller T value, which brings the function closer to the softmax function, makes q i more similar to the probability distribution after activation. Conversely, a larger T value results in a smoother probability distribution curve from the softmax output, allowing the student model to acquire knowledge with richer information entropy. \n\nAfter obtaining the logits from the teacher model, the student model's logits are calculated in a similar manner. Here, z S i represents the logit output of the student model for the ith class. \n\nSubsequently, a loss function is employed to enable the student model to absorb and compress the knowledge from the teacher model. \n\nHere, Q i and P i represent the probability distributions of the teacher and student models, respectively. The difference between these probability distributions is calculated using the Kullback-Leibler divergence, with the temperature parameter used to adjust it, ensuring that the gradients do not become too small as the temperature value increases. \n\nThroughout the knowledge distillation process, in addition to computing the soft target loss, it is also necessary to calculate the hard target loss.",
            "score": 0.6222798497220533,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2276
                },
                {
                    "start": 2279,
                    "end": 2428
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 135,
                    "end": 139,
                    "matchedPaperCorpusId": "235694435"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "247779043"
                },
                {
                    "start": 147,
                    "end": 151,
                    "matchedPaperCorpusId": "266620794"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "233378598",
            "title": "Undistillable: Making A Nasty Teacher That CANNOT teach students",
            "text": "Knowledge distillation (Hinton et al., 2015) helps the training process of \"student\" networks by distilling knowledge from one or multiple well-trained \"teacher\" networks. The key idea is to leverage soft probabilities output of teacher networks, of which incorrect-class assignments reveal the way how teacher networks generalize from previous training. By mimicking probabilities output, student networks are able to imbibe the knowledge that teacher networks have discovered before, and the performance of student networks is usually better than those being trained with labels only. In what follows, we formally formulate the learning process of knowledge distillation. \n\nGiven a pre-trained teacher network f \u03b8 T (\u2022) and a student network f \u03b8 S (\u2022), where \u03b8 T and \u03b8 S denote the network parameters, the goal of knowledge distillation is to force the output probabilities of \n\n, the student network f \u03b8 S could be learned by the following: \n\nwhere KL(\u2022, \u2022) and X E(\u2022, \u2022) are Kullback-Leibler divergence (K-L divergence) and cross-entropy loss, respectively. The introduced \"softmax temperature\" function \u03c3 \u03c4s (\u2022) (Hinton et al., 2015) produces soft probabilities output when a large temperature \u03c4 s (usually greater than 1) is picked, and it decays to normal softmax function \u03c3(\u2022) when \u03c4 s equals 1. Another hyper-parameter \u03b1 is also introduced to balance between knowledge distillation and cost minimization.",
            "score": 0.6222683603788502,
            "section_title": "REVISITING KNOWLEDGE DISTILLATION",
            "char_start_offset": 8694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1413
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28271484375
        },
        {
            "corpus_id": "271162073",
            "title": "MUSCLE: A Model Update Strategy for Compatible LLM Evolution",
            "text": "Now that we have metrics to indicate model update regression, we propose a knowledge distillation approach to minimize this regression for the taskspecific models M v1 and M v2 . Typically, knowledge distillation minimizes the KL divergence between the soft targets \u03c3(z t ) and \u03c3(z s ), where z s and z t are logits predicted by student and teacher models, respectively. \n\n, i denotes the i'th token, n is the total number of tokens available for training, T is temperature parameter, and \u03c3 denotes softmax. Most knowledge distillation works consider the distillation from a trained teacher to an untrained student (Tian et al., 2022;Rajasegaran et al., 2020). Recent work (Roth et al., 2024) tackles the goal of knowledge transfer between pre-trained student and teacher models while retaining student knowledge gained a priori, and shows that standard knowledge distillation between pre-trained models struggles to transfer knowledge without performance drops. Complementary to this work focusing on performance and maintaining prior knowledge, we tackle compatibility with prior models through knowledge transfer.",
            "score": 0.6216284216408663,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 12431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1116
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11614990234375
        },
        {
            "corpus_id": "278327499",
            "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
            "text": "Knowledge Distillation is a technique to transfer knowledge from a large, complex model (the teacher) to a smaller, efficient model (the student) [132]. The goal is to compress the teacher's expertise into the student while retaining performance. Two primary approaches exist: Model Distillation and Data Distillation. \n\nModel Distillation. Model Distillation directly transfers knowledge via the teacher's outputs. The student learns by mimicking the teacher's softened probability distribution (soft labels) over classes, which contains richer information than hard labels (one-hot vectors). A temperature-scaled softmax is used to smooth the outputs, and the student's loss combines both the teacher's guidance and ground-truth labels \n\nwhere   and   are the teacher's and student's softened probabilities for class ,   and   are logits from the teacher and student, and  is the temperature. \n\nThen, the loss function is computed as: \n\nwhere KL( \u2225 ) is the Kullback-Leibler divergence between teacher and student distributions; CE(,  \u2032 ) is cross-entropy loss between student predictions  \u2032 = softmax(  ) and true labels ;  is the weight balancing distillation and cross-entropy losses;  2 compensates for gradient scaling caused by temperature. Moreover,   ,   is logits from teacher and student;  is the emperature ( > 1 smooths probabilities,  = 1 recovers standard softmax);  is the distillation weight (typically 0.5); ,  is the teacher/student softened probabilities;  is the ground-truth labels. \n\nData Distillation. Data Distillation generates synthetic data that encapsulates the teacher's knowledge for student training. Synthetic data  syn ,  syn is generated such that training the student on it mimics training on the original dataset with the teacher. Methods include gradient matching or leveraging the teacher to label synthetic samples. \n\nMinimize the difference between student gradients on synthetic data and teacher gradients on real data: \n\nwhere L syn is the loss on synthetic data ( syn ,  syn ); L real is loss on real data (,  );  is the model parameters. Moreover,  syn ,  syn is the synthetic inputs and labels (learned via optimization);  is the parameters of the student model.",
            "score": 0.6209048447476986,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1856
                },
                {
                    "start": 1859,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46240234375
        },
        {
            "corpus_id": "276885408",
            "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
            "text": "Although knowledge distillation has been proven effective as a model compression technique and further extended successfully to diffusion model acceleration, the theoretical explanation for its efficacy has remained elusive. How 'dark knowledge' is effectively captured from teacher models and utilized to guide student learning remains a fundamental theoretical question [6]. Lopez-Paz et al. [19] presented a unified theoretical framework that connects distillation with privileged information, establishing a generalized framework for understanding machine-to-machine knowledge transfer. Viewing distillation as a transfer of privileged information, TAKD [25] showed that an assistant model of intermediate capacity could more effectively mediate the knowledge flow between teacher and student models.",
            "score": 0.6207528082461894,
            "section_title": "Privileged Information in Distillation",
            "char_start_offset": 7563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 804
                }
            ],
            "ref_mentions": [
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00609588623046875
        },
        {
            "corpus_id": "221340727",
            "title": "MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation",
            "text": "Knowledge Distillation (KD), proposed by Hinton et al. [10], aims at transferring knowledge from a deep teacher network, denoting as T, to a shallow student network, denoting as S. In order to transfer knowledge from teacher to student, the loss for training student network is modified by adding KL divergence between the teacher's and the student's output probability distributions. Formally, given labeled dataset D of N samples D = {(x 1 , y 1 ) , . . . , (x N , y N )}, we can write the loss function of student network as following, \n\nwhere \u03b1 is the hyper-parameter to control the relative importance of the two terms, \u03c4 is the temperature hyper-parameter, \u03b8 T and \u03b8 S are the parameters of teacher T and student S respectively. L CE refers to the cross entropy loss and KL refers to the KL divergence which measures how one probability distribution is different from another. \n\nBy minimizing this modified loss function, the student network will try to match the one-hot ground-truth distribution (the first term) while lowering its discrepancy with output probability distribution from teacher T (x i ; \u03b8 T ) to transfer the knowledge of negative classes learned by teacher network. The probability distribution over classes can be obtained by employing softmax function, where the probability of sample s i belonging to class j can be expressed as, \n\nwhere C is number of classes, z i is the output logits of sample x i , \u03c4 is the same temperature hyper-parameter as (1). The role of \u03c4 is to control the smoothness of the output probability distribution. The higher the temperature, the higher the entropy of distribution, i.e. the smoother distribution.",
            "score": 0.6203454911556172,
            "section_title": "Background: Knowledge Distillation",
            "char_start_offset": 10547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 538
                },
                {
                    "start": 541,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "85528498",
            "title": "Improving Route Choice Models by Incorporating Contextual Factors via Knowledge Distillation",
            "text": "Knowledge distillation, also known as model compression, aims to learn a small or shallow neural network (normally called the student model, denoted as S(\u00b7)) with limited training examples and computational power by transferring the generalization ability from a large well-trained neural network (called the teacher model, denoted as T (\u00b7)), as shown in Fig.2 and Fig.4. During training, the student model will be guided by the teacher. The student attempts to match its softened softmax outputs with that of the teacher, and its hard softmax outputs with the ground truth. Given the output z i before the last layer of a neural network, usually called logits, the softmax transforms z i to a probability P i using the following equation.\n\nNormal softmax tends to set the probability for one class to one and that for the rest to zero. This makes it hard to distill hidden knowledge to the student with the teacher as the source. To improve the generalization ability of student model and efficiently use the hidden knowledge, Hinton et. al. [27] proposed high temperature softmax function in lieu of using the normal softmax (temperature=1 in this case). Then the probability P i is given by the following equation.\n\nLet the input features be denoted by x. Let the output from the teacher model T (\u00b7) with parameters \u03b8 t be T (x; \u03b8 t ). Let the output from the student model S(\u00b7) with parameters \u03b8 s be S(x; \u03b8 s ). Knowledge distillation from the teacher to the student is achieved by minimizing the distillation loss mathcalL d , between the networks, given by the following equation, L d (X, Y ) =\u03b1 n L 1 (y n , S(x n ; \u03b8 s )) + \u03b2 n L 2 (S(x n ; \u03b8 s ), T (x n ; \u03b8 t )) where X is a batch of training data that has n training examples, Y is the vector of corresponding labels for the training data X. The distillation loss L d comprises of two losses, L 1 and L 2 (both cross entropy loss functions), which correspond to supervised loss constrained by the ground truth data and softened loss constrained by the teacher model, respectively. The constants \u03b1 and \u03b2",
            "score": 0.6200372993183216,
            "section_title": "D. Knowledge Distillation",
            "char_start_offset": 11507,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "268297332",
            "title": "Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples",
            "text": "In the context of knowledge distillation, the objective is to train a student neural network, which is denoted as f \u03b8 S (\u2022), to mimic the behavior of a pre-trained teacher network, denoted as f \u03b8 T (\u2022). Both networks have their respective parameter sets: \u03b8 T for the teacher network and \u03b8 S for the student network. The training dataset contains pairs of samples and their corresponding labels, denoted as (x (i) , y (i) ). The logit response of a sample x (i) from the network f \u03b8 (\u2022) is indicated by p f \u03b8 (x (i) ). The softmax temperature function, \u03c3 \u03c4s (\u2022), which is proposed by Hinton et al. in 2015, transforms logits into soft probabilities when a large temperature \u03c4 s , which is usually greater than 1, is applied; and it behaves like the standard softmax function \u03c3(\u2022), when \u03c4 S is set to 1 [1]. To achieve knowledge distillation, a combined loss function is utilized for training the student network, as shown in Eqn. (1). \n\nThis combined loss function consists of two terms. The first term measures the KL divergence between the softened logits of the teacher network, \u03c3 \u03c4 S (p f \u03b8 T (x (i) )), and the student network, \u03c3 \u03c4 S (p f \u03b8 S (x (i) )). This divergence quantifies the difference in the distributions of soft probabilities generated by the two networks. The second term is the cross-entropy loss between the softened probabilities of the student network, \u03c3(p f \u03b8 S (x (i) )), and the ground truth label, y (i) . This term ensures that the student network is learning to predict the correct labels directly. \n\nTo control the balance between knowledge distillation and conventional cost minimization, a hyperparameter \u03b1 is usually used. By adjusting the value of \u03b1, the relative importance of knowledge transfer from the teacher network and the direct supervision of the student network can be controlled during the training process [1].",
            "score": 0.6167384455904918,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 801,
                    "end": 804,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1851,
                    "end": 1854,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "247362975",
            "title": "A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges",
            "text": "model (student). The authors in [208] name this concept knowledge distillation. Figure 10 shows an illustration of the student-teacher network paradigm. The edge devices do not have to store or run the actual DL model locally with this knowledge transfer. Instead, the teacher can be trained, whereas only the student is deployed to the edge devices. The teacher transfers the knowledge of predicted class probabilities to the student as a soft target. The distillation process occurs when a hyperparameter temperature T is included in the softmax function. When T = 1, it will be a standard softmax function. When T increases, the probability distribution of the output generated by the softmax becomes softer, this means that the model can provide more information to which classes the teacher found more similar to the predicted class [208]. The student learns the class probabilities generated by the teacher, known as soft targets. It is pointed out in [208] that hyperparameter T should not be very high, i.e., a lower T usually works better. This is reasonable as when T is high, the teacher model will incorporate more knowledge, and the student model cannot capture so much knowledge because it is smaller in the size of the parameters.\n\nThe authors in [209] extend the work in [208] and find that using the teacher's intermediate hidden layers of information can improve the final performance of the student. With this information, the authors propose FitNets. Unlike [208], the student in FitNets is deeper and thinner than the teacher so that it can generalize better or run faster. Simulation results confirm that the deeper models generalize better, and the thinner model significantly reduces the computational burden on resourceconstrained devices. In the smallest network, the student is 36 times smaller than the teacher, with only a 1.3% drop in performance. Although there is a slight trade-off between efficiency and performance, the student is always significantly faster than the teacher, and the student is small in size. This shows the effectiveness of knowledge distillation in deploying lightweight models on resource-constrained mobile devices. As such, the cognitive avatars in the Metaverse can interact with human players in real time and release their power to help users handle some simple but tedious work.",
            "score": 0.6140468446030796,
            "section_title": "C. Scalable AI Model Training",
            "char_start_offset": 118831,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45751953125
        },
        {
            "corpus_id": "275573808",
            "title": "Knowledge distillation approach for skin cancer classification on lightweight deep learning model",
            "text": "The knowledge distillation algorithm introduced by Hinton et al. [6] uses two loss functions to accomplish the distillation. The first loss function compares the student model's predictions (softmax distribution) with the one-hot encoded hard labels of the data. The purpose of the second loss function was to compare the softmax distributions (soft labels) of the student and teacher models. A well-trained teacher model typically generates a high pseudo-probability for the correct answer, which is similar to one-hot encoding. However, it also produces non-zero pseudo-probabilities for incorrect classes. Unlike others, from these incorrect likelihoods, the student model can learn a lot of useful information from the false soft labels, as shown by Hinton et al [6]. Consider training a child to classify fruits. When knowing about apples, it may be useful to understand how identical the attributes of an apple are to those of pear fruit and how dissimilar they are from an orange. The soft target loss function performs a very similar effect by guiding the learner to the complete spectrum of class likelihoods. Therefore, the student model as a learner may require much less data to acquire all the knowledge of the teacher model by specifying classes that are overlooked from the training data. When conducting knowledge distillation using Hinton's algorithm, it is necessary to loosen the softmax distributions from the teacher and the student wherein the pseudo likelihoods are distributed evenly. The final output layer of a deep model is the softmax layer which converts the logits z i for i-th class into the probability p i belongs to the i-th class by the following equation: \n\nUsually the hard targets are known as the one-hot label which is a category vector with one entry being 1 and the rest being 0, such as [0, 0, 1, 0]. On the other hand, the soft targets are somewhat probabilistic distributions of different categories, such as [0.15, 0.2, 0.6, 0.05]. The prediction of the soft target by the teacher model includes dark knowledge and can be utilized as a coach to transfer information from the teacher network to the small network. Furthermore, a temperature factor T is introduced to superintend the significance of each soft target as follows.",
            "score": 0.6139095061957474,
            "section_title": "Knowledge distillation with Hinton algorithm",
            "char_start_offset": 19872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2272
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "260447668",
            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
            "text": "In this case, the form of knowledge is converted to a simpler one while the knowledge itself is kept the same. This approach can help avoid using cumbersome networks in actual deployment. \n\nCompared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part. \n\n3) Loss Function for KD. The proposed KD-based neural network lightweight method is depicted in Fig. 3. After the teacher network is trained, the student network is trained using a combination of distillation loss and ordinary MSE loss.",
            "score": 0.6136532508196916,
            "section_title": "B. Key Idea and Training Framework",
            "char_start_offset": 18765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2175
                },
                {
                    "start": 2178,
                    "end": 2202
                },
                {
                    "start": 2203,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2414
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "257771365",
            "title": "SELF-VS: Self-supervised Encoding Learning For Video Summarization",
            "text": "Knowledge distillation, first proposed by (Hinton et al. 2015) is a technique that allows for the transfer of knowledge from a pre-trained model, known as the teacher, to a smaller model, known as the student. The basic idea behind knowledge distillation is that the student model will be able to improve its performance by mimicking the behavior of the teacher model. This method is mostly utilized for model compression and learning from multiple teacher. However, recently in (Touvron et al. 2021) authors demonstrated that distillation training can not only encapsulate knowledge in smaller models but also aid in situations where there is a shortage of proper amounts of data. This is known as \"softdistillation\" and is formulated by the following equation: \n\nwhere Z t is the logits of the teacher model and Z s the logits of the student model. \u03c4 is denoted as the temperature for the distillation, \u03bb as a coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy (L CE ) on ground truth labels y, and \u03c8 is the softmax function.",
            "score": 0.6131527343478125,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1062
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293701171875
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "Since Hinton et al. [1] introduce the basic concept of knowledge distillation, where the dark knowledge in teacher models is given by the temperature-scaled representations of the softmax function, various kinds of information have been employed as the sources of knowledge for distillation from teachers to students. FitNets [18] distills intermediate features of a teacher network, where the student network transforms the intermediate features using guided layers and then calculates the difference between the guided layers and the intermediate features of teacher network. The position of distillation is shifted to the layers before the ReLU operations in [19], which also proposes the novel activation function and the partial L 2 loss function for effective knowledge transfer. Zagoruyko and Komodakis [20] argue importance of attention and propose an attention transfer (AT) method from teachers to students while Kim et al. [21] compute the factor information of the teacher representations using an autoencoder, which is decoded by students for knowledge transfer. Relational knowledge distillation (RKD) [22] introduces a technique to transfer relational information such as distances and angles of features. \n\nCRD [23] maximizes mutual information between a teacher and a student via contrastive learning. \n\nThere exist a couple of methods to perform knowledge distillation without teacher models. For example, ONE [24] distills knowledge from an ensemble of multiple students while BYOT [25] transfers knowledge from deeper layers to shallower ones. Besides, SSKD [26] distills self-supervised features of teachers to students for transferring richer knowledge.",
            "score": 0.6123281016757396,
            "section_title": "What to distill",
            "char_start_offset": 4237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1220
                },
                {
                    "start": 1223,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1675
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 23,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1227,
                    "end": 1231,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 1428,
                    "end": 1432,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1578,
                    "end": 1582,
                    "matchedPaperCorpusId": "219636179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5439453125
        },
        {
            "corpus_id": "249712477",
            "title": "Multi scale Feature Extraction and Fusion for Online Knowledge Distillation",
            "text": "The key idea of knowledge distillation is that soft targets contain the dark knowledge which can be used as a supervisor to transfer knowledge to the student model. Given a labeled dataset D{x i , y i } N i=1 , with N samples, x i is the ith input sample and y i \u2208 {1, 2, ..., M } is the corresponding ground-truth label. M is the total number of classes in the dataset. Consider n student models {S j } n j=1 , the logit produced by the last fully connected layer of the student S j is denoted as z j = {z 1 j , z 2 j , ..., z M j }. Then the probability of the jth student for the sample x i over the mth class p m j (x i ) can be estimated by a softmax function, where T is the temperature which produces a more softened probability distribution as it increases. Specifically, when T = 1, it is defined as the original softmax output, we consider writing it as p m j (x i ); otherwise it is rewritten as pm j (x i ). For multi-class classification, the objective is to minimize the cross-entropy loss between the softmax outputs and the ground-truth labels, \n\nwhere l i = 1 if y i = m, and 0 otherwise. Knowledge transfer is facilitated by matching the softened probability of the student model pm j (x i ) and the teacher model pm t (x i ). We introduce the distillation loss of j \u2212 th student model in the form of Kullback-Leibler Divergence \n\n. \n\n(3)",
            "score": 0.6119414565116239,
            "section_title": "Problem Definition",
            "char_start_offset": 8018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1060
                },
                {
                    "start": 1063,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1350
                },
                {
                    "start": 1353,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43896484375
        },
        {
            "corpus_id": "270123240",
            "title": "Federated and Transfer Learning for Cancer Detection Based on Image Analysis",
            "text": "Knowledge distillation transfers knowledge from a complex model (teacher) to a simpler model (student), aiming to retain much of the teacher's performance.\n\nwhere L hard is the traditional loss (e.g., cross-entropy against true labels), L soft is the distillation loss (e.g., KL divergence between the teacher's and student's predictions), \u03b1 is a hyperparameter balancing the two losses, and T is the temperature scaling the logits before applying softmax.\n\nOverall, TL provides a broad framework for leveraging knowledge from related tasks while Fine-Tuning focuses on adjustments to a pre-trained model for a new task.On the other hand, domain adaptation addresses the discrepancy between source and target domain distributions.Moving on, knowledge distillation focuses on transferring knowledge from a complex model to a simpler one.",
            "score": 0.6104809873760813,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 20876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 157,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 836
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.248046875
        },
        {
            "corpus_id": "269282659",
            "title": "Dynamic Temperature Knowledge Distillation",
            "text": "Knowledge distillation [Hinton et al., 2015] is a very effective model compression technology that aims to transfer knowledge from a more bloated and complex teacher model to a more compact and lightweight student model so that the model can be deployed on devices with more restricted resources.Typically the output of the teacher is too sharp (confident) and makes it difficult for the student model to learn the subtle differences between the incorrect classes.As a result, soft labels regulated by \"temperature\" are often used to improve the KD performance.In practice, the loss function of knowledge distillation usually consists of two parts: cross-entropy (CE) loss with hard labels, and KL-divergence loss with soft labels.Temperature has a great impact on the smoothness of the soft labels.Larger temperatures make the soft label smoother, and smaller temperatures make the soft label sharper.A few research studies have observed that a fixed temperature might hinder the KD process, and therefore devoted to exploring the dynamic regulation of temperature during the KD process.For instance, Annealing KD [Jafari et al., 2021] believed that tuning the temperature can bridge the capacity gap between the teacher and the student networks, and proposed to control the temperature through simulated annealing.CTKD [Li et al., 2023] showed that the task difficulty level can be controlled during the student's learning career through a dynamic and learnable temperature.These methods overlooked the differences in smoothness between the output distributions of the student and teacher networks, and they all apply the same temperature to both the teacher and the student.Furthermore, all the above methods need to add new modules or increase training costs, which greatly reduces the flexibility of their usage.As a result, empirically fixed temperatures remain in most recently developed KD algorithms (for example, DKD [Zhao et al., 2022]).NormKD [Chi et al., 2023] originates from a perspective akin to ours, employing normalization of the output logits to establish a consistent degree of smoothness across the distribution.However, their approach does not account for the collaborative dynamics be-tween teacher and student models, resulting in students with markedly low confidence levels that render them less practical for real-world deployment.",
            "score": 0.6091234026504448,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1316
                },
                {
                    "start": 1316,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 1948
                },
                {
                    "start": 1948,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 1321,
                    "end": 1338,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035736083984375
        },
        {
            "corpus_id": "51883340",
            "title": "Training Small Networks for Scene Classification of Remote Sensing Images via Knowledge Distillation",
            "text": "By introducing the high-temperature softmax into our framework in former subsection, we divide the whole training process of knowledge distillation into two procedures. First, train the teacher model directly on dataset, which is shown in Figure 2a. The target is to let T(X), the softmax output of the teacher model, fit the ground truth Y GT . Then, distill the knowledge via high-temperature softmax, as shown in Figure 2b. The student model outputs two branches: high-temperature softmax outputs distill knowledge from the teacher model and the normal softmax outputs learn to match the ground truth label. Thus, the total loss of KD process L KD (X) is: \n\nwhere S T and T T (X) denote the T-temperature softmax output of the student model and the teacher model respectively. In extreme conditions, such as lacking of training samples, the teacher's high-temperature output can even provide supervision to the student model without any ground truth data (set \u03bb = 0). In prediction or production environment, the trained student model only outputs normal softmax result, as shown in Figure 2c. As the higher-temperature softmax output from the teacher model contains different information than the ground truth dataset, our KD framework provides the student model with more categorical information in scene classification tasks.",
            "score": 0.6087142631435156,
            "section_title": "KD Training Process",
            "char_start_offset": 12251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3671875
        },
        {
            "corpus_id": "263620565",
            "title": "Can a student Large Language Model perform as well as it's teacher?",
            "text": "In recent years, the landscape of deep learning has been characterized by models that are increasingly large and intricate. While such models, often boasting billions of parameters, consistently set new benchmarks in accuracy, their computational intensity presents deployment challenges, especially in environments with limited computational resources, such as edge devices [Tao et al., 2020]. Knowledge distillation offers a viable solution to this quandary, facilitating the transfer of knowledge from a sophisticated, high-capacity \"teacher\" model to a more compact \"student\" model, aiming to retain as much of the performance as possible [Hinton et al., 2015]. \n\nCentral to knowledge distillation is the principle that learning can be enhanced when models are trained not just on hard labels but also on the richer, probabilistic outputs of a teacher model. These soft labels can be perceived as capturing the teacher's confidence distribution across classes, providing nuanced insights which hard labels might overlook [Bucilu\u01ce et al., 2006]. \n\nA critical component of this approach is temperature scaling, which modulates the granularity of these soft labels. The temperature parameter, introduced by Hinton et al. [2015], plays a pivotal role in controlling the \"sharpness\" of the teacher's output distributions, thus influencing the quality of the information relayed to the student model. \n\nThe training of the student model is then typically guided by a weighted loss function that balances between the conventional cross-entropy loss and the divergence from the teacher's outputs, usually measured using Kullback-Leibler divergence [Lopez-Paz et al., 2015]. \n\nHowever, the process is not without complexities. The optimal architecture of the student model, the quality of the teacher, and the precise balance of hyperparameters are all determining factors in the success of the distillation [Polino et al., 2018]. The intricacies of these factors and their interplay remain a focal point of contemporary research. \n\nIn conclusion, knowledge distillation emerges as a key technique in the deep learning toolkit, bridging the divide between cutting-edge performance and practical, efficient deployment. Its continued exploration holds the promise of further refining and expanding its applicability across diverse domains.",
            "score": 0.6086674770378184,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2025
                },
                {
                    "start": 2028,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2332
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "220887044"
                },
                {
                    "start": 1025,
                    "end": 1047,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "221716228",
            "title": "A Roadmap to Domain Knowledge Integration in Machine Learning",
            "text": "The performance of a machine learning algorithm is measured in terms of its average performance on multiple models running over a same dataset. Instead of using multiple models, one way is to combine the models to build a robust model that can give effective prediction. But building, training and testing of such a model is expensive and resource heavy. Transferring some knowledge from a bigger model to a small and light model can give a significant performance enhancement [19].\n\nIn different classification task, the probabilities of predicting a class is produced by the imposing \"softmax\" function on the output layer of the network. Softmax takes the logits and convert it to a probability by comparing it with other classes.\n\nwhere z i is referred as lots, and T is the temperature variable to produce a softer probability distribution. The idea behind the teacher-student knowledge distillation architecture is to transfer knowledge from a complex model to a simple model. For example, transfer knowledge from the teacher model to the student model. The motivation behind the idea is to avoid making the one-hot encoded values as hard targets for the student model and to train it on a transfer set of softened probabilities that is predicted by the teacher model.",
            "score": 0.6060196750624905,
            "section_title": "1) Knowledge Distillation for Resource Optimization:",
            "char_start_offset": 21702,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.323486328125
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "KD is a well-known tool to compress a possibly larger (teacher) model, F m \u00f0X; W m \u00de to a smaller (student) model F s \u00f0X; W s \u00de. Here, the input space X belongs to label space y 2 R n , where n represents input the number of classes available in the dataset. We assume a supervised classification setting where model outputs are a m \u00bc F m \u00f0X; W m \u00de; a s \u00bc F s \u00f0X; W s \u00de; fa m ; a s 2 R n g before activation such that a m and a s represent the un-activated output of teacher and student models, respectively. The student model outputs are activated using the softmax function [66] to produce a hard embedding h s such that h s (i) > > h s (j), {1 <= i, j <= n, i 6 \u00bc j} where the logits are probability distributions P n i\u00bc1 h s \u00f0i\u00de \u00bc 1; h s \u00f0i\u00de 2 R \u00fe . However, transfer of knowledge is done through proper softening of the output of the teacher a m and student model a s , which is done by a modified softmax activation containing a hyper-parameter \u03c4: \n\nThe acquisition of soft labels helps utilize the dark knowledge stored in the teacher model for proper knowledge transfer to the student model. The outputs h s , o s , o m and label y are used to provide gradient information for the student to learn better using the distillation loss which is back-propagated to update model parameters. The distillation loss (L DL ) consists of the weighted union of the softmax Cross Entropy (L CE ) [67] and the Kullback-Liebler Divergence (L KLD ) [68] loss with an added weight. \n\nKnowledge distillation varies from a traditional classification because of L KLD , which facilitates the student to learn. The hyper-parameter \u03c4 usually gets a value within the range \u03c4 = 2 to 20. \u03c4 contributes to softmax distributions with different levels of softness. Proper softening of the logits enables better distribution of information to the student, S. As a result, the hyperparameter temperature is essential in knowledge distillation for proper knowledge transfer.",
            "score": 0.6055260289709908,
            "section_title": "Solution using Knowledge distillation (KD) [26]",
            "char_start_offset": 12373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 1392,
                    "end": 1396,
                    "matchedPaperCorpusId": "110510"
                },
                {
                    "start": 1442,
                    "end": 1446,
                    "matchedPaperCorpusId": "37718089"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47802734375
        },
        {
            "corpus_id": "252846538",
            "title": "Boosting Graph Neural Networks via Adaptive Knowledge Distillation",
            "text": "KD training process. Detailed Technique. From the above analysis, temperature directly controls the trade-off between true label knowledge and dark knowledge. Additionally, existing work (Zhang and Sabuncu 2020) has proved that temperature scaling helps to yield more calibrated models. To transfer more useful knowledge from teachers to students, we make the predefined hyper-parameter temperature adaptive for all nodes: each node is associated with an adaptive temperature based on the confidence of teachers for each node. Specifically, we take the entropy of the teacher's logits t t t v to measure the teachers' confidence for each node. The lower entropy means more confident the teacher is for the specific node. Then we compute the temperature for each node with learnable parameters by the following formulation:\n\nwhere \u03c3 represents sigmoid operation. We use \u03c4 v = \u03c4 v \u00d7 (\u03c4 max \u2212 \u03c4 min ) + \u03c4 min to limit the temperature into a fixed range [\u03c4 min , \u03c4 max ]. In the experiments, we discover that involving the distribution of the logits of the teacher in the temperature is more effective than only considering the teachers' confidence after two training steps:\n\nwhere CONCAT represents the concatenation function.",
            "score": 0.6045719007188104,
            "section_title": "Distillation via Adaptive Temeperature",
            "char_start_offset": 14380,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "240544883",
            "title": "Deep Neural Network Compression for Plant Disease Recognition",
            "text": "In the traditional three-stage iterative pruning process, after each pruning step, the model is retrained to restore the accuracy loss incurred by pruning. In this paper, knowledge distillation [37] is used instead of retraining to obtain a better precision recovery effect, as shown in Figure 5. In knowledge distillation, the teacher model uses output soft labels to enable the student model to learn dark knowledge; this process can be defined as: \n\nwhere the temperature parameter T is used to control the smoothness of the output to preferably transfer the knowledge of the teacher network. L KL is the Kullback-Liebler (KL) divergence between the soft labels of the two network models. The soft label output S stu of the student network can be given by: \n\nwhere z S is the output of the student network without a softmax layer. Equation ( 5) controls the degree of smoothness of the output, and it is the softmax function when T = 1. S Tea can also be calculated in this way. Under the effect of knowledge distillation, the total loss function of the student network model becomes: \n\nwhere y True denotes the ground-truth label of the output, and the latter term is the classic cross-entropy loss. \u03b1 and \u03b2 are weight hyperparameters.",
            "score": 0.6043282136274915,
            "section_title": "Retraining",
            "char_start_offset": 18151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 198,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "257102399",
            "title": "Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System",
            "text": "Knowledge distillation is a model compression technique proposed by Hinton et al. in 2015 [9], which could effectively train a smaller student model by learning from a larger teacher model. As shown in Figure 2, a general knowledge distillation framework consists of three major components: teacher-student network architecture, knowledge, as well as distillation algorithm. The main idea of knowledge distillation is that the student model mimics the teacher model in order to achieve a comparable or even a superior performance [10]. Teacher network usually has more complicated network architecture and learns from the data first. After the training of teacher network, the useful knowledge could be distilled from the teacher network. Hereafter, student network learns the knowledge distilled from teacher network to mimic teacher's behavior together with the training data. The core of knowledge distillation is how to simplify model while maintaining relatively good performance. \n\nOne promising and efficient way to distill knowledge is to mimic the logit output of last layer in the teacher model (i.e., teacher's prediction). Soft target, i.e., the probability that the input data   belongs to each class, is proposed as the logit of teacher network. For a -class classification problem, the soft target of teacher network \u0398  could be calculated by the softmax function as: \n\nWhere   denotes the -th training sample,  is the number of classes,    is logit of last layer for the th class and  denotes temperature factor which could control importance of each target. For example, a higher  produces a softer probability distribution among different classes (assign probability with less difference to each class). Soft target contains informative dark knowledge of teacher network and could enhance performance of student network. Empirically, when the student model is very small compared to the teacher model, lower temperatures work better [9]. This is because a very small model might not be able to capture all the information when we raise the temperature. Therefore, soft target could be seen as knowledge extracted from teacher network and it could be transferred to student network by matching output of two networks. To measure the output similarity between teacher network \u0398  and student network \u0398  , the Kullback Leibler (KL) divergence is often employed, which could be formulated in Eq. ( 2),",
            "score": 0.604290383183476,
            "section_title": "Knowledge distillation",
            "char_start_offset": 15159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2414
                }
            ],
            "ref_mentions": [
                {
                    "start": 530,
                    "end": 534,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "233363076",
            "title": "Continual Learning for Named Entity Recognition",
            "text": "Knowledge Distillation (Hinton, Vinyals, and Dean 2015) is a technique to transfer the knowledge between models with a process called \"distillation\". KD was proposed to transfer knowledge from a cumbersome model into a smaller and more efficient one. This technique has been used to compress huge language models, such as BERT (Devlin et al. 2019), into smaller ones, e.g., DistilBERT (Sanh et al. 2019). \n\nKD works by first training a teacher model on some task. After training, the teacher will be able to assign a probability distribution p T over the categories of the task. p T is typically computed by the softmax over the logits z j of the last layer of a neural network architecture, for each category j. The softmax is defined as sof tmax(z j ) = exp(zj /Tm) l exp(z l /Tm) , where T m is a temperature hyper-parameter1 , which is typically set to 1. The teacher is trained by minimizing the crossentropy between the one-hot distribution and its predicted output distribution. Then, a student model is trained by imitating the teacher output distribution over a transfer dataset. That is, the student is trained by computing the KL divergence between p T (also referred to as the \"soft targets\") and the student output probability distribution p S . KD has been used for CL in the image domain (Shmelkov, Schmid, and Alahari 2017; Li and Hoiem 2018; Rannen et al. 2017) -for example, by updating an image classifier to support new categories while preventing catastrophic forgetting (Li and Hoiem 2018).",
            "score": 0.603992532413713,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7955,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1512
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 55,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 385,
                    "end": 403,
                    "matchedPaperCorpusId": "203626972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20556640625
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "In this section, we explore the effect of different temperatures on distillation performance with different teacher-student architectures. In MTKD-SSR, the temperature T 1 in Eq. ( 3) is used to soften the transferred knowledge on the teacherstudent architecture during the stage-wise response distillation, and T 2 in Eq. ( 5) is used to soften the knowledge within the student network during the cross-stage review distillation. As stated in (Chen et al., 2021;Jafari et al., 2021), a proper temperature makes the transferred knowledge informative for improving the performance. Figures 3 and 4 show the Top-1 accuracy of our MTKD-SSR on CIFAR-100, the mAP accuracy on Market-1501, and the Recall@1 accuracy on CUB200-2011 with different temperatures T 1 and T 2 in {1.5, 2.0, 3.0, 4.5, 6.0, 8.0}. We see that the temperature has a large impact on the distillation performance. Specifically, we use the temperatures as T 1 = 2.0 or 3.0 and T 2 = 2.0 on CIFAR-100, T 1 = 1.5, 2.0 or 3.0 and T 2 = 3.0 or 4.5 on Market-1501, and T 1 = 1.5 or 2.0 and T 2 = 4.5 or 6.0 on CUB200-2011. The temperature can not only form the soften informative knowledge during the stage-wise distillation, but also adjust the reviewed knowledge within the student structure during cross-stage distillation. This also implies that the knowledge at each stage of student plays a different role for student self-reflection. The possible reason for the cross-stage review distillation is that the introduction of the temperature parameter can further well distill the dark knowledge from the preceding layers of the student network to its subsequent layers. Through the introduction of two temperatures in the proposed MTKD-SSR, the dark knowledge contained in both teacher and student networks can be fully explored for learning the student.",
            "score": 0.6033679794872385,
            "section_title": "Analysis",
            "char_start_offset": 28464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "259138742",
            "title": "A Weakly Supervised Approach to Emotion-change Prediction and Improved Mood Inference",
            "text": "Similar to [18], we employ knowledge distillation [19], a technique used to transfer knowledge from a larger (teacher) model to a smaller (student) model. In this method, the goal is to train the student model to mimic the output probabilities of the teacher model, in addition to the predicting the true labels. Fig. 3 (right) presents the Teacher-Student Network (TS-Net), where ResMoodEmo is used as the teacher model (see Sec. IV-B), and ResMood is used as the student model (see Sec. IV-A). The teacher model, trained with both mood and \u2206 labels, distills knowledge to the student, which is only trained with mood labels. Since the performance of the student model alone is evaluated, \u2206 labels are not utilised during the testing phase. The SoftMax layer of the student model has a hyper-parameter called temperature (T ), which regulates the softness of the output class probabilities. Using low temperature values produces a sharper probability distribution, facilitating the student to focus on the relative differences in the probabilities of the classes. A weighted sum of the distillation loss, L D , measuring the difference between the outputs of the teacher and student models, and the student loss, L S , a typical supervised loss is optimised in the TS-Net, L = \u03b1L S + (1 \u2212 \u03b1)L D , where \u03b1 is a training hyperparameter.",
            "score": 0.6025932899870281,
            "section_title": "C. Teacher-Student Network",
            "char_start_offset": 19056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1335
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 15,
                    "matchedPaperCorpusId": "252683187"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34521484375
        },
        {
            "corpus_id": "269317596",
            "title": "Shared Knowledge Distillation Network for Object Detection",
            "text": "Firstly, we will recap the Knowledge Distillation method proposed by Hinton et al. [11]. This widely used method involves using category similarity as a guide for student networks. To regularize the network's learning, temperature is introduced to soften the initial categorical information, also referred to as 'dark knowledge'. The output probability of the teacher network and student network can be calculated as Equations ( 1) and (2). \n\nwhere T represents temperature, which can adjust the softening of the output probability. z i z j are the logits input for softmax, p i represents the output probability of each category in the teacher network, and q i represents the output probability of each category in the student network. Cross-entropy between the distilled teacher and student models calculates the soft loss. The hard loss is calculated based on the predicted values of the student model and the true values. \n\nThe ground truth label (also known as the hard target label) for the i-th sample is represented by y i . The teacher and student models' predicted probability are represented by p i and q i , respectively. The total loss function is calculated as below. \n\nThe loss function used in Knowledge Distillation involves two types of losses: soft target loss L soft and hard target loss L hard . The former guides the student to replicate the teacher's probability distribution, whereas the latter reflects the guidance from the actual ground truth labels. The parameter \u03b1 balances the effect of these two losses. During the Knowledge Distillation process, the student receives both the challenging and soft target knowledge. The loss function can be written as follows: \n\nwhere the loss of the cross-entropy is represented by L CE . The softmax function is represented by \u03c3. y represents the ground truth label. The output logits of the student and teacher networks are denoted by z S and z T . The balancing hyperparameter is represented by \u03b1.",
            "score": 0.6014693859911298,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "244079262",
            "title": "Knowledge Distillation of Grassmann Manifold Network for Remote Sensing Scene Classification",
            "text": "When we draw them in a line chart, it's easy to find that the optimal temperature is approximately concentrated from 4 to 6. Therefore, the other experiments will be conducted at a temperature of 4. When the temperature is so low that it approaches 1, the softmax layers become naive, and the difference between positive and negative labels is too large. When the temperature is very high, we can see from the softmax formula with the temperature that the difference between all labels, including positive labels and negative labels, is very small, so the ability to distinguish between different categories is reduced. Property of Parameter \u03b1. The knowledge distillation utilizes the information learned from the teacher network and student network. The teacher network can learn more knowledge than the student network from a lot of data and store it in the soft targets, which is used by the student network. On the other side, the student network gets information from the ground truth at the same time. As mentioned above, \u03b1 is the weighted coefficient between the two loss functions, i.e., L so f t and L hard . The L hard uses the original supervised information to limit the scope of the solution space, and the L so f t contains more prior knowledge learned from the cumbersome teacher model. Therefore, \u03b1 can manipulate which of the two types of information is more dominant in guiding the convergence of the network. \n\nIn order to figure out the effect of \u03b1, we experiment on three datasets with different \u03b1. The value of \u03b1 is from 0 to 1 in intervals of 0.1 in all datasets. According to Equation (10), the vanilla GrNet is the special case of distillation when the \u03b1 is 0. The results are listed in Table 7. The Figure 13 shows that the optimal \u03b1 is approximately between 0.2 and 0.5. When \u03b1 is greater than 0.5, the accuracy will decrease as \u03b1 increases. When the \u03b1 is equal to 1, the loss function is only composed of L so f t , that is, the training process is completely controlled by the prior knowledge of the teacher network. Due to the lack of supervision of the ground truth, incorrect knowledge from the teacher network will also be passed to the student network, which leads to the decline of the accuracy of the model.",
            "score": 0.6013273983513434,
            "section_title": "Comparison Experiments with Shallow CNNs",
            "char_start_offset": 36595,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 1609,
                    "end": 1613,
                    "matchedPaperCorpusId": "22975757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "256843895",
            "title": "End-to-end emotional speech recognition using acoustic model adaptation based on knowledge distillation",
            "text": "In many cases, an ensemble model combining two or more networks provides a good performance [26]. Although the ensemble strategy majorly enhances the performance of a model, a whole ensemble model is cumbersome and demands excessive computational power when accessed by many users, especially if each model employs a large neural network. [15] proposed knowledge distillation to overcome the limitations of an ensemble-heavy model. The performance of knowledge distillation almost matches that of the big teacher model on a distilled student model but requires fewer parameters than the teacher model. Particularly, knowledge distillation offers model compression.\n\nNeural networks typically compute the class probability in a softmax output layer that converts the logit z i computed for each class into a probability q i , as described in (1). In this computation, the z i is compared with other logits. The softmax function outputs a one-hot binary vector indicating the class assignments of z i . This typical labeling system is called a hard target shown in Fig. 2.\n\nIn (1), the temperature parameter T creates a soft label (for a standard softmax function, T = 1). As pointed out in [15], a hard target accepts only the highest probability and dumps the others. The probability of abandonment might also play a role in transfer learning.\n\nTypically, the softmax layer outputs an integer value. However, if T is greater than 1, the probability distribution is softened over the classes. After training on the target data set with assuming a soft target distribution of each case in the target data set, the softmax function of the high-temperature teacher model transfers information to the student model. The student model is trained on the same high-temperature softmax model, but in the post-training test stage, the softmax function is reverted to a standard softmax with T = 1. Figure 3 and Eqs. (2) to (5) show the calculation process of knowledge distillation loss.\n\nLoss \u00bc 1\u2212\u03b1 \u00f0 \u00deC hard x; y \u00f0 \u00de\u00fe\u03b1C soft x; q \u00f0 \u00de \u00f04\u00de\n\nThe total loss in the network is the sum of the distillation and student losses [15]. The distillation loss is contributed by the soft label of the teacher model and the soft prediction of the student model, and the student loss",
            "score": 0.6011448640016336,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5693,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 96,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 841,
                    "end": 844,
                    "matchedPaperCorpusId": "27415606"
                },
                {
                    "start": 1906,
                    "end": 1909,
                    "matchedPaperCorpusId": "220632998"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "11820063"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "3603145",
            "title": "Knowledge Transfer with Jacobian Matching",
            "text": "Knowledge Distillation (Hinton et al., 2015) first showed that one can use softmax with temperature to perform knowledge transfer with neural nets. Ba & Caruana (2014) found that squared error between logits worked better than the softmax method, and they used this method to train shallow nets with equivalent performance to deep nets. Romero et al. (2014) and Zagoruyko & Komodakis (2017) showed how to enhance distillation by matching intermediate features along with the outputs, but use different methods to do so. Sau & Balasubramanian (2016) found that adding noise to logits helps during teacher-student training. We show that the use of the Jacobian can be interpreted as adding such noise to the inputs analytically.",
            "score": 0.5985640758251485,
            "section_title": "Introduction",
            "char_start_offset": 2009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 726
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236328125
        },
        {
            "corpus_id": "273850157",
            "title": "Towards Personalized Federated Learning via Comprehensive Knowledge Distillation",
            "text": "Knowledge distillation (KD) transfers knowledge from a trained teacher model to an untrained student model [15]. Let p t and p s denote the outputs of the teacher and student models, respectively. To smooth the output distribution, a temperature parameter \u03c4 is used in the softmax function p \u03c4 = exp(zi/\u03c4 ) j exp(zj /\u03c4 ) , where z i is the i-th output element. KD aims to minimize the discrepancy between the teacher and student models. The loss function is defined as follows: \n\nwhere L CE is the Cross-Entropy loss between the soft labels p s and the hard labels y, L KL is the Kullback-Leibler divergence loss between the soft labels p \u03c4 s and the soft labels p \u03c4 t , \u03bb is the weight factor. \n\nRecently, KD has become a critical technique in FL [16], offering innovative solutions to tackle the challenge of catastrophic forgetting, exemplified by methods such as pFedSD [7]. This method facilitates the transfer of knowledge from the historical model to the local model for each client. However, it places excessive emphasis on utilizing personalized knowledge from the historical model while overlooking generalized knowledge of the global model. \n\nAs shown in Fig. 2, We propose a comprehensive knowledge distillation method using multi-teacher knowledge distillation. In our method, we retained the local model from previous training, referred to as the historical model, to maintain personalized knowledge. The global model, in contrast, embodies generalized knowledge. We employ both the global and historical models as teachers, with the local model serving as the student, to facilitate a thorough knowledge transfer that balances generalization and personalization. The loss function is defined as follows: \n\nwhere p \u03c4 k represents the soft labels of the local model w k , p \u03c4 g represents the soft labels of the global model w g , and p \u03c4 h represents the soft labels of the historical model w h . \n\nTo facilitate effective knowledge transfer, we implement an annealing mechanism. Initially, the student model primarily learns from the teacher model's soft labels. As training advances, \u03bb undergoes annealing, gradually increasing its emphasis on the hard labels.",
            "score": 0.5979699845516084,
            "section_title": "B. Comprehensive Knowledge Distillation",
            "char_start_offset": 7182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 874,
                    "end": 877,
                    "matchedPaperCorpusId": "254089825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4404296875
        },
        {
            "corpus_id": "269635406",
            "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management",
            "text": "Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution.\n\nThe student model learns to match both true labels and softened probabilities from the teacher.The objective function combines cross-entropy loss with respect to true labels and an additional term for the softened probabilities:\n\nwhere H is cross-entropy, y are true labels, \u03c3 is the softmax function, and \u03bb balances the two terms.Knowledge distillation's efficiency is practical for scenarios requiring lightweight models, such as mobile or embedded systems.It enables complex model emulation with remarkable efficiency, broadening access to advanced machine learning for various applications, including financial investments.",
            "score": 0.5970264166437512,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10828,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 263
                },
                {
                    "start": 265,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 493
                },
                {
                    "start": 495,
                    "end": 596
                },
                {
                    "start": 596,
                    "end": 724
                },
                {
                    "start": 724,
                    "end": 892
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "The purpose of knowledge distillation is to make the distribution of outputs of a teacher network and a student network close enough.Those distributions are obtained through the softmax function.In the general softmax function, index e first enlarges the distance between logits and then normalizes.Its final output is a vector close to one-hot, of which one entity is very large and the others are very small.Output through such a softmax will lead to the loss of the relationship between classes in the process of knowledge transfer, which leads to a student network that cannot fully learn the knowledge from the teacher network.Therefore, we adopted a more general approach [12].Our task focuses on the fully supervised classification task, which means that x \u2208 X is input and y \u2208 Y is the label.Suppose that a softmax classifier is used to model a posterior predictive distribution with the input x; the predictive distribution is\n\nwhere f represents the logits outputs by the classifier parameterized with \u03b8 and T is the temperature parameter.This is a concept borrowed from Boltzmann distribution in statistical mechanics.It can be easily proved that the outputs of softmax will converge to a one-hot vector when the temperature T tends to 0, and the outputs of softmax will be softer when the temperature T tends to be infinity.Therefore, we can use a higher T to make the distribution produced by softmax soft enough when training the student model.Furthermore, we let the softmax outputs of the student model approximate the teacher model's.In such a manner, the student model can learn a lot of dark knowledge which cannot be learned from hard targets.The normal temperature T is only in the training phase.We use Mixup [46] to construct the associated samples and take the cross-entropy function used in Mixup as the cross-entropy function of our associated samples.\n\nwhere x is the original sample, x \u2032 is the sample generated in the association process and x as is the associated sample.\n\nL MCE (x as , y a , y b ; \u03b8) = \u03b1 * L CE (x as , y a ; \u03b8)\n\nwhere y a and y b are the corresponding labels of x and x \u2032 .",
            "score": 0.5959099343749563,
            "section_title": "Associative Learning for Self-Distillation",
            "char_start_offset": 15033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 299
                },
                {
                    "start": 299,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 632
                },
                {
                    "start": 632,
                    "end": 683
                },
                {
                    "start": 683,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 935
                },
                {
                    "start": 937,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1878
                },
                {
                    "start": 1880,
                    "end": 2001
                },
                {
                    "start": 2003,
                    "end": 2059
                },
                {
                    "start": 2061,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 682,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "233714221",
            "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
            "text": "KD is a technique to transfer knowledge from one model (i.e., a teacher) to another (i.e., a student), usually from a larger model to a smaller one. The student learns from more informative sources, the predictive probabilities from the teacher, besides one-hot labels. Hence, it can attain a similar performance compared to the teacher although it is usually a much smaller model, and show comparable or even better performance when the student has the same capacity as the teacher [8,48].\n\nFor an input x and a K-dimensional one-hot target y, a model produces the logit vector z(x) = [z 1 (x), \u00b7 \u00b7 \u00b7 , z K (x)], and then outputs the predicted probabilities P (x) = [p 1 (x), \u00b7 \u00b7 \u00b7 , p K (x)] by a softmax function. Hinton et al. [17] suggest to utilize temperature scaling to soften these probabilities for better distillation:\n\nwhere \u03c4 denotes a temperature parameter. By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):\n\nTherefore, the existing methods that use the soft targets for regularization can be interpreted within the framework of knowledge distillation. For example, LS [25] is equivalent to distilling the knowledge from the teacher which produces uniformly distributed probabilities on any inputs.",
            "score": 0.5944191254112605,
            "section_title": "Knowledge Distillation as Softening Targets",
            "char_start_offset": 8351,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 486,
                    "end": 489,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1403,
                    "end": 1407,
                    "matchedPaperCorpusId": "174802983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "235428017",
            "title": "KDAS-ReID: Architecture Search for Person Re-Identification via Distilled Knowledge with Dynamic Temperature",
            "text": "Input: the SuperNet weights w; the architecture parameter \u03b1; the training set D T and the evaluation set D E ; a class-balance data sampler; 1 Create a mixed operation \u00f4(i.j) parameterized by \u03b1 (i,j) for each edge (i, j); 2 Split D T into the search training set D train and the search validation set D val ; 3 while not terminated do 4 Update the temperature T of the distillation loss in Equation (7).; Knowledge distillation is wildly used in model compression because of the feature that it aims to transfer knowledge from a pre-trained teacher model to a smaller and faster student model. In the process of knowledge distillation, an appropriate soft target set was obtained by increasing the temperature parameter of the softmax layer of the teacher model. Then, for the student model to be trained, the same temperature parameter value is used to match the soft target set of the teacher model as part of the total objective function of the student model. Thus, it can induce the training of the student model and realize the transfer of knowledge. Hinton et al. [5] redefined knowledge distillation of training a student network to approach to the teacher's output after the softmax layer. As we know, neural networks typically use a softmax output layer that converts the logit, and the class probabilities q produced as follows: \n\nwhere q i is the probability of a class, l i is the logit, and l j indicate other logits. The softmax output converges to a one-hot vector as the temperature T approaches 0. As the temperature T approaches infinity, the softmax output is softer. Therefore, when training the new model, a higher temperature T can be used to make the distribution produced by softmax soft enough, and then the softmax output of the new model (at the same temperature) is approximate to the original model. The normal temperature T = 1 was used to predict after the training. The normal temperature T = 1 was used to predict after the training. A softer probability distribution over classes computed by a higher value of T.",
            "score": 0.5932647502006716,
            "section_title": "Algorithm 1: The KDAS-ReID Algorithm",
            "char_start_offset": 15247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 336,
                    "matchedPaperCorpusId": "85502762"
                },
                {
                    "start": 1070,
                    "end": 1073,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5576171875
        },
        {
            "corpus_id": "258575536",
            "title": "A Study of Compressed Language Models in Social Media Domain",
            "text": "In a traditional classification problem, a standard training objective is to minimize the cross-entropy between the model's predicted distribution and the hard targets (one-hot Comparison of the large-scale language models and their compressed versions on the irony detection task empirical distribution of the gold class). In contrast to training with a cross-entropy over the hard targets, Hugging Face trained the student model with a distillation loss over the soft target (probability distribution of the teacher's output). \n\nwhere t is the logits from the teacher and s is the logit of the student. In this case, the student model can leverage the knowledge of the teacher model. \n\nIn addition, following Hinton (Hinton, Vinyals, and Dean 2015), they also used a softmax-temperature to further expose the mass of the distribution over the classes. The formula for softmax-temperature is as follows: \n\nwhere T controls the smoothness of the output distribution and z i is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. \n\nThe entire model structure is shown in Figure 1.",
            "score": 0.592897839313269,
            "section_title": "Model Training",
            "char_start_offset": 5900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1208
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 750,
                    "matchedPaperCorpusId": "4941467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.165283203125
        },
        {
            "corpus_id": "239616535",
            "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
            "text": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions. Thus, the loss for the student is defined as:\n\nwhere L CE is a standard cross-entropy loss on hard labels, L KD is a distillation loss, which is a crossentropy loss on soft labels, and \u03bb \u2208 [0, 1] controls the balance between hard and soft targets. To be specific, knowledge distillation (Hinton et al., 2015) minimizes the Kullback-Leibler divergence between soft targets from a teacher and probabilities from a student. The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 . The distillation loss is defined as follows:\n\n, \u03c3 is a softmax function. The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels). Following Hinton et al. (2015), we scale the loss by \u03c4 2 in order to keep gradient magnitudes approximately constant when changing the temperature. We omit \u03c4 for brevity.\n\nLimitations. This KD can be applied to multimodal setups and student models in this distillation are directly trained to mimic a teacher's outputs. As a result, the student and teacher models may significantly differ in outputs with a single-modality input, i.e., modality-specific outputs, which may lead to inefficient distillation (Fig. 1). To better mimic the teacher's behaviors, we introduce a multimodal KD approach, modality-specific distillation, in the next section.",
            "score": 0.5927799936249645,
            "section_title": "Conventional Knowledge Distillation",
            "char_start_offset": 4824,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6875
        },
        {
            "corpus_id": "276938088",
            "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
            "text": "Knowledge distillation is a technique to imitate a performance that a deep learning model has, but reduce the size on another model. It applies the outputs of a model to train another model having comparable accuracy. These two distinct models are similar to the way information is delivered in human society, with one acting as the\"teacher\"and the other as the\"student\". Softmax plays a role in comparing logits generated by models with each other by converting probability distributions. It delivers the logits of a teacher to a student with compression through a parameter named temperature. Tuning this variable reinforces the distillation performance. Although only this parameter helps with the interaction of logits, it is not clear how temperatures promote information transfer. In this paper, we propose a novel approach to calculate the temperature. Our method only refers to the maximum logit generated by a teacher model, which reduces computational time against state-of-the-art methods. Our method shows a promising result in different student and teacher models on a standard benchmark dataset. Algorithms using temperature can obtain the improvement by plugging in this dynamic approach. Furthermore, the approximation of the distillation process converges to a correlation of logits by both models. This reinforces the previous argument that the distillation conveys the relevance of logits. We report that this approximating algorithm yields a higher temperature compared to the commonly used static values in testing.",
            "score": 0.5915096492892683,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73046875
        },
        {
            "corpus_id": "234336288",
            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
            "text": "Knowledge distillation (KD) was introduced by [30] as: \n\n\u2022 Train a large model that performs and generalizes very well. This is called the teacher model. \n\n\u2022 Take all the data you have and compute the predictions of the teacher model. The total dataset with these predictions is called the knowledge, and the predictions themselves are often referred to as soft targets. This is the knowledge distillation step. \n\n\u2022 Use the previously obtained knowledge to train the smaller network, called the student model. IV. TEACHER-STUDENT LEARNING Knowledge distillation starts with training a larger model, the teacher 'T'. As it is trained on a heavier platform (GPU), it achieves high performance. Then a lightweight model known as student 'S' is deployed to learn from 'T'. Now, 'S' is supposed to give comparable performance as 'T' but with less memory and more speed. \n\nTo improve knowledge transfer from teacher to student various types of methods are researched. Assuming a trained 'T' has already eliminated some label errors contained in the ground truth data, the authors in [29] treated the hard label predicted by 'T' as the underlying knowledge. While in [30], the soft label produced by 'T', i.e., the classification probabilities, are focused to provide more information to transfer. In general, knowledge is transferred from the 'T' to 'S' by minimizing a loss function in which the target is the distribution of class probabilities predicted by 'T'. This probability distribution has the correct class at a very high probability (close to '1') with all other class probabilities very close to '0'. As such, it does not provide much information beyond the ground truth labels already provided in the dataset. For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process. The distillation related work can be categorized as below:",
            "score": 0.5908875132273588,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 8373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 57,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1077,
                    "end": 1081,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70556640625
        },
        {
            "corpus_id": "257102363",
            "title": "Detecting software vulnerabilities using Language Models",
            "text": "The technique of teacher-presented knowledge distillation involves training a student model using both soft and hard labels provided by the teacher. The soft labels, or targets, are calculated by applying the softmax function to the teacher's logits with a temperature parameter T. A higher T results in softer targets. In this study, we follow the suggestion of Lan, Zhu, and Gong ( 2018) and set T to 3 for all methods. To transfer knowledge from the teacher model to the student model, the predicted distribution q of the student model is aligned with the target distribution t, both calculated with a temperature of 3. The distillation loss function, which measures the difference between the two distributions, is represented by the Kullback-Leibler (KL) divergence equation (3). The total loss for training the model with both hard and soft labels is given by equation ( 4), where the distillation loss Ldis is multiplied by T2 before combination to maintain its contribution roughly unchanged. It's important to note that the probabilities predicted by the student model q are computed from the logits with a temperature of 1 when aligning with hard labels, and with a higher temperature when aligning with soft targets. Throughout the paper, the notation q represents the T=1 version and q represents the high-temperature version.",
            "score": 0.5906773134190424,
            "section_title": "IV. DISTILLATION WITH A TEACHER MODEL",
            "char_start_offset": 6230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1338
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "249062897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "266671498",
            "title": "Insulator Defect Detection Based on ML-YOLOv5 Algorithm",
            "text": "Knowledge distillation is a technique that leverages knowledge transfer to train a lightweight student network from a larger teacher network. In this study, we apply the knowledge distillation method proposed by Mehta et al. [34] to distill the improved model. The knowledge distillation process is illustrated in Figure 7. As shown in the figure above, the output values of the Softmax function, with an added variable T, are used as soft targets to prevent minimal contributions to the loss function when the probability distribution has low entropy. The Softmax function is defined by the following equation: \n\nIn the equation, q i represents the true probability, z is a vector, and z i and z j are elements of the vector. T represents the temperature, where higher T values result in a larger entropy in the Softmax distribution, increasing the focus on training negative labels. \n\nThe overall loss consists of algorithmic and knowledge distillation losses, with \u03bb D used to balance these two types of losses. The total loss function is expressed as shown in Equation (2). \n\nIn this study, we made improvements to the YOLOv5s, a lightweight network model, and YOLOv5m, a deeper network model, as follows: \n\n(1) Integrated depthwise separable convolutions into the backbone. \n\n(2) Replaced the C3 module in the neck with an enhanced C2f_DG. \n\n(3) Replaced the feature pyramid network (FPN) with the modified feature pyramid network (MFPN). \n\nThe improved models based on YOLOv5s and YOLOv5m are, respectively, named M-YOLOv5s and M-YOLOv5m. We employed M-YOLOv5m as a teacher network to train the student model M-YOLOv5s. Through knowledge distillation, we improved model accuracy without adding extra parameters.",
            "score": 0.5906363280918445,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 20496,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1443
                },
                {
                    "start": 1446,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1717
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 229,
                    "matchedPaperCorpusId": "21713986"
                },
                {
                    "start": 1073,
                    "end": 1076,
                    "matchedPaperCorpusId": "26856801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470458984375
        },
        {
            "corpus_id": "268247468",
            "title": "Logit Standardization in Knowledge Distillation",
            "text": "The development of deep neural networks (DNN) has revolutionized the field of computer vision in the past decade. However, with increasing performance and capacity, the model size and computational cost of DNN have also \u2020 Corresponding author. . Vanilla knowledge distillation implicitly enforces an exact match between the magnitudes of teacher and student logits. It is an unnecessary side-effect because it is found sufficient to preserve the innate relations between their logits. Given the capacity gap between them, it is also challenging for a lightweight student to produce logits with the same magnitude as a cumbersome teacher. In contrast, the proposed Z-score logit standardization pre-process mitigates the side-effect. The standardized student logits have arbitrary magnitude suitable for the student's capacity while preserving the essential relations learned from the teacher. \n\nbeen expanding. Despite of a tendency that larger models have greater capacity, many efforts have been made by researchers to cut down model size without sacrificing much accuracy. In addition to designing lightweight models, knowledge distillation (KD) has emerged as a new approach to achieve this goal. It involves transferring the knowledge of a pre-trained heavy model, known as the teacher network, to a small target model, known as the student network. \n\nHinton et al. [13] firstly proposes distilling a teacher's knowledge into a student by minimizing a Kullback-Leibler (KL) divergence between their predictions. A scaling factor of the softmax function in this context, called temperature T , is introduced to soften the predicted probabilities. Traditionally, the temperature is set globally beforehand as a hyper-parameter and remains fixed throughout training. CTKD [24] adopts an adversarial learning module to predict sample-wise temperatures, adapting to varying sample difficulties. However, existing logit-based KD approaches still assume that the teacher and student should share temperatures, neglecting the possibility of distinct temperature values in the KL divergence. In this work, we demonstrate that the general softmax expression in both classification and KD is derived from the principle of entropy maximization in information theory.",
            "score": 0.5895390540318873,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 1774,
                    "end": 1778,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "264052001",
            "title": "Decentralized Heterogeneous Federal Distillation Learning Based on Blockchain",
            "text": "Knowledge distillation is a technique that extracts valuable insights from complex models and condenses them into a singular, streamlined model, thereby enabling its deployment in real-world applications. Knowledge distillation [15] is a knowledge transfer and model compression algorithm proposed by Geoffrey Hinton et al. in 2015. For a specific character, through the use of a knowledge distillation algorithm, the information of an ideally trained teacher network containing more knowledge can be transferred to a smaller untrained student network. \n\nIn this paper, the loss function L student of the student network can be defined as: \n\nLCE is the cross entropy loss function, LKL is the Kullback Leibler (KL) divergence, p student and p teacher are the outputs of the network after the softmax activation function, z is the output logits of the neural network, and T is the temperature, which is generally set as 1. The primary purpose of temperature is to reduce the loss of knowledge contained in the small probability results caused by excessive probability differences. KL divergence can measure the difference between the two models. The larger the KL divergence, the more significant the distribution difference between the models, and the smaller the KL divergence, the smaller the distribution difference between the two models. The formula of KL divergence is: \n\nwhere P(x) and Q(x) respectively represent the output of different networks after the softmax activation function.",
            "score": 0.5887109723978058,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 232,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193359375
        },
        {
            "corpus_id": "264146621",
            "title": "Efficient Apple Maturity and Damage Assessment: A Lightweight Detection Model with GAN and Attention Mechanism",
            "text": "Knowledge distillation mainly consists of two components: a teacher model and a student model. \n\nAssuming the teacher model is T and the student model is S, with input x, the output of the teacher model is T (x) and the output of the student model is S(x). The objective of knowledge distillation is to enable the student model to learn the knowledge of the teacher model by minimizing the output difference between the two models. \n\nFirst, define a temperature parameter T d for smoothing the output probability distribution of the models. Then, calculate the output probability distributions of the teacher and student models using softmax with temperature: \n\nNext, minimize the difference in probability distributions between the teacher and student models. The Kullback-Leibler (KL) divergence is used to measure this difference: \n\nIn addition, to ensure that the student model performs well on the labels, minimize the cross-entropy loss between the output probability distribution P S(x) and the true label y: \n\nFinally, linearly weight the two loss functions to obtain the final loss function: \n\nHere, \u03b1 is the weight coefficient used to balance the impact of KL divergence loss and cross-entropy loss. By minimizing the loss function L, the objective of knowledge distillation is achieved, and the student model learns the knowledge of the teacher model.",
            "score": 0.5886106590449227,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 16742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1362
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "231846715",
            "title": "HGAN: Hybrid Generative Adversarial Network",
            "text": "Knowledge distillation is mostly used in image classification problem where the output of neural network is a probability distribution over categories. The probability is calculated by applying a softmax function over logits which are the output of the last fully connected layer. Hinton et al. [19] used logits to transfer the embedded information in a teacher network to student network. In order to train a student network F to generate student logits F (x i ), a parameter called temperature T is introduced. Afterwards, the generalized softmax layer converts logits vector \n\nwhere higher temperature T produces softer probability over categories. Hinton et al. [19] proposed to minimize the KL divergence between teacher and student output as follows: \n\nIn [50] instead of forcing the student to exactly mimic the teacher by minimizing KL-divergence in Equation ( 4), the knowledge is transferred from teacher to student via discriminator in a GAN-based approach.",
            "score": 0.5880416209531626,
            "section_title": "Knowledge distillation:",
            "char_start_offset": 8816,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.317626953125
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "knowledge distillation is an effective framework to transfer knowledge from a neural network to another, which typically consists of two branches: a teacher T , which is usually a complex model or accompanied by some extra knowledge, and a student S, which is a small network that learns from the teacher (Hinton, Vinyals, and Dean 2015). In standard knowledge distillation model, the teacher network T is trained with ground-truth (i.e., hard labels) and outputs soft labels PT = softmax( ZT /\u03c4 ), in which ZT is the pre-softmax logits and \u03c4 is the temperature parameter that is normally set to 1. Similarly, one can define PS = softmax( ZS /\u03c4 ) for the student network S. In the training stage, S is required to match not only the ground-truth one-hot labels, but also the probability outputs of the teacher model: \n\nwhere L S GT is the ground-truth loss using one-hot labels, L KD is the knowledge distillation loss using teacher's soft labels and \u03bb is the coefficient to trade off such two terms. Typically, L GT is often the cross entropy loss in classification problems, and L KD is the Kullback-Leibler divergence to quantify the difference of output distribution from student to teacher: \n\nHenebry was 86 and lived in St Martinville. [0, \u2026, 0.69, \u2026, 0.17, ..., 0.14 where G is the one-hot distribution of ground-truth and G(i) is the i-th element of G. The soft distribution PT outputted by teacher model is more smooth by assigning non-zero probabilities to more than one class and yields smaller variance in gradients, which contain hidden information (also known as dark knowledge) about the relationship between different classes. By learning from soft labels, the student network inherits such dark knowledge and often has a faster convergence speed (Chen et al. 2017).",
            "score": 0.5876398852541661,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 816
                },
                {
                    "start": 819,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1195
                },
                {
                    "start": 1198,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1782
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 337,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1763,
                    "end": 1780,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41796875
        },
        {
            "corpus_id": "269922330",
            "title": "Federated Learning for Time-Series Healthcare Sensing with Incomplete Modalities",
            "text": "where the  is the Kullback-Leibler divergence [1], calculated between   , the softmax probabilities derived from the global knowledge transfer model's output, and    , the softmax probabilities obtained from the client's local model output. represents a temperature used to soften the probability distribution of the logits, facilitating the learning of more generalizable knowledge.\n\nTo summarize, all clients use classification loss (crossentropy loss) to train their local models for the primary task.Multimodal clients additionally incorporate contrastive loss (Equation 1), while both unimodal and multimodal learner clients integrate knowledge transfer loss (Equation 7) into their optimization objectives.",
            "score": 0.5873000702434479,
            "section_title": "Client Role Assignment.",
            "char_start_offset": 18777,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 240,
                    "end": 383
                },
                {
                    "start": 385,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038970947265625
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "We follow the basic notations as introduced in [31].Specifically, we consider a C-class classification problem with Y = [C] = {1, 2, \u2022 \u2022 \u2022 , C}.We denote the \"logits\" as the output scores of a sample x before applying the softmax function, which is represented as f (x) \u2208 R C .Correspondingly, the softened probability vector is denoted as p(x; \u03c4 ) = SF(f (x); \u03c4 ), where SF(\u2022; \u03c4 ) denotes the softmax function with temperature \u03c4 .f y and p y denote the ground-truth class's logit and probability, while g = [f c ] c\u0338 =y and q = [p c ] c\u0338 =y represent the vector of non-ground-truth classes' logits and probabilities.y denotes the ground-truth class.\n\nThe most standard KD [9] contains two stages of training.The first stage trains complex teachers, and then the second stage transfers the knowledge from teachers to a smaller student by minimizing the KL divergence between softened probabilities.Usually, the loss function during the second stage (i.e., the student's learning objective) is a combination of cross-entropy loss and distillation loss:\n\nwhere the upper script \"t\"/\"s\" denotes \"teacher\"/\"student\" respectively.The dark knowledge in the above distillation equation is the teacher's label, i.e., p t (\u03c4 ).In this paper, we aim to study the influence of teacher capacity on the teacher's label.That is, we majorly compare the relationships between p tlarge and p tsmall which are provided by a larger and a smaller teacher, denoted as t large and t small , respectively.Commonly, the training and test accuracy of t large will be higher than that of t small because the larger teacher has a huge capacity to capture more information.The process of KD has the effect of improving the student network's performance when compared with its independent training.An explanation for this improvement is that the dark knowledge contained in larger teachers could help the student better capture the semantic information.",
            "score": 0.5871238781153166,
            "section_title": "III. BACKGROUND AND PRELIMINARIES",
            "char_start_offset": 7996,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 52,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 650
                },
                {
                    "start": 652,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 1051
                },
                {
                    "start": 1053,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1306
                },
                {
                    "start": 1306,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1924
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "252780669"
                },
                {
                    "start": 673,
                    "end": 676,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "271855826",
            "title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation",
            "text": "However, we note that ATS (Asymmetric Temperature Scaling) [16] highlights a potential issue in knowledge distillation when the teacher model exhibits over-confidence. Specifically, if the teacher assigns excessively high scores to the correct class and lower or less varied scores to incorrect classes, applying a uniform temperature to scale the logits of the neural network can lead to reduced class discriminability. This phenomenon can adversely impact the effectiveness of the knowledge distillation process. To address this issue, ATS proposes the use of two distinct temperature values: one for the correct class and another for the incorrect classes. Inspired by the insight from ATS, we introduce a novel mechanism called Adaptive Asymmetric Temperature Scaling. Unlike ATS, which employs two pre-defined temperature parameters shared across all input samples, our method adaptively learns the temperature for each positive sample while only keep the temperature for the negative samples pre-defined. This adaptive approach aims to tailor the temperature scaling to the specific characteristics of each positive sample, thereby enhancing the effectiveness of the knowledge distillation process. \n\nHence, we further improve the score distributions of the student and teacher model: P (\u210e, ,{ , \u2032 } ) and P (\u210e, ,{ , \u2032 } ) . Specifically, we assign an adaptive temperature to each positive triple, determined by the teacher model's confidence in that triple. To quantify the teacher model's confidence in the positive triple (\u210e, , ), we utilize its score probability (i.e., P (\u210e, ,\u02c6 ) as shown in Eq. 5. Here, we set the as 1) in this paper. A higher score probability for a positive triple indicates greater confidence by the teacher model. We also think that other methods, such as using the entropy of the teacher's score distribution over the set of positive and negative samples (i.e., P (\u210e, ,{ , \u2032 } ) ), could also be explored. We consider these alternative measures as potential avenues for future research. \n\nSpecifically, we calculate the temperature (\u210e, , ) for each positive triple (\u210e, , ) using a two-layer MLP with ReLU as the activation function and the hidden dimension as 32. Formally,",
            "score": 0.5866106813210267,
            "section_title": "Client Local Training with Knowledge Distillation",
            "char_start_offset": 17360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "252780669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12408447265625
        },
        {
            "corpus_id": "267374660",
            "title": "Unleashing the Potential of Knowledge Distillation for IoT Traffic Classification",
            "text": "Knowledge distillation is a technique of ML that involves transferring knowledge from one model to another. The idea is to train a smaller, more lightweight model to mimic the behavior of a larger, more complex model by learning from the teacher's outputs [14], [15]. More specifically, the basic idea of knowledge distillation is to train the student model to predict the same outputs as the teacher model, but with fewer computational resources [16]. This is achieved by using the teacher model's outputs as ''soft targets'', also known as soft labels, during training, rather than the ''hard labels'' used in standard supervised learning. Soft targets are probability distributions over the possible outputs, rather than the actual output values themselves. By using these soft targets, the student model is encouraged to learn a more general and robust representation of the data. \n\nKnowledge distillation has been used successfully in a variety of applications, including image classification [17], object detection [18], and natural language processing [19]. It is particularly useful in scenarios where computational resources are limited, such as on mobile devices or embedded systems. Additionally, it has been shown that knowledge distillation can improve the performance of the student model, even when the teacher model is not perfect, by helping the student model learn from the teacher's mistakes [20]. \n\nThe basic mathematics of knowledge distillation can be understood through the following steps: \n\n\u2022 Softmax function: The softmax activation function is used to convert the output of a neural network (logits or scores) into probabilities. It is defined as follows: \n\nwhere z i is the input to the i th neuron, K is the total number of neurons, and e is the base of the natural logarithm. The softmax function ensures that the output probabilities sum up to one. \n\n\u2022 Temperature parameter: In knowledge distillation, the teacher model's output is modified using a temperature parameter T . The modified output probabilities are given by: \n\nwhere z i is the input to the i th neuron of the teacher model and K is the total number of neurons. The temperature parameter T controls the ''softness'' of the probabilities.",
            "score": 0.5860781037234769,
            "section_title": "A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 7577,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "212633769"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "52290108"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "195218687"
                },
                {
                    "start": 1411,
                    "end": 1415,
                    "matchedPaperCorpusId": "233296935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "201660247",
            "title": "Online Sensor Hallucination via Knowledge Distillation for Multimodal Image Classification",
            "text": "At the test time the student network is expected to perform better inference pertaining to the initial teacher guided training process. [40] recently proposed a graph-based distillation method to learn in presence of side information for action detection and action recognition tasks in videos. \n\nKnowledge distillation refers to the process of intelligently transferring the knowledge of an already trained, larger/complex learning module or an ensemble of several models to a rather smaller counterpart which is expected to be easier to comprehend and deploy. Knowledge distillation for machine learning models was first introduced by [41] while [42] presented a more general approach towards distillation within the scope of a feed-forward neural network and for the purpose of handwritten digit recognition. [41] considers the operations directly on the logits obtained from the softmax layer for learning the smaller model and minimize the squared difference between the logits from cumbersome and the smaller model. [42] scale the final logits by a temperature parameter to obtain soft thresholds and these are used to train the smaller model. They also show that matching the logits as done by [41], is a special case of the technique they propose. \n\nHowever, from the perspective of the distillation framework mentioned in [42], the knowledge gathered by a deep network is not interpreted merely as the network parameters (weights and biases) obtained during training but also includes the final logits generated by the network corresponding to the inputs. Precisely, [42] propose a much general methodology for distillation of knowledge from a larger teacher network to a drastically smaller student network by using a parameter termed as the temperature. Essentially, the output logits of teacher network are normalized with respect to the temperature parameter and subsequently the softmax operation is carried out on them in order to obtain a softer threshold which leads to an improved and more general knowledge transfer to the student network. The outputs of neural networks are typically class probabilities which are obtained by using the softmax function which converts the output logits z i , computed for each class into a probability, by comparing it to other logits. \n\nConsider a scenario where we have data from multiple modalities available during training, but while testing only one or a subset of these are available.",
            "score": 0.5857243328843712,
            "section_title": "Related work",
            "char_start_offset": 12651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2288
                },
                {
                    "start": 2291,
                    "end": 2444
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "51869487"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1201,
                    "end": 1205,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71826171875
        },
        {
            "corpus_id": "232061472",
            "title": "Heterogeneous Defect Prediction Based on Federated Transfer Learning via Knowledge Distillation",
            "text": "In this article, knowledge distillation [26] is used to achieve communication between participants and learn information from others. Knowledge distillation enables the original model of each participant to generate a set of soft targets. The average of soft targets is used to update their own private models. Neural networks usually generate class probabilities by using the softmax layer. Knowledge distillation converts the logit of each class into a probability by comparing the logits of each class with other classes. Knowledge distillation converts the logit z i of each class into a probability q i by comparing the logits of each class with other classes. where T is temperature, which is a concept introduced from the Boltzmann distribution in statistical mechanics. When T tends to 0, the softmax output will converge to a one-hot vector. When T tends to infinity, the output of softmax becomes ''soft''. Therefore, when a new model is trained, make the softmax output of the new model approximate the original model (higher T), and set T = 1 for prediction at the end of training. Essentially this is equivalent to data augmentation, adding a-priori information about the correlation between categories. The relationship learned by the original model is packed in the data, and the new model is trained with the stronger data, taking full account of the distance between the classes and the variance information within the class. The performance of the new model has been improved to achieve the effect of ''distillation''. In this article, the average of logits of multiple parties is used to realize the update of the private model in knowledge distillation. \n\nAs FIGURE 4 is a schematic diagram of knowledge distillation, the weighted average of two different loss functions is used to improve the performance of the new model. The first loss function is the cross entropy with the soft targets, the second one is the cross entropy with the correct labels. The former requires that the original model and the new model to use the same temperature, and the latter temperature is 1. \n\nEach sample has a cross-entropy gradient when it is used for matching logits. v i represents the logit (probability p i ) of the soft target, and the gradient of the logit (z i ) probability q i for the new model is:",
            "score": 0.5853360879877162,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 15321,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2315
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2406005859375
        },
        {
            "corpus_id": "272341595",
            "title": "Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation",
            "text": "The knowledge distillation loss function defines the loss function used to transfer knowledge from the teacher models to the student model during training. The Kullback-Leibler divergence, denoted as KL, is a measure from information theory that quantifies how one probability distribution diverges from a second, expected probability distribution [44]. In the context of knowledge distillation, KL is used to calculate the loss between the softened predictions of student model (proposed multimodal model) and the teacher models (subject-independent and subjectspecific models). The distillation losses for each pair of teacher-student predictions are computed using the scaled softmax function with a temperature parameter T , and are then averaged to compute the final distillation loss [11]. The complete loss function, including the trade-off between student loss and distillation loss controlled by the hyperparameter \u03b1, is defined as follows: \n\n(1) \n\nwhere: \n\n\u2022 P k teacher and P student are the softmax predictions of the teacher and student models, respectively, for each set of models. \n\n\u2022 KL is the Kullback-Leibler divergence function applied to the softened predictions of the teacher and student models to compute the distillation loss. \n\n\u2022 T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training. \n\n\u2022 \u03b1 is a hyperparameter that balances the contribution of the student's loss and the distillation loss in the overall training loss. The distillation loss is used to update the final multimodal model by minimizing the difference between the teacher and student model predictions. By transferring knowledge from the teacher models to the student model, the final multimodal model can capture both subject-independent and subject-specific features, leading to improved performance and generalization.",
            "score": 0.5852483558562601,
            "section_title": "2) KNOWLEDGE DISTILLATION LOSS",
            "char_start_offset": 31151,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 352,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "220332637",
            "title": "Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models",
            "text": "In this section, we review the basics of knowledge distillation, which is used in the next section in one of the defensive mechanisms. Knowledge distillation is the procedure to distill the knowledge learned in one model (teacher model) into another model (student model) which is usually smaller in size. \n\nThe student model, which mimics the performance of teacher model, can then be deployed to save computational cost. Dark Knowledge [26] is proposed by Hinton, et al., to improve the model's distillation performance. Instead of using the predicted hard labels as training targets, dark knowledge uses the predicted probability p = [p 1 , p 2 , ..., p c ] (\u2200i, 0 < p i < 1, and p i is the probability of a sample being predicted to belong to class i) of the teacher model as the training target. The probability scores are also known as soft targets, in contrast to the hard targets of the original {0, 1} labels. Dark knowledge argues that the probability scores capture the relationship between classes. Taking the MNIST dataset for example, digits 1 and 7 appear more similar than 1 and 8, so the prediction of a hand-written digit with a ground truth of 1 is usually classified with a higher probability of 7 than 8. By utilizing the relationship between classes, the teacher model communicates more information to the student model, which helps train the student model to better mimic the complex non-linear function learned by the teacher model. \n\nAn issue with using the standard softmax function in (2) in the final output layer of the baseline DNN is that it causes the output probability scores to concentrate on one class, thereby reducing the correlation between the output classes. To increase the output class correlation, Hinton, et al., [26] propose using a temperature parameter, T , to normalize the logit value. Higher values of T cause the probability scores to be distributed more evenly thereby better reflecting the correlation between classes. Specifically,",
            "score": 0.5851221489949312,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 15258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29052734375
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "To set the learning rate for our default knowledge distillation based transfer approach using KL divergence (as in Section 4.1), we conducted a parameter search over a set of 33 teacherstudent pairs randomly selected from the timm Wightman (2019) library, with learning rates lr \u2208 {1e-2, 1e-3, 1e-4, 1e-5}, for which we found a learning rate of 1e-4 to generally work best, albeit regardless of chosen values, the average transfer delta was consistently negative. \n\nFollowing Section 4.1, we also extend KL distillation with a cross-entropy classification loss. In this case, hyperparameters were determined over a grid comprising learning rates lr \u2208 {1e-2, 1e-3, 1e-4, 1e-5}, softmax temperatures T \u2208 {0.1, 1, 4, 10} and weightings \u03bb \u2208 {0.05, 0.1, 0.25, 0.5}. Again, we found that a learning rate of 1e-4 was the most effective on average, but found particular variance in the weighting \u03bb, where we observed that a larger \u03bb value -placing higher emphasis on distillation -is better suited for transferring knowledge from a stronger teacher to a weaker student, while a smaller \u03bb seems to be preferable when transferring knowledge from a weaker teacher to a stronger student. This further highlights the trade-off between knowledge gain and retention, where for a weaker teacher, retention plays a much more crucial part to ensure overall high performance, as student knowledge is overwritten. \n\nFor the softmax temperature, we found that a small temperature of 0.1 limits the decrease in the student's performance when transfering from a weaker teacher model, but also limiting the knowledge transfer in general. This results in only small increases in the student's performance even when transferring from a stronger teacher model. Hinton et al. (2015) propose to use a larger temperature of 4 to match soft targets to better represent smaller probabilities in the output of a single sample. However, we do not find larger temperatures to benefit the transfer performance.",
            "score": 0.5846888092682687,
            "section_title": "A.1 IMPLEMENTATION OF DISTILLATION-BASED KNOWLEDGE TRANSFER VARIANTS",
            "char_start_offset": 32246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1974
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321533203125
        },
        {
            "corpus_id": "233210322",
            "title": "Data-Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015) is a popular model compression approach by training a compact student model (S) to mimic the softmax outputs of a pre-trained cumbersome teacher model (T ). Let W T and W S be the parameters of the teacher and the student model, respectively. The softmax outputs (class probabilities) of the teacher and the student model are represented as P T = T (x, W T ) = softmax(a T ) and P S = T (x, W S ) = softmax(a S ), respectively, where x is the training sample and a is the pre-softmax activation of a model. During the KD process, a temperature \u03c4 is usually used for softening the class probability (Eq. (1)). A larger \u03c4 can produce softer class probabilities so that information with higher entropy is enclosed in the targets, and the student model can be trained with a larger learning rate and converge faster.\n\n(1) W S can be learned by minimizing the loss function in Eq.\n\nwhere y is the one-hot ground truth vector, L CE (\u00b7) is the cross-entropy loss, and \u03bb c is a scaling factor that balances the importance of the two losses.",
            "score": 0.5845916472474367,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 11010,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1510009765625
        },
        {
            "corpus_id": "234353710",
            "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning",
            "text": "Bengio et al. [14] first propose a progressive learning paradigm which organizes the training data from easiest to hardest. In active learning [38,39], similar indicators have been used to guide decisions about which data point to label next. In general, we all rely on some indicators to evaluate the training data. However, to our knowledge, applying this indicator in terms of the consistency regularization to SSL is new, and has not been explored before. \n\nKnowledge distillation. Hinton et al. [40] apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label. However, the method needs to set the value of temperature empirically, which is shared by all training samples. Our method can automatically define the temperature of each training sample according to its uncertainty, and use its own temperature to decide how much influence it has on training the student model.",
            "score": 0.58406965573035,
            "section_title": "Recently, Qiao et al. propose deep Co-Training",
            "char_start_offset": 11142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1267
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "127253432"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6943359375
        },
        {
            "corpus_id": "258714078",
            "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
            "text": "Following the knowledge distillation process, a teacher model shares its knowledge with student models so that the students not only learn from the dataset but also directly from the teacher. In many real-life applications, especially in mobile or edge devices with computational and hardware constraints, deploying the lighter student model (instead of the larger teacher model) can be a pragmatic solution. At present, Hinton's knowledge distillation [26] can be considered the most widely used model compression technique due to its efficacy and simplicity. The distillation process softens output logits of the teacher model to transfer information from the teacher to the student model. The student model uses the logit distribution of the teacher model across all training instances as an altered training set. Soft outputs of the teacher model act as the ground truth (supervision) while learning the student model. This auxiliary information provided by the teacher (contained in the soft outputs) is known as dark knowledge [26,27]. Dark knowledge also contains helpful information of class similarity in a dataset that is helpful for the model to learn [26]. For proper knowledge transfer from the teacher to the student, the softening process requires tuning a hyperparameter named Temperature, \u03c4. Hinton et al. [26] used a \u03c4 value between 2 and 20 inclusively. However, \u03c4 can still be increased to test with softer outputs. An improper value of \u03c4 produces inefficient models with a lackluster performance. Therefore, to attain a suitable student model, knowledge distillation frameworks must iterate several times with different values of the hyperparameter \u03c4. Preparations and tuning around such frameworks raise substantial financial and environmental costs by engaging longer training time and higher carbon footprint. In this paper, we estimate the overhead of existing methods and propose a mitigation process by using a stochastic process related to a hyperparameter instead of a grid search [28] (see Fig 1). \n\nTo tackle these problems, this paper proposes a stochastic technique of assigning the hyperparameter \u03c4. This process of assigning a stochastic temperature facilitates training while drastically reducing costs and preserving the performance of the assembled student model. This system of training a model compression system can achieve comparable performance to a finetuned system while reducing the time, power costs, and carbon footprint.",
            "score": 0.5838536242897927,
            "section_title": "Introduction",
            "char_start_offset": 2075,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "750954"
                },
                {
                    "start": 2010,
                    "end": 2014,
                    "matchedPaperCorpusId": "15700257"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "233407431",
            "title": "Self-distillation with Batch Knowledge Ensembling Improves ImageNet Classification",
            "text": "Knowledge distillation [21] can be considered as regularizing the training of the student network using soft targets that carry the \"dark knowledge\" of the teacher network. \n\nThe student network generally consists of a backbone encoder F and a classifier C to perform classification. For each training sample x, its logit vector is encoded as z = C(F (x)). The predictive probability vector p \u03c4 can be obtained via a softmax function on the logits, i.e., the probability of class k can be formulated as \n\nwhere \u03c4 is a temperature hyper-parameter, and K is the number of total classes. Let y \u2208 {1, . . . , K} denotes the ground truth label and q \u03c4 is the soft target produced by the teacher network. The cross-entropy loss and the KL divergence between the predictions and soft targets are minimized jointly to train the student via \n\nwhere p(y) denotes the probability normalized without a temperature, and \u03bb weights the two terms. \n\nRecent works [14,28,40,47] found that ensembling diverse \"dark knowledge\" from multiple teachers or students can form better soft targets, leading to better final performance (see Figure 2 (a)&(b) for details). However, this strategy would increase much more computational and memory overhead to enable multiple networks or branches training. To tackle the challenge, we introduce batch knowledge ensembling in a single network via self-distillation, as illustrated in Figure 2 (c).",
            "score": 0.5837932387881867,
            "section_title": "Revisit of Knowledge Distillation",
            "char_start_offset": 8697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "219965421"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 954,
                    "end": 957,
                    "matchedPaperCorpusId": "54447578"
                },
                {
                    "start": 957,
                    "end": 960,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "235732222",
            "title": "A Lottery Ticket Hypothesis Framework for Low-Complexity Device-Robust Neural Acoustic Scene Classification",
            "text": "Teacher-Student Learning (TSL), also named as Knowledge Distillation (KD), is a widely investigated approach for model compression [17,18]. Specifically, it transfers knowledge from a large and complex deep model (teacher model) to a smaller one (student model). The main idea is to establish a framework that makes the student directly mimicking the final prediction of teacher. Formally, the soften outputs of a network can be computed by p = sof tmax( \u03b1 \u03c4 ), where \u03b1 is the vector of logits (pre-softmax activations) and \u03c4 is a temperature parameter to control the smoothness [17]. Accordingly, the distillation loss for soft logits can be written as the Kullback-Leibler divergence between the teacher and student soften outputs. In this work, we followed the approaches in [7] to build a large two-stage ASC system, serving as the teacher model. Then a teacher-student learning method is used to distill knowledge to a low-complexity student model, as shown in Figure 1 (b).",
            "score": 0.5820351581810405,
            "section_title": "Teacher-Student Learning (TSL)",
            "char_start_offset": 3409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 979
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 138,
                    "matchedPaperCorpusId": "704391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15380859375
        },
        {
            "corpus_id": "277632659",
            "title": "Crowd counting at the edge using weighted knowledge distillation",
            "text": "Knowledge distillation (KD) in neural networks was proposed by Hinton et al. in 5 which is inspired by the work 48 demonstrating that \"knowledge learned by a large ensemble of models can be transferred to a single model\". Hinton et al. 5 introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case. However, the method was limited to supervised learning. To extend the KD concept to any type of deep learning model, Romero et al. 6 proposed feature-based KD to learn the intermediate representation (activation maps). They proposed to match the intermediate layer outputs of the teacher and student models to guide the student model. Several works extended this model to propose different methods to match feature activation maps such as using attention maps 49 , neuron selectivity 50 , and factors 51 . Similar to the response-based KD in which the knowledge is distilled from the output layer, in feature-based KD the knowledge is distilled from a single intermediate layer. \n\nIn contrast, 7 proposed relations-based KD in which a Gram matrix is calculated as an inner product between the feature maps of two layers. The model has been further extended by 52 to learn from more than one teacher. \n\nAuthors in 53 highlighted that the transfer of knowledge is impacted by the difference in the capacity of the teacher and student models. Knowledge distillation from multiple teacher networks is proposed in 54 .",
            "score": 0.5801159363881268,
            "section_title": "Related work",
            "char_start_offset": 9297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 81,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 112,
                    "end": 114,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 236,
                    "end": 237,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1477,
                    "end": 1479,
                    "matchedPaperCorpusId": "52012952"
                },
                {
                    "start": 1670,
                    "end": 1671,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1836,
                    "end": 1838,
                    "matchedPaperCorpusId": "13745134"
                },
                {
                    "start": 1889,
                    "end": 1891,
                    "matchedPaperCorpusId": "212908749"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "268247468",
            "title": "Logit Standardization in Knowledge Distillation",
            "text": "Knowledge distillation [13] is designed to transfer the \"dark\" knowledge from a cumbersome teacher model to a lightweight student model. By learning from the soft labels of teacher, student can achieve better performance than training on hard labels only. The traditional method trains a student by minimizing a difference such as KL divergence between its predicted probability and the teacher's. The prediction of probability is commonly approximated by the softmax of logit output. KD algorithms can be classified into three types, i.e., logit-based [3,13,17,22,28,47,49,50], feature-based [1,4,5,10,12,23,25,27,31,37,44], and relation-based [15,19,29,30,39,43] methods. \n\nA temperature is introduced to flatten the probabilities in logit-based methods. Several works [2,13,26] explore its properties and effects. They reach an identical conclusion that temperature controls how much attention student pays on those logits more negative than average. A very low temperature makes student ignore other logits and instead mainly focus on the largest logit of teacher. However, they do not discuss why teacher and student share a globally predefined temperature. It was unknown whether temperature can be determined in an instance-wise level until CTKD [24] proposed predicting sample-wise temperatures by leveraging adversarial learning. However, it assumes that teacher and student should share temperatures. It was still undiscovered whether teacher and student can have divergent temperatures. ATKD [9] proposes a sharpness metric and chooses adaptive temperature by reducing the gap between teacher and student. However, their assumption of a zero logit mean relies on numerical approximation and limits its performance. Additionally, they do not thoroughly discuss where the temperature is derived from and whether distinct temperatures can be assigned. In this work, we provide an analytical derivation based on the entropy-maximization principle, demonstrating that students and teachers do not necessarily share a temperature. It is also found sufficient to preserve the innate relationship of prediction, instead of exact logit values of teacher [15].",
            "score": 0.5797170981186508,
            "section_title": "Related Work",
            "char_start_offset": 4999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2161
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "260933721"
                },
                {
                    "start": 562,
                    "end": 565,
                    "matchedPaperCorpusId": "222124879"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 596,
                    "end": 598,
                    "matchedPaperCorpusId": "247762862"
                },
                {
                    "start": 598,
                    "end": 600,
                    "matchedPaperCorpusId": "233296935"
                },
                {
                    "start": 600,
                    "end": 603,
                    "matchedPaperCorpusId": "258309453"
                },
                {
                    "start": 603,
                    "end": 606,
                    "matchedPaperCorpusId": "102483181"
                },
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "236912875"
                },
                {
                    "start": 609,
                    "end": 612,
                    "matchedPaperCorpusId": "248986384"
                },
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 618,
                    "end": 621,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 645,
                    "end": 649,
                    "matchedPaperCorpusId": "248986690"
                },
                {
                    "start": 649,
                    "end": 652,
                    "matchedPaperCorpusId": "245006036"
                },
                {
                    "start": 652,
                    "end": 655,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 655,
                    "end": 658,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 658,
                    "end": 661,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18310546875
        },
        {
            "corpus_id": "274436184",
            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
            "text": "Knowledge distillation seeks to transfer knowledge from a cumbersome, complicated teacher model to a more lightweight student model, enabling the student to achieve performance comparable to the teacher. In this work, we emphasize the transfer of fair and rich knowledge during the distillation process, rather than solely focusing on model compression. \n\nTo facilitate knowledge transfer, the knowledge distillation approach was designed by Hinton et al. (Hinton et al., 2015) to align the softened outputs generated by both the teacher and student models. Two supervisory signals are utilized in the training of the student model: (1) Hard Labels: The true labels from the training dataset. (2) Soft Labels: The label predictions of the teacher model. Formally, let Z tea represent the output logits from the teacher model, and Z stu denote the output logits from the student model, the loss to be minimized is defined as: \n\nwhere L CE denotes the cross-entropy loss and the hyperparameter \u03b1 regulates the relative contribution of the two loss functions. The knowledge distillation loss, L KD , is given by: \n\nwhere H KL is the KL-divergence, and \u03c4 is a temperature hyperparameter that regulates the smoothness of the two logit distributions by appropriately scaling them. A higher temperature makes the distribution flatter, generating softer predictions; a lower temperature enlarges the difference between the two distributions, generating harder predictions.",
            "score": 0.5789240748563089,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 477,
                    "matchedPaperCorpusId": "3144218"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11962890625
        },
        {
            "corpus_id": "269157594",
            "title": "Comprehensive Survey of Model Compression and Speed up for Vision Transformers",
            "text": "Knowledge distillation techniques, such as soft and hard distillation, facilitate the transfer of knowledge from a complex 'teacher' model to a simpler 'student' model.Soft distillation focuses on minimizing the Kullback-Leibler (KL) divergence between the softened logits (or outputs) of both the teacher and student models.This is formally captured by the distillation objective:\n\nwhere LCE denotes the cross-entropy loss, \u03c8 represents the softmax function, Zt and Zs are the logits from the teacher and student models, respectively, \u03c4 is the temperature parameter enhancing softness of distributions, and \u03bb balances the contributions of the KL divergence and the cross-entropy loss.Conversely, hard distillation uses the teacher's predictions as definitive labels for training the student, simplifying the process by directly comparing the student's predictions against these labels:\n\nwhere yt = argmaxcZt(c) represents the hard label decision by the teacher model.\n\nThe DeiT [27] method introduces a novel approach specific to Transformers, incorporating a 'distillation token' into the architecture that functions analogously to the class token but focuses on mimicking the teacher's predictions.This mechanism allows for a direct interaction between the distillation token and other components through the self-attention layers, demonstrating superior performance in distillation.Our experimental setup involves applying the DeiT framework for knowledge distillation on the CIFAR dataset, adjusting for computational resource constraints.",
            "score": 0.578731207331808,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 381
                },
                {
                    "start": 383,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1201
                },
                {
                    "start": 1201,
                    "end": 1386
                },
                {
                    "start": 1386,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 979,
                    "end": 983,
                    "matchedPaperCorpusId": "229363322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319091796875
        },
        {
            "corpus_id": "257427325",
            "title": "Hierarchical Network with Decoupled Knowledge Distillation for Speech Emotion Recognition",
            "text": "In 2015, Hinton et al. [16] proposed the concept of Knowledge Distillation (KD) to transfer knowledge by minimizing the Kullback-Leibler divergence between the predicted logit of the teacher and student models. Knowledge distillation was subsequently validated on tasks such as speech recognition and image recognition. KD constructively introduces the distillation temperature T into the softmax equation, along with the classification probability Q = [q 1 , q 2 , . . . , q i , . . . , q C ] \u2208 R 1\u00d7C ; the probability of the i-th class is written as q i , the number of classes is written as C, the logit of the i-th class is written as z i , such that q i is calculated as follows: \n\ntypically, T is set to 1, with one-hot classification used for the label. However, this form of labeling makes the training of the neural network too absolute, meaning that information contained in the incorrect classes is lost. In contrast, when the temperature of T is set to a value greater than 1, the classes whose probability was previously suppressed to 0 will also have a small probability proportional to the distillation temperature. However, this method still limits the potential of logit distillation, as will be discussed in section 3.3.",
            "score": 0.5778413298755708,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1238
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "276929258",
            "title": "Generalized Kullback-Leibler Divergence Loss",
            "text": "A larger temperature can smooth the predicted scores from teacher models in knowledge distillation. Thus, it can potentially alleviate the hard training convergence problem. However, the larger temperature will also eliminate the useful \"dark knowledge\", i.e., class relationships, for transfer learning. As listed in Table 2, performance on \"Many\" classes with a temperature of 2.0 or 1.5 is obviously better than that with a temperature of 1.0, confirming that the smoothness can facilitate training convergence of classes with high predicted scores in knowledge distillation. Meanwhile, GKL-KD achieves much better overall performance than KL-KD with various temperatures, demonstrating the superiority of the weight function \u03c6 * (x m , x n ) design in GKL loss.",
            "score": 0.577516730562009,
            "section_title": "Analysis on Training Convergence for Knowledge Distillation.",
            "char_start_offset": 20630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 765
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037261962890625
        },
        {
            "corpus_id": "258832674",
            "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?",
            "text": "#Syn Images Epoch Acc (%) GLIDE [40] 200K 100 44.58 SD [50] 200K 100 39.95 DiT [46] 200K 100 54.64 Knowledge Distillation. Originally proposed by Hinton et al. [22], knowledge distillation aims to transfer the knowledge of a pretrained heavy teacher model to a lightweight student model. After the distillation, the student can master the expertise of the teacher and be used for final deployment. Specifically, the Kullback-Leibler (KL) divergence loss is utilized to match the output distribution of two models, which can be simply formulated as follows: \n\nL kd (q t , q s ) = \u03c4 2 KL(\u03c3(q t /\u03c4 ), \u03c3(q s /\u03c4 )), (8) where q t and q s denote the logits predicted by the teacher and student. \u03c3(\u2022) is the softmax function and \u03c4 is the temperature hyperparameter which controls the softness of probability distribution.",
            "score": 0.5774795875867946,
            "section_title": "Method",
            "char_start_offset": 10489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 814
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "245335280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26171875
        },
        {
            "corpus_id": "272884937",
            "title": "Enhancing Romanian Offensive Language Detection Through Knowledge Distillation, Multi-task Learning, and Data Augmentation",
            "text": "KD. KD [13] aims to reduce the size of a larger model, referred to as the teacher, by transferring its knowledge to a smaller, faster, and similarly performing model known as the student. The fundamental principle behind this approach revolves around compressing the knowledge contained within the teacher model, with the student learning to mimic his predictions [3]. \n\nThe temperature parameter T is critical in the knowledge distillation process. It serves as a mechanism to control the level of confidence in the predictions made by the teacher model. A higher temperature leads to a more uniform probability distribution, allowing the student to explore diverse options, while a lower temperature accentuates the differences between classes, focusing on the information deemed more relevant by the teacher (i.e., exploitation) [13,20]. The temperature adjustment is applied at the softmax function, which computes the probability distribution over classes. Given the input x i , the probability p i,k for class k is computed based on the network logits z i as follows [13]: \n\nThe knowledge distillation architecture involves training the teacher and the student neural networks on the same dataset. A hyperparameter \u03b1 controls the interpolation of partial losses, considering the teacher's soft predictions and the ground truth labels. The distillation loss is calculated using the cross-entropy loss (L CE ) between the ground truth and the hard predictions and the Kullback-Leibler (KL) divergence loss (L KL\u2212KD ) between the soft labels and soft predictions [20,16]: \n\nwhere p t (x i , T ) represents the softmax outputs of the teacher model, and p s (x i , T ) represents the student's softmax output, both computed using Eq. 6. MTKD. In natural language processing, the simultaneous learning of multiple tasks presents a considerable challenge. MTL addresses this challenge by training a single model to solve numerous tasks concurrently. However, optimizing a model for various tasks with different complexities can result in performance imbalances, where specific tasks dominate while others suffer [18]. \n\nTo tackle the performance imbalance problem in MTL scenarios, multi-task learning with knowledge distillation (MTKD) has been proposed by [7].",
            "score": 0.574999435562253,
            "section_title": "Knowledge Distillation Models",
            "char_start_offset": 12850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2116
                },
                {
                    "start": 2119,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "243865194"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "243865194"
                },
                {
                    "start": 1570,
                    "end": 1573,
                    "matchedPaperCorpusId": "233189631"
                },
                {
                    "start": 2111,
                    "end": 2115,
                    "matchedPaperCorpusId": "220514465"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3720703125
        },
        {
            "corpus_id": "230770298",
            "title": "Modality-specific Distillation",
            "text": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions. \n\nThus, the loss for the student is defined as: \n\nwhere L CE is a standard cross-entropy loss on hard labels, L distill is a distillation loss, which is a crossentropy loss on soft labels, and \u03bb \u2208 [0, 1] controls the balance between hard and soft targets. \n\nTo be specific, knowledge distillation (Hinton et al., 2015) minimizes Kullback-Leibler divergence between soft targets from a teacher and probabilities from a student. The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 . The distillation loss is defined as follows: \n\n(2) where \n\n\u03c3 is a softmax functoin. The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels). Following (Hinton et al., 2015), we scale the loss by \u03c4 2 in order to keep gradient magnitudes approximately constant when changing the temperature. We omit \u03c4 for brevity. Limitations. This KD can be applied to multimodal setups and student models in this distillation are directly trained to mimic a teacher's outputs without access to teacher's modality-specific behaviors. As a result, the student and teacher models may significantly differ in their modality-specific outputs, which leads to inefficient distillation. To better mimic the teacher's behaviors, we propose modality-specific distillation in the next section.",
            "score": 0.5748762676900064,
            "section_title": "Conventional Knowledge Distillation",
            "char_start_offset": 6139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 459
                },
                {
                    "start": 462,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "246867020",
            "title": "Meta Knowledge Distillation",
            "text": "In the original formulation of knowledge distillation (KD) (Hinton et al., 2015), a student model is trained to match the targets provided by a teacher model, which tries to provide more information than the ground truth one-hot labels. The targets are usually softened by a fixed temperature in the softmax function and the entropy increases as the temperature increases. \n\nFormally, we consider a student model S with parameters \u03b8 s for distilling the teacher. We denote the output logits from the teacher and the student as z s and z t , respectively. The student is trained to minimize the cross-entropy (CE) loss between its predicted probability p s and the teacher's output probability p t , where p s and p t are softened by their corresponding temperatures \u03c4 s and \u03c4 t respectively. \n\nwhere p s = softmax(z s /\u03c4 s ), p t = softmax(z t /\u03c4 t ). \n\nIn prior arts, \u03c4 s and \u03c4 t are manually chosen as the same constant greater than 1. However, as we discussed in Sec. 2, the temperatures play a key role in distillation, and the optimal temperatures vary in different distillation setups, which are difficult to tune manually. Besides, the temperatures control the degree of softness of the targets provided by the teacher. Using a fixed temperature for different distillation setups, including teach/student architectures, augmentation types, datasets, can be sub-optimal. Intuitively, the temperatures are expected to be adjusted according to according to the different training setups. \n\nWe take an explicit approach to tackle this problem: we aim at meta-learning the temperatures of both the teacher and student to dynamically adjust the softness of the learning targets according to the student's on-the-fly performance on the validation set, \n\nwhere L t and L v denote the training and validation losses, and \u03c6 = {\u03c4 s , \u03c4 t } are meta-parameters (temperatures) to be optimized on the meta level.",
            "score": 0.57435495990281,
            "section_title": "Problem Formulation",
            "char_start_offset": 9833,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1905
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59326171875
        },
        {
            "corpus_id": "270737904",
            "title": "On Reducing Activity with Distillation and Regularization for Energy Efficient Spiking Neural Networks",
            "text": "In this paper, we only focus on Response-based KD but we explore various techniques listed and explained below :\n\n-Mean Squared Error (MSE) Knowledge Distillation: this approach involves employing MSE in the KD loss function.The objective is to minimize the squared differences between the logits generated by the teacher and student networks.The MSE loss function is defined as follows:\n\nHere, logits t and logits s represent the predicted logits (presoftmax activations) of the teacher and student models, respectively, for the x i input.\n\n-Soft Targets (ST) Knowledge Distillation: the teacher's outputs are used as new labels (they are named Soft Targets or soft labels).In this method, a temperature \u03c4 is applied to the Softmax function of both the student and the teacher.The temperature parameter controls the smoothness of the probability distribution and therefore impacts the degree of knowledge transfered from the teacher to the student.\n\nWhere p(x i , \u03c4 ) is the probability distribution of the x i input and \u03c4 is the temperature parameter.Accordingly, the loss term for ST KD is defined as follows:\n\nWhere p t and p s represent the probability distributions of the teacher and student models, respectively, for the x i input while D KL denotes the Kullback-Leibler divergence.\n\n-Soft Targets Knowledge Distillation with Heterogeneous Temperature (ST-HET KD): this technique proposes varying the temperature between the teacher and the student.The idea is to leverage the potential benefits of temperature heterogeneity in optimizing neural activity while preserving the performance.For ST-HET KD, the distillation loss is defined as:\n\nIn addition to Eq. ( 4), \u03c4 t and \u03c4 s represent the temperature of the teacher and student models, respectively.The distillation loss, whether MSE or ST, is added to the total loss function during training.The total loss shown in Eq. 1 becomes a weighted combination of the standard loss and the distillation loss (Eq.L KD can be either Eq. 2, or Eq. 4 or Eq. 5).",
            "score": 0.5739896146910033,
            "section_title": "III. METHODOLOGY A. Response-based Knowledge Distillation",
            "char_start_offset": 7946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 114,
                    "end": 225
                },
                {
                    "start": 225,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 387
                },
                {
                    "start": 389,
                    "end": 540
                },
                {
                    "start": 542,
                    "end": 675
                },
                {
                    "start": 675,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 949
                },
                {
                    "start": 951,
                    "end": 1053
                },
                {
                    "start": 1053,
                    "end": 1112
                },
                {
                    "start": 1114,
                    "end": 1290
                },
                {
                    "start": 1292,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1596
                },
                {
                    "start": 1596,
                    "end": 1647
                },
                {
                    "start": 1649,
                    "end": 1760
                },
                {
                    "start": 1760,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 1966
                },
                {
                    "start": 1966,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "234557589",
            "title": "A Distilled BERT with Hidden State and Soft Label Learning for Sentiment Classification",
            "text": ", where z t and z s represent the outputs of the last transformer of the teacher and the student model, respectively, namely the last hidden state of them. T represents temperature, which affects the smoothness of the outputs. In our study, we set T = 1 and use it as the standard softmax function, because our preliminary results show that other T values lead to similar results.  is Kullback-Leibler divergence between  and  , where s t and s s are the soft labels (logits) of the teacher and the student model, respectively. \n\nThe overall learning objective for knowledge distillation is denoted by their sum:",
            "score": 0.5735462539094857,
            "section_title": "\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65",
            "char_start_offset": 2510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 527
                },
                {
                    "start": 530,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24169921875
        },
        {
            "corpus_id": "252683187",
            "title": "To Improve Is to Change: Towards Improving Mood Prediction by Learning Changes in Emotion",
            "text": "In addition to model fusion and end-to-end optimisation, we employ knowledge distillation [12] to transfer knowledge from a larger teacher network to a smaller student network. Fig. 4 (right) presents the Teacher-Student Network, where the teacher and student networks are as described in Sec. 3.2 and 3.1, respectively. The pre-trained teacher network utilizing both mood and \u0394 labels (2-CNN+MLP) distills knowledge, while training the student network (1-CNN) only requires mood labels. As inference is again based on the student network, \u0394 labels are not required during test time. \n\nThe student model's SoftMax layer involves a hyper-parameter called the temperature  , which controls the smoothness of the output probabilities. Setting  > 1 increases the weight of smaller logit 1 values, thus revealing more information about inter-class relations than the one-hot labels [12]. The Kullback-Leibler (KL) divergence is used to compute the distillation loss, while sparse categorical cross-entropy is used as the student loss function. The overall loss of the teacher-student model    is the weighted sum of the student loss  stu and distillation loss  dis : \n\nwhere  is a training hyper-parameter. The fine-tuned hyperparameters include a batch size \u2208 {16, 64, 128},  \u2208 {3, 5, 7} and  \u2208 {0.05, 0.1, 0.15, 0.2, 0.25, 0.3}.",
            "score": 0.5734215918293573,
            "section_title": "Teacher-Student Network",
            "char_start_offset": 11693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1161
                },
                {
                    "start": 1164,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39111328125
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": ", we can use the temperature scaling [14] in Softmax to obtain softer probability predictions(p i (x : T)) that result in a better distillation of knowledge compared to the hard labels of\n\nwhere T refers to the temperature parameter.\n\nThe temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures.\n\nVanilla form of knowledge distillation originally proposed in [14] in classification problem learns with a combined total loss of hard and soft labels as:\n\nHere, z s and z t are soft logits of the student and teacher network, respectively, with the temperature parameter T in the Softmax function \u03c3, y is the ground truth, H is the loss function, and \u03b1 is the hyperparameter for the weight of the distillation. \u03b1 controls the weight",
            "score": 0.5734161849197388,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 26503,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1914,
                    "end": 1918,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "268681594",
            "title": "Attention is all you need for boosting graph convolutional neural network",
            "text": "Knowledge distillation is a technique for enhancing the performance of student models through the transfer of advanced and complex knowledge from large and high-performance teacher models.By feeding the output of the teacher model as a supervised signal to the student model, the student model can leverage the experience and knowledge of the teacher model to enhance its ability to learn the distinguishing features and patterns of the task, ultimately improving overall performance.The knowledge distillation first proposed in [3] by having the student output label probabilities to match the teacher output soft label probabilities, with a specific loss function formula of Eq. ( 5):\n\nwhere H denotes the cross-entropy loss or KL divergence, and T 1 denotes the distillation temperature, which allows for smoother labels for distillation.z T and z S are the labeling probabilities of teachers and students, respectively.",
            "score": 0.5725385563761534,
            "section_title": "Knowledge distillation",
            "char_start_offset": 19270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 686
                },
                {
                    "start": 688,
                    "end": 841
                },
                {
                    "start": 841,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0144500732421875
        },
        {
            "corpus_id": "231925118",
            "title": "Learning Student-Friendly Teacher Networks for Knowledge Distillation",
            "text": "We propose a novel knowledge distillation approach to facilitate the transfer of dark knowledge from a teacher to a student. Contrary to most of the existing methods that rely on effective training of student models given pretrained teachers, we aim to learn the teacher models that are friendly to students and, consequently, more appropriate for knowledge transfer. In other words, at the time of optimizing a teacher model, the proposed algorithm learns the student branches jointly to obtain student-friendly representations. Since the main goal of our approach lies in training teacher models and the subsequent knowledge distillation procedure is straightforward, most of the existing knowledge distillation methods can adopt this technique to improve the performance of diverse student models in terms of accuracy and convergence speed. The proposed algorithm demonstrates outstanding accuracy in several well-known knowledge distillation techniques with various combinations of teacher and student models even in the case that their architectures are heterogeneous and there is no prior knowledge about student models at the time of training teacher networks.",
            "score": 0.5724258873041317,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0262603759765625
        },
        {
            "corpus_id": "198179767",
            "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching",
            "text": "Deep neural networks can generate features from any layers. The knowledge distillation technology usually uses different layer's features or outputs as knowledge to transfer from teacher network to student network. The high layer features are mostly closer to the object parts for performing a specific task. However, the lower layer features are usually the typical generic features (i.e., edges and corners). Therefore, we can take the features generated from the lower parts of the DNNs as the intermediate hints. All these features contain valuable dark knowledge which can be transferred to guide student network's training process. \n\nLet us respectively denote x and y as the input of the DNNs and one-hot labels of our architecture. We let P T be the teacher network's softmax output as P T = sof tmax(a T ). Specifically, P T is obtained by applying softmax function on the un-normalized log probability values a T . Similarly, the same image fed to the student network to get the predictions P S = sof tmax(a S ). In the intermediate layers of the DNN, we denote the activation tensor A \u2208 R C\u00d7X\u00d7W with its corresponding layer. The pairs of teacher and student attention maps are denoted as F (A j T ) and F (A j S ) in vectorized form respectively [21]. And the standard cross entropy is denoted as H. Hinton et al. [19] extend previous works by training a compact student network to mimic the output probability distribution of teacher network. They name this informative and representative knowledge as dark knowledge. It contains the relative probabilities of 'incorrect' classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 the student network will be trained to optimize the following loss function: \n\nMishra et al. [22] propose a new perspective view to jointly train a teacher network (full-precision) and a student network (low-precision) from scratch using knowledge distillation. The total loss function is as following: L(x; W t , W s ) = \u03b1H(y true , P T )+\u03b2H(y true , P S )+\u03b3H(a T , P S ) \n\n(2) In this case, the teacher and student network both train from scratch.",
            "score": 0.5723435810354849,
            "section_title": "B. Formulation",
            "char_start_offset": 17544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 1257,
                    "end": 1261,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1325,
                    "end": 1329,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "3643430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2083740234375
        },
        {
            "corpus_id": "245974615",
            "title": "Knowledge Distillation via Weighted Ensemble of Teaching Assistants",
            "text": "Originally proposed by Bucila, Caruana, and Niculescu-Mizil (2006) [6] and popularized by Hinton, Vinyals, and Dean (2015) [2] knowledge distillation compress the knowledge of a large and computational expensive model (often an ensemble of neural networks) to a single computational efficient neural network. The idea of knowledge distillation is to train the small model, the student, on a transfer set with soft targets provided by the large model, the teacher. Knowledge distillation has been commonly used in a number of learning tasks since then. Modeling knowledge transfer between teacher and student has also been done using adversarial methods. Using several teachers was still a good way to improve robustness. Some studies also proposed deep mutual learning which allows an ensemble of student models to learn collaboratively and teach each other during training. \n\nThe main idea of using knowledge distillation is that student network (S) to be trained not only using the true labels information but also observation of how the teacher (T) works with the unseen data provided. Because the teacher model has more more generalization power, the idea is to train the student model is such way that it can mimic the behaviour of that generalization. The teacher network is complex in size being deeper and wider. \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss and is given as, \n\nIn knowledge distillation one also tries to match the softened outputs of teacher y t =softmax(a t ) and student y s =softmax(a s ) via Kullback-Leibler divergence loss, \n\nby using a temperature parameter \u03c4 which has an additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss equation which used KD loss and cross-entropy loss, \n\nwhere \u03bb is the parameter used to trade-off between these two losses. This method is used for knowledge distillation from teacher and student models.",
            "score": 0.572113640511867,
            "section_title": "B. Knowledge distillation",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1795
                },
                {
                    "start": 1798,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2048
                },
                {
                    "start": 2051,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1744384765625
        },
        {
            "corpus_id": "252118741",
            "title": "Distilling Deep RL Models Into Interpretable Neuro-Fuzzy Systems",
            "text": "Distillation is a method to transfer knowledge from a teacher model T to a student model S. In supervised learning, this is done by using the output distributions of the teacher to train the student [15]. As these output distributions (soft targets) contain confidence levels of the teacher over all output classes, they contain more information than the original labels (hard targets), allowing the student model to learn more efficiently. However, the output distribution for a fully trained teacher model usually has the correct class at very high probability, with all the other classes very close to 0. As such, the soft target hardly provides any information beyond the ground truth labels encoded in the original hard targets. For this reason, the original softmax output function \u03c3(z) of the teacher network is replaced with a temperature softmax function \u03c3 \u03c4 (z) that converts each logit, z i , computed for each class into a probability, q i , by comparing z i with the other logits:\n\nNote that, if the temperature \u03c4 = 1, we obtain the original softmax function. If \u03c4 > 1, the result of this function is a smoothed version of the normal softmax. The student is then trained on these smoothened distributions using the classical crossentropy loss function.\n\nPolicy distillation is the application of this idea to reinforcement learning agents. In [14], a dataset is built by having the teacher model interact with the environment and recording the encountered states and output Q-values. The student is then trained to mimic this behaviour (see Figure 1a). As we require the student to output Q-values, which do not form valid probability distributions, we replace the cross-entropy loss with a temperature Kullback-Leibler divergence loss:\n\nWhere q T is the output of the teacher model and q S the output of the student model. Note that the output of a softmax function on a vector of Q-values is usually a very smooth probability distribution, as Q-values often have very small yet important differences. For this reason, the temperature \u03c4 in policy distillation is usually lower than 1, resulting in sharper probability distributions.\n\nMore recent advances in policy distillation have shown better empirical results if an -greedy student policy is followed during distillation [19]- [21]. This means that, for some small (e",
            "score": 0.5716464958682421,
            "section_title": "IV. POLICY DISTILLATION",
            "char_start_offset": 6088,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "275993890",
            "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design",
            "text": "Employing ResEmoteNet as a teacher model within the framework of knowledge distillation, the methodology prioritizes the extraction of hard labels and soft predictions from the teacher model, culminating in a composite of distillation loss functions. The input data is subsequently propagated through the student model for processing. Fig. 2 illustrates the proposed method. \n\nIn the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training. Where  , is the \"softened\" probability distribution of the teacher model and  , is the \"softened\" probability distribution of the student model. In the context of Hard Label Learning, the Cross-entropy loss is employed to guarantee that the student model sustains its accuracy concerning the primary task while preserving direct supervision through ground truth labels; this loss can be calculated between the predictions of the student model and the ground truth labels. \n\nWhere C is the number of classes,  , is the ground truth label for class c (1 for the correct class, 0 otherwise), ` , is the predicted probability (softmax output) for class c for sample i. Then the total loss function is a weighted combination of two components:  =  +  \u00d7  2 \u00d7  (6) Where \u03b1 is the weight for hard label loss, \u03b2 is the weight for distillation loss, T is the temperature parameter, CrossEntropyLoss is Cross-entropy loss with hard labels and KLDivLoss is KL divergence loss between softened predictions.",
            "score": 0.5707056986401122,
            "section_title": "B. Our Knowledge Distillation Method:",
            "char_start_offset": 6372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1494
                },
                {
                    "start": 1497,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 2016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "247181099",
            "title": "Single-head lifelong learning based on distilling knowledge",
            "text": "Knowledge distillation [16] is a technique that can transfer knowledge from a neural network that has already been trained, to another network through soft probabilities. This technique is widely used in the field of model compression by condensing the knowledge in a neural network into soft probabilities. Hence, the student network can efficiently obtain knowledge from the teacher network using these soft targets. Therefore, the student network can be trained more easily. Typically, neural networks usually use a \"softmax\" operator to generate the output of the model. It divides the logit z i by the summation of other logits to transform a logit into a probability q i . The key point of the knowledge distillation is that it will divide all logits by the hyperparameter T first. Where T is temperature, it is usually set to 1 in the general training situation for neural networks. However, in a knowledge distillation scenario, we will set our temperature T greater than 1 to get a softer probability. The larger the temperature T we set, the smoother the probability we get. Through this soft target from the teacher network, the student network can understand the correlation between classes. By using this extra information that is not contained in the ground truth, the student network can train itself more easily and perform better. In the lifelong learning field, knowledge distillation [9,13,14,15] provides an effective way to transfer knowledge from a pre-updated model to a new one if only a few examples from previous tasks are available. The neural network can alleviate catastrophic forgetting by recalling knowledge that it has learned from prior tasks. For example, Farquhar and Gal [19] studied knowledge distillation in continual learning with various evaluation strategies and highlighted shortcomings of the common practice to use one dataset with different pixel permutations.",
            "score": 0.5697170401180266,
            "section_title": "A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 6341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1403,
                    "end": 1406,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 1406,
                    "end": 1409,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1409,
                    "end": 1412,
                    "matchedPaperCorpusId": "173187918"
                },
                {
                    "start": 1412,
                    "end": 1415,
                    "matchedPaperCorpusId": "220665451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44482421875
        },
        {
            "corpus_id": "255393673",
            "title": "Knowledge-Guided Causal Intervention for Weakly-Supervised Object Localization",
            "text": "Knowledge distillation [9], [13] aims to transfer insights from a large teacher model to a compact student model [28], \n\nwhich can be split into knowledge and distillation. Knowledge encompasses response-based [13], [48] and feature-based [22], [23], [66] aspects. Response-based knowledge pertains to prediction logits, effective for simple classification distillation. However, for intricate distillation, relying solely on teacher logits is inadequate, prompting the exploration of featurebased knowledge to enhance distillation efficacy. Distillation, the mode of knowledge transfer, includes offline [12], [15], online [3], [52], and self-distillation [21], [60]. To facilitate smooth knowledge absorption by student models, a \"soft target\" approach is adopted, involving a \"softmax\" with a temperature T applied to teacher knowledge as additional supervision for students. \n\nIn this study, we design a multi-source knowledge guidance framework to guide the absorption of classification and localization knowledge during model training, elaborated in \u00a7III-C.",
            "score": 0.5689469859613014,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 10405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 878
                },
                {
                    "start": 881,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 210,
                    "end": 214,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 605,
                    "end": 609,
                    "matchedPaperCorpusId": "21679091"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 629,
                    "end": 633,
                    "matchedPaperCorpusId": "219531897"
                },
                {
                    "start": 657,
                    "end": 661,
                    "matchedPaperCorpusId": "53713470"
                },
                {
                    "start": 663,
                    "end": 667,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.237060546875
        },
        {
            "corpus_id": "233210081",
            "title": "Dual discriminator adversarial distillation for data-free model compression",
            "text": "We define the teacher model as T with parameters W t , and the student model as S with parameters W s . Furthermore, z t and z s are logits of teacher and student. There has been a temperature hyperparameter in vanilla knowledge distillation [24] for softening the output of the teacher model. So we define P t = softmax(z t \u2215 ) and P s = softmax(z s \u2215 ) as the softened output predictions of the teacher and student respectively. Then the student network is trained to mimic the behavior of the teacher network by optimizing the following loss function:\n\nwhere H is the cross-entropy loss between the true labels and outputs of student network, KL is the Kullback-Leibler divergence between the softened outputs of teacher and student models. is hyper parameter that adjust the balance between these two terms.",
            "score": 0.5687963327416293,
            "section_title": "Teacher-student learning paradigm",
            "char_start_offset": 15804,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 242,
                    "end": 246,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13916015625
        },
        {
            "corpus_id": "262046518",
            "title": "Distilling Hubert with LSTMs via Decoupled Knowledge Distillation",
            "text": "The method of knowledge distillation proposed in [4] consists of transferring knowledge from a potentially large and cumber-arXiv:2309.09920v1 [eess.AS] 18 Sep 2023 some classification model to a distilled one using a soft target distribution for each sample in a transfer dataset, calculated for each of C classes as \n\nwhere l c represents the logit of class c, and \u03c4 is a temperature scaling. The authors argue that soft targets provide much more information than hard labels and show that they are an effective way of communicating patterns discovered by a teacher model, being leveraged as guidance for training smaller models even with less data than the teacher. The knowledge distillation loss [4,5] is defined as \n\nwhere KL(\u2022||\u2022) is the Kullback-Leibler divergence, and p T and p S are the softmax-probability scores output by the teacher and the student model, respectively. The final loss is a linear combination of L KD and the cross-entropy loss between the student's class probabilities and the ground-truth hard labels. More recently, Knowledge Distillation (DKD) [5] was proposed. The authors first show that the KD loss can be split into two parts: Target Class Knowledge Distillation (TCKD) and Non-Target Class Knowledge Distillation (NCKD). The first one is argued to transfer the knowledge about the \"difficulty\" of the training samples, while the latter contains valuable knowledge about the non-target classes. The reformulated loss reads as \n\nwhere b \u2208 R 2 are the binary probabilities of the target class and all the other non-target classes together, and p \u2208 R C\u22121 represents the probabilities among non-target classes, discarding the target one. The NCKD term is shown to be coupled and inversely tied to the teacher's confidence on the target output, p T . This means that a good teacher would have a high p T , suppressing the NCKD component. As a consequence, the effectiveness of distillation of valuable information about the non-target classes would be limited for well-predicted samples.",
            "score": 0.5686380704439609,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 3750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 52,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 701,
                    "end": 704,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 704,
                    "end": 706,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1078,
                    "end": 1081,
                    "matchedPaperCorpusId": "247476179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2021484375
        },
        {
            "corpus_id": "271488200",
            "title": "A Review of Recent Hardware and Software Advances in GPU-Accelerated Edge-Computing Single-Board Computers (SBCs) for Computer Vision",
            "text": "Knowledge distillation involves transferring knowledge from a large, cumbersome model (the teacher) to a smaller, more efficient model (the student). Techniques such as Hinton's Knowledge Distillation [84] and Born-Again Networks [85] aid in this knowledge transfer. Recent advancements incorporate attention mechanisms and multi-stage distillation to enhance the student model performance and accuracy. Knowledge distillation is pivotal for transferring knowledge from large, complex models to smaller, efficient ones, facilitating deployment on resource-constrained edge-computing devices. Various approaches have evolved over time, enabling the compression of complex models without significant performance loss. Early techniques focused on transferring knowledge from a large, well-trained teacher model to a smaller student model, typically involving mimicking the teacher's behaviour through soft labels or intermediate representations. Notable early approaches include the following: Recent advancements in knowledge distillation have focused on integrating selfdistillation techniques and exploring the synergy between different distillation approaches. These developments aim to enhance the scalability and adaptability of knowledge distillation methods, enabling the efficient deployment of compact and accurate CV models in diverse edge-computing applications.",
            "score": 0.568546608072215,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 43009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1371
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "220364494"
                },
                {
                    "start": 230,
                    "end": 234,
                    "matchedPaperCorpusId": "238203535"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.011199951171875
        },
        {
            "corpus_id": "54436113",
            "title": "Snapshot Distillation: Teacher-Student Optimization in One Generation",
            "text": "The last factor, also being the one that was most studied before, is how knowledge is delivered from teacher to student. There are two arguments, both of which suggesting that a smoother teacher signal preserves richer information, but they differ from each other in the way of achieving this goal. The distillation algorithm [19] used a temperature term T to smooth both input and output scores, and the tolerant teacher algorithm [49] trained a less confident teacher by adding a regularization term in the first generation (a.k.a. the patriarch), and this strategy was verified the advantageous over the non-regularized version [11].\n\nIn the context of snapshot distillation, we follow [19] to divide the teacher signal (in logits, the neural responses before the softmax layer) by a temperature coefficient T > 1. In the framework of knowledge distillation, the student signals should also be softened before the KL divergence is computed with the teacher signals. The reason is that, the student with a shallow architecture is not capable of completely mimicking the same outputs of the teacher with a deep architecture [2,19], and thus matching the soft versions of their outputs is a more rational choice. The aim of knowledge distillation is to match the outputs, forcing the student to predict what the teacher predicts as much as possible. However, our goal is to generate secondary information in T-S optimization, instead of matching. As a result, we do not divide the student signal by T . This strategy also aligns with Eqn 1 used in the very first iterations (i.e., no teacher signals are provided). In experiments, we observe a faster convergence as well as consistent accuracy gain -see Section 4.1 for detailed numbers. We name it as asymmetric distillation.",
            "score": 0.5685178217629786,
            "section_title": "Principle #3: Secondary Information",
            "char_start_offset": 17584,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 631,
                    "end": 635,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2125244140625
        },
        {
            "corpus_id": "212644537",
            "title": "Multitask Emotion Recognition with Incomplete Labels",
            "text": "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels. Knowledge Distillation has been proved effective in model compression, continual learning [27] and domain adaptation [1]. However, its application to multitask learning with missing labels is under-researched. \n\nKnowledge distillation for regression is not as common as for classification. Some work in face alignment [26], [32] has used the L1 or L2 distance as the distillation loss function. \n\nTo enable the use of knowledge distillation using the hyperparameter temperature for valence-arousal estimation, we transform the regression task to a classification task by discretizing the continuous values. Then we can use temperature to control the smoothness of the soft labels.",
            "score": 0.568488483274487,
            "section_title": "C. Knowledge Distillation",
            "char_start_offset": 6487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1200
                }
            ],
            "ref_mentions": [
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 637,
                    "end": 640,
                    "matchedPaperCorpusId": "206742843"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "8911820"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "8350200",
            "title": "Distilling Word Embeddings: An Encoding Approach",
            "text": "As said, existing approaches to knowledge transfer between two neural networks mainly follow a two-step strategy: first training a teacher network; then using the teacher network to guide a student model by matching softmax, depicted in Figure 1a.\n\nFor a classification problem, softmax is typically used as the output layer's activation function. Let z \u2208 R nc be the input of softmax. (nc is the number of classes.) The output of softmax is\n\nTake a 3-way classification problem as an example. If a teacher model estimates y = (0.95, 0.04, 0.01) for three classes, it is valuable information to the student model that Class 2 is more similar to Class 1 than Class 3 to Class 1.\n\nHowever, directly imposing constraints on the output of softmax may be ineffective: the difference between 0.04 and 0.01 is too small. Ba et al. [1] match the input of softmax, z, rather than y. Hinton et al. [7] raise the temperature T during training, which makes the estimated probabilities softer over different classes. The temperature of 3, for instance, softens the above y to (0.64, 0.22, 0.14) . Matching softmax can also be applied along with standard crossentropy loss (with one-hot ground truth), or more elaborately, the teacher model's effect declines in an annealing fashion when the student model is more aware of data [11].",
            "score": 0.5679303387722985,
            "section_title": "BACKGROUND OF MATCHING SOFT-MAX",
            "char_start_offset": 3714,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1314,
                    "end": 1318,
                    "matchedPaperCorpusId": "2723173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "The goal of KD [8] is to transfer the dark knowledge encapsulated in the soft probability output of the teacher model to the student model. In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by \n\nwhere p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher. The core idea of KD lies in minimizing the KL divergence loss function to align the soft logits of the teacher and student. The loss of KD L KD is defined as \n\nwhere p T and p S are the output logits of the teacher T and the student S, respectively. A notable limitation of KD is its uniform treatment of all samples, regardless of their inherent difficulty. In practice, different samples may present varying levels of challenge, a one-size-fits-all approach might not optimally transfer the insights of the teacher. This paper aims to address this limitation by introducing a sample-wise reweighting scheme. The original KD method computes the distillation loss using the logits from both teacher and student. ER-KD introduces a novel step by reweighting the loss with the entropy of the teacher's predictions at the instance level. This entropy serves as an indicator of sample difficulty, guiding the student to focus more on challenging samples. By ensuring a balanced knowledge transfer, ER-KD reduces the student's overconfidence prediction and aligns it more closely with the teacher's.",
            "score": 0.5676894088672171,
            "section_title": "Preliminaries",
            "char_start_offset": 9275,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1745
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95361328125
        },
        {
            "corpus_id": "265444951",
            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
            "text": "KD aims to transfer the dark knowledge from a complex teacher model to a lightweight student model by aligning their logits that are softened with a temperature hyperparameter. This enables the student to approximate the teacher's performance in a compact form. \n\nLogit and Feature Distillation Logit distillation methods aim to align the teacher and student output logits, valued for their simplicity and wide applicability. In contrast, feature distillation focuses on minimizing divergence in intermediate feature representations, offering enhanced learning but at the cost of increased computational demands. \n\nBoth pathways have demonstrated state-of-the-art performance across various tasks and domains. While most of these methods follow a static training approach, recent techniques have further refined the distillation process through dynamic approaches. \n\nAdaptive Distillation Diverging from the static approaches, adaptive distillation methods pave the way for more dynamic and tailored knowledge transfer [3,13,18,20,31,37,38]. These methods either dynamically provide prior knowledge [38], modulate hyperparameters [13,18,20], or adjust distillation strategies [3,31,37] based on the teacher-student performance gap. Within this landscape, the Curriculum Temperature for Knowledge Distillation (CTKD) [13] emerges as a simple yet effective approach. This approach introduces curriculum training and adversarial temperature learning to KD, progressively exposes the student to complexities, and pushes it to address harder challenges through instance-wise temperature modulation. Nonetheless, CTKD relies on temperature modulation which dictates the overall softness of labels. In contrast, our ER-KD reweights the KD loss with the entropy of the teacher's predictions, precisely emphasizing challenging samples while reducing the focus on simpler ones.",
            "score": 0.5672750064187573,
            "section_title": "Related work",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 1019,
                    "end": 1022,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1022,
                    "end": 1025,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1028,
                    "end": 1031,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1037,
                    "end": 1040,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "249626454"
                },
                {
                    "start": 1130,
                    "end": 1134,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 1137,
                    "end": 1140,
                    "matchedPaperCorpusId": "244119770"
                },
                {
                    "start": 1176,
                    "end": 1179,
                    "matchedPaperCorpusId": "239024317"
                },
                {
                    "start": 1182,
                    "end": 1185,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1316,
                    "end": 1320,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1405029296875
        },
        {
            "corpus_id": "254910794",
            "title": "Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation",
            "text": "When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results. \n\nFormally, Let (x,Y) in D us where x is an eventnote and Y is the corresponding label. Now our student model M S , given an input x will output logits L S which can be shown as L S = M S (x). These logit values are softened by using the temperature \u03c4 and used in the softmax function \u03c3 to get the soft probabilities denoted by \u0176S\u03c4 = \u03c3 (L S /\u03c4 ). On the other hand, Y S denotes the hard probabilities in Y S = \u03c3 (L S ) to be used by the CE (cross entropy) loss. \n\nThe teacher model M T , outputs the score for each inputs from dataset D sn . Assuming, (I,Y) in D us where i is the set of structured features, gathered from the dataset D s and Y is the corresponding label that is identical to the label of dataset D us . Therefore, The score provided by the model, can be denoted as L T i = M T (I i ) and the hard probability distribution for each input can be shown as Y T i == \u03c3 (L T i /\u03c4 ), the soften probability would be then,in \u0176T \u03c4 = \u03c3 (L T /\u03c4 ). The final loss function now can be derived as Equation 12: \n\nIV. RESULTS \n\nSeveral classification algorithms and techniques have been used to classify the opioid patients from both structured and unstructured datasets.",
            "score": 0.5670786799050526,
            "section_title": "G. COMBINING MODELS BY KNOWLEDGE DISTILLATION",
            "char_start_offset": 33784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 48
                },
                {
                    "start": 49,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "252780810",
            "title": "Pruning Adversarially Robust Neural Networks without Adversarial Examples",
            "text": "In knowledge distillation [25], [38], a student model learns to mimic the output of a teacher. Consider a well-trained teacher model T , and a student model h \u03b8 that we wish to train to match the teacher's output. Let \u03c3 : R k \u2192 [0, 1] k be the softmax function, i.e., \u03c3(z) j = e z j j e z j , j = 1, . . . , k. Let \n\nbe the softmax outputs of the two models weighed by temperature parameter \u03c4 > 0 [25]. Then, the knowledge distillation penalty used to train \u03b8 is: \n\nwhere L is the classification loss of the tempered student network h \u03c4 \u03b8 and KL is the Kullback-Leibler (KL) divergence. Intuitively, the knowledge distillation loss L KD treats the output of the teacher as soft labels to train the student, so that the student exhibits some inherent properties of the teacher, such as adversarial robustness.",
            "score": 0.5667867973778331,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 9433,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 314
                },
                {
                    "start": 317,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 808
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 36,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22216796875
        },
        {
            "corpus_id": "232232785",
            "title": "BreakingBED - Breaking Binary and Efficient Deep Neural Networks by Adversarial Attacks",
            "text": "Knowledge distillation (KD) is the transfer of knowledge from a teacher to a student network [40,21]. The student can be a smaller DNN, which is trained on the soft labels of the larger teacher network, achieving an improvement in an accuracy-efficiency trade-off. The student represents a compressed version of the teacher, condensing its knowledge. This paper focuses on KD training, using Kullback-Leibler (KL) divergence between the teacher and the student output distribution formulated as the loss function in Eq. ( 1). Here, \u03c3(f t (I)) and \u03c3(f s (I)) represent the softmax output logits of the teacher and student network respectively, computed for a sample image I in a mini batch of N samples. \n\nDuring the knowledge transfer using the teacher's logits, a softmax temperature T 1 is used. During the evaluation, we use T = 1 to obtain softmax-cross entropy loss.",
            "score": 0.5662235675026571,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 2441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 871
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 97,
                    "matchedPaperCorpusId": "204838340"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2115478515625
        },
        {
            "corpus_id": "232320310",
            "title": "ZS-IL: Looking Back on Learned Experiences For Zero-Shot Incremental Learning",
            "text": "softmax of logits) of the i th class in l new incremented class(es). \n\nFor the distillation purpose, we adopt knowledge distillation from network output (logits), know as dark knowledge [17,46,7], and our objective is to minimize the Euclidean distance between the stored logits in the transfer set O = [o 1 (x), . . . , o K (x)] and those generated by the learner network \u00d4 = [\u00f4 1 (x), . . . , \u00f4K (x)] as follows: \n\nHaving a look at both cost functions, we define the total loss function by a linear combination as bellow: \n\nwhere \u03bb is predefined parameter to control the degree of distillation.",
            "score": 0.5661302614971488,
            "section_title": "Zero-Shot Incremental Learning",
            "char_start_offset": 13393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 71,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040985107421875
        },
        {
            "corpus_id": "274776589",
            "title": "Knowledge Migration Framework for Smart Contract Vulnerability Detection",
            "text": "The concept of Knowledge Distillation (KD) was initially proposed by Hinton et al [7]. The method entails transferring knowledge from a teacher model to a student model, thereby reducing the computational overhead while maintaining high performance. The core principle involves softening the Softmax output by a temperature parameter to enhance the similarity between categories and improve the generalisation ability of the student model. The underlying formula is q i = exp(z i /T) (Ej) exp(z j /T) , where z i represents the teacher model output and q i denotes the temperature-smoothed output. \n\nThe research methods employed in knowledge distillation can be broadly classified into three categories. The initial category encompasses temperature-based distillation methodologies. In a related contribution, Wei Y. [8] put forth a dynamic temperature adjustment mechanism that entails a reduction in temperature over the course of training steps, with the objective of optimising the student model. In a similar vein, Long J. [9] adopted a dual-temperature mechanism, utilising higher temperatures for those samples that are more challenging to classify and lower temperatures for those that are less so. The second category comprises feature alignment and self-distillation methods. The paper [10] employs mean squared error to align the probabilities of the teacher and student models, whereas the Born-Again model [11] utilises self-distillation for the transfer of knowledge. The paper [12] improves the performance of the student model by regularising feature norms. Similarly, Park et al. [13] put forth a proposal for the transfer of knowledge via the establishment of relationships between samples (such as distance or angle) with the objective of maintaining the diverse features of the teacher model. Nevertheless, in high-dimensional data spaces, this method is vulnerable to noise and may encounter difficulties in effectively capturing the intricate distribution of data. Furthermore, knowledge distillation has been extensively employed in a multitude of domains. The Cross-Task Distillation framework [14] is a multi-task learning method that enhances the performance of the student model in multiple tasks by sharing teacher knowledge across tasks. These knowledge distillation methods continue to rely on external labels or source training data, which constrains their applicability in scenarios with no or limited labelled data.",
            "score": 0.5659285558981394,
            "section_title": "Knowledge distillation",
            "char_start_offset": 6320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2267
                },
                {
                    "start": 2268,
                    "end": 2449
                }
            ],
            "ref_mentions": [
                {
                    "start": 1029,
                    "end": 1032,
                    "matchedPaperCorpusId": "270909666"
                },
                {
                    "start": 1420,
                    "end": 1424,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1598,
                    "end": 1602,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 2119,
                    "end": 2123,
                    "matchedPaperCorpusId": "247011924"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "264323617",
            "title": "Deep Learning-Based Eye Gaze Estimation for Automotive Applications Using Knowledge Distillation",
            "text": "In this stage, the knowledge distillation algorithm is used. Forward propagation is performed for the teacher and student networks, while backpropagation is applied only to the student network. The main loss function is defined using two different loss functions: student loss and distillation loss. \n\nKnowledge from a model is categorized into three different types: Response-based knowledge, Feature-based knowledge, and Relation-based knowledge [32]. Response-based knowledge focuses on the final output layer, where the student model will learn the predictions of the teacher model. Feature-based knowledge leverages the knowledge of the data from teacher intermediate layers to train a student model. Relation-based knowledge focuses on the correlation between feature maps, graphs, similarity matrix, feature embeddings, or probabilistic distributions. In this paper, response-based knowledge is used because it showed the best results in different tasks and applications. \n\nThe block diagram for the knowledge distillation process is shown in Fig. 1. \n\nThe knowledge transfer from teacher to student is performed by minimizing the main loss function with the target to yield the same probabilities as using the teacher model. Basically, it refers to the output of the softmax function applied to non-normalized predictions. These predictions of the teacher model are usually found under the name of logits. Most often, the correct class of the probability distribution has a higher level compared to the other class probabilities that are close to zero. Therefore, the outcome provided by this probability distribution is very similar to the ground truth labels of the dataset. Regarding this behavior, Hinton et al. [15] introduced the softmax parameter temperature. By denoting this parameter with T , the probability p i of class i from the logit z i is computed as in (1). \n\nWhen the parameter T is set to 1, the equation becomes a standard softmax function. Setting a value higher than 1 for T , the probability distribution will provide more information about the classes where the teacher model reported a prediction close to the correct class. This is the knowledge of the teacher model that is transferred to the student model using the distillation algorithm. When the distillation loss function is calculated using the soft labels, the same value of T is used to calculate the softmax on the student's logits.",
            "score": 0.5658653691969704,
            "section_title": "3) TRAINING THE STUDENT NETWORK WITH KNOWLEDGE TRANSFER FROM THE TEACHER NETWORK",
            "char_start_offset": 23846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1057
                },
                {
                    "start": 1060,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2276
                },
                {
                    "start": 2277,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1879,
                    "end": 1882,
                    "matchedPaperCorpusId": "28182923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "247486748",
            "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing",
            "text": "\"Knowledge distillation\" is a concept that first appeared in 2014 and that was an idea that was proposed by Geoffrey Hinton [30]. \"Knowledge distillation\" refers to a method of transferring knowledge from a large pretrained model (teacher model) to a small model to be used in practice (student model), in terms of the model distribution. In a deep learning model, the more parameters and the more computation, the better the feature extraction, and, accordingly, the performance of the model improves. However, since it uses a lot of memory and requires a lot of computing power, the efficiency is reduced in actual use cases. In other words, knowledge distillation is a method that is used to improve the performance of a small model by transferring the knowledge of a large model to a small model in the learning process, without changing the structure of the small model, so that the small model can perform as well as the large model. The architecture of the knowledge distillation model is shown in Figure 1. Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model. In classification tasks, neural networks generally use a Softmax output layer to transform the output into a probability, whereas knowledge distillation uses a smooth refinement of the Softmax. The Softmax equation of knowledge distillation is as follows: Knowledge distillation is a procedure for model compression in which the student model is trained to match the teacher model. In this process, the loss function is minimized and is transferred from the teacher model to the student model, and the goal is to match the softened teacher logits of the output with the actual label. The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model.",
            "score": 0.565325663008369,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 128,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "271399352",
            "title": "A continual learning framework to train robust image recognition models by adversarial training and knowledge distillation",
            "text": "To overcome the problem of overly relying on the real labels and ignoring the teacher's knowledge, during the training we add a hyperparameter (Temperature) to soften the teacher label. This way, the student no longer pursues the model accuracy (Loss 1), and yet pays some attention to the similarity between different labels instead. Thus, the teacher model's learning performance can be promoted. As indicated above, a larger parameter derives a more uniform distribution for the teacher model's output. The Loss 2 loss function and the Loss 1 loss function are to be balanced to exploit the corresponding advantages. In this work, we adopt the technique presented in (You et al., 2017) to achieve the balance. It is to average multiple teachers' SoftMax layer outputs, use Temperature to soften the labels, and then perform the Temperature on the student (the student model's SoftMax layer output). Finally, the cross-entropy is calculated. This method is called multi-teacher knowledge distillation (MTKD hereafter). This loss is defined in Equation (3) as below: \n\nwith \n\nHere",
            "score": 0.5638259464905668,
            "section_title": "Loss 2: KL (Kullback-Leibler) divergence loss under teacher's guidance",
            "char_start_offset": 35766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1081
                }
            ],
            "ref_mentions": [
                {
                    "start": 670,
                    "end": 688,
                    "matchedPaperCorpusId": "26021416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "268230792",
            "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
            "text": "Knowledge Distillation (KD) aims at transferring the knowledge of a teacher LLM T to a smaller student LLM S. The standard approach is to train the student to mimic the output distribution of the teacher and the true labels.Let f T (x) and f S (x) be the logit outputs of the teacher and student networks evaluated on an input data x, respectively.P T and P S be the softmax outputs of the teacher and student networks, respectively, Z T and Z S be the logit outputs of the teacher and student networks, respectively, and y be the true labels.The student is trained to minimize the linear combination of the supervised learning loss and the distillation loss:\n\nwhere \u03bb is a weight hyperparameter, L CE is the cross-entropy loss, L KL is the Kullback-Leibler divergence, \u03c3 is the softmax function, and the second term is the distillation loss which follows the softmax-temperature using a temperature parameter t (Hinton et al., 2015).At inference time, t is set to 1 to recover the standard softmax.y is known as the hard label because of its one-hot encoding while \u03c3(f T (x)/t) is the soft label.",
            "score": 0.563641413161954,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 8673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 659
                },
                {
                    "start": 661,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1097
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.334716796875
        },
        {
            "corpus_id": "265384964",
            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
            "text": "This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization. \n\nIn 2015, Hinton et al. discovered the advantages of training the distilled model to not only produce the soft labels from the teacher but also the correct labels relying on the ground truth labels. Consequently, we compute the \"standard\" loss by comparing the predicted class probabilities of the student with the ground truth labels. This loss is named the \"student loss\". When calculating the class probabilities for the student loss, we employ T = 1. The comprehensive loss function, which integrates both the distillation and student losses, is determined as follows: \n\nHere, x represents the input, W represents the student model's parameters, y denotes the ground truth labels, H signifies the cross-entropy loss function, \u03c3 represents the softmax function characterized by the \"temperature\" T, and \u03b1 and \u03b2 are constants. The logits of the student and teacher are denoted as z s and z t , respectively. The general structure of knowledge distillation is depicted in Figure 2. \n\nHere,  represents the input,  represents the student model's parameters,  denotes the ground truth labels,  signifies the cross-entropy loss function,  represents the softmax function characterized by the \"temperature\" , and  and  are constants. The logits of the student and teacher are denoted as  and  , respectively. The general structure of knowledge distillation is depicted in Figure 2.",
            "score": 0.5629285631782668,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 12021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1214
                },
                {
                    "start": 1217,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1610
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "231648215",
            "title": "Learning to Augment for Data-Scarce Domain BERT Knowledge Distillation",
            "text": "Before we dive into our method, we first introduce the process of knowledge distillation. The distillation process aims to transfer the knowledge of a large teacher network to a small student network. The objective is defined as follows:\n\nwhere f s and f t represent the features of student and teacher models respectively. L(\u00b7) is a loss function that evaluates the difference between the teacher and student models. Inspired by the success of transformer networks such as BERT (Sun et al. 2019;Jiao et al. 2019;Wang et al. 2020), our Figure 1: Overview of the proposed Learning to Augment (L2A) method. The generator generates augmented data based on both source and target domain data from a statistic stationary distribution (P s ). The reinforced selector selects useful augmented data to help the task of knowledge distillation and updates its policy according to the student network performance. distillation model is based on the BERT network (Vaswani et al. 2017). We consider three types of distillation strategy: L att based on attention information, L hidden on intermediate hidden representations, and L dark on the prediction outputs or dark knowledge, detailed as follows:\n\nwhere A i represents the attention matrix corresponding to the i-th self-attention head of the last BERT layer and h is the number of attention heads. H s , H t denotes the output of the last layer of student network and teacher network, respectively. W denotes a transformation matrix that transforms the hidden states of the student network into the same space as the teacher network's states. For dark knowledge based distillation, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits as follows:\n\nwhere g s and g t are the logits from the student and teacher respectively. T KD denotes the temperature value which controls the smoothness of the output distribution. Note that for the regression problem, the above loss is reformulated as the mean square error between the student's and the teacher's logits.\n\nWe combined the above three types of loss as our final KD loss, namely: L KD = L att + L hidden + L dark .",
            "score": 0.5627201981419054,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7626,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "201670719"
                },
                {
                    "start": 951,
                    "end": 972,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "260234949",
            "title": "Lightweight Tunnel Defect Detection Algorithm Based on Knowledge Distillation",
            "text": "Figure 5. The multidimensional knowledge distillation process, where the leftmost component is the teacher model learning process, is connected to the middle student model through the distillation parameter \"T\" to form Soft loss, and the rightmost component is the real label with parameter \"T\" of 1, connected to The rightmost component is the real label with parameter \"T\" of 1, connected to the student model to form Hard loss, and the two losses are combined to form the full multidimensional knowledge distillation loss. \n\nTo help train the student network in depth, the outputs of the intermediate layers of the teacher network were introduced, and then each intermediate and output layer parameter was optimized by a loss function as in Equation ( 9): \n\nOf these, the f t (x) and f s (x) are the feature maps of the middle layer of the teacher and student models respectively. When the feature maps of the teacher and student models are not in the same shape, it is common to apply the transformation functions \u03c6 t ( f t (x)) and \u03c6 s ( f s (x)). L F ( ) denotes the similarity function used to match the feature maps of the teacher and student models, and in this study L F ( ) denotes the cross-entropy loss function. \n\nThe training process is distilled by distilling the output with the high-temperature match to obtain a hard loss L hard defined as the cross-entropy loss between the ground truth label and the student model as in Equation ( 10): \n\nSoft loss L soft is defined as the cross-entropy between the teacher model and the student model as in Equation ( 11): \n\nwhere v i denotes the output of the teacher network, z i denotes the output of the student network and p T i , and q T i denotes the value of the softmax output of the teacher network and the student network at temperature T for class i, respectively. c i denotes the value of ground truth at class i. c j \u2208 {0, 1} N denotes the total number of samples. The total loss in the output layer is obtained through Equation ( 8) as Equation ( 12):",
            "score": 0.562344974077664,
            "section_title": "Multidimensional Knowledge Distillation",
            "char_start_offset": 21009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1577
                },
                {
                    "start": 1580,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2021
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "211043581",
            "title": "Transfer Heterogeneous Knowledge Among Peer-to-Peer Teammates: A Model Distillation Approach",
            "text": "Distillation is a method to transfer knowledge from a teacher model Z i to a student model Z j . The traditional distillation targets from a classification network are typically obtained by passing the weighted sums of the last network layer through a softmax function. Traditional distillation process starts with a powerful teacher network and performs one-way knowledge transfer to an untrained student. However, a pre-trained expert is always unavailable in social settings. \n\nRecently, [17] extended model distillation to a setting where a pool of untrained students which learn simultaneously to solve a classification task. Trained in this way, it turns out teammates learn significantly better than when learning independently. Similarly, in our scenario, the value functions of each untrained student are given by a softmax layer. Given N feature vectors \u03a6 = {(s n , a n )} N n=1 with K atoms, we adopt the distillation setup of [17] and minimize the Kullback-Leibler (KL) divergence, which is shown as: \n\nIn our framework, there are two roles: the role of the teacher agent i, (i.e., an agent whose value function Z i gives teaching demonstrations) and the role of the student agent j, (i.e., an agent who receives teaching demonstrations and enhances its value function Z j ). Agent i and j are able to teach each other via model distillation, but we only consider a oneway interaction for clarity: agent j is willing to minimize D KL (Z i Z j ). So how does KL-divergence metric work? We have argued that Z j := W j (t)\u03c6 n : W (t) \u2208 R K\u00d7d with feature vector \u03c6 n = (s n , a n ). Assume the weight matrix initializes at zero, W j (0) = 0, and with Theorem 1 in [18], we could argue that weight matrix W j (t) fulfills almost surely: \n\nwhen we adopt the notion of gradient flow (stepsize is finite and sufficiently small). As a result, LTCR solves the problem of transferring the knowledge of categorical Q-function Z i .",
            "score": 0.5618372687943028,
            "section_title": "Distilling to Teach",
            "char_start_offset": 8058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 1672,
                    "end": 1676,
                    "matchedPaperCorpusId": "174800711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1456298828125
        },
        {
            "corpus_id": "277940458",
            "title": "Scaling Laws for Data-Efficient Visual Transfer Learning",
            "text": "(2) Output Layer Distillation Loss: While maintaining the ability to learn from original task labels, we employed a weighted combination loss: \n\nwhere   represents the cross-entropy loss,   is the model's prediction, and   is the ground truth label; the KL term represents the Kullback-Leibler divergence between the output distributions of the small and large models,  is the softmax function,   and   are the logits from the teacher (smaller) and student (larger) models respectively,  is the temperature parameter used to smooth the probability distributions, and  is a weight coefficient balancing original task learning and knowledge distillation. \n\nThis hierarchical learning approach and carefully designed loss function enable us to analyze how knowledge effectively transfers across models of increasing complexity and to identify optimal pathways for small-to-large model guidance. However, through preliminary experiments, we observe an intriguing phenomenon: the effectiveness of distillation appears to have a non-linear relationship with pre-training data scale. Under certain data regimes, distillation strategies excel; in others, traditional training methods prove more effective. This discovery prompts us to investigate the boundary conditions of knowledge distillation, leading to the development of a new theoretical framework.",
            "score": 0.5618333341189418,
            "section_title": "Model Distillation: Optimization Strategy",
            "char_start_offset": 15628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47509765625
        },
        {
            "corpus_id": "251040920",
            "title": "HIRE: Distilling High-order Relational Knowledge From Heterogeneous Graph Neural Networks",
            "text": "Aggregation Function RGCN [13] h \n\nnodes are imprecise. The node-level knowledge distillation is applied to transfer soft labels (p \u03c4 t i ) predicted by the teacher network to the student network, allowing students to learn smoother knowledge [17]. The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class. . Besides the teacher's soft labels, it can further boost the training performance with existing ground truth labels, as stated by Hinton et al. [17], which can significantly improve the performance of the student model. Accordingly, the NKD loss can be written as a weighted average of the cross-entropy loss and the distillation loss: \n\nwhere L CE = \u2212 N i y i log p s i is the basic cross-entropy loss, i denotes the i-th node, \u03b1 is the hyperparamemter for balancing the cross-entropy loss and the distillation loss. \n\nNevertheless, NKD cannot address the semantic relation modeling issue because it ignores the crucial correlation between different types of nodes in heterogeneous graphs, which can be solved in the next section.",
            "score": 0.5605887099178611,
            "section_title": "Models",
            "char_start_offset": 13265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "5458500"
                },
                {
                    "start": 243,
                    "end": 247,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1247,
                    "end": 1251,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69921875
        },
        {
            "corpus_id": "244156385",
            "title": "Evaluation of Deep Neural Network Compression Methods for Edge Devices Using Weighted Score-Based Ranking Scheme",
            "text": "Distilling knowledge (i.e., useful information) from a large model (the teacher) to a small model (the student) has shown excellent results in model compression. The concept of knowledge transfer is based on the idea that large models are robust and can learn complex patterns from data such that useful information can be transferred to the small model to mimic the behaviour of the large model. \n\nWe implemented the teacher-student model using a temperature-based softmax function at the output layer in Keras [38]. This technique was inspired by [21]. The teacher model (i.e., VGG16) was trained on our dataset, and the class probabilities vector for each data point (i.e., observation) was calculated and extracted. These probabilities vectors, also called soft labels, were distilled to the small model as the target label during training. We also trained the small model using the hard labels, and the overall losses generated by the small model were combined and weighted, as shown in Equation ( 6): \n\nLoss total is the total loss, which is the combination of the student and distillation losses. The student loss is computed using the standard loss function by making the temperature parameter (T = 1). The temperature parameter controls the amount of information that can be distilled to the student. However, we need to keep in mind that the student has a threshold that limits the amount of information that it can retain from the teacher. The \u03b1 and \u03b2 are constants associated with the individual loss function taking the respective unnormalized log probabilities (z s , z t ) for each class label.",
            "score": 0.5603274164121004,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 24563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49853515625
        },
        {
            "corpus_id": "212855595",
            "title": "Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction",
            "text": "This subsection illustrates the objective functions of teacher and student networks, we introduce two additional loss functions to help our model efficiently transfer dark knowledge (i.e., the global type constraints and the relevance among relations) from teacher to student. \n\nThe Teacher Network In vanilla knowledge distillation, the teacher network is trained to fit one-hot labels. However, the ultimate goal of teacher is to provide better guidances for student, rather than achieve high accuracy simply. Inspired by Yang et al. (2019), we introduce a top score difference (TSD) loss to make the teacher's distribution softer. More concretely, we first pick up K classes with the highest confidence scores from the teacher's output, and then compute the gap between the confidence scores of the primary class and other K-1 classes: \n\nwhere \u03c1 k refers to the value of k-th largest element in the output distribution of teacher. Based on the global statistics, K is set to 3 empirically. We add the penalty term to standard ground-truth loss L T GT when training the teacher, facilitating it to distribute confidence to a few secondary relations: \n\nThe Student Network Typically, knowledge distillation transfers dark knowledge from the final output of teacher. Chen et al. (2017) demonstrate that using the intermediate representation of teacher as hint can stabilize the training process and improve the final performance of student. Here, we utilize the Kullback-Leibler divergence to measure the differences of corresponding branches between the teacher and student networks as hint learning loss: \n\nwhere L T L and L T G are the local and global logits of the teacher network respectively, L S L and L S A are the local and auxiliary logits of the student network respectively. Intuitively, it encourages the results of MAA to be similar with the scaled global logits of the teacher network. The loss of knowledge distillation is calculated as the sum of L HT and L KD with a weight factor \u03bb ht . As a result, the updated loss of student network is defined as follows:",
            "score": 0.5580709803381455,
            "section_title": "Objective Functions",
            "char_start_offset": 15526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2078
                }
            ],
            "ref_mentions": [
                {
                    "start": 524,
                    "end": 542,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1267,
                    "end": 1285,
                    "matchedPaperCorpusId": "29308926"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "209515527",
            "title": "Modeling Teacher-Student Techniques in Deep Neural Networks for Knowledge Distillation",
            "text": "Assume training data tuples of inputs and labels (. ) \u2208  which D is a set of training data. Let T be Teacher network with parameters   and S be Student network with parameters   . \n\nThe distillation equation can be written as Eq. ( 1) to minimize . \n\nIn Eq. ( 1),   is cross-entropy which is computed on the labels  \ufffd  that are predicted by the student and ground truth labels y with temperature =1.   is distillation loss, which is crossentropy computed on softmax output of teacher and student with temperature  . Hence, (.   . )  (.   . ) represent of softmax output of the student and the teacher, respectively.  is hyper-parameters to balance the influence of each loss [7][1]. \n\nHow the knowledge is transferred between the teacher(s) and student(s) is specified in this stage. As illustrated in Fig. 5, there are three steps in knowledge distillation playing an essential role in having an efficient TS model. These steps are including the determination of knowledge types, location of distillation, and methods of knowledge transfer. \n\nIn the base TS models, soft-labels (also known as logits) are considered as distilled knowledge. However, the knowledge can be distilled from each location of the teacher model, including the end of the model and between layers. In [14], knowledge is transferred between blocks of the teacher to student. Distillation-loss is realized through a cross-entropy function that is applied to the output of the student and softlabels of the teacher. Different knowledge types are considered in previous studies, such as soft labels, hard labels, etc. In [9], knowledge type in the form of the mutual information between intermediate layers is maximized and several functions are used to minimize the loss of intermediate layers. In [7], knowledge  types are in the form of a similarity matrix between teacher and student models. Transfer Methods is another step in the knowledge distillation stage. In [15] a multilevel approach for knowledge transfer is presented in which an information mask is provided by the teacher, and the student is trained with both of the information masks and teacher's ground truth. Also, in [16] and [17], there is more than one level of distillation.",
            "score": 0.5580306619906003,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 10817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 1940,
                    "end": 1944,
                    "matchedPaperCorpusId": "182952755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205322265625
        },
        {
            "corpus_id": "234067330",
            "title": "Optimising Hardware Accelerated Neural Networks with Quantisation and a Knowledge Distillation Evolutionary Algorithm",
            "text": "Distillation loss: cross entropy of the teacher's high temperature (T = \u03c4) output with the students high temperature output. \n\nThe loss function for knowledge distillation (from [30]) is: \n\nwhere x is an input, W are the parameters of the student network, y the ground truth vector and \u03c3(z; T = \u03c4) is the softmax function applied to logit vector z and temperature T = \u03c4. The student and teacher logit vectors are s and t, and hyper-parameters \u03b1 and \u03b2 are arbitrary constants. \n\nIn the NEMOKD methodology (Section 3.4), student models in the initial CNN architecture population are partially trained using knowledge distillation.",
            "score": 0.55785343011039,
            "section_title": "2.",
            "char_start_offset": 12417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 127,
                    "end": 187
                },
                {
                    "start": 190,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 628
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2391357421875
        },
        {
            "corpus_id": "222096654",
            "title": "Class-Incremental Learning of Convolutional Neural Networks Based on Double Consolidation Mechanism",
            "text": "Knowledge distillation is not originally proposed for incremental learning, but it has been widely used in this domain. Knowledge distillation is a simple and effective method for model compression and model training, which was proposed by Hinton et al. [15] in 2015. In the context of deep learning, to achieve better prediction, over-parameterized deep neural networks are often constructed. Such kind of network has very strong learning ability but requires a large amount of memory and computing resources, which is very unfavorable for deployment. However, it is empirically difficult to train a small model from scratch to achieve the equivalent performance of a large model. Fortunately, by using the knowledge distillation method, knowledge can be migrated from a large network or network cluster to a smaller network for more efficient deployment. \n\nThe small model need to be trained is called the student network, and the large model that has been trained is called the teacher network. Our goal is to make the output of the student network sufficiently close to that of the teacher network, but there is a problem with doing this directly: the Softmax output of the teacher network is similar to a one-hot vector, with one value large and the others small. In this case, the output of the teacher network provides very limited supervision information. Compared with the ''hard'' output which is almost one-hot, we hope the output can be more ''soft,'' that is, the probability distribution can be more moderate. Knowledge distillation presents an effective method. Let's consider a generalized Softmax function: \n\nwhere z i is the output value of the model's ith output node before Softmax function performed, and T is the temperature parameter. It is easy to prove that q i converges to a one-hot vector as temperature T approaches 0, and becomes softer as T gets higher. \n\nThe knowledge distillation method assists the training of the student network by introducing the soft-target related to the teacher network as a part of the total loss, to realize knowledge transfer. The whole process of knowledge distillation is shown in Fig. 2. In Fig. 2, the predicted output of the left teacher network is transformed by generalized Softmax to obtain the soft probability distribution. The value is between 0 and 1, and the value distribution is relatively moderate.",
            "score": 0.5572910765121868,
            "section_title": "B. KNOWLEDGE DISTILLATION",
            "char_start_offset": 9329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.451416015625
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "D EPLOYING large-scale neural networks on portable de- vices with limited computation and storage resources is challenging [1], and efficient architectures such as Mo-bileNets [2], [3] and ShuffleNets [4], [5] have been designed for lightweight deployment.However, the performances of these lightweight networks are usually not comparable to the larger ones.Commonly, second learning [6], [7] or knowledge distillation (KD) [8]- [10] could be utilized to transfer the knowledge of a more complex and well-performed network (i.e., the teacher) to the smaller ones (i.e., the student).The dark knowledge in KD is still a mystery that has attracted lots of studies [9], [11]- [13], and their goal is to answer the following question: what's the knowledge that the teacher provides and why they are effective in KD?\n\nIn the original KD method [9], the student aims to mimic the teacher's behavior by minimizing the Kullback-Leibler (KL) divergence between their output probabilities.That is, the logits and softened probabilities, i.e., the inputs to the final softmax operator and the corresponding outputs, are the specific knowledge transferred in KD.With the development of KD methods, the output-level knowledge has been extended to various types [14], including the intermediate features [15]- [20], the sample relationships [21]- [27], the parameters [28], [29], and the collaborative or online knowledge [10], [30] etc.However, the outputs of neural networks are much easier to visualize, analyze, and understand.Therefore, we focus on the original KD [9] and aim to understand the dark knowledge (i.e., the logits and softened probabilities) provided by the teachers.Unlike previous studies, we majorly study the output-level dark knowledge provided by teachers with various capacities, which receives little attention in previous studies.",
            "score": 0.5570940221600975,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 256,
                    "end": 358
                },
                {
                    "start": 358,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1150
                },
                {
                    "start": 1150,
                    "end": 1423
                },
                {
                    "start": 1423,
                    "end": 1517
                },
                {
                    "start": 1517,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 176,
                    "end": 179,
                    "matchedPaperCorpusId": "12670695"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "4555207"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "24982157"
                },
                {
                    "start": 206,
                    "end": 209,
                    "matchedPaperCorpusId": "51880435"
                },
                {
                    "start": 384,
                    "end": 387,
                    "matchedPaperCorpusId": "3039694"
                },
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "1024861"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 662,
                    "end": 665,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 667,
                    "end": 671,
                    "matchedPaperCorpusId": "8125776"
                },
                {
                    "start": 673,
                    "end": 677,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 839,
                    "end": 842,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1290,
                    "end": 1294,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 1296,
                    "end": 1300,
                    "matchedPaperCorpusId": "243860797"
                },
                {
                    "start": 1327,
                    "end": 1331,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1333,
                    "end": 1337,
                    "matchedPaperCorpusId": "219965004"
                },
                {
                    "start": 1354,
                    "end": 1358,
                    "matchedPaperCorpusId": "198160865"
                },
                {
                    "start": 1360,
                    "end": 1364,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "253020769"
                },
                {
                    "start": 1414,
                    "end": 1418,
                    "matchedPaperCorpusId": "232380330"
                },
                {
                    "start": 1556,
                    "end": 1559,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06463623046875
        },
        {
            "corpus_id": "267068302",
            "title": "Effective Intrusion Detection in Heterogeneous Internet-of-Things Networks via Ensemble Knowledge Distillation-Based Federated Learning",
            "text": "Knowledge distillation, first proposed by Hinton [18], allows transferring the knowledge of a large, complex model (known as the teacher) to a smaller, simpler model (known as the student). The motivation behind knowledge distillation is to train the student model to mimic the behaviour of the teacher model. The process of knowledge transfer usually needs a proxy dataset as the medium. As a result, most of the works will choose a mutually exclusive dataset while some others may use an autoencoder or GAN to generate some synthetic dataset. Once the proxy dataset is chosen, the class probabilities of the model's last layer (logits) or feature representations of middle hidden layers (feature maps) are usually used as the soft targets for knowledge transfer. The reason is that they contain more valuable information than the hard labels used in normal training. \n\nFor the original knowledge distillation, loss typically consists of two terms: a standard cross-entropy loss term and a distillation loss term. The former uses the hard label as the target while the latter uses a soft target. For the federated learning scenario, we usually do not have the publicly labelled dataset. Therefore, the combined loss is reduced to only the distillation loss term. The distillation loss term is often formulated as the Kullback-Leibler (KL) divergence between the teacher and student's softmax output probabilities, shown in Equation (1). \n\nwhere S and T represent the student and teacher logits respectively. \u03c3 is the softmax function and a temperate T is added.",
            "score": 0.5570758377273222,
            "section_title": "III. AN EFFECTIVE HETEROGENEOUS INTRUSION DETECTION SYSTEM FRAMEWORK A. Knowledge Distillation",
            "char_start_offset": 7450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1562
                }
            ],
            "ref_mentions": [
                {
                    "start": 1433,
                    "end": 1436,
                    "matchedPaperCorpusId": "18668597"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1201171875
        },
        {
            "corpus_id": "271961991",
            "title": "Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher",
            "text": "Knowledge distillation (KD) is a widely-used technique to transfer knowledge from a large teacher model into a much smaller student model with minimum sacrifice of teacher model's predictive power [4,11]. The vanilla training objective in KD such as KL loss [11,22,28] encourages the student's outputs to be close to the teacher's outputs as much as possible. This objective implicitly assumes the teacher's output distribution on the distillation data is perfectly aligned with the ground truth distribution. However, in many applications, this assumption does not hold and the teacher's output distributions can be biased from the ground truth due to various factors, such as the inductive bias encoded in the teacher model architecture [34], miscalibration in the training procedure [22], or the bias in the teacher model training set [20,21]. Enforcing the student to blindly imitate the teacher's outputs can make the student inherit such biases and produce suboptimal predictions. \n\nTo overcome this challenge, one common approach [11] suggests scaling the teacher's logits via a temperature parameter. A proper temperature value can enhance the quality of the teacher model's output distribution by making it closer to the true label distribution [22]. However, the shifting space offered by temperature scaling is limited, and the optimal temperature value relies on resourceintensive grid search. Along a separate line, label smoothing [29] is proposed to regularize the neural networks, and modulated loss functions [18,19] are designed to address various statistical issues in model training such as overfitting and data imbalance. Despite their potential, there is a lack of work that explores tailoring such techniques for more robust knowledge distillation.",
            "score": 0.5565382635142753,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1771
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 262,
                    "end": 265,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "235390933"
                },
                {
                    "start": 739,
                    "end": 743,
                    "matchedPaperCorpusId": "235293810"
                },
                {
                    "start": 786,
                    "end": 790,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 838,
                    "end": 842,
                    "matchedPaperCorpusId": "244430074"
                },
                {
                    "start": 1254,
                    "end": 1258,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1526,
                    "end": 1530,
                    "matchedPaperCorpusId": "248405706"
                },
                {
                    "start": 1530,
                    "end": 1533,
                    "matchedPaperCorpusId": "47252984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07989501953125
        },
        {
            "corpus_id": "268060109",
            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
            "text": "Softened outputs. In the process of knowledge distillation, the teacher model's outputs are 'softened' by adjusting the softmax temperature. This is done to create a more informative output distribution, which is crucial for transferring the teacher's knowledge to the student model.",
            "score": 0.5561271625795897,
            "section_title": "Distillation process and softmax loss calculation",
            "char_start_offset": 22876,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 17
                },
                {
                    "start": 18,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 283
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239501953125
        },
        {
            "corpus_id": "268248015",
            "title": "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models",
            "text": "Originally proposed by Hinton et al. [15], knowledge distillation aims to transfer the knowledge of a pretrained heavy teacher model to a lightweight student model. After the distillation, the student can master the expertise of the teacher and be used for final deployment. Specifically, the Kullback-Leibler (KL) divergence loss is utilized to match the output distribution of two models, which can be formulated as follows: \n\nL kd (q t , q s , \u03c4 ) = \u03c4 2 KL(\u03c3(q t /\u03c4 ), \u03c3(q s /\u03c4 )). \n\n( \n\nwhere q t and q s denote the logits predicted by the teacher and student. \u03c3(\u2022) is the softmax function and \u03c4 is the temperature [15,31] which controls the softness of distribution.",
            "score": 0.5560116045873226,
            "section_title": "Background",
            "char_start_offset": 12288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 623,
                    "end": 626,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.361083984375
        },
        {
            "corpus_id": "246904890",
            "title": "Preservation of the Global Knowledge by Not-True Distillation in Federated Learning",
            "text": "We explore how the MSE loss on Not-True Distillation acts. In Table 14, the MSE version FedNTD (FedNTD (MSE)) shows better accuracy and less forgets as \u03b2 grows, but at some degree, the model diverges; thereby cannot reach the original FedNTD, which exploits softmax and KL-Divergence loss to distill the knowledge in the global model. We explain it as matching all not-true logits using MSE logits is too strict to learn the global knowledge since the dark knowledge is mainly contained in top-k logits. FedNTD controls the class signals by using temperature-softened softmax.",
            "score": 0.5552892117615574,
            "section_title": "L MSE Loss for Not-True Distillation",
            "char_start_offset": 30932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1767578125
        },
        {
            "corpus_id": "265445906",
            "title": "Bridging Classical and Quantum Machine Learning: Knowledge Transfer From Classical to Quantum Neural Networks Using Knowledge Distillation",
            "text": "Initially, we set up experiments to find the best hyperparameters. In the case of knowledge distillation, hyperparameters \u03c4 and \u03b1 play the most important role in the model performance. Thus, we create the hyperparameter space \u03c4 = [1, 2, 5] and \u03b1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], and perform experiments by dividing the training sets into training and validation sets. This helped us to select the optimal values of temperature (\u03c4 ) and alpha (\u03b1), which are  From the heatmap, we observe that the student model benefits from smaller values of T and \u03b1 in general. Specifically, lower temperatures tend to sharpen the probability distribution, which helps the student model learn more confidently from the teacher's predictions. This signifies that the student model benefits from a more focused and less noisy learning signal. Similarly, a smaller \u03b1 indicates a lower reliance on the distilled knowledge from the teacher, which seems to improve the student's performance. This implies that the student model can effectively leverage the teacher's knowledge to achieve higher accuracy with lower weightage on supervisory signals through the distillation process. Concurrently, it implies that the student model possesses a certain degree of inherent capability or intuition that allows it to perform well even without heavily relying on the distilled knowledge from the teacher. This inherent capability might be attributed to the student model's structure, its initial conditions, or its ability to generalize from limited information. The observed trend suggests that while knowledge distillation helps in improving the student model's performance, overly depending on the teacher's output could potentially constrain the student model's ability to explore and learn from the data itself.",
            "score": 0.5552769442563141,
            "section_title": "Preliminary Results",
            "char_start_offset": 44641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1808
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0253753662109375
        },
        {
            "corpus_id": "250526328",
            "title": "Large\u2010scale knowledge distillation with elastic heterogeneous computing resources",
            "text": "In this section, we present the background of elastic deep learning distillation, i.e., knowledge distillation. Then, we introduce the methods for distributed and decentralized training and elastic computing resources. \n\nModel compression based on knowledge transferring was first proposed in 7 in order to compress an ensemble of models or a large model to a compact model. The ensemble of models or the large model take much storage space and require a long time for inference while the compact model requires relatively small storage space and a short time for inference. Knowledge distillation is based on the popular machine learning Softmax function and a temperature 8 , which is defined in Formula 1. \n\n, where is the output from the output layer of a teacher neural network; is a temperature, which indicates the impact of the output from the teacher model. The Softmax output layer computes the probability that the input data corresponds to each class with the corresponding computed logit. The probability is related to a temperature T, i.e., which represents the impact of the distilled knowledge of the teacher model on the student model. A higher temperature corresponds to a weaker impact. As shown in Figure 1, during the training of knowledge distillation, two neural networks are used: teacher model and student model. The student model is trained based on the combination of two loss values. The first loss value is calculated from a soft prediction, which contains the probability of each class calculated from the teacher model. The soft prediction is calculated using Formula 1. The other loss value corresponds to a hard prediction, which is the ground truth label from the training data. \n\nTwo types of work are also related to this paper, i.e., mutual knowledge distillation and model pruning. When there is no available teacher model, the teacher and student can be trained at the same time, i.e., an ensemble of students can learn collaboratively and teach each other throughout the training process 16,17 , which is the mutual knowledge distillation 9 . Note that the mutual knowledge distillation is also denoted \"online knowledge distillation\" in 9 . However, we use the \"online knowledge distillation\" to represent the knowledge distillation with the teacher model and the student model deployed in the same GPU card during the training process of knowledge distillation in this paper.",
            "score": 0.5551455351655227,
            "section_title": "RELATED WORK",
            "char_start_offset": 3556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 218
                },
                {
                    "start": 221,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 294,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 674,
                    "end": 675,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2031,
                    "end": 2033,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 2079,
                    "end": 2080,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 2178,
                    "end": 2179,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44140625
        },
        {
            "corpus_id": "275118790",
            "title": "Exemplar-condensed Federated Class-incremental Learning",
            "text": "Another important part of the dual-distillation structure is knowledge distillation, which is used to transfer knowledge from previous tasks to new tasks, thereby mitigating the problem of catastrophic forgetting. We directly implement Knowledge Distillation (Rebuffi et al. 2017 Mathematically, let p t\u22121 (x) denote the probability distribution (softmax output) of the teacher model after training on the previous task t \u2212 1, and p t (x) denote the distribution of the student model being trained on the current task t. The goal is to minimize the following objective function: \n\nHere, L ce represents the task-specific cross-entropy loss for the current task t, such as cross-entropy loss, and L KL is the loss function of Kullback-Leibler divergence KL(p t\u22121 (x) \u2225 p t (x)) between the teacher's and student's output distributions, which serves as the distillation loss. The parameter \u03bb controls the balance between the task loss and the distillation loss. By minimizing this objective, the student model learns to perform well on the new task while retaining knowledge from previous tasks, thus reducing catastrophic forgetting.",
            "score": 0.5543187542838088,
            "section_title": "Prior Knowledge Supervision.",
            "char_start_offset": 28320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1132
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 279,
                    "matchedPaperCorpusId": "206596260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0794677734375
        },
        {
            "corpus_id": "264490946",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model",
            "text": "Knowledge Distillation (KL distillation) was pioneered by Hinton et al. (2015), which suggests minimizing the KL divergence between a teacher and a student model's soft targets \u03c3(z t ) and \u03c3(z s ): with temperature T . We use Eq. 1 as our base transfer objective (KL-Dist transfer), as it still remains popular and competitive (Beyer et al., 2022;Rajasegaran et al., 2020;Tian et al., 2020) (see Supp. for other objectives). KL distillation is often extended with auxiliary classification losses (e.g. crossentropy L XE , (Hinton et al., 2015;Tian et al., 2020;Rajasegaran et al., 2020;Beyer et al., 2022)) to stabilize the distillation process. We denote the \u03bb-weighted combination as XE-KL distillation, and the associated transfer as XE-KL-Dist. transfer or XE-KL: \n\nMost knowledge distillation research considers the distillation from a trained teacher to an untrained student, in stark contrast to our goal of knowledge transfer between pretrained models while retaining student knowledge already gained a priori. And indeed, when applied to knowledge transfer between pretrained models in \u00a75.1, standard knowledge distillation struggles to transfer knowledge without performance drops for most teacher-student pairs. We measure this via the transfer delta \u2206 transf = acc(f s kt ) \u2212 acc(f s ), which quantifies the change in the student's top-1 accuracy, with f s kt being the student model following the knowledge transfer.",
            "score": 0.5540982736068658,
            "section_title": "KNOWLEDGE TRANSFER THROUGH KNOWLEDGE DISTILLATION",
            "char_start_offset": 14411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 372,
                    "matchedPaperCorpusId": "219720852"
                },
                {
                    "start": 372,
                    "end": 390,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 543,
                    "end": 561,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 561,
                    "end": 586,
                    "matchedPaperCorpusId": "219720852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03271484375
        },
        {
            "corpus_id": "270377282",
            "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm",
            "text": "However, negative labels also contain a great deal of information. Some negative labels may correspond to higher probabilities than others, and even contain more information than positive labels. In order to obtain more information from negative labels, knowledge distillation introduces a temperature variable, denoted by T. By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels. \n\nwhere q i is the \"softened\" probability vector, obtained by exponential operation and normalization; Z i indicates the logit value for the current category; j denotes the number of output nodes (number of categories); Z j represents the logit value for each category output by the full connectivity layer; T is the temperature parameter, and when T = 1, the function is the original Softmax function. The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,23 . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function. The second step is to perform Softmax computation on the logits output of the student network to get the hard predicted values. The hard predicted values were compared to the actual labels and the difference between them was measured using the student loss function. The two parts of the loss function are added together to get the total loss function, which is calculated as where V loss is the value of total loss function; V loss-SL is the value of the student loss function; V loss-KD is the value of distillation loss function; \u03b1 is the scaling factor, which is used to adjust the weights of the two loss functions. When \u03b1 is equal to zero, this corresponds to the network not being distilled and trained using only the student loss function.",
            "score": 0.5539877391483787,
            "section_title": "Fermented grains surface temperature detection based on YOLO v5n",
            "char_start_offset": 12832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2246
                },
                {
                    "start": 2247,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 989,
                    "end": 991,
                    "matchedPaperCorpusId": "264462123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "3643430",
            "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy",
            "text": "We introduce the concept of knowledge distillation in this section. Bucilu\u01ce et al. (2006), Hinton et al. (2015) and Urban et al. (2016) analyze this topic in great detail. \n\nwhere, W T and W A are the parameters of the teacher and the student (apprentice) network, respectively, y is the ground truth, H(\u2022) denotes a loss function and, \u03b1, \u03b2 and \u03b3 are weighting factors to prioritize the output of a certain loss function over the other. In equation 1, lowering the first term of the cost function gives a better teacher network and lowering the second term gives a better student network. The third term is the knowledge distillation term whereby the student network attempts to mimic the knowledge in the teacher network. In Hinton et al. (2015), the logits of the teacher network are divided by a temperature factor \u03c4 . Using a higher value for \u03c4 produces a softer probability distribution when taking the Softmax of the logits. In our studies, we use cross-entropy function for H(\u2022), set \u03b1 = 1, \u03b2 = 0.5 and \u03b3 = 0.5 and, perform the transfer learning process using the logits (inputs to the Softmax function) of the teacher network. In our experiments we study the effect of varying the depth of the teacher and the student network, and the precision of the neurons in the student network.",
            "score": 0.5533745402440111,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 12530,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 171
                },
                {
                    "start": 174,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 89,
                    "matchedPaperCorpusId": "11253972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2247314453125
        },
        {
            "corpus_id": "225066713",
            "title": "Empowering Knowledge Distillation via Open Set Recognition for Robust 3D Point Cloud Classification",
            "text": "Interestingly, there is also a slight improvement in the open set capabilities of the distilled student model, which we attribute to the supervision recieved from the teacher network during the proposed distillation training. This finding implies that the along with the transfer of dark knowledge from teacher to student as proposed by [6], distillation process also transfers open set capability to the distilled student.",
            "score": 0.5533458400220429,
            "section_title": "Knowledge Distillation Performance",
            "char_start_offset": 23750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0121002197265625
        },
        {
            "corpus_id": "258762557",
            "title": "Student-friendly Knowledge Distillation",
            "text": "The process of vanilla knowledge distillation (KD) [15] is shown in Figure 1(a). For the training data x with the label y in a dataset with K classes, the outputs of the teacher model and the student model are g t \u2208 R K and g s \u2208 R K , respectively. Using the softmax function yields the student's prediction p s = softmax (g s ) \u2208 R K , and we can then compute the cross-entropy loss between the student's prediction and the ground-truth label: \n\nUsing the softmax function with temperature, we obtain the softened teacher prediction p t = softmax (g t /T ) \u2208 R K and the softened student prediction p s = softmax (g s /T ) \u2208 R K . Through the temperature, the predictions become smoother over each class, so the distillation loss is better able to reflect the differences between the other classes in addition to the correct one. Then, we can compute the distillation loss between the softened predictions with the KL divergence: \n\nThe total loss of KD is: \n\nwhere \u03b1 and \u03b2 are coefficients used to balance the two parts. Knowledge distillation optimizes the student model by optimizing this loss function.",
            "score": 0.5527324229347481,
            "section_title": "Vanilla knowledge distillation",
            "char_start_offset": 4944,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.460205078125
        },
        {
            "corpus_id": "253107531",
            "title": "Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation",
            "text": "Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge of (commonly) a larger and better performing teacher model to a student model (Buciluundefined et al., 2006). The idea is to mix the ground-truth label with the model probability distribution of a teacher model, resulting in an adaptive version of label smoothing (Tang et al., 2021).\n\n(2) \u03d5 and \u03b8 denote the parameters of a teacher model and a student model respectively.P indicates a probability distribution smoothed with a temperature. Similar to label smoothing, \u03d5 remains unchanged in training; thus a student model is learned to minimize the KL divergence between its probability distribution and that of the teacher model. WhenP \u03d5 follows a uniform distribution with the temperature set to 1, the loss function of knowledge distillation is identical to that of uniform label smoothing.\n\nTraining a large teacher model can be computationally expensive; for this reason, there have been attempts to replace the teacher model with the student model itself, called self-knowledge distillation (Zhang et al., 2019;Yuan et al., 2020;Kim et al., 2021;Zhang and Sabuncu, 2020). TF-KD (Yuan et al., 2020) trains a student with a pre-trained teacher that is identical to the student in terms of structure. SKD-PRT (Kim et al., 2021) utilizes the previous epoch checkpoint as a teacher with linear increase in \u03b1. (Zhang and Sabuncu, 2020) incorporates beta distribution sampling (BETA) and self-knowledge distillation (SD), and introduce instance-specific prior label distribution. (Yun et al., 2020) utilizes self-knowledge distillation to minimize the predictive distribution of samples with the same class, encouraging consistent probability distribution within the same class.",
            "score": 0.5525520441880896,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6578,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 44,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 159,
                    "end": 189,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1116,
                    "end": 1133,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1133,
                    "end": 1157,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1165,
                    "end": 1184,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 1293,
                    "end": 1311,
                    "matchedPaperCorpusId": "233714221"
                },
                {
                    "start": 1391,
                    "end": 1416,
                    "matchedPaperCorpusId": "219558831"
                },
                {
                    "start": 1560,
                    "end": 1578,
                    "matchedPaperCorpusId": "214727822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0153656005859375
        },
        {
            "corpus_id": "248987313",
            "title": "The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains",
            "text": "We define the degree of contribution balance to be simply evaluating the standard deviation of all parameter sensitivity. A lower standard deviation means that there is a more balanced contribution. \n\nA Case Study on Knowledge Distillation We here take naive knowledge distillation (KD) (Hinton et al., 2015) as a case study. We tie the success of KD to the more balanced contribution  among parameters. KD aims to transfer knowledge from a teacher model to a student model. Specifically, the student model tries to minimize the Kullback-Leibler (KL) divergence between its output p s and the gold label y, and between p s and output of the teacher p t . We here formulate a naive KD objective. \n\nmin KL(y \n\nCommonly, the teacher is a high-capacity model and student is more compact. However, recent studies (Furlanello et al., 2018;Zhang et al., 2019;Fang et al., 2020) show that the student model can significantly outperform the teacher when the student use the same architecture (and consequently, number of parameters) as the teacher, termed self-distillation. \n\nUsing the previously described machine translation task in Section 2.2, we conduct self-distillation experiments and iterate self-distillation twice, i.e., the student taught by the regular transformer model becomes a new teacher for the next student. In Table 1, we report sacreBLEU (Post, 2018). Similar to the previous literature, model performance substantially increase after each round of self-distillation. This surprising result is referred to in the literature as 'dark knowledge' (Gotmare et al., 2018;Zhang et al., 2019). Some studies try to understand the 'dark knowledge', e.g., in the view of regularization (Yuan et al., 2020) or ensemble (Allen-Zhu and Li, 2020), but they only explain how it leads to performance improvements instead of how the model itself changes. Here, we argue that the 'dark knowledge' transferred from teachers to such students is actually due to the more balanced contribution among parameters. We visualize the sensitivity distribution of all models via violin plots with their standard deviation in Figure 3 4 .",
            "score": 0.5523484348020891,
            "section_title": "Degree of Contribution Balance",
            "char_start_offset": 7244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 705
                },
                {
                    "start": 708,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 833,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 833,
                    "end": 852,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 852,
                    "end": 870,
                    "matchedPaperCorpusId": "231592453"
                },
                {
                    "start": 1352,
                    "end": 1364,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 1558,
                    "end": 1580,
                    "matchedPaperCorpusId": "53107519"
                },
                {
                    "start": 1580,
                    "end": 1599,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 1690,
                    "end": 1709,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0233306884765625
        },
        {
            "corpus_id": "252565150",
            "title": "Selective Cross-Task Distillation",
            "text": "If we already have a well-trained model f T on label set C T , we can extract \"dark knowledge\" from it [48], [49] \n\n(2) Here \u03c1 \u03c4 is the softmax function with temperature \u03c4 , \n\nC is the number of classes and the dimension of f (x), and \u03bb \u2265 0 in Equ. ( 2) is a hyper-parameter that balances two terms. This method considers the teacher's output class distribution as a soft label, and uses KL divergence to pull student's output closer to it. The teacher model usually has a larger capacity [41], and its output can contain more information than one-hot label such as class correlations and instance relationships [50]. Trained with both instance labels and teacher's supervision, the student model can obtain a higher accuracy and converge faster.",
            "score": 0.5522588578317763,
            "section_title": "Standard Knowledge Distillation",
            "char_start_offset": 18611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 173
                },
                {
                    "start": 176,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 746
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 107,
                    "matchedPaperCorpusId": "174800711"
                },
                {
                    "start": 109,
                    "end": 113,
                    "matchedPaperCorpusId": "235826057"
                },
                {
                    "start": 489,
                    "end": 493,
                    "matchedPaperCorpusId": "212908749"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "211068747"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419189453125
        },
        {
            "corpus_id": "272969060",
            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
            "text": "Logits-based KD was first proposed in [9], where the student learns the logits knowledge from the teacher through KL divergence. In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions: \n\nFeature-based KD transfers the knowledge of intermediate layers features from the teacher to the student. Generally, we can formulate such distillation methods as: \n\nwhere F t i and F s i are the corresponding features of the teacher and student at i-th layer. w(\u2022) is a mapping function that aligns the dimensions of features from the student to the teacher. \n\nFrom the above distillation process, it can be seen that logits-based knowledge distillation focuses on constraining the overall knowledge distribution. In contrast, feature-based knowledge distillation imposes pixel-level constraints on the student network. These two methods have distinct optimization objectives during the distillation process, and directly integrating them can lead to suboptimal solutions. In this study, we aim to achieve comprehensive knowledge transfer by conducting knowledge distillation on the intermediate layer features and the final layer logits. To address this issue, we introduce UniKD, a meticulously designed framework that adjusts the intermediate layer features for a thorough and consistent knowledge transfer, using the same constraints employed for the final layer's logits.",
            "score": 0.5514709252116656,
            "section_title": "Preliminaries",
            "char_start_offset": 8573,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "259378480",
            "title": "Review of Recent Distillation Studies",
            "text": "A new temperature scaling method, the asymmetric temperature scale (ATS), is proposed here to make the distribution of error classes during distillation more discriminable. the ATS applies different temperatures to logits in correct and incorrect classes, allowing for more flexible and differentiated distillation instruction during instruction. This approach is particularly beneficial when teachers are overconfident, as it reduces the size of correct category logits, increases the diversity of error category logits, and ultimately increases the variance and accuracy of the derivations. Experiments using different datasets and network architectures are described here, including ResNet, WideResNet, ResNeXt, VGG, ShuffleNet, AlexNet, MobileNet, and DSCNN. the training setup mainly follows previous studies with slight modifications based on the dataset. The learning rate decayed every 30 cycles, with a batch size of 128 for CIFAR and 64 for the other datasets. The researchers investigated the phenomenon that having more accurate teacher models does not necessarily lead to better student model learning in knowledge distillation. They identified three factors of successful teachers:correct instruction, smooth regularization, and class differentiation. They propose a solution called asymmetric temperature scaling (ATS) to enhance the derived variance of large teachers to make their distillation labels more discriminatory when teaching. Experiments have shown that the method is effective. So far, researchers are studying why large teachers have difficulty teaching knowledge distillation (KD) effectively. They hope their research provides a new perspective on the KD field, but they do not anticipate any negative social impact from their work.",
            "score": 0.5514415108875619,
            "section_title": "Symmetric temperature scaling",
            "char_start_offset": 17771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1763
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044342041015625
        },
        {
            "corpus_id": "258048760",
            "title": "A Survey on Recent Teacher-student Learning Studies",
            "text": "The classical KD method, which minimizes the loss of kl dispersion between two predictions obtained from the teacher/student model, has a fixed temperature in the softmax layer. A new approach, called Course Temperature for Knowledge Distillation (CTKD), is proposed here, which improves the performance of knowledge distillation by gradually increasing the learning difficulty of students through dynamic and learnable temperatures. The loss of distillation between the instructor and the student is maximized by learning the temperature in a reverse gradient during the student's training. The method is easy to implement and achieves overall improvement at negligible additional computational cost. The concepts of two domains, course learning and knowledge distillation, are discussed here. Course learning is a training strategy that organizes learning tasks into incremental difficulty and has applications in several fields such as computer vision and natural language processing. Knowledge distillation is a method for transferring knowledge from a pre-trained teacher model to a smaller student model. Existing approaches have been designed to transfer knowledge in various forms, including logic-based, representation-based, and relationalbased approaches. Temperature hyperparameters control the difficulty level of the distillation process, with lower values concentrated on the maximum logarithm and higher values flattening the distribution. Most of the work fixes temperature as a hyperparameter, but recently MKD proposes to learn temperature through meta-learning. The proposed CTKD method is more efficient than the MKD method and can be seamlessly integrated into the existing KD framework. The technique of knowledge distillation course temperature (CTKD) consists of a learnable temperature module to predict the suitable temperature for distillation and a gradient inversion layer to invert the gradient of the temperature module during back propagation. The technique uses an easy-to-hard curriculum that progressively increases the learning difficulty for students. A method called adversarial distillation is involved here, which involves optimizing the student model to minimize taskspecific losses and distillation losses. Distillation losses are calculated using a dynamic temperature value called tau. The authors suggest using an adversarial approach to learn the optimal temperature value where the temperature module is optimized in the opposite direction of the student to maximize the distillation loss.",
            "score": 0.5511322897922417,
            "section_title": "Curriculum distillation",
            "char_start_offset": 24089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2249
                },
                {
                    "start": 2250,
                    "end": 2330
                },
                {
                    "start": 2331,
                    "end": 2537
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "278171126",
            "title": "Swapped Logit Distillation via Bi-level Teacher Alignment",
            "text": "Knowledge Distillation. According to the classical KD [15], the softmax function plays an important role in converting the logit into a probability over classes, indicating how likely each class is for a given input z \u2208 R C . The softmax function can be calculated by: Fig. 2 Overview of SLD framework. After obtaining the logit outputs, swapping methods are applied to teacher logits, producing main teacher and pseudo-teacher logits. Loss scheduling is proposed to avoid teacher conflicts when both teachers are integrated. Prediction augmentation is used to generate outputs at various softness levels. Real misclassified samples of the teacher's prediction illustrate how a model prediction can mislead the student model's learning since the target and the prediction share similarities in color, shape, and textures (where the red text is the target, and the black one is the false prediction). \n\nwhere z j and p j represents the logit and probability output on the j-th class, respectively. T is the temperature scaling to scale the smoothness of the distribution. To measure the match of the teacher and student's output, the Kullback Leibler (KL) divergence loss is adopted as follows: \n\nwhere L KD is the KL loss, p tea j and p stu j are the teacher's and student's probability output on the j-th class, respectively.",
            "score": 0.5508894019334746,
            "section_title": "Background",
            "char_start_offset": 7885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1326
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1895751953125
        },
        {
            "corpus_id": "276938088",
            "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
            "text": "These methods reported improved performance over the conventional global temperature setting, suggesting that dynamic temperature settings promote knowledge distillation. Sun et al. [19] showed the irrelevance between the temperatures of different samples using the Lagrange multiplier and that the issue of the shared temperatures is solved by z-score standardization. The temperature derived from this multiplier provides different temperatures for various samples. \n\nThe adaptive temperature can treat it as the approximation of the probabilities. The logits by a student get close to ones of a teacher by the parameter. Several works explored alternatives to softmax used in KL divergence. Taylor Softmax [21] is an alternative method to softmax that uses Taylor expansion to approximate functions. Taylor expansion expresses a function as the sum of n derivatives. This approximation is valid within a certain range of neighboring points. SM-Taylor softmax [1] is a generalization of Taylor softmax and introduces soft margins. The additivity of Taylor expansion has desirable properties for matrix operations. In this study, we approximate the softmax and log softmax functions and show that the approximation converges to the correlation between logits according to temperature scaling.",
            "score": 0.5508082461664172,
            "section_title": "Related Work",
            "char_start_offset": 5288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1293
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "978861"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "227127574"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2034912109375
        },
        {
            "corpus_id": "258156357",
            "title": "A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition",
            "text": "Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy. The student loss as given in Equation (3) computes the difference between the student's prediction and the ground truth based on which the student model then generates the output class probabilities using standard softmax function as given in Equation ( 4). The reason for using standard softmax function instead of the normalized softmax function is that the student loss computes the difference between the predictions made by the student and the ground truth. \n\nHere in Equation ( 3), the variable h represents the final predictions for a specific class (derived from the student probability vector) and the variable y denotes the ground truth. The final loss function is the weighted sum of the distillation loss and the student loss as given in Equation ( 5) \n\nwhere Student_Loss and Distillation_Loss denote the student loss and the mutual distillation loss of the teacher and the student model, respectively, and the variable \u03b1 is the weight factor of the final loss that defines contribution of the student and the distillation loss in the final loss.",
            "score": 0.5501498647227242,
            "section_title": "Spatio-Temporal Knowledge Distillation from Teacher to Student Model",
            "char_start_offset": 31830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1770
                },
                {
                    "start": 1773,
                    "end": 2066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6474609375
        },
        {
            "corpus_id": "276428017",
            "title": "Decoupled Feature and Self-Knowledge Distillation for Speech Emotion Recognition",
            "text": "The concept of knowledge distillation, first introduced by Hinton [27], involves transferring knowledge by minimizing the KL divergence between the predictions of a teacher network and a student network. Various distillation techniques have since been proposed [28], [29], [30], [31], [32], and recent developments have led to self-distillation techniques [33], [34], [35]. \n\nZipf's Law is a statistical principle that describes the inverse relationship between word frequency and rank in natural language, which has parallels in neural network outputs in deep learning [36]. After sufficient training, the final softmax layer's outputs, when sorted and averaged, exhibit a Zipf distribution similar to that of word frequencies. This observation has informed self-distillation in studies like [36] and [37], which decouples the distillation loss function into target and non-target classes, applying Zipf's Law to the non-target component to improve performance. Our approach builds on this by introducing a distillation temperature T in the design of smoothed target labels, amplifying the differences in target probabilities across samples before normalizing the output with softmax.",
            "score": 0.5499208117755424,
            "section_title": "B. SELF-KNOWLEDGE DISTILLATION",
            "char_start_offset": 6693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1185
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "252876184"
                },
                {
                    "start": 267,
                    "end": 271,
                    "matchedPaperCorpusId": "257654415"
                },
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "259138547"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "235780986"
                },
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "256877744"
                },
                {
                    "start": 356,
                    "end": 360,
                    "matchedPaperCorpusId": "249436459"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "236919340"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "246607907"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "251066725"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "251066725"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "257687846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2476806640625
        },
        {
            "corpus_id": "265445809",
            "title": "Cosine Similarity Knowledge Distillation for Individual Class Information Transfer",
            "text": "This manipulation decreases the biased prediction of student model and allows the student model to dynamically learn from the teacher's knowledge, rather than being restricted by the teacher's fixed distribution. As a result, our method can more effectively enhance the knowledge distillation process compared to existing approaches by fine-tuning predictions for non-target classes Li et al. (2022). This is supported by entropy analysis, which is detailed in the experiments section. Furthermore, we suggest dynamic temperature scaling for individual samples, a method we refer to as \"Cosine Similarity Weighted Temperature\" (CSWT), which was mentioned as process (2). This approach adjusts temperatures based on the similarity between prediction of teacher and student model. The CSWT conveys more confident information by setting lower T i when the cosine similarity is high, and transfer richer information about the non-target class by setting higher T i when the cosine similarity is low. This effect has the advantage of providing more optimized knowledge for the student model than using a fixed temperature scaling factor. \n\nOur contributions can be summarized as follows: \n\n\u2022 We treat the predicted values from both the student and teacher models as vectors and employ cosine similarity to minimize the angle between these two vectors. Due to the scale-invariant property of cosine similarity, students learn more insights from teachers that encompass diverse possibilities. \n\n\u2022 We suggest a Cosine Similarity Weighted Temperature (CSWT), which adjusts the temperature based on the cosine similarity value, enabling the student model to receive the most suitable information for each sample. \n\n\u2022 Extensive experiments conducted on various datasets serve as evidence for the effectiveness of our proposed methods. Our approach achieves results comparable to that of the teacher model and, in some cases, even outperforms it.",
            "score": 0.5498432848497219,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 399,
                    "matchedPaperCorpusId": "252780669"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06439208984375
        },
        {
            "corpus_id": "277994315",
            "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
            "text": "Several studies have utilized the temperature of the softmax function, with knowledge distillation (KD) [5,13,14,15] and temperature scaling [6,16] serving as representative examples. Knowledge distillation is a method for transferring knowledge from a teacher model to a student model. This method aims to improve performance and reduce the number of model parameters. In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance. Liu et al. [13] proposed Meta KD, which optimizes T via meta-gradients computed on a validation set. Curriculum Temperature KD (CTKD) [14] introduces an adversarial learning scheme that directly learns the temperature parameter itself. Sun et al. [15] achieve a dynamic temperature by applying logit standardization independently to both the teacher and the student models. Furthermore, temperature scaling [6] reduces the model's prediction bias by correcting temperature parameters. Balanya et al. [16] utilized adaptive temperature in the temperature scaling. In contrastive learning, the temperature parameter is also adjusted to modulate the influence of negative samples [17]. \n\nIn addition to these approaches, Agarwala et al. [10] also highlighted the importance of tuning the temperature parameter in classification tasks. Furthermore, relaxed softmax [18] also proposes a tuning strategy for inverse temperature. In fields other than classification, temperature parameters have attracted attention in deep metric learning. Xu [19] proposed a heated-up strategy that involves training the model with increasing temperature. \n\nHowever, none of the above studies discusses the relationship between the dimensionality of the feature maps M and the optimal temperature T * . While these studies note the importance of tuning the temperature parameter, they do not sufficiently address the question of whether the optimal temperature parameter can be estimated without training the model. \n\nBuilding upon these general insights into temperature tuning, a study in sensor-based human activity recognition has explored the relationship between feature dimensionality and temperature [9]. Denoting the number of dimensions of the output of the feature extractor (e.g., ConvNets) as M , the experimental results indicate a potential relationship between the optimal temperature T * and M . However, there were several limitations: \n\n1.",
            "score": 0.5496870777242786,
            "section_title": "Temperature parameter",
            "char_start_offset": 8052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2398
                },
                {
                    "start": 2399,
                    "end": 2439
                },
                {
                    "start": 2442,
                    "end": 2444
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 107,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 110,
                    "end": 113,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 144,
                    "end": 147,
                    "matchedPaperCorpusId": "251224012"
                },
                {
                    "start": 643,
                    "end": 647,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 916,
                    "end": 919,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1009,
                    "end": 1013,
                    "matchedPaperCorpusId": "251224012"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "229297730"
                },
                {
                    "start": 1370,
                    "end": 1374,
                    "matchedPaperCorpusId": "53530110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "252510341",
            "title": "Automatic Modulation Classification with Neural Networks via Knowledge Distillation",
            "text": "computed using the same high temperature in the softmax of the distilled model that was used to generate the soft targets from the cumbersome model. The second is cross entropy with the correct labels. Since the magnitudes of the gradients produced by the soft target scale as 1/T 2 , it is important to multiply them by T 2 when using both hard and soft targets [19].\n\nThe process of knowledge distillation is shown in Figure 5, and its loss function [35] is\n\nwhere T and \u03b1 are hyperparameters, T refers to the temperature of distillation, and \u03b1 refers to the proportion of soft loss in the total loss.",
            "score": 0.5492898876731451,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 13019,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 367,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138671875
        },
        {
            "corpus_id": "268729379",
            "title": "Lightweight Knowledge Distillation-Based Transfer Learning Framework for Rolling Bearing Fault Diagnosis",
            "text": "Knowledge distillation models typically consist of two main stages: the first stage involves the teacher model inferring the training data to obtain soft labels for the classification task, while the second stage entails training the student model using the richer information contained in the soft labels [28].This process is illustrated in Figure 2. Normally, the model's prediction results represent the probability predictions for each class in the classification task after passing through the Softmax classification layer.However, these probabilities often do not contain information about the similarity between different classes, which can weaken the learned feature information to some extent.Therefore, the teacher model introduces a temperature factor T into the Softmax function to capture the similarity information between different classes in the classification task, as shown in the following equation:\n\nwhere z i and z j are the inputs to the Softmax function, q i represents the predicted probabilities for each class in the classification task, and T is the temperature factor.Introducing the temperature factor makes the output probabilities of Softmax smoother.When T = 1, Equation ( 5) is equivalent to the traditional Softmax classifier.\n\nSensors 2024, 24, x FOR PEER REVIEW 5 of 17\n\n(1 )\n\nwhere \u03b1 represents the weighting of the model loss considering soft labels.",
            "score": 0.5489835685815639,
            "section_title": "Knowledge Distillation Model with Soft Labels",
            "char_start_offset": 9576,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 702
                },
                {
                    "start": 702,
                    "end": 918
                },
                {
                    "start": 920,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1260
                },
                {
                    "start": 1262,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1311
                },
                {
                    "start": 1313,
                    "end": 1388
                }
            ],
            "ref_mentions": [
                {
                    "start": 306,
                    "end": 310,
                    "matchedPaperCorpusId": "250931984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56005859375
        },
        {
            "corpus_id": "258486778",
            "title": "A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation",
            "text": "Self distillation student/ teacher where T is the \"temperature\" parameter that is usually set to 1 in the regular softmax, whereas in the distillation setting, it is used to control the softness of the probability distribution over classes. \n\nThere are several variants of distillation, using different types of loss functions and different options for what dataset is used to distill the knowledge to the student model (called transfer set or proxy dataset). For example, the transfer set could be a large public unlabeled dataset, a public labeled dataset, or even the teacher's original training data. If the transfer set is labeled, the student can be trained using a linear combination of two loss functions: \n\nwhere  and (1\u2212) are the weights assigned to the Kullback-Leibler (KL) divergence (i.e., distillation loss) and the cross entropy (CE) loss, respectively,  is the true label hot encoded,   and   are the class probabilities (soft targets) predicted by the student model and the teacher model, respectively. Figure 1 illustrates the knowledge distillation pipeline. Distillation approaches. The distillation literature has investigated three learning approaches of knowledge distillation: offline distillation, online distillation, and self-distillation. Figure 2 summarizes the distillation approaches. We provide a brief description below; for details, we refer the reader to a recent survey [8]. \n\nThe offline distillation is a traditional way commonly used for model compression. A pre-trained teacher is used to train an untrained student using a loss term that encourages the student's predictions to match the predictions of the teacher model [10]. Most previous knowledge distillation works use the offline method [8]. In the online distillation, both the teacher and the student are untrained, and they learn collaboratively and teach each other throughout the training process [1,27]. In self-distillation, the same networks are used for the teacher and the student models, and the knowledge is transferred in the same model, from the deeper layers to the shallow layers [26].",
            "score": 0.5489192353095854,
            "section_title": "Untrained student",
            "char_start_offset": 4704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 713
                },
                {
                    "start": 716,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 1407,
                    "end": 1410,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1735,
                    "end": 1738,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 1903,
                    "end": 1906,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277099609375
        },
        {
            "corpus_id": "265351966",
            "title": "Leveraging different learning styles for improved knowledge distillation in biomedical imaging",
            "text": "Knowledge Distillation [17] is an approach introduced to transfer the knowledge in terms of probability outputs, p i , from a complex, highly parameterized pre-trained teacher network f (X, \u03d5) to a simple and compact student network g(X, \u03b8) to achieve model compression while retaining the high performance of the teacher.Given a training set with N samples X = {x i } N i=1 with corresponding labels Y = {y i } N i=1 , the teacher network f (X, \u03d5), is trained on the ground truth labels.The probabilistic output of a teacher network for a sample x i is defined as p i given by the extended softmax as:\n\nwhere z c corresponds to the logits, C is the number of classes, and T is the temperature parameter to get a smoother output\n\nPredictions, Predictions Features, Features Features, Predictions T \u2192 S 2 , S 1 \u2192 S 2 Predictions, Predictions Features, Features Predictions, Features probability distribution of the classes.Generally, the objective function for the teacher network is the standard Cross-Entropy (CE) error defined as:\n\nNow, the student networks are trained on the combined loss of Cross-Entropy (CE), and Knowledge Distillation (KD), where the CE helps the student networks to adhere to the ground truth labels and KD assists them to align their learning with that of the teacher.Here, Kullback Leibler (KL) divergence [28] is used for L KD p to measure the correspondence between the teacher and student predictions p i and s i respectively as:\n\nFinally, the loss function for the student network is the weighted (\u03b1) summation of the cross entropy (L CE ) and knowledge distillation (L KD p ) terms:\n\nwhere hyperparameter \u03b1 is used to balance the contributions of the hard target loss (L CE ) and soft target loss (L KD p ) during the distillation process for each student.The knowledge can be transferred in an online or offline manner from the teacher to the student networks.",
            "score": 0.5487360500743952,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6794,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 322
                },
                {
                    "start": 322,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 602
                },
                {
                    "start": 604,
                    "end": 728
                },
                {
                    "start": 730,
                    "end": 922
                },
                {
                    "start": 922,
                    "end": 1032
                },
                {
                    "start": 1034,
                    "end": 1295
                },
                {
                    "start": 1295,
                    "end": 1460
                },
                {
                    "start": 1462,
                    "end": 1615
                },
                {
                    "start": 1617,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1894
                }
            ],
            "ref_mentions": [
                {
                    "start": 1334,
                    "end": 1338,
                    "matchedPaperCorpusId": "116908168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "269982135",
            "title": "Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch",
            "text": "Knowledge Distillation (KD) could transfer the ``dark knowledge\"of a well-performed yet large neural network to a weaker but lightweight one. From the view of output logits and softened probabilities, this paper goes deeper into the dark knowledge provided by teachers with different capacities. Two fundamental observations are: (1) a larger teacher tends to produce probability vectors that are less distinct between non-ground-truth classes; (2) teachers with different capacities are basically consistent in their cognition of relative class affinity. Abundant experimental studies verify these observations and in-depth empirical explanations are provided. The difference in dark knowledge leads to the peculiar phenomenon named ``capacity mismatch\"that a more accurate teacher does not necessarily perform as well as a smaller teacher when teaching the same student network. Enlarging the distinctness between non-ground-truth class probabilities for larger teachers could address the capacity mismatch problem. This paper explores multiple simple yet effective ways to achieve this goal and verify their success by comparing them with popular KD methods that solve the capacity mismatch.",
            "score": 0.5485980386399107,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0110321044921875
        },
        {
            "corpus_id": "233324119",
            "title": "Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose Estimation",
            "text": "The key to knowledge distillation is to let a small network (student) imitate not only the output of a large network (teacher), but also true labels of datasets. Let   and   be the logits of the student and teacher, respectively. T is a temperature parameter to soften the output of the student and teacher.   =    (  / ) and   =    (  / ) are the soften outputs of the student and teacher, respectively. To encourage the student to mimic the output of the teacher, a KL-divergence loss   can be minimized as follows: \n\nTo minimize the gap between the output of the student model softmax(  ) and true labels  of datasets, the cross-entropy loss   can be penalized as follows: \n\nFinally, the overall loss function can be denoted by adding a balance factor  as follows:",
            "score": 0.5477491598380645,
            "section_title": "APPROACH 3.1 Background",
            "char_start_offset": 9697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 767
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08282470703125
        },
        {
            "corpus_id": "226246319",
            "title": "Channel Planting for Deep Neural Networks using Knowledge Distillation",
            "text": "Knowledge distillation can transfer the knowledge of DNNs with a large parameter (teacher networks) to smaller shallow networks (student networks). Ba et al. [13] proposed to use L2 loss between the input vectors of the softmax activation function (logits) of the teacher network and the student network. Hinton et al. [14] introduced to use the KL-divergence with a temperature parameter to make the softmax outputs of the teacher network and the softmax outputs (probability) of the student network similar. Romero et al. [31] introduced to map the student hidden layer to the prediction of the teacher hidden layer. Zhang et al. [32] presented a deep mutual learning (DML) strategy where, rather than one-way transfer between a static pre-defined teacher network and a student network, an ensemble of students learn collaboratively and teach each other throughout the training process.",
            "score": 0.5475467245395806,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 5365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 888
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 632,
                    "end": 636,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328857421875
        },
        {
            "corpus_id": "245353487",
            "title": "Multi-Modality Distillation via Learning the teacher's modality-level Gram Matrix",
            "text": "With the rapid increases in computing power, it is not surprising that various complex deep neural networks with a large number of parameters such as UNITER [1], and BERT [2] have been increasingly used for computer vision natural and language processing and have achieved great success [6] [7] [8]. \n\nHowever, it can not be efficiently deployed on device with limited computing and storage capability [15][16] [17]. To address the above issues, research mainly focuses on model compression such as knowledge distillation [18], [19], model quantization [19], [20] [21] [22] and model pruning [23], [24]. Among them, the knowledge distillation approach has been widely used due to its advantages, such as low performance sacrifice, easy implementation and hardware-friendly. \n\nThe vanilla knowledge distillation involves training a small model (student) to match a large pre-trained model (teacher). In order to transfer the knowledge from the teacher model to the student, a loss function is optimized to match ground-truth labels as well as softened teacher logits. As the \"temperature\" scale function is applied to the softmax, the logit distributions learn by teacher become softer, which can reveal inter-class relationships effectively. Based on intuition, the key to the success of KD is mainly that more fine-grained supervised information for improving the student model performance are provided across different categories in soft targets rather than discrete labels. Unlike previous interpretations, new concepts propose that soft target regularization functions as smoothing regulation for preventing overconfident predictions by student models. \n\nIn recent research [25] [26] [27] [28] [29] [30] [31], the main goal of knowledge distillation is to transfer the feature information of samples from teachers to students. For example, [32] imitated the teacher network by asking students to learn to return Logits before the softmax layer. [33] let students share some lower semantic levels with teachers and train them at the same time, but they also let students learn teachers' Logits knowledge.",
            "score": 0.5467828818941393,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 5137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2107
                }
            ],
            "ref_mentions": [
                {
                    "start": 157,
                    "end": 160,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 402,
                    "end": 406,
                    "matchedPaperCorpusId": "3885464"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 569,
                    "end": 573,
                    "matchedPaperCorpusId": "85548624"
                },
                {
                    "start": 1678,
                    "end": 1682,
                    "matchedPaperCorpusId": "244680427"
                },
                {
                    "start": 1693,
                    "end": 1697,
                    "matchedPaperCorpusId": "221739203"
                },
                {
                    "start": 1703,
                    "end": 1707,
                    "matchedPaperCorpusId": "221397580"
                },
                {
                    "start": 1844,
                    "end": 1848,
                    "matchedPaperCorpusId": "13540098"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3759765625
        },
        {
            "corpus_id": "270077283",
            "title": "Self-Knowledge Distillation via Progressive Associative Learning",
            "text": "In this section, we first briefly introduce the most related works of knowledge distillation.Then we specifically review recent self-distillation works.\n\nKnowledge distillation is a widely used paradigm for model compression, which transfers knowledge from a complex teacher model to a compact student model.To be specific, the teacher network has high accuracy and huge parameters, while the student network is not as accurate as the teacher network but has fewer parameters.Through knowledge distillation, we hope that the student network can approach or exceed the teacher network as much as possible.In this way, we obtain a compact student network with a similar prediction effect as the teacher network.Ba et al. [30] first proposed a method that uses the teacher's logits before the softmax as the regression target to train the student network, which completes the imitation of the teacher network by forcing the student network to mimic the teacher network's logits.Hinton et al. [12] first proposed to use the soft outputs of the pretrained teacher network as dark knowledge to supervise the training of the student network.They introduced a temperature hyperparameter T and formulated the problem as \"knowledge distillation\".The student network is forced to learn the soft targets of the teacher network, which are obtained through using a high temperature T on the softmax inputs.In the process of knowledge transfer, soft targets often contain richer information than one-hot targets.Romero et al. [13] extended the knowledge distillation method proposed by Hinton et al.In their method, the student network can be deeper and narrower than the teacher network and improve the performance by learning the outputs of the teacher network and the features of the middle layer.All the above methods are offline distillation methods [31,32], which need a pretrained teacher network.\n\nIn contrast to these methods, online knowledge distillation trains the student network under the supervision of a teacher from scratch.For example, Zhang et al. [33] proposed a mutual learning method, which uses multiple neural networks.Zhao et al. [9] proposed a collaborative training method, which uses both an expert teacher and a from-scratch teacher to supervise the student.To reduce the computational cost, Zhou et al. [34] proposed to employ two different networks which share some low parameters and train separately.",
            "score": 0.5463925498011,
            "section_title": "Related Work",
            "char_start_offset": 5661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 152
                },
                {
                    "start": 154,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1236
                },
                {
                    "start": 1236,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1497
                },
                {
                    "start": 1497,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1889
                },
                {
                    "start": 1891,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2128
                },
                {
                    "start": 2128,
                    "end": 2272
                },
                {
                    "start": 2272,
                    "end": 2418
                }
            ],
            "ref_mentions": [
                {
                    "start": 719,
                    "end": 723,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 989,
                    "end": 993,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "258888057"
                },
                {
                    "start": 2052,
                    "end": 2056,
                    "matchedPaperCorpusId": "26071966"
                },
                {
                    "start": 2140,
                    "end": 2143,
                    "matchedPaperCorpusId": "198179767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35205078125
        },
        {
            "corpus_id": "272367610",
            "title": "SAFE: Advancing Large Language Models in Leveraging Semantic and Syntactic Relationships for Software Vulnerability Detection",
            "text": "Knowledge distillation [55]- [58] is a technique used to transfer knowledge from one or more teacher models to a student model. In its simplest form, the student model is trained on a transfer set of data using soft target probability distributions produced by the teacher models, employing a temperature T in the softmax function. The student model is trained with the same temperature T , but uses a temperature of 1 after training. Inspired by [55], we transfer knowledge from the trained-teacher-A and trained-teacher-B models to the Student-S model by minimizing the Kullback-Leibler (KL) divergence between the softmax outputs of the teacher models and the student model's distilled tokens. Particularly, let O tA and O tB be the logit output values of the trained-teacher-A and trained-teacher-B models for a given source code data sample X with its corresponding vulnerability label Y . The Student-S model is trained to minimize the following objective function: \n\n) Where \u03d5 T is the softmax function with the temperature T . When the temperature T is set to 1, we denote the softmax function as \u03d5. The trade-off hyper-parameters \u03b3, \u03b4, and \u03b7 (where \u03b3 + \u03b4 + \u03b7 = 1) used to set the weights of utilizing the pre-trained large language model backbone in the Student-S model as well as enhancing the semantic and syntactic relationships transferred from the trained-teacher-A and trained-teacher-B models. \n\n3) Inference (testing) phase: Given a source code data sample X from the testing set D test , we based on the representation of the class token [cls] (i.e., O S cls ) of the trained Student-S model in our proposed SAFE approach to predict the vulnerability Y of data X. \n\nThe comprehensive algorithm of our SAFE approach is exhibited in Algorithm 1.",
            "score": 0.5461731309672204,
            "section_title": "B. Methodology",
            "char_start_offset": 20333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "254069919"
                },
                {
                    "start": 447,
                    "end": 451,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43017578125
        },
        {
            "corpus_id": "252668749",
            "title": "Using Knowledge Distillation to improve interpretable models in a retail banking context",
            "text": "Following the steps of Liu, Wang, and Matwin (2018), we also consider using logits instead of the probabilities resulting from the teacher's softmax layer, as an alternative representation of the dark knowledge being transferred. \n\nThe logits in a neural network are computed with the following formula: \n\nwhere \u0398 l is the weight matrix of the last hidden layer, and x l\u22121 the output activation matrix of the penultimate hidden layer supplemented by the bias vector. \n\nThe student model is trained on this target variable (8). For later prediction purposes, the softmax activation function ( 1) is applied on the student's outputs.",
            "score": 0.5461292734274341,
            "section_title": "Matching Logits",
            "char_start_offset": 23307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 466
                },
                {
                    "start": 469,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 631
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 51,
                    "matchedPaperCorpusId": "57189239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0496826171875
        },
        {
            "corpus_id": "208526905",
            "title": "Online Knowledge Distillation with Diverse Peers",
            "text": "In teacher-presented knowledge distillation, a student model is trained by the teacher-predicted soft distribution t together with hard ground-truth labels. The soft distribution or targets t are calculated with softmax of logits as Equation (2) with a temperature T > 1. A higher T means softer distributions. Following the suggestion in (Lan, Zhu, and Gong 2018), we set T to 3 in this paper for all methods.\n\nKnowledge is transfered by aligning the student predicted distribution q also after softmax with the same temperature (i.e., T = 3) to the target distribution. Specifically, Kullback-Leibler (KL) divergence between t and q may be used to define the distillation loss\n\nWith both hard and soft labels, the total loss for training a teacher-presented distillation model is\n\nwhere L dis is multiplied by T 2 before combination to ensure that the contribution of distillation term keeps roughly unchanged if the temperature is changed (Hinton, Vinyals, and Dean 2015). It is worthy to note that the predicted probabilities of the student model q are computed from the logits with temperature T = 1 when aligning with hard ground-truth labels but with a higher temperature when aligning with soft targets in distillation training. To be more clear, we use q for the T = 1 version, and q for the high temperature version through out this paper.",
            "score": 0.5460452869416035,
            "section_title": "Distillation with a teacher model",
            "char_start_offset": 8099,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 364,
                    "matchedPaperCorpusId": "48352434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415283203125
        },
        {
            "corpus_id": "249209818",
            "title": "Parameter-Efficient and Student-Friendly Knowledge Distillation",
            "text": "It is worth noting that our methods greatly reduce the variance of logits compared to the vanilla KD, and both our proposed methods have relatively small variance. In Figure 3, it shows the degree of sharpness for different temperatures in vanilla KD and our methods. Consistent with the analysis of Equation 10, since the teacher's output is unchanged, that is, the variance is unchanged, by increasing the temperature, the student's output is indeed smoother, resulting in a larger gap. Figure 4 shows the Top-1 accuracy of the corresponding methods. In general, lower gaps are associated with higher accuracy (lower temperature, lower gap value), but very close gaps may introduce anomalies (see the case of temperature 2 and 4). This phenomenon shows that smoothing the output of teachers and students within a certain range can improve the friendly transferability of knowledge (in terms of improving the accuracy and reducing the gap). In the face of each sample, it is impossible to simply manually adjust the common temperature to achieve a dynamic smooth output to improve the students' ability to accept the teacher's knowledge. With our methods, the smoothness of the output can be dynamically adjusted according to the gradient of each sample to achieve better knowledge transfer (compared to the vanilla KD with the best temperature setting, our methods have improved a lot about accuracy, and a large reduction in the gap). Student-friendly knowledge leads to better performance. In this part, we show that trainable part in teacher models can narrow the gap between student and teacher and make knowledge transfer process more student-friendly thus achieving better accuracy. We further explore the relationship between network consistency (sharpness gap in Figure 3, KL-Divergence in Figure 5 & 6 and CKA [22] in Figure 7 & 8 ) and accuracy in Figure 4. The three metrics mentioned above measure the final degree of consistency of the teacher-student network from different perspectives. A lower sharpness gap represents a closer knowledge representation of the teacher-student, and a lower KL represents the final convergence degree of the lower bound through distillation learning, while the a larger CKA represents larger similarity of the students and teachers. We get the following interesting findings: 1) From Equation 10, it is clear that the gap also decreases when the student network trained with the vanilla KD.",
            "score": 0.5457289330515862,
            "section_title": "How to Narrow the Gap?",
            "char_start_offset": 14112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2281
                },
                {
                    "start": 2282,
                    "end": 2439
                }
            ],
            "ref_mentions": [
                {
                    "start": 1821,
                    "end": 1825,
                    "matchedPaperCorpusId": "141460329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018341064453125
        },
        {
            "corpus_id": "237478571",
            "title": "BERTtoCNN: Similarity-preserving enhanced knowledge distillation for stance detection",
            "text": "Knowledge distillation is a common model compression and transfer learning method. Through the \"teacher\" model and the \"student\" model, the implicit knowledge(dark knowledge) learned from the \"teacher\" model of complex network is distilled into the simple",
            "score": 0.5455460837193756,
            "section_title": "Knowledge distillation",
            "char_start_offset": 5594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021942138671875
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "The idea behind knowledge distillation is to have the student network (S) be trained not only via the information provided by true labels but also by observing how the teacher network (T) represents and works with the data. The teacher network is sometimes deeper and wider (Hinton, Vinyals, and Dean 2015), of similar size (Anil et al. 2018;Zhang et al. 2017), or shallower but wider (Romero et al. 2014). \n\nLet a t and a s be the logits (the inputs to the final softmax) of the teacher and student network, respectively. In classic supervised learning, the mismatch between output of student network softmax(a s ) and the ground-truth label y r is usually penalized using cross-entropy loss \n\n(1) \n\nIn knowledge distillation, originally proposed by Bucila, Caruana, and Niculescu-Mizil;Ba and Caruana (2006;2014) and popularized by Hinton, Vinyals, and Dean (2015), one also tries to match the softened outputs of student y s = softmax(a s /\u03c4 ) and teacher y t =softmax(a t /\u03c4 ) via a KLdivergence loss \n\nHyperparameter \u03c4 referred to temperature is introduced to put additional control on softening of signal arising from the output of the teacher network. The student network is then trained under the following loss function: \n\nwhere \u03bb is a second hyperparameter controlling the trade-off between the two losses. We refer to this approach as Baseline Knowledge Distillation (BLKD) through the paper.",
            "score": 0.5454858232390217,
            "section_title": "Background and Notations",
            "char_start_offset": 8436,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1229
                },
                {
                    "start": 1232,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1403
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 306,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 342,
                    "end": 360,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 385,
                    "end": 405,
                    "matchedPaperCorpusId": "3323727"
                },
                {
                    "start": 751,
                    "end": 788,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 834,
                    "end": 866,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1923828125
        },
        {
            "corpus_id": "268650296",
            "title": "Comparative analysis of strategies of knowledge distillation on BERT for text matching",
            "text": "The model output is the category probabilities processed by the softmax layer. In this approach, all negative labels are treated uniformly, and similarity information between labels is mostly ignored, which leads to a reduction in the amount of knowledge output from the teacher model [4]. Figure 1 demonstrates the entire training process. To take advantage of this similarity knowledge, T, a temperature variable, is designed to soften the output classification information of the traditional softmax layer: \n\nThe higher the temperature, the smoother the obtained labels are and the more information they carry. Optimising the loss function in this way allows the teacher model to pass more knowledge to the student model. However, since the teacher model also has a certain error rate, the loss function also needs to incorporate the loss between the predicted and true values of the student model to reduce the likelihood of errors being passed on to the students. Therefore, the loss function for the whole of knowledge distillation is weighted by these two components. That is",
            "score": 0.5453419332271585,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 1375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1082
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.488525390625
        },
        {
            "corpus_id": "271356843",
            "title": "A Lightweight Fault Diagnosis Method Based on Knowledge Distillation Under Time-Varying Rotational Speeds",
            "text": "In order to integrate the loss of the student network's own learning task with the loss of imitating the teacher network's output, a balance coefficient is introduced, which \u03bb is used to regulate the relative weights of the two in the total loss function. In addition, in order to optimize the student network's learning of the teacher network's output distribution, a temperature parameter T is employed to make the output probability distribution of the teacher network smoother and more balanced, thus prompting the student network to capture and learn more comprehensively the decision boundaries and intrinsic knowledge structure of the teacher network. By minimizing the loss function, the student network can be made to fit the teacher network as closely as possible, making the learning process more efficient. Usually, the prediction result of the model represents the probability prediction of each category in the classification task after the Softmax classification layer, and the probability of each category obtained in this way often does not contain the similarity information between the categories, which will weaken the feature information learned by the model to a certain extent. Therefore, the teacher model introduces a temperature factor T in Softmax to obtain the similarity information between categories in the classification task, as shown in equation (6). \n\nDuring the training process of the student model, knowledge distillation introduces the prediction result of the teacher model as an additional target while learning the error between the input data and the real sample label. The cross-entropy loss function is chosen as the calculation function of the loss between the model test probability values and the real labels, then the loss after distillation of the teacher model and the student model L soft as well as the loss between the predicted values of the student model and the real labels L hard are shown in Eqs. ( 7) and ( 8), respectively: \n\nwhere p j and q j denote the predicted probability distributions of the teacher and student models after distillation of the temperature factor T, respectively, c j for the real label of the classification task, and q j denote the predicted probability distribution of the student model when the temperature factor T is 1, which is also referred to as the hard-labeled prediction of the student model. The loss of the whole knowledge distillation model is L soft and L hard two parts, as shown in equation ( 9):",
            "score": 0.5449521477278001,
            "section_title": "C. KNOWLEDGE DISTILLATION",
            "char_start_offset": 12872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2388
                },
                {
                    "start": 2389,
                    "end": 2498
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "243832889",
            "title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family",
            "text": "Knowledge Distillation (KD; Hinton et al., 2015b) is a method to transfer, or distill, knowledge from one model to another-usually from a large model to small one-such that the small student model learns to emulate the performance of the large teacher model. KD can be formalized as minimizing the objective function: \n\nwhere l is the loss function that measures the difference in performance between the teacher f T and the student f S , and x i is the ith input. The conventional loss function l used in practice is a linear combination of the traditional cross entropy loss L CE and the Kullback-Leibler divergence L KL of the pre-softmax outputs for f T and f S : \n\nwhere \u03c3 is the softmax function \u03c3(x) = 1/(1 + exp(\u2212x)), and \u03c4 is the softmax temperature. Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1. \n\nThe parameter \u03b1 represents the weight trade-off between the KL loss and the cross entropy loss L CE . end while 11: end for 12: Obtain the best performing configuration \u03b8 * for the student network generator. 13: Sample k architectures from G(\u03b8 * ), train them to completion, and obtain test performance. \n\nThe L KD loss is characterized by the hyper-parameters: \u03b1 and \u03c4 ; popular choices are \u03c4 \u2208 {3, 4, 5} and \u03b1 = 0.9 (Huang & Wang, 2017;Zagoruyko & Komodakis, 2016;Zhu et al., 2018). Numerous other methods (Polino et al., 2018;Huang & Wang, 2017;Tung & Mori, 2019) can be formulated as a form of Equation ( 2), but in this paper we use the conventional loss function l. \n\nTraditionally in KD, both the teacher and the student network have predefined architectures. In contrast, AutoKD defines a search space of student network architectures and finds the optimal student by leveraging neural architecture search, as detailed below.",
            "score": 0.5444693937194403,
            "section_title": "KNOWLEDGE DISTILLATION",
            "char_start_offset": 8368,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1793
                }
            ],
            "ref_mentions": [
                {
                    "start": 1326,
                    "end": 1343,
                    "matchedPaperCorpusId": "48352434"
                },
                {
                    "start": 1408,
                    "end": 1425,
                    "matchedPaperCorpusId": "198179476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64404296875
        },
        {
            "corpus_id": "272945756",
            "title": "Fast reconstruction of milling temperature field based on CNN-GRU machine learning models",
            "text": "Knowledge distillation is an instructor-student training structure that typically utilizes a student model with a simpler network structure to learn the knowledge provided by an instructor model that has been trained with a more complex network structure; this approach trades a slight performance loss for faster computation and smaller model parameters. Knowledge distillation works by training the student model with both the predictions of the teacher model (soft labeling) and the real data (hard labeling), and calculating the weighted total loss of the student model on both the soft and hard labels, essentially \"migrating\" the knowledge learned by the teacher model to the student model. The structure of the knowledge distillation strategy used in this paper is shown in Figure 6. \n\nThe specific knowledge distillation strategy process is as follows: \n\n(1) The raw data that has been preprocessed is input to both the teacher model and the student model, the teacher model is the CNN-GRU model constructed in the previous section, and the student model is a small model with a single CNN layer and a single GRU layer. ( 2  (5) The distillation loss and the student loss are weighted to obtain the total loss, and the gradient of each parameter is updated in the backpropagation process. \n\nThe following are the calculation formulas involved in the knowledge distillation operation process: \n\nKnowledge distillation soft labeling calculation formula as Equation ( 5): \n\nwhere T is the distillation temperature coefficient, used to control the \"hardness\" of the soft label. When T is larger, the soft label distribution area is uniform, more softened, when T is smaller, the soft label distribution closer to the hard label. \n\nDistillation loss of the loss function LOSSsoft formula is as Equation ( 6): \n\nwhere k is the total number of samples, p i (u i ,T) is the ith output of the teacher model at temperature coefficient T, and p i (z i ,T) is the ith output of the student model at temperature coefficient T. \n\nThe loss function LOSShard for student loss is formulated as Equation ( 7): \n\nwhere y i 7is a vector of hard labels representing the class i output of the unsoftened student model. \n\nThe total loss of knowledge distillation can be expressed as Equation ( 8): \n\n( )",
            "score": 0.5443624351049843,
            "section_title": "Temperature boundary condition estimation model based on knowledge distillation with gated convolutional recurrent networks",
            "char_start_offset": 29942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1296
                },
                {
                    "start": 1299,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2282
                },
                {
                    "start": 2285,
                    "end": 2288
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030792236328125
        },
        {
            "corpus_id": "235421655",
            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
            "text": "Efficiently transferring knowledge from large teacher network to small student network has been a traditional topic which has drawn more and more attention in recent years. Adopting this teacherstudent learning paradigm, many distillation methods have been investigated. [2] first proposed model compression that transfers information from a large model or from an ensemble of models to a small model without a significant drop in accuracy. [11] proposed the concept of soft label which is distilled from the teacher model to the student model. Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized. \n\nExtra information other than the outputs, for example, the intermediate-level supervision from the teacher model, can additionally boost the performance of knowledge distillation. Application of the intermediate representations was first introduced in FitNet [26] to provide hints to improve the training of the student model. Thanks to the additional hint-based training, the trained deep student network showed better accuracy with fewer parameters compared to the original wide teacher network. \n\nTo explore the relationships between different feature maps, [40] proposed a flow of solution process (FSP), which is defined by the Gram matrix between two layers. Many other different algorithms have been proposed to improve the process of distilling knowledge in more complex settings. [3,15,38,41] As mentioned above, knowledge distillation has been extensively studied and effectively used for ANNs in the literature. However, some recent works show how off-the-shelf knowledge distillation commonly used in ANNs can be adapted for SNNs. [16] first proposed a study that applied knowledge distillation to SNNs. In this work, output spike train of the teacher SNN model was involved in the distillation process, but the performance was unsatisfactory.",
            "score": 0.5442193778387782,
            "section_title": "Knowledge distillation",
            "char_start_offset": 7213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 274,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1589,
                    "end": 1593,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1823,
                    "end": 1826,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1826,
                    "end": 1829,
                    "matchedPaperCorpusId": "26071966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "237492140",
            "title": "Class-Distribution-Aware Calibration for Long-Tailed Visual Recognition",
            "text": "Self-distillation (SD) or knowledge distillation (KD) is a kind of transfer of knowledge from a teacher model to a student model by commonly minimizing an objective function (Ba & Caruana, 2013;Crowley et al., 2018). A popular way to do this is by calculating the Kullback-Leibler divergence loss between the temperature scaled probability of teacher and student models (Hinton et al., 2015). Hardmax and softmax with temperature are used to perform self distillation for overparameterized neural network (Dong et al., 2019).",
            "score": 0.5440842732251245,
            "section_title": "A.3. Self-Distillation",
            "char_start_offset": 13385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 525
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 215,
                    "matchedPaperCorpusId": "23316647"
                },
                {
                    "start": 370,
                    "end": 391,
                    "matchedPaperCorpusId": "44117253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33349609375
        },
        {
            "corpus_id": "270389751",
            "title": "Multistage feature fusion knowledge distillation",
            "text": "Knowledge distillation (KD), as initially proposed by Hinton et al. 10 , aims to supervise the training convergence of a student network, a smaller model, with a teacher network, a larger model.This method controls the extent of knowledge transfer between two networks using a temperature parameter, T, to control the transfer of soft-label dark knowledge.This approach has given rise to variations, including intermediate feature layer distillation and multistage soft label distillation.\n\nIn the context of intermediate feature layer distillation, the challenge of inconsistent multistage feature knowledge distribution is a critical issue.FitNet 11 employs squared distance constraints to measure the similarity of intermediate layer features between teacher and student networks.AT 15 uses multi-layer attention maps to extract features between the teacher network and the student network, and builds a knowledge transfer mechanism between the two.CC 16 proposed a correlation congruence method to reduce the correlation consistency distribution between teachers and students across multiple sample instances, and improve the distribution consistency between student models and teacher models in the classification output of multiple instances.AB 17 proposed a method for knowledge transfer by extracting the activation boundaries formed by hidden neurons, which enables students to learn the separation boundaries between different activation regions formed by each neuron in the teacher, thereby reducing the differences between the student network and the teacher network.FT 18 proposed two convolutional modules, the reader and the translator, which are used to extract the feature information of teachers and the translator to extract the feature information of students.Through distillation training, the differences between the two modules are reduced, achieving the imitation and learning of the teacher network by www.nature.com/scientificreports/ the student network.NST 19 proposed a new KT loss function to minimize the maximum average difference in neuron feature distribution between the teacher model and the student model, significantly improving the performance of the student model.CRD 20 is a knowledge distillation method based on contrastive learning, which preserves mutual information between teachers and students by optimizing the distillation loss function.OFD 21 uses a novel distance function and edge residual function to distill essential information between teacher and student networks.ReviewKD 22 utilizes a multilevel composite knowledge approach to transfer dark knowledge at the feature level, achieving state-of-the-art performance.",
            "score": 0.5435257710202751,
            "section_title": "Knowledge distillation",
            "char_start_offset": 2815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 194,
                    "end": 356
                },
                {
                    "start": 356,
                    "end": 489
                },
                {
                    "start": 491,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1248
                },
                {
                    "start": 1248,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2204
                },
                {
                    "start": 2204,
                    "end": 2387
                },
                {
                    "start": 2387,
                    "end": 2522
                },
                {
                    "start": 2522,
                    "end": 2673
                }
            ],
            "ref_mentions": [
                {
                    "start": 68,
                    "end": 70,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 955,
                    "end": 957,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1251,
                    "end": 1253,
                    "matchedPaperCorpusId": "53213211"
                },
                {
                    "start": 1582,
                    "end": 1584,
                    "matchedPaperCorpusId": "3608236"
                },
                {
                    "start": 2208,
                    "end": 2210,
                    "matchedPaperCorpusId": "204838340"
                },
                {
                    "start": 2391,
                    "end": 2393,
                    "matchedPaperCorpusId": "102483181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.122314453125
        },
        {
            "corpus_id": "270878760",
            "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
            "text": "As implied by its name, logic-based KD [43] is a distillation paradigm that employs logic within teacher models for knowledge transfer.We can formulate the general knowledge distillation loss function as follows:\n\nwhere  s ,  t \u2208 R  denote the logits output of the student and teacher network, respectively. is a temperature parameter that adjusts the smoothness of the logits. represents the number of classes.The Kullback-Leibler divergence (KLD) [43] loss can also be replaced with other functions, such as Reverse Kullback-Leibler (RKL) [20,53,65,96] distillation, Jenson-Shannon (JS) [129] distillation, etc.",
            "score": 0.5432969293509109,
            "section_title": "Logits-based KD",
            "char_start_offset": 3560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 212
                },
                {
                    "start": 214,
                    "end": 307
                },
                {
                    "start": 307,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 613
                }
            ],
            "ref_mentions": [
                {
                    "start": 541,
                    "end": 545,
                    "matchedPaperCorpusId": "3826036"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "251493147"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "107645"
                },
                {
                    "start": 589,
                    "end": 594,
                    "matchedPaperCorpusId": "233168978"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0037212371826171875
        },
        {
            "corpus_id": "274269913",
            "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation",
            "text": "Knowledge distillation (KD) is a powerful technique for transferring knowledge from a larger, more complex model (the teacher) to a smaller, more efficient model (the student). The goal is to retain the performance benefits of the larger model while reducing the computational complexity, enabling deployment in resource-constrained environments. \n\nIn the knowledge distillation framework, let x \u2208 R d denote the input data, and let y \u2208 {1, 2, . . . , C} represent the corresponding true labels, where C is the number of classes. The teacher model is a pre-trained model with parameters \u03b8 T , and it outputs a set of logits z T = f T (x; \u03b8 T ). These logits are then passed through a softmax function to obtain the predicted probability distribution: \n\nSimilarly, the student model, with parameters \u03b8 S , produces its own set of logits z S = f S (x; \u03b8 S ) and corresponding probability distribution: \n\nThe central idea in knowledge distillation is to train the student model such that its output distribution p S closely matches the teacher's output distribution p T , while also ensuring that the student performs well on the actual classification task. \n\nA key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from. \n\nThe loss function used to train the student model is a weighted combination of two components: the supervised loss and the distillation loss. The supervised cross-entropy loss ensures that the student model performs well on the actual classification task by minimizing the cross-entropy between the true labels, denoted as y, and the student's predictions, p S . This loss is calculated as follows: \n\nwhere y i represents the one-hot encoded true label. In addition to this, the distillation loss encourages the student model to mimic the softened output distribution of the teacher model.",
            "score": 0.5432051829267694,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 14052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 346
                },
                {
                    "start": 349,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1401
                },
                {
                    "start": 1404,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2182
                },
                {
                    "start": 2185,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "267479822",
            "title": "Improving single\u2010stage activity recognition of excavators using knowledge distillation of temporal gradient data",
            "text": "The rationale for using a temperature parameter is that by increasing the sharpness of the obtained probability distribution, it will facilitate the selection of the most probable value, thus increasing the confidence of the model in its prediction (Caron et al., 2021;T. Chen et al., 2020).\n\nTo further improve the knowledge transfer from the teacher network (i.e., the TG backbone) to the student network (i.e., the 3D CNN RGB backbone), a lower temperature value is selected for the probability distribution of the teacher network, compared to that of the student network.By using a lower temperature value for the teacher network, the resulting probability distribution is sharper, and the model is more confident in its predictions.This increased confidence helps the student network learn more effectively by receiving stronger supervision from the teacher network, leading to improved performance.A detailed ablation study examining the effect of different teacher and student temperature parameters is presented in Section 5.1.2.\n\nFinally, the total loss used for training the proposed DIGER method comprises three main components.The YOWO loss (Equation 7) used for training of the RGB (\ue238   ) and TG (\ue238   ) backbones as outlined in Sections 3.1 and 3.2, respectively, and the knowledge distillation loss (\ue238  ).Equation ( 14) presents the final utilized loss function in which the parameter  is used to adjust the effect of the knowledge distillation loss during the training.In this work, the best results were obtained using the value of  = ; however, a comprehensive ablation study is presented in Section 5.2 to further demonstrate the effect of  on the final model performance.",
            "score": 0.5429514709132564,
            "section_title": "Knowledge distillation",
            "char_start_offset": 37637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 291
                },
                {
                    "start": 293,
                    "end": 575
                },
                {
                    "start": 575,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1037
                },
                {
                    "start": 1039,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1484
                },
                {
                    "start": 1484,
                    "end": 1690
                }
            ],
            "ref_mentions": [
                {
                    "start": 249,
                    "end": 269,
                    "matchedPaperCorpusId": "233444273"
                },
                {
                    "start": 269,
                    "end": 290,
                    "matchedPaperCorpusId": "211096730"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0240478515625
        },
        {
            "corpus_id": "256459285",
            "title": "Knowledge Distillation on Graphs: A Survey",
            "text": "Knowledge distillation aims to train a smaller student model by transferring the knowledge from a larger teacher model [Gou et al., 2021;Wang and Yoon, 2021]. The main idea is to enforce the student model to mimic the teacher model, where the logits, activations, neurons, and features can all be regarded as the knowledge that guides the learning of the student model [Hinton et al., 2015;Ahn et al., 2019;Heo et al., 2019]. In particular, the knowledge distillation loss L kd between the student and the teacher can be defined as follows: \n\n) where DIV indicates the divergence loss (e.g., Kullback-Leibler divergence), k T and k S are the knowledge obtained from the teacher and student models, respectively. A vanilla knowledge distillation considers the logits as the knowledge and employs a Softmax function with temperature to derive the soft targets p, which show the probabilities of the input belonging to various classes. Hence, the Eq. 3 can be reformulated as L kd = DIV(p T , p S ), where p T and p S represent the soft targets derived from the teacher and students, respectively. \n\nSimilarly, in the cases where knowledge is not logits, k T and k S in Eq. 3 can be replaced accordingly, e.g., f T and f S for the learned features of the teacher and student models. After calculating L kd , the student model is trained by a joint objective of both the origin downstream task loss and the knowledge distillation loss L kd , where the former facilitates the student to learn from the original specific task, and the latter targets at transferring the knowledge from the teacher to the student.",
            "score": 0.541887642041327,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1606
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 137,
                    "matchedPaperCorpusId": "247362682"
                },
                {
                    "start": 137,
                    "end": 157,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 390,
                    "end": 407,
                    "matchedPaperCorpusId": "118649278"
                },
                {
                    "start": 407,
                    "end": 424,
                    "matchedPaperCorpusId": "247362682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "260334695",
            "title": "BearingPGA-Net: A Lightweight and Deployable Bearing Fault Diagnosis Network via Decoupled Knowledge Distillation and FPGA Acceleration",
            "text": "I. 2) Decoupled knowledge distillation. Despite numerous forms of knowledge distillation, our method adopts the response-based KD, which utilizes the teacher model's logits for knowledge transfer. This is because the limited hardware resource in a low-cost FPGA cannot hold two or more convolutional layers. Then, there are no hidden features for featurebased KD to distill. \n\nIn classical knowledge distillation, soft labels, which are the logits produced by the teacher, are deemed as distilled knowledge [27]. Soft labels are obtained by the softmax function converting the logits z i of a neural network into the probability p i of the i-th class: \n\nwhere C represents the number of classes, and T \u2208 R + serves as the temperature factor. When the temperature T is higher, the probability distribution over classes becomes smoother. A lower value of T (where T < 1) sharpens the output, increasing the disparity in probability values of different classes. \n\nFor classification, the cross-entropy (CE) loss is adopted to measure the difference between the probability of the predicted label p and the ground truth y: \n\nand KL-Divergence measures the similarity between the probability labels of the teacher p T and the student p S , \n\nKD combines two loss functions: \n\n) where \u03b1 is the scale factor to reconcile the weights of two loss functions, and T 2 keeps two loss functions at the same level of magnitude. Combining L KL and L KD helps the student get the guidance of the teacher and the feedback from the ground truth. This ensures that the student model learns effectively and reduces the risk of being misled. \n\nHowever, L KL implies that all logits from the teacher are transferred to the student on an equal booting. Intuitively, student models should have the ability to filter the knowledge they receive. In other words, knowledge relevant to the current target category should be reinforced, while knowledge far from the target category should be attenuated. The coupling in classical KD harms the effectiveness and flexibility across various tasks. To address this, decoupled knowledge distillation, which factorizes L KL into a weighted sum of two terms (the target class and non-target class), was proposed [31].",
            "score": 0.5418837223387147,
            "section_title": "III. METHOD",
            "char_start_offset": 11609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 961,
                    "end": 1118
                },
                {
                    "start": 1121,
                    "end": 1234
                },
                {
                    "start": 1237,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2231
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.268310546875
        },
        {
            "corpus_id": "238639290",
            "title": "Data-Efficient Sensor Upgrade Path Using Knowledge Distillation",
            "text": "In classification tasks, a neural network outputs a discrete probability distribution p in which p i is the output probability for class i. This is achieved by applying the softmax function to the output logits z: \n\nin which T is a temperature parameter, usually set to 1. When a higher temperature value is used, the absolute differences between the output probabilities of different classes become less pronounced, thereby producing a distribution that is less peaked. Knowledge distillation [7] uses the probability distribution generated by a large teacher neural network-or an ensemble of networks-as soft targets to train a small student neural network. Usually a high temperature is used to soften the probability distribution. The resulting distillation loss L D is then given by the cross-entropy between the probability distributions generated by the student and teacher networks: \n\nwhere z t and z s are the logits of the teacher and the student respectively. When ground truth labels are available, these can be used in conjunction with the soft targets provided by the teacher network. In this case, the student loss L S is also a cross-entropy loss defined as follows: \n\nwhere y is the one-hot vector encoding the ground truth label. The total loss is then given by the convex combination of both the distillation loss and the student loss: \n\nwhere \u03b1 is a hyperparameter, in addition to the temperature parameter T.",
            "score": 0.5412094942973568,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 6399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 213
                },
                {
                    "start": 216,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1429
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 497,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.469970703125
        },
        {
            "corpus_id": "252510341",
            "title": "Automatic Modulation Classification with Neural Networks via Knowledge Distillation",
            "text": "The main idea behind knowledge distillation is that to achieve superior performance, the student model must imitate the teacher model. Knowledge types, distillation techniques and teacher-student learning architecture all play critical roles in student learning. [19]. A general teacher-student framework for knowledge distillation is shown in Figure 4. The teacher-student framework is the most fundamental framework for knowledge distillation. The teacher network in this paper is the pre-trained Inception-ResNet, and the student networks are a CNN with three convolutional layers and a Mini-Inception-ResNet. The output of the teacher network is distilled to form a soft and hard target to calculate the student network's loss function.  Neural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit z i , computed for each class into a probability q i , by comparing z i with the other logits [18].\n\nwhere T is a temperature that is normally set to 1. Using a higher value for T produces a softer probability distribution over the classes. To understand the knowledge distillation, a benchmark model, which is the distillation combined with student losses, is given in Figure 5. The basic process of knowledge extraction is as follows: The same input enters the teacher and student networks; the temperature of the softmax layer of the teacher network is set to t; the output produced is soft targets; the temperature of the softmax layer of the student network is set to t; and the soft targets produced together with those produced by the teacher network produce a distillation loss; the temperature of the student network's softmax layer is set to 1; and the resulting soft targets and true labels of the data generate the both the student and distillation losses as well as a combined loss, which comprises the overall loss.  When calculating losses in KD, the weighted average of two different objective functions is used. The first is cross entropy with the soft targets, which is computed using the same high temperature in the softmax of the distilled model that was used to generate the soft targets from the cumbersome model. The second is cross entropy with the correct labels. Since the magnitudes of the gradients produced by the soft target scale as 1/T 2 , it is important",
            "score": 0.5402679145602817,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 10977,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "274993680",
            "title": "Predicting Subsurface Layer Thickness and Seismic Wave Velocity Using Deep Learning: Knowledge Distillation Approach",
            "text": "Knowledge distillation (KD), introduced by Hilton et al. [12], presents a promising solution to this challenge by transferring knowledge from a large, complex teacher model to a smaller, more efficient student model. KD is a technique that involves training a smaller, computationally efficient student model to mimic the behavior of a larger, more complex teacher model. The student model learns to achieve this by minimizing a loss function that includes both the traditional cross-entropy loss between the student's predictions and the true labels, as well as a distillation loss that encourages the student to mimic the teacher's soft predictions. The soft predictions of the teacher model are obtained by applying a SoftMax function to the logits (pre-activation outputs) of its final layer. These soft predictions provide a more informative representation of the teacher's uncertainty compared to hard predictions, which are binary or categorical labels. By matching the teacher's soft predictions, the student model can learn not only the correct class labels but also the confidence associated with each prediction.",
            "score": 0.540251132877626,
            "section_title": "A. KNOWLEDGE DISTILLATION",
            "char_start_offset": 6735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "220280430",
            "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution",
            "text": "In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling). Results are reported in Table 10. Student is ResNet-18 and teacher is an ensemble of 8 ResNet-18 models trained with CutMix, having a top-1 accuracy of 84.6% on the CIFAR100 dataset. \n\nWe observe that XCL is not sensitive to temperature (Table 11). Note that finding an optimal T requires extensive hyper-parameter tuning. XCL does not require hyper-parameter tuning, and compared to the best KD with temperature scaling reduces the accuracy gap by 59%.",
            "score": 0.5390094735753235,
            "section_title": "Knowledge Distillation with Temperature Scaling",
            "char_start_offset": 24958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 729
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66650390625
        },
        {
            "corpus_id": "276938088",
            "title": "Adaptive Temperature Based on Logits Correlation in Knowledge Distillation",
            "text": "Notation Given that a dataset D containing total N samples {x n , y n } K n=1 , x n \u2208 R H\u00d7W and y n \u2208 [1, N ] are the K th input images and the correct label, respectively. H, W and N notates the height and the width of the input and the number of classes. A model generates the logit v i against the input D. Applying z-score standardization v i yields z i following z i = 0. For the convenience, the normalized values divided by a scaling parameter \u03c4 called by temperature notates t i . The logits predicted by a teacher model and a student model describes t p i and t q i , respectively. The conversion of logits to class probabilities archives with the softmax function. The i th sample's probabilities are \n\nwhere t p i and t q i are logits scaled by \u03c4 . \n\nKnowledge Distillation A temperature manages the smoothness of transfering from a teacher model to a student model. When \u03c4 = 1, the output obtains a comparison with the logit z p i and z q i . The purpose of this hyper-parameter is the promotion of transferring the relationship of logits in a sample [19]. The dynamic change implies to the accuracy improvement [13]. Here, we prove that KL divergence converges the correlation of logits by the approximation of knowledge distillation loss. The loss follows \n\nwhere p and q are probability distributions applied softmax into logits divided by \u03c4 .",
            "score": 0.5388164255603953,
            "section_title": "Preliminaries",
            "char_start_offset": 7285,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1358
                }
            ],
            "ref_mentions": [
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "268247468"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "254069919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44677734375
        },
        {
            "corpus_id": "271039191",
            "title": "Understanding the Gains from Repeated Self-Distillation",
            "text": "Knowledge distillation [11] was initially proposed as a way to transfer the knowledge learnt by a larger teacher model to a smaller student model, which can then be deployed in limited resource settings.The process is as follows: Train a teacher (T ) model using ground-truth labels, then use its predictions to supervise the training of a student (S) model via a combined per-sample loss, \u03be \u2022 \u2113 \u0177T , y S (\u03b8) + (1 \u2212 \u03be) \u2022 \u2113 y, y S (\u03b8) ,\n\nwhere \u2113 denotes the loss function, y is the ground-truth label, \u0177T denotes the teacher's prediction, and y S (\u03b8) denotes the student's prediction, parameterized by the learnable \u03b8.The extra hyperparameter \u03be is called the imitation parameter [24], generally restricted to \u03be \u2208 [0, 1].It gives additional freedom to the student to balance importance between labels and teacher's predictions.The student trained via this distillation objective (i.e., utilizing the teacher's predictions through \u03be \u0338 = 0) has been widely observed to generalize better than when trained only on the labels (i.e., \u03be = 0).This gain has been attributed to 'dark knowledge' that is (i) impossible to be directly extracted from the training data by the small model, but (ii) easily learnt by the large model and transferred to the small model.\n\nChallenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models.One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T .This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD).Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35,43].At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1).",
            "score": 0.5387543347356846,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1252
                },
                {
                    "start": 1254,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1630
                },
                {
                    "start": 1630,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1297,
                    "end": 1301,
                    "matchedPaperCorpusId": "14659675"
                },
                {
                    "start": 1324,
                    "end": 1327,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1823,
                    "end": 1826,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1826,
                    "end": 1829,
                    "matchedPaperCorpusId": "54986302"
                },
                {
                    "start": 1829,
                    "end": 1832,
                    "matchedPaperCorpusId": "219558831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0384521484375
        },
        {
            "corpus_id": "263620704",
            "title": "I2KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding",
            "text": "Knowledge Distillation is a technique first proposed by [34]. The goal of knowledge distillation is to transfer the knowledge from a large, complex model which is known as the teacher to a smaller, simpler model which is known as the student. This is achieved by training the student model to mimic the behavior of the teacher model, using either the predicted outputs or intermediate representations of the teacher model. Existing knowledge distillation methods generally fall into two categories. The first category focuses on using dark knowledge [35,36] and the second category focuses on sharing information about the relationships between the layers of the teacher model [37,38]. In our method, we apply knowledge distillation to facilitate knowledge transfer between different languages and achieve the mutual guidance between intent and slots.",
            "score": 0.5386858404966207,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4817,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 851
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 60,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "2723173"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "26021416"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "263861232"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014617919921875
        },
        {
            "corpus_id": "208158386",
            "title": "Neural Network Pruning With Residual-Connections and Limited-Data",
            "text": "We use knowledge distillation [14] to fine-tune the small model. The original large model plays the role of a teacher model and the pruned small model is the student model. A typical strategy is to train the student model under the supervision of soft target (the logits of teacher models) and hard target (the groundtruth label). However, because the teacher model has not seen the new data, its output (logits) may be noisy. \n\nInspired by PENCIL [38] and R2-D2 [37], we can update the noisy logits during model training via SGD. However, updating logits is dangerous. The quality of soft targets can be updated to become worse if the student model is not accurate enough. To avoid this situation, we divide the whole training process into two steps: fine-tuning on original small dataset with knowledge distillation plus mixup and fine-tuning on expanded dataset with label refinement. Note that the expanded new data and our proposed refinement method are only used in step 2. \n\nStep 1: knowledge distillation with mixup. Knowledge distillation [14] and mixup [40] are two widely used techniques, which are really helpful for the training with limited data. We propose to combine these two techniques together. First, a new input is generated via mixup: \n\nwhere (x i , y i ) and (x j , y j ) are two random examples, \u03bb \u2208 [0, 1] is drawn from a Beta distribution. The new input x is then fed into teacher and student models. Let u denote the output logits of the teacher model, and v denote the student's output. Our total loss is calculated as: \n\nwhere p = softmax(u/T ), q = softmax(v/T ) are the softmax output under temperature T . L KL and L CE denote KL-divergence loss and cross-entropy loss, respectively. \u03b1 \u2208 [0, 1] controls the balance between losses. With these two techniques, the pruned small model can converge into a good local minima. \n\nStep 2: knowledge distillation with label refinement. We then fine-tune the small model on our expanded dataset and update logits to remove label noises.",
            "score": 0.5385901384669685,
            "section_title": "Label Refinement",
            "char_start_offset": 16101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1547
                },
                {
                    "start": 1550,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 448,
                    "end": 452,
                    "matchedPaperCorpusId": "83458813"
                },
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "3162051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0938720703125
        },
        {
            "corpus_id": "253098943",
            "title": "Respecting Transfer Gap in Knowledge Distillation",
            "text": "Knowledge distillation (KD) [21] transfers knowledge from a teacher model, e.g., a big, cumbersome, and energy-inefficient network, to a student model, e.g., a small, light, and energy-efficient network, to improve the performance of the student model. A common intuition is that a teacher with better performance will teach a stronger student. However, recent studies find that the teacher's accuracy is not a good indicator of the resultant student performance [8]. For example, a poorly-trained teacher with early stopping can still teach a better student [8,11,77]; or, a teacher with a smaller model size than the student is also a good teacher [77]; or, a teacher with the same architecture as the student helps to improve the student-self-distillation [13,82,81,27]. \n\nShould we view KD in a perspective of domain transfer [12,63], we would better understand the above counter-intuitive findings. From Figure 1, we can see that teacher predictions and ground-truth labels indeed behave differently. Although the teacher is trained on the balanced dataset, its predicted probability distribution over the dataset is imbalanced. Even on the same training set with the same model parameter, teachers with different temperature \u03c4 yield different \"soft label\" distributions from the ground-truth ones. This implies that human and teacher knowledge are from different domains, and there is a transfer gap that drives the \"dark knowledge\" [21] transferring from teacher to student-regardless of \"strong\" or \"weak\" teachers, it is a valid transfer as long as there is a gap. However, the transfer gap affects the distillation performance on the under-represented classes, i.e., classes on the tail of teacher predictions, which is overlooked in recent studies. Take CIFAR-100 as an example. We rank and divide the 100 classes into 4 groups according to the ranks of predicted probability. As shown in Table 1, compared to vanilla training, KD achieves better performance in all the subgroups. However, the increase in the top 25 classes is much higher than that in the last 25 classes, i.e., averagely 5.14% vs. 0.85%.",
            "score": 0.5382877197753463,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1887
                },
                {
                    "start": 1888,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 463,
                    "end": 466,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 559,
                    "end": 562,
                    "matchedPaperCorpusId": "203642130"
                },
                {
                    "start": 565,
                    "end": 568,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "219962714"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 763,
                    "end": 766,
                    "matchedPaperCorpusId": "159041406"
                },
                {
                    "start": 766,
                    "end": 769,
                    "matchedPaperCorpusId": "232302458"
                },
                {
                    "start": 769,
                    "end": 772,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "8548594"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "29160099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0243377685546875
        },
        {
            "corpus_id": "235489777",
            "title": "Knowledge Distillation via Instance-level Sequence Learning",
            "text": "The idea of knowledge distillation is to train the student network not only via the true labels information but also by mimicking the teacher's class probabilities or feature representation (activations of hidden layers). In other words, the teacher network could provide valuable dark knowledge as extra supervisory information besides the ground-truth labels. \n\nLet us denote x, y as the input of the DNN and one-hot labels of our architecture. \u03c6 s and \u03c6 t represent the student network and teacher network with parameters W s and W t respectively. Given an input image x, the teacher network outputs the final predictions as P T which are obtained by applying the softmax function on the un-normalized log proba-bility values a T , i.e. P T = sof tmax(a T ). Similarly, the same image is fed into the student network to obtain the predictions P S = sof tmax(a S ). The standard cross-entropy is denoted as H. In classical supervised learning, the mismatch between the output of the student network softmax and the ground-truth label y is usually penalized using cross-entropy loss: \n\nHinton et al. [11] extend previous works [19] by training a compact student network to mimic the output probability distribution of the teacher network. They name this informative and representative knowledge as dark knowledge and try to match the softened outputs of teacher and student via a KLdivergence loss: \n\nIt contains the relative probabilities of incorrect classification results provided by teacher networks. When we perform knowledge distillation with a temperature parameter \u03c4 , the student network will be trained to optimize the following loss function: \n\nwhere \u03bb is a second hyper parameter controlling the trade-off between the two losses. The teacher network is sometimes deeper and wider than the above approaches, but sometimes has the similar size as the student network [26][27] [28]. Snapshot Distillation [28] proposes to finish teacher-student optimization within one generation which acquires teacher information from the previous iterations of the same training process. Inspired by this, we propose to employ the snapshot of student from the previous epochs to design curriculum for efficient knowledge distillation. \n\nInstance-level sequence learning for knowledge distillation.",
            "score": 0.5381827183321622,
            "section_title": "B. Formulation",
            "char_start_offset": 12183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1655
                },
                {
                    "start": 1658,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2231
                },
                {
                    "start": 2234,
                    "end": 2294
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1105,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1128,
                    "end": 1132,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1879,
                    "end": 1883,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34814453125
        },
        {
            "corpus_id": "203642142",
            "title": "Distillation \u2248 Early Stopping? Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network",
            "text": "Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing [24]. Among these tasks, image classification is considered as one of the fundamental tasks since classification networks are commonly used as base networks for other problems. In order to achieve higher accuracy using a network with similar complexity as the base network, distillation has been proposed, which aims to utilize the prediction of one (teacher) network to guide the training of another (student) network. In [17], the authors suggested to generate a soft target by a heavy-duty teacher network to guide the training of a light-weighted student network. More interestingly, [14,5] proposed to train a student network parameterized identically as the teacher network. Surprisingly, the student network significantly outperforms the teacher network. Later, it was suggested by [49,19,9] to transfer knowledge of representations, such as attention maps and gradients of the classifier, to help with the training of the student network. In this work, we focus on the distillation utilizing the network outputs [17,14,45,5,46]. \n\nTo explain the effectiveness of distillation, [17] suggested that instead of the hard labels (i.e one-hot vectors), the soft labels generated by the pre-trained teacher network provide extra information, which is called the \"Dark Knowledge\". The \"Dark knowledge\" is the knowledge encoded by the relative probabilities of the incorrect outputs. In [17,14,45], the authors pointed out that secondary information, i.e the semantic similarity between different classes, is part of the \"Dark Knowledge\", and [5] observed that the \"Dark Knowledge\" can help to refine noisy labels. In this paper, we would like to answer the following question: can we theoretically explain how neural networks learn the Dark Knowledge? Answering this question will help us to understand the regularization effect of distillation. \n\nIn this work, we assume that the teacher network is overparameterized, which means that it can memorize all the labels via gradient descent training [12,11,34,1].",
            "score": 0.537312522543212,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 117,
                    "matchedPaperCorpusId": "1779661"
                },
                {
                    "start": 909,
                    "end": 911,
                    "matchedPaperCorpusId": "21596346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "268739697",
            "title": "TCN-attention-HAR: human activity recognition based on attention mechanism time convolutional network",
            "text": "The specific formula for attention is as follows: where e t represents the attention weight calculated based on the network's output layer at time t. The attention weight is determined using weight parameters U and w , along with a bias vector b . Ultimately, the classification of human activities is accomplished through the Softmax classification layer. The formula for this classification process is as follows: \n\nwhere z is the output of the softmax layer, and k is the number of activity categories. \n\nAs a model compression method, knowledge distillation, as shown in Fig. 5, mainly uses large and complex neural network models as teacher models, simple and lightweight neural network models as student models, and transfers the knowledge learned from the teacher model to the student model, significantly improving the accuracy of the student model. The student model can adjust distillation losses through temperature (T). Given the probability of Softmax(z i , T) , class i is calculated based on Logit to obtain z_ I. The specific formula for adding the temperature softmax function is: Therefore, the soft loss ( L soft ) makes Cross entropy for the softmax generated by the teacher model and the softmax generated by the student model, and hard ( L hard ) is the student loss of the standard softmax function. The complete Loss function L of knowledge distillation process is the weighted average value of soft loss and hard loss, which is defined as: \n\nexp(e t ) \n\nt j=0 e j (7) where H is the Cross entropy Loss function, z t and z s represents the logarithm of the teacher model and the student model,\u03b1 As the distillation loss coefficient, \u03b2 As a loss coefficient for students.",
            "score": 0.5371350248634513,
            "section_title": "Model architecture",
            "char_start_offset": 12648,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1694
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37646484375
        },
        {
            "corpus_id": "219636179",
            "title": "Knowledge Distillation Meets Self-Supervision",
            "text": "The seminal paper by Hinton et al. [16] show that the knowledge from a large ensemble of models can be distilled and transferred to a student network. Specifically, one can raise the temperature of the final softmax to produce soft targets of the teacher for guiding the training of the student. The guidance is achieved by minimizing the Kullback-Leibler (KL) divergence between teacher and student outputs. An interesting and inspiring observation is that despite the teacher model assigns probabilities to incorrect classes, the relative probabilities of incorrect answers are exceptionally informative about generalization of the trained model. [16] and SSKD. We extend the mimicking on normal data and on a single classification task to the mimicking on transformed data and with an additional self-supervision pretext task. The teacher's self-supervision predictions contain rich structured knowledge that can facilitate more rounded knowledge distillation on the student. In this example, contrastive learning on transformed images serves as the self-supervision pretext task. It constructs a single positive pair and several negative pairs through image transformations t(\u2022), and then encourages the network to recognize the positive pair. The backbone of the teacher and student are represented as ft and fs, respectively, while the corresponding output is given as t and s with subscript representing the index \n\nThe hidden knowledge encapsulated in these secondary probabilities is sometimes known as \"dark knowledge\". \n\nIn this work, we are fascinated on how one could extract richer \"dark knowledge\" from neural networks. Existing studies focus on what types of intermediate representations of teacher networks should student mimic. These representations include attention map [46], gram matrix [44], gradient [39], pre-activation [15], and feature distribution statistics [17]. While the intermediate representations of the network could provide more fine-grained information, a common characteristic shared by these medium of knowledge is that they are all derived from a single task (typically the original classification task). The knowledge is highly task-specific, and hence, such knowledge may only reflect a single facet of the complete knowledge encapsulated in a cumbersome network.",
            "score": 0.5370986367110661,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2305
                }
            ],
            "ref_mentions": [
                {
                    "start": 35,
                    "end": 39,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 649,
                    "end": 653,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 1808,
                    "end": 1812,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1823,
                    "end": 1827,
                    "matchedPaperCorpusId": "3603145"
                },
                {
                    "start": 1886,
                    "end": 1890,
                    "matchedPaperCorpusId": "215744940"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5224609375
        },
        {
            "corpus_id": "236984375",
            "title": "Combine-Net: An Improved Filter Pruning Algorithm",
            "text": "Knowledge distillation (Figure 3) is put forward by Hinton et al. [7]. It is a widely used knowledge transfer technology in the deep learning field. First, a well-trained, robust, high-precision teacher network is needed. Its output is softened with temperature T to provide more information entropy, which extracts hidden knowledge behind its output layer. Then, a relatively small student network is trained to imitate the teacher network's probability output distribution, obtaining a better output result. The main idea of knowledge distillation. The label of the input image is cat; the probability is expressed as {0, 1, 0}. After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q'', which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: After the inference of teacher network and student network, the algorithm outputs classification results q and q', so that the image is declared as a cat. However, this image also shows some dog traits, which is not shown obviously in q and q'. After softening the teacher network, the dark knowledge appears. The classification result is q\", which provides more dark knowledge. Training the student network with the teacher network makes the student network more accurate on the basis of the teacher network's characteristics. \n\nTo improve the efficiency of knowledge distillation, Haitong Li [20] used KL divergence to replace cross-entropy loss (CE) to make the final loss function become: \n\nwhere Q T s , Q T t are the softmax probability distribution of the student network and teacher network after softening according to temperature T.",
            "score": 0.5369238840667231,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 15408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2167
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 69,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "225397480",
            "title": "A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning",
            "text": "Testing Process The neural network often obtains the network output through the softmax [17] layers at the end. By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  . In order to calculate the similarity between the output vector and the ideal vector in the neural network, cross entropy is usually describe the distance between the two probability distributions. KL divergence (relative entropy) refers to describe the difference between two probability distributions. \n\nIn knowledge distillation, cross entropy is a loss function commonly used with softmax in multiclassification problems: \n\nFor the two probability distributions P and Q, the relative entropy is defined as follows: \n\nThe combined neural network loss function formula is as follows: \n\nThe   ,  in the above formula is called  ,that is, the cross-entropy loss obtained by the soft targets generated by the teacher network, Cross Entropy ( ,  ) is called  , which is the cross-entropy loss obtained by using the hard targets in the training set. Use  to adjust the proportion of the two loss values. \n\nIn knowledge distillation, the teacher network only conducts one-way knowledge transfer to the student network during the learning process. In the method proposed in this paper, multiple networks are used for training at the same time. During the training process, each network not only accepts the supervision of the true value label, and the fixed probability estimation of the teacher network, but also refers to the dynamic probability estimation of other networks. This method can further improve the generalization ability. Through the process, feedback information is obtained from the learning state between the student networks to optimize the training process and achieve collaborative training. In each iteration, we calculate the predictions of different models and update the parameters of the network according to the predictions of other models.",
            "score": 0.5362948524795161,
            "section_title": "Training Process",
            "char_start_offset": 5694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2152
                },
                {
                    "start": 2153,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2483
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "267740612",
            "title": "FedD2S: Personalized Data-Free Federated Knowledge Distillation",
            "text": "KD is referred to any method of transferring knowledge from one or multiple teacher models into a student model [24]. This process levearges a public dataset, denoted by D p , to align the mapping functions of teacher and student models. . Specifically, the logit outputs of the teacher model, when applied to the public dataset, are passed through a softmax function to generate soft labels. Along with the ground-truth outputs, which are used to train the student model in a conventional way, soft labels are utilized as regularizers to constrain the loss of the student model. Typically, a Kullback-Leibler divergence function is employed to minimize the discrepancy between the soft labels of the teacher model and the predictions made by the student model, as follows [24]: \n\nwhere F s (\u2022; \u03b8 s ) and F t (\u2022; \u03b8 t ) are the student and teacher models, parameterized by \u03b8 s and \u03b8 t , respectively. In addition, L KL is the per-sample Kullback-Leibler loss function, and \u03c4 is the so-called temperature hyper-parameter used to soften generated logits. Notably, throughout this paper, we assume that a soft-max function with temperature \u03c4 is employed in the output layer of models. \n\nIn FKD, depending on the method, both clients and the server can play the role of either the teacher or the student. In [25], clients first update their local models using their respective local datasets, then, each client generates a set of soft labels by making predictions on a shared public dataset. On the server side, these local soft labels are averaged to create global soft labels, which represent global knowledge. Finally, global knowledge is utilized in Eq. ( 5) to execute knowledge distillation. This allows for the transfer of knowledge from other clients to each individual client. \n\nThe performance of knowledge sharing in FKD methods depends significantly on the distribution of the public dataset. However, in real-world scenarios, accessing a public dataset that accurately represents the entire distribution of local datasets is often impractical. Therefore, it is necessary to develop methods that facilitate the exchange of knowledge between clients and the server without the need for a shared dataset.",
            "score": 0.5359884161162569,
            "section_title": "B. Knowledge Distillation",
            "char_start_offset": 9430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.369873046875
        },
        {
            "corpus_id": "15232595",
            "title": "Knowledge Transfer Pre-training",
            "text": "The idea that a well-trained DNN model can be used as a teacher to help training other models was proposed by Ba and Hinton [18], [19], [20]. The basic assumption is that the teacher model learns rich knowledge from the training data and this knowledge can be used to guide the training of child models which are simple and hence unable to learn many details without the teacher's guide. To distill the knowledge from the teacher model, the logit matching approach proposed by Ba [18] teaches the child model by encouraging its logits (activations before softmax) close to those generated by the teacher model in terms of square error, and the dark knowledge distiller model proposed by Hinton [19] encourages the output of the child model close to those of the teacher model in terms of cross entropy. This knowledge transfer idea has been applied to learn simple models from complex models so that the simple model can approach the performance of the complex model [20], [23]. \n\nWe focus on the dark knowledge distiller model rather than logit matching as it showed better performance in our experiments. This model uses a well-trained DNN as the teacher model to predict the targets of the training samples, and these targets are used to train the child model. The predicted targets are actually posterior probabilities of the targets associated with the DNN output, and they are soft because the class identities with these targets are not as deterministic as with the original one-hot hard targets. To make the targets softer, a temperature T was introduced in [19] to scale the logits. This is formulated by p i = e z i /T j e z j /T , where i, j indexes the target classes. As argued by Hinton [19], a larger T allows more information of non-targets to be distilled.",
            "score": 0.5357710857161136,
            "section_title": "III. PRE-TRAINING WITH DARK KNOWLEDGE TRANSFER A. Dark knowledge distiller",
            "char_start_offset": 8898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1773
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 134,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 136,
                    "end": 140,
                    "matchedPaperCorpusId": "704391"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 967,
                    "end": 971,
                    "matchedPaperCorpusId": "704391"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1701,
                    "end": 1705,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1239013671875
        },
        {
            "corpus_id": "274981678",
            "title": "CBNN: 3-Party Secure Framework for Customized Binary Neural Networks Inference",
            "text": "The standard approach in Knowledge Distillation (KD) [14] employs a teacherstudent paradigm, where a large and complex teacher network imparts knowledge to a smaller student network trained on the identical dataset. Figure 1 illustrates the entire KD algorithm's workflow. Distillation: DNNs typically convert the layer output, known as logit z i , into a class probability q T i for each class using the softmax function. \n\nThe concept of temperature T , introduced by Hinton [14], determines the softness of the probability distribution across classes. \n\nTeacher-Student Network: During training, as depicted in Figure 1, the computation of the student model's loss function follows the standard protocol. The dataset comprises inputs and labels, with the label of the ground truth referred to as the hard label. The student loss is determined using the crossentropy loss function H. \n\nThe term class denotes the accurate outcome of the classification process. Depending on the hard label y i , if y i = 1 when i corresponds to the correct class, or y i = 0 when i does not match the correct class, the student loss can be simplified as follows: \n\nAs Figure 1 illustrates, the teacher model employs the softmax function under a specific temperature T to generate predictions q T i , referred to as soft labels. Given the identical temperature T and inputs, the student model computes predictions p T i , known as soft predictions. Following the formula 3, the teacher loss is defined as: \n\nDuring the training process, the overall loss function L is computed as a combination of the student loss and the teacher loss. \n\nThe essence of knowledge distillation lies in the utilization of a higher temperature T to generate soft labels in the teacher model, facilitating the transfer of knowledge. The weighting factor \u03bb serves to prioritize the output of a specific loss function relative to others. Additionally, y denotes the hard label corresponding to the soft label p T . By training the student model with both soft and hard labels, it can achieve comparable accuracy to the more complex teacher model.",
            "score": 0.5355791916763588,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 554
                },
                {
                    "start": 557,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1147
                },
                {
                    "start": 1150,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "237492204",
            "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks",
            "text": "Knowledge Distillation in Classification Tasks. Transferring knowledge from one model to another is a research topic that has obtained noteworthy attention during recent years. Ba and Caruana (Ba and Caruana 2014) trained a single and small neural network to imitate the logits of a large and complex neural network. Then, Hinton et al. (Hinton, Vinyals, and Dean 2015) introduced KD and dark knowledge to claim that the deeper teacher model can successfully distill its knowledge into the smaller student neural network by matching their soft targets (softmax distributions). Nowadays, a lot of successive papers have been written to propose different techniques to KD for model compression purposes. Romero et al. (Romero et al. 2015) distilled the feature representations of the teacher's intermediate layers to the student for improving the training stage of the student network. Transferring the attention maps (Zagoruyko and Komodakis 2017;Huang and Wang 2017;Tarvainen and Valpola 2017), the inner products of intermediate activation maps (Yim et al. 2017), and relational knowledge between training samples (Park et al. 2019;Tung and Mori 2019;Peng et al. 2019;Liu et al. 2019) are some other methods to promote the distillation process from one model to another. However, these approaches ignored the possibility of available subclass knowledge within the classes and therefore did not take advantage of hidden subclass knowledge to improve student performance. By contrast, in this study, we use subclass knowledge to enhance the generalization ability of the teacher network. \n\nSubclass Knowledge Distillation. The distillation of knowledge can be improved by increasing the amount of information that the teacher can transfer to the student. M\u00fcller et al. (M\u00fcller, Kornblith, and Hinton 2020) compelled the teacher to create semantically meaningful subclasses for each class during its training phase with auxiliary contrastive loss. The student is then trained to mimic the invented teacher's subclasses predictions (probabilities). When the number of training samples per class is the same, they measured the number of bits of label information about how the teacher generalizes through subclass distillation in binary classification tasks.",
            "score": 0.5354846708654206,
            "section_title": "Related Work",
            "char_start_offset": 4680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 213,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 337,
                    "end": 369,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 916,
                    "end": 946,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 966,
                    "end": 993,
                    "matchedPaperCorpusId": "263861232"
                },
                {
                    "start": 1046,
                    "end": 1063,
                    "matchedPaperCorpusId": "206596723"
                },
                {
                    "start": 1115,
                    "end": 1133,
                    "matchedPaperCorpusId": "131765296"
                },
                {
                    "start": 1133,
                    "end": 1152,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 1152,
                    "end": 1169,
                    "matchedPaperCorpusId": "102483463"
                },
                {
                    "start": 1169,
                    "end": 1185,
                    "matchedPaperCorpusId": "198185886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11798095703125
        },
        {
            "corpus_id": "22951388",
            "title": "Distilling Model Knowledge",
            "text": "The term \"knowledge distillation\" originated from the recent work of Hinton et al. (2015). Whereas in this thesis we are using it more broadly to mean \"transferring knowledge from a cumbersome model to a convenient model\", in their work they use it only in the context of model compression. They focus on neural networks, or ensembles thereof, that have a softmax output. Their goal is to compress them to smaller neural networks with a softmax output. Their method is in principle similar to the standard one of Bucil\u0203 et al. (2006), however they come up with a novel insight that leads to a slightly different approach.\n\nTheir main observation is that, in the output of the softmax layer, the relative magnitudes of the different components, even of those that are very small, contain a significant amount of knowledge. However, when training with cross entropy, the training signal is typically dominated by the largest component, which is often very close to 1. In order to extract the knowledge hidden in the small differences between the tiny components, they propose scaling the logits by a factor of 1 /T . Parameter T can be interpreted as a \"temperature\", and the higher it is set the smoother the softmax output becomes, thus making the small differences in the probability components become more pronounced. The drawback of their approach is that T , as a newly introduced free hyperparameter, needs to be tuned.\n\nIncreasing the temperature is not the only way of enhancing the differences between tiny probability components. The same effect can be achieved by transforming the probabilities to a different domain. For example, Ba and Caruana (2014) use the logit domain, and in our derivative square error loss we use the log domain, both of which succeed in enhancing the small differences. Indeed, Hinton et al. (2015) argue that in the limit of setting the temperature infinitely high, their method approximates a transformation to the logit domain.",
            "score": 0.5348223209793098,
            "section_title": "Knowledge distillation",
            "char_start_offset": 48205,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 513,
                    "end": 533,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1641,
                    "end": 1662,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53125
        },
        {
            "corpus_id": "252780669",
            "title": "Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again",
            "text": "Knowledge Distillation (KD) aims at transferring the knowledge of a well-performed neural network (the {\\it teacher}) to a weaker one (the {\\it student}). A peculiar phenomenon is that a more accurate model doesn't necessarily teach better, and temperature adjustment can neither alleviate the mismatched capacity. To explain this, we decompose the efficacy of KD into three parts: {\\it correct guidance}, {\\it smooth regularization}, and {\\it class discriminability}. The last term describes the distinctness of {\\it wrong class probabilities} that the teacher provides in KD. Complex teachers tend to be over-confident and traditional temperature scaling limits the efficacy of {\\it class discriminability}, resulting in less discriminative wrong class probabilities. Therefore, we propose {\\it Asymmetric Temperature Scaling (ATS)}, which separately applies a higher/lower temperature to the correct/wrong class. ATS enlarges the variance of wrong class probabilities in the teacher's label and makes the students grasp the absolute affinities of wrong classes to the target class as discriminative as possible. Both theoretical analysis and extensive experimental results demonstrate the effectiveness of ATS. The demo developed in Mindspore is available at https://gitee.com/lxcnju/ats-mindspore and will be available at https://gitee.com/mindspore/models/tree/master/research/cv/ats.",
            "score": 0.5345908446973282,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0740966796875
        },
        {
            "corpus_id": "258341405",
            "title": "Multi-target Knowledge Distillation via Student Self-reflection",
            "text": "Previous knowledge distillation methods often transfer knowledge via minimizing the Kullback-Leibler (KL) divergence between the logits from the last layer of the teacher network and those from the last layer of the student network. Our method splits both the student and the teacher networks into several same level blocks, each of which is associated with an auxiliary classifier to get the softmax outputs. That is to say, each block's auxiliary classifier of the teacher network corresponds to the same block's auxiliary classifier of the student network. The logit outputs as the knowledge will then be distilled from the teacher network's auxiliary classifier to the student network's auxiliary classifier at the same stage, which can enable student network to learn more knowledge from teacher network at different levels. The distillation loss between student network and teacher network is formulated as: where K is the number of auxiliary classifiers. Note that the temperature used in p s i and p t i is denoted as T 1 according to Eq. ( 1). The superscripts s and t represent the student network and the teacher network, respectively. L K L (.) represents the Kullback-Leibler (KL) divergence, which is defined as: \n\nBy using the multi-stage offline teacher-student knowledge distillation, we allow each student's shallow/deep block to mimic the corresponding teacher's shallow/deep block via learning from teacher logit knowledge.",
            "score": 0.5343011133235943,
            "section_title": "Stage-Wise Response Distillation",
            "char_start_offset": 16603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1443
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "257079238",
            "title": "Distilling Calibrated Student from an Uncalibrated Teacher",
            "text": "The primary goal of KD is to distill the dark knowledge from the teacher network into the student network by comparing the temperature-scaled softmax logits of the teacher and student. The loss term generally consists of two losses, KL divergence loss (L div ) and a task-specific loss (L task ). \n\nwhere N is the total number of samples in the dataset. KL loss is the Kullback-Leibler Divergence score which describes the difference between the probability distribution of soft labels of teacher T and student S. In conventional KD [10], the KL-divergence loss and the task-specific loss are linearly combined as follows: \n\nwhere \u03b1 is a balancing factor. Generally, the value of \u03b1 is kept high to ensure high knowledge transfer from the teacher to the student, however, doing so makes the student network highly uncalibrated as shown in Fig. 1. Therefore, we scale the KL-divergence loss by taking mean across the dimension of the logits (i.e. dividing it by the logit size, which is same as number of classes). Using this scaled KLdivergence loss we observe a improvement in the calibration and a drop in accuracy, as shown in Fig. 2. We encounter a trade-off between accuracy and calibration error in the framework of KD1 . Our approach leverages the distillation framework to take care of the accuracy while improving the calibration with the help of data augmentation techniques.",
            "score": 0.5340371462944078,
            "section_title": "Knowledge distillation",
            "char_start_offset": 8738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 296
                },
                {
                    "start": 299,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 622
                },
                {
                    "start": 625,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.398681640625
        },
        {
            "corpus_id": "260704139",
            "title": "Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation",
            "text": ". As for collaborative learning, we calculate classification loss as follows: \n\nL   = cross_entropy(z  , y),  \u2208 {, ,  } \n\nIn the knowledge distillation part, we calculate the distillation loss as follows: \n\nwhere T is the temperature parameter,  is the softmax operation, and (, ) means the KL divergence between the soften outputs  from teacher network and  from student network. Because at the beginning of the model training, the predictions of each student network are not accurate enough, we need to decrease the weight of the distillation loss during the early training stage. Therefore, we adopt a time-dependent unsupervised ramp-up function  () [22]. When the training epoch is 0,  () is 0. Then,  () increases exponentially as the training epoch progresses. When the training epoch reaches ,  () takes a value of 1. Then the final total loss is as follows:",
            "score": 0.533958355382333,
            "section_title": "3",
            "char_start_offset": 19286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 80,
                    "end": 119
                },
                {
                    "start": 122,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 866
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "244130170",
            "title": "Synthetic Unknown Class Learning for Learning Unknowns",
            "text": "Following the conventional DNN classifier training scheme, the teacher network (T ) is first trained with the aim of minimizing the cross-entropy loss function. DNN classifiers are usually overgeneralized, producing very high confidence scores even for unknowns during testing. Thus, we introduce an additional network, the student network, and apply a knowledge distillation technique [7] based on the pretrained teacher network to relax the posterior probabilities of the student network for known classes. Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature  \u03c4 in the softmax function as follows: \n\nand \n\nwhere l ik is the logit of sample x i for the k-th known class and l u ik is the logit for the k-th synthetic unknown class. The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function: \n\nwhere D is a known training sample set, |D| is the size of D, H is the cross-entropy function, and Q \u03c4 i and P \u03c4 i are temperature-scaled probability vectors for the teacher and student, respectively. \n\nUnlike original knowledge distillation [7], we do not consider the cross-entropy between the true labels and the nonscaled probability vector of the student because our purpose is to minimize overgeneralization while leaving room for learning unknowns. In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes. This chance is further amplified via the synthetic unknown class learning approach described in the next subsection.",
            "score": 0.5336955927444893,
            "section_title": "Knowledge distillation",
            "char_start_offset": 12967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 389,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 1404,
                    "end": 1407,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "220332637",
            "title": "Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models",
            "text": "Specifically, \n\nwhere p \u2208 R c\u00d71 are the output probability scores w.r.t. each class, softmax (2) is a function which is usually applied to a vector to generate probability scores, z \u2208 R c\u00d71 are the logit values (i.e., the output of the previous layer which is input to the softmax function) output by a deep neural network f (x) applied on an input x. Intuitively when T is large, the difference between the maximum and minimum normalized logit values is small. Thus, the output probability values will be pushed towards a more uniform distribution. \n\nIf the student model matches the soft targets on a large transfer dataset, then we can say that the student model distills most of the knowledge stored in the larger teacher model. Note that the transfer set does not need to be constrained to the original data used for training, but could be any data. Therefore, we can formulate the model distillation problem as soft target alignment via the cross-entropy loss between the probability scores of the student model and teacher model. \n\nIn the distillation process, the student model typically uses the same temperature as the teacher model. During training, the temperature needs to be tuned for the best performance. When deploying the student model, the standard softmax function (2) should be utilized to set the probability scores back to their normal values.",
            "score": 0.5330698045520493,
            "section_title": "V. KNOWLEDGE DISTILLATION",
            "char_start_offset": 17231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 16,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1451416015625
        },
        {
            "corpus_id": "254069919",
            "title": "Curriculum Temperature for Knowledge Distillation",
            "text": "Knowledge distillation (Hinton, Vinyals, and Dean 2015) (KD) has received increasing attention from both academic and industrial researchers in recent years. It aims at learning a comparable and lightweight student by transferring the knowledge from a pretrained heavy teacher. The traditional process is implemented by minimizing the KL-divergence loss between two predictions obtained from the teacher/student model with a fixed temperature in the softmax layer. As depicted in (Hinton, Vinyals, and Dean 2015;Liu et al. 2022;Chandrasegaran et al. 2022), the temperature controls the smoothness of distribution and can faithfully determine the difficulty level of the loss minimization process. Most existing works (Tung and Mori 2019;Chen et al. 2020;Ji et al. 2021) ignore the flexible role of the temperature and empirically set it to a fixed value (e.g., 4). Differently, MKD (Liu et al. 2022) proposes to learn the suitable temperature via meta-learning. However, it has certain limitations that require an additional validation set to train the temperature module, which complicates the training process. Besides, it mainly focuses on the strong data augmentation condition, neglecting that most existing KD methods work under normal augmentation. Directly combining MKD with existing distillation methods under strong augmentation may cause severe performance degradation (Das et al. 2020). \n\nIn human education, teachers always train students with simple curricula, which start from easier knowledge and gradually present more abstract and complex concepts when students grow up. This curriculum learning paradigm has inspired various machine learning algorithms (Caubri\u00e8re et al. 2019;Duan et al. 2020). In knowledge distillation, LFME (Xiang, Ding, and Han 2020) adopt the classic curriculum strategy and propose to train the student gradually using samples ordered in an easy-to-hard sequence. RCO (Jin et al. 2019) propose to utilize the sequence of the teacher's intermediate states as the curriculum to gradually guide the learning of a smaller student.",
            "score": 0.5324126153085782,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1399
                },
                {
                    "start": 1402,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2069
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 55,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 480,
                    "end": 512,
                    "matchedPaperCorpusId": "232232777"
                },
                {
                    "start": 512,
                    "end": 528,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 717,
                    "end": 737,
                    "matchedPaperCorpusId": "198179476"
                },
                {
                    "start": 737,
                    "end": 754,
                    "matchedPaperCorpusId": "208526905"
                },
                {
                    "start": 882,
                    "end": 899,
                    "matchedPaperCorpusId": "73729180"
                },
                {
                    "start": 1696,
                    "end": 1713,
                    "matchedPaperCorpusId": "213005467"
                },
                {
                    "start": 1747,
                    "end": 1774,
                    "matchedPaperCorpusId": "209862398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.236328125
        },
        {
            "corpus_id": "212908749",
            "title": "Improved Knowledge Distillation via Teacher Assistant",
            "text": "Despite its huge popularity, there are few systematic and theoretical studies on how and why knowledge distillation improves neural network training. The so-called dark knowledge transferred in the process helps the student learn the finer structure of teacher network. Hinton, Vinyals, and Dean (2015) argues that the success of knowledge distillation is attributed to the logit distribution of the incorrect outputs, which provides information on the similarity between output categories. Furlanello et al. (2018) investigated the success of knowledge distillation via gradients of the loss where the soft-target part acts as an importance sampling weight based on the teachers confidence in its maximum value. Zhang et al. (2017) analyzed knowledge distillation from the posterior entropy viewpoint claiming that soft-targets bring robustness by regularizing a much more informed choice of alternatives than blind entropy regularization. Last but not least, Lopez-Paz et al. (2015) studied the effectiveness of knowledge distillation from the perspective of learning theory (Vapnik 1998) by studying the estimation error in empirical risk minimization framework. \n\nIn this paper, we take this last approach to support our claim on the effectiveness of introducing an intermediate network between student and teacher. Moreover, we empirically analyze it via visualizing the loss function.",
            "score": 0.5322273065201422,
            "section_title": "Related Work",
            "char_start_offset": 6977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1390
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 302,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 713,
                    "end": 732,
                    "matchedPaperCorpusId": "829159"
                },
                {
                    "start": 961,
                    "end": 984,
                    "matchedPaperCorpusId": "3323727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03143310546875
        },
        {
            "corpus_id": "189762324",
            "title": "Linear Distillation Learning",
            "text": "Knowledge distillation (KD) is a method of transferring \"knowledge\" from one machine learning model called teacher to another one called student. The idea behind KD is that a teacher network is a high-capacity model with desired high performance and a student network is a more lightweight model [1,19,27]. A student cannot match the teacher, but the distillation process brings the student closer to the predictive power of the teacher. Distillation idea was brought to the neural network community by Hinton et. al. [7]. \n\nIn distillation learning, knowledge is transferred by training a student model, using a soft target distribution for comparison with the output layer. This distribution is produced by a cumbersome model with a high temperature in its output softmax \n\nwith z i are logits and T is temperature. Another scenario of knowledge distillation training is transferring knowledge from an ensemble of highly regularized models to a smaller model [7]. \n\nDistillation can also be applied for adversarial permutation [15], born-again neural networks [6] and Global Additive Explanations [26]. Furthermore, Sau and Balasubramanian [20] proposed to add random perturbations into soft labels for simulating learning from multiple teachers. \n\nSurprisingly, a distillation method often allows smaller student network to be trained to mimic the larger and deeper models very accurately, while the student trained on the one-hot hard targets cannot achieve the same results. The clear reason for this awaits to be discovered.",
            "score": 0.531916108482862,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 4344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1530
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 299,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 299,
                    "end": 302,
                    "matchedPaperCorpusId": "1923568"
                },
                {
                    "start": 302,
                    "end": 305,
                    "matchedPaperCorpusId": "16550689"
                },
                {
                    "start": 1029,
                    "end": 1033,
                    "matchedPaperCorpusId": "2672720"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "53531803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2427978515625
        },
        {
            "corpus_id": "272366594",
            "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
            "text": "However, reverse KL can come at the expense of diversity, often producing fewer output variations. In Baby LLaMA, temperature scaling is applied to the logits of teacher models (e.g., GPT-2 and LLaMA) to provide smoother output distributions for distillation Timiryasov et al. (2023). This allows the student model to retain the nuances of the teacher's probabilistic predictions, especially in low-data regimes. The soft probabilities are controlled by the temperature parameter T , ensuring that the model retains fine-grained knowledge: \n\n. \n\nExpanding on this, KPTD adopts a similar approach, but it leverages entity definitions and domainspecific knowledge to generate the transfer set. The student model learns to match the teacher's output distribution based on this enriched set of entities, which is crucial for maintaining up-todate knowledge in pre-trained models Mirzadeh et al. (2020); Gong et al. (2022); Chen et al. (2020). Following this, the authors of GKD refine this further by using token-level forward and reverse divergence, focusing on critical tokens during distillation Jafari et al. (2021); Kim and Rush (2016). This design ensures that the student captures vital aspects of sequence-level tasks, such as machine translation and summarization, where token importance fluctuates throughout the sequence. \n\nIn parallel with previous work, in MetaICL Min et al. (2022) and Multitask- ICT Huang et al. (2022), soft distillation is adapted to transfer in-context learning (ICL) abilities from large models to smaller ones, helping the student generalize across tasks. These models use few-shot learning to transfer the multitask capabilities of large LLMs, effectively allowing the student to leverage the soft output distributions for task generalization. After this, the work from Zhao et al. (2024a) further enhances this process by introducing a multi-stage collaborative approach, where the student generates refined pseudolabels to improve the next stage of distillation, pushing the boundaries of semi-supervised learning through iteration. \n\nAbove all, the soft distillation loss can be expressed as: \n\nwhere:",
            "score": 0.5317034352758216,
            "section_title": "Soft Knowledge Distillation",
            "char_start_offset": 12729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1328
                },
                {
                    "start": 1331,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 2068
                },
                {
                    "start": 2071,
                    "end": 2129
                },
                {
                    "start": 2132,
                    "end": 2138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1326904296875
        },
        {
            "corpus_id": "258960507",
            "title": "GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking",
            "text": "Knowledge distillation [20] aims to transfer knowledge from a stronger pre-trained teacher model to a compact student model.\n\nOne of the key ideas of knowledge distillation is to inject the posterior knowledge (e.g., the provenance of the golden answer) into the teacher, and the student is supervised by matching the output distribution of the teacher during the training procedure.\n\nConventional methods conduct knowledge distillation by minimizing the Kullback-Leibler divergence loss (KLD loss) between the output distribution of the teacher model and the student model [8,20,25,56]. Recent studies treat knowledge distillation as a learning-to-rank problem [6,46,57]. Thus, the distillation process can be regarded as a teacher teaching a student how to order the top-ranked candidates correctly. Another line of work improves the distillation performance by progressively distilling knowledge from a teacher model to a student model. For example, Lin et al. [33] proposed a progressive distillation approach for dense passage retrieval, which gradually improves the capability of teachers and students through both model and data perspectives. Li et al. [32] enhanced the distillation performance by progressively increasing the learning difficulty of the student. They realized the progressive distillation by controlling the distillation temperature [20] in an easy-to-hard curriculum [3,4], where the distillation temperature is a hyper-parameter that controls the smoothness of probability distributions and can faithfully determine the difficulty level of the distillation process [7,34].\n\nIn contrast to the above work, we train a generative passage estimator as a teacher and distill the ranking order estimated from it to the student, which allows the student to learn how to order the top-ranked passages. Moreover, we realize the knowledge distillation in an easy-to-hard manner by progressively increasing the difficulty of the sampled candidate passages for training the student, which enables the student progressively learn to distinguish the provenance of answers from many plausible candidates.",
            "score": 0.5315628248971515,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 9611,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 577,
                    "matchedPaperCorpusId": "218915023"
                },
                {
                    "start": 583,
                    "end": 586,
                    "matchedPaperCorpusId": "250426422"
                },
                {
                    "start": 662,
                    "end": 665,
                    "matchedPaperCorpusId": "207163577"
                },
                {
                    "start": 665,
                    "end": 668,
                    "matchedPaperCorpusId": "233237266"
                },
                {
                    "start": 668,
                    "end": 671,
                    "matchedPaperCorpusId": "252368357"
                },
                {
                    "start": 964,
                    "end": 968,
                    "matchedPaperCorpusId": "252545066"
                },
                {
                    "start": 1393,
                    "end": 1396,
                    "matchedPaperCorpusId": "256454415"
                },
                {
                    "start": 1396,
                    "end": 1398,
                    "matchedPaperCorpusId": "873046"
                },
                {
                    "start": 1592,
                    "end": 1595,
                    "matchedPaperCorpusId": "250113551"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01363372802734375
        },
        {
            "corpus_id": "57189239",
            "title": "Improving the Interpretability of Deep Neural Networks with Knowledge Distillation",
            "text": "Earlier research in this area focuses on designing inherently interpretable models such as decision lists [17], decision sets [18], additive models [19], sparse linear models [20], etc. However, this approach presents a severe constraint on the selection of algorithms. Besides, although human can comprehend these models, they fail to model more complex problems with good accuracy performance. \n\nIn this paper, we apply the most recent model-agnostic approach [21] which performs post-hoc explanations on the trained models. Past research focus on either global interpretations [22] [23] or local explanations [24] [25] [26]. We concentrate on the global interpretations. In this paper, we adopt knowledge distillation to improve the global interpretation results. \n\nKnowledge distillation refers to the process of transferring the dark knowledge learned by a teacher model (usually sophisticated and large) to a student model (usually shallow and small). Dark Knowledge [27] [28] is the salient information hidden in the \"soft targets\": predicted probabilities for all classes, which are more informative than the \"hard targets\": predicted classes. Maybe the pioneer work to distill the knowledge from a neural network into another algorithm is by Craven and Shavlik [29] who used a symbolic algorithm: the decision tree [30] to approximate the functions learned by a neural network with one hidden layer using hard targets. \n\nKnowledge distillation originates from model compression [31]. In [31], the teacher model was built using the ensemble selection algorithm [32], which was then used to label unseen unlabeled data: the training data for the student model (also called the transfer data). This approach uses the hard targets produced by the teacher model. A followed work [33] distills deep nets into shallow feed-forward nets adopting the method of \"matching logits\" (scores before the softmax activations), which would avoid the information loss when passing through logits to the probability space. Then the concept of \"knowledge distillation\" was officially introduced in [27]. It is a more general solution to transfer knowledge from a cumbersome model to a compact model.",
            "score": 0.5307164113259568,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 766
                },
                {
                    "start": 769,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "17699665"
                },
                {
                    "start": 126,
                    "end": 130,
                    "matchedPaperCorpusId": "12533380"
                },
                {
                    "start": 148,
                    "end": 152,
                    "matchedPaperCorpusId": "14190268"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "207211836"
                },
                {
                    "start": 462,
                    "end": 466,
                    "matchedPaperCorpusId": "3366554"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "6930911"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "14664111"
                },
                {
                    "start": 622,
                    "end": 626,
                    "matchedPaperCorpusId": "13029170"
                },
                {
                    "start": 973,
                    "end": 977,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "84176918"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "6930911"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1496,
                    "end": 1500,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 1569,
                    "end": 1573,
                    "matchedPaperCorpusId": "1345193"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 2087,
                    "end": 2091,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "231800060",
            "title": "Improved knowledge distillation by utilizing backward pass knowledge in neural networks",
            "text": "In the original KD, the process of transferring knowledge from a teacher to a student model accomplishes by minimizing a loss function between the logits of student and teacher networks. This loss function has been used in addition to the regular training loss function for the student network. In other words, we have an additional loss term in the KD loss function between the softmax outputs of teacher and student networks which is softened by a temperature term. \n\nwhere S(x) and T (x) are student and teacher networks respectively. \u03c4 is the temperature parameter and \u03b1 is a coefficient between [0, 1]. This loss function is a linear combination of two loss functions. \n\nThe first loss function minimizes the difference between the output of the student model and the given true label. The second loss function minimizes the difference between the outputs of the student model and the teacher model. Therefore the whole loss function minimizes the distance between the student and both underlying and teacher functions. Since the teacher network is assumed to be a good approximation of the underlying function, it should be close enough to the underlying function of data samples. Fig. 2-(a) shows a simple example with three data points, an underlying function, a trained teacher and a potential student function that satisfies the KD loss function in eq. 1. However, the problem is, even though the student satisfies the KD objective function and intersects the teacher function close to the training data samples, there is no guarantee that it would fit the teacher network in other regions of the input space as well. In this work, we try to address this problem by deploying the backward gradient information w.r.t the input (we refer to as backward knowledge) in the two networks.",
            "score": 0.5292678537253066,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 3476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1792
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32177734375
        },
        {
            "corpus_id": "259228825",
            "title": "Research on Safety Helmet Detection Algorithm Based on Improved YOLOv5s",
            "text": "Knowledge distillation is a technique utilized to extract the knowledge of a large teacher model and condense it into a small student model. It can be understood as a large teacher neural network teaching his knowledge to a small student network [35][36][37]. \n\nThe process is transferred from the teacher network to the student network. The teacher network is generally bloated; therefore, the teacher network provides knowledge to the student network. The student network is a relatively small network and can thus obtain a lightweight network model. Knowledge distillation adopts the teacher-student mode. In this mode, the teacher is the output party of \"knowledge\", and the student is the receiver of \"knowledge\" [38]. \n\nThe teacher has a strong learning ability and can transfer the learned knowledge to the student model with a lower learning ability, so as to improve the generalization ability of the student model. The complicated and cumbersome but easy-to-use teacher model has no upper limit; it is purely a tutor, and in reality, a simple and flexible student model is deployed. The knowledge distillation process is shown in Figure 9   First, distill a deeper teacher network with a better extraction ability to obtain a logit, and distill it at T temperature. Then, use the classification prediction probability distribution in the Softmax layer to obtain soft targets. At the same temperature T, the logits in the student network are distilled, and then the category prediction probability distribution in Softmax is used to obtain the loss function L soft . Its expression is: \n\nwhere C j is the true label value of the j-th class. Finally, L hard and L soft are weighted and summed to obtain the final loss function L. This loss function can prevent the wrong information from the teacher network from being transmitted to the student network by comparing it with the real label. In this study, the improved YOLOv5s model was used as the teacher network, and the YOLOv5s model with the large target detection layer removed by structural pruning was used as the student model for knowledge distillation to obtain the final model and reduce the amount of calculation and parameters of the improved network model.",
            "score": 0.528904740726668,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 38607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 259
                },
                {
                    "start": 262,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "250957345"
                },
                {
                    "start": 250,
                    "end": 254,
                    "matchedPaperCorpusId": "231969372"
                },
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "244283694"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "219559263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1357421875
        },
        {
            "corpus_id": "251460030",
            "title": "Improving Deep Mutual Learning via Knowledge Distillation",
            "text": "Knowledge distillation in [8] is one of the most popular knowledge transfer methods today, and it uses a teacher-student framework. The basic idea of this method is that a pretrained teacher network (i.e., a cumbersome network or the biggest network) using certain hyperparameters are then used to train an untrained student network (i.e., small network) for the purpose of transferring knowledge. This process uses a distillation knowledge equation where a temperature (T) is involved and can be varied to obtain a soft probability output from a class C image which can be calculated as: \n\nSuppose the teacher network is marked as G t and the student network is G s , then the distillation loss can be defined as: \n\nAs a result, the student loss function contained in Figure 2 is minimized during the training process based on ( 6) and (7) as: \n\nwhere \u03bb is a balancing value between the two losses. The main purpose of the teacherstudent framework is to force the student output probability to imitate or match the pre-trained teacher network's probability output.",
            "score": 0.5288868815271165,
            "section_title": "DML and KD",
            "char_start_offset": 10613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03460693359375
        },
        {
            "corpus_id": "277104482",
            "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
            "text": "To further enhance the performance of the MaTVLM, we propose a knowledge distillation method that transfers knowledge from transformer layers to Mamba-2 layers. We use a pre-trained VLM as the teacher model and our MaTVLM as the student model. We will introduce the distillation strategies in the following. Probability Distribution Distillation First, our goal is to minimize the distance between probability distributions of the models, just the logits output by the models before applying the softmax function. This approach is widely adopted in knowledge distillation, as aligning the output distributions of the models allows the student model to gain a more nuanced understanding from the teacher model's prediction. To achieve this, we use the kullback-leibler (KL) divergence with a temperature scaling factor as the loss function. The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model. The loss function is defined as follows: \n\nThe softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively: \n\nwhere T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model. \n\nwhere T li (x) and S li (x) represent the outputs of the teacher model and the student model at layer l i , respectively. \n\nSequence Prediction Loss Finally, except of the distillation losses mentioned above, we also calculate the crossentropy loss between the output sequence prediction of the student model and the ground truth. This loss is used to guide the student model to learn the correct sequence prediction, which is crucial for the model to perform well on downstream tasks. The loss function is defined as: \n\nwhere y is the ground truth sequence, and \u0177s is the predicted sequence from the student model.",
            "score": 0.5285917456665504,
            "section_title": "Knowledge Distilling Transformers into Hybrid Models",
            "char_start_offset": 11610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 1979
                },
                {
                    "start": 1982,
                    "end": 2076
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "261429373",
            "title": "Knowledge distillation for efficient standard scanplane detection of fetal ultrasound",
            "text": "The difference between the probability distributions generated by the softmax is computed with the Kullback-Leibler (KL) divergence loss. \n\nA hyperparameter \u03c4 , called temperature, controls the softening of the signal arising from the output of the teacher network; a t and a s are the logits of the teacher and student networks, respectively. More details on this loss can be found at [17] (Sec. Distillation) where the Cross-Entropy loss is adopted, but it has been proved it is equivalent to the KL divergence ([23] Sec. 2). L FMD : it is computed on the intermediate feature maps, as a hand-crafted version of the adaptive method proposed in [21]. As shown in Fig. 2 the knowledge transfer is performed taking into account only three convolutional layers with RELU activation function for the student and for the teacher: the first, the intermediate, and the last layers of the teacher as associated with the corresponding layers of the student, through an appropriate feature map loss, combining Mean Square Error (MSE) losses: \n\nSince the two networks should be of different size scales and based on different architectures, in order to align the shapes of the features between the teacher and the student, an average pooling [24] is performed to both student and teacher's features map. A projection including a stack of convolutional layers of the size 1 \u00d7 1, 3 \u00d7 3, and finally 1 \u00d7 1 is further applied to the student's features to reduce the gap between capabilities of the networks.",
            "score": 0.5284593531252093,
            "section_title": "Proposed methodology",
            "char_start_offset": 10533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 646,
                    "end": 650,
                    "matchedPaperCorpusId": "227335337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.114990234375
        },
        {
            "corpus_id": "256900863",
            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
            "text": "Distillation (LLM-DKD) 1) Teacher Logits: LLM-DKD takes the Negative Euclidean distance [39] between teacher model output and each class as teacher logits: \n\nwhere \u0177t is the label of the t-th class, z M is teacher logits as shown in Fig. 2, \n\n2) Target Class Knowledge and Non-target Class Knowledge: For a given data from the t-th class, the soft labels can be denoted as u = [u 1 , u 2 , . . . , u t , . . . , u C ] \u2208 R 1\u00d7C , where u i is the soft label of the i-th class. Each element in u can be obtained by the softmax function with temperature \u03c4 : \n\nwhere z i represents the logit of the i-th class. LLM-DKD decouples fuzzy dark knowledge into target class knowledge and non-target class knowledge by: \n\nwhere u t represents soft labels of the target class, containing knowledge about the \"difficulty\" of data. u \\t represents soft labels of non-target class, containing knowledge making knowledge distillation work [32]. \n\nto independently model probabilities among non-target classes: \n\n3) Fuzzy Dark Knowledge Decoupling Process of LLM-DKD: Widely used KL divergence [14] is employed for decoupling fuzzy dark knowledge, which can be expressed as: \n\nAlgorithm 1 Teacher model and Student model. \n\nthe number of fuzzy rules K, the regularization parameter L, the maximum iteration epoch \u03b8, the threshold parameter \u03be, the learning rate \u03b7. \n\nOutput: the outputs of teacher model and student model. Procedure: \n\nStep 1: Randomly select the center v k i of Gaussian membership function in (2b) from five fixed fuzzy partition {0, 0.25, 0.5, 0.75, 1}, set the width \u03b4 k i to be a positive value, and compute the normalized fuzzy membership degree by (2a)-(2b). \n\nStep 2: Calculate antecedent parameter matrixs of teacher model and student model by (3a)-(3c). \n\nStep 3: The consequent parameter of teacher model q g can be determined by using",
            "score": 0.5283967540584956,
            "section_title": "B. Least Learning Machine based Decoupling Knowledge",
            "char_start_offset": 13868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1761
                },
                {
                    "start": 1764,
                    "end": 1844
                }
            ],
            "ref_mentions": [
                {
                    "start": 922,
                    "end": 926,
                    "matchedPaperCorpusId": "247476179"
                },
                {
                    "start": 1076,
                    "end": 1080,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "272586100",
            "title": "Recognition of Rice Species Based on Gas Chromatography-Ion Mobility Spectrometry and Deep Learning",
            "text": "The deep learning model trained above has a relatively complex structure with many parameters, and the hardware cost for deployment is relatively high to achieve a faster reasoning speed. However, knowledge distillation uses a more complex pretrained teacher model to guide lightweight student model training and transfer the knowledge of the teacher model to the student network. Model compression can reduce the hardware requirements of the deployment platform and improve the inference speed of the model. \n\nIn the classification problem, the output layer of the model uses the Softmax function to predict the class probability value, and its training process is to obtain the maximum likelihood for the true value, All negative labels are treated equally, but the actual situation is that different negative samples have different contributions to the loss function. In knowledge distillation, the concept of a soft target is introduced, which assigns probabilities to each class by improving the class probabilities that are output by the Softmax layer. The soft goal is toincorporate the temperature parameter T into the Softmax function: \n\n\u2211 j e z j /T (11) where z i is the output value of the logical unit of class i, and the larger the value is, the more likely the result belongs to this class. \n\nT represents the temperature or strength of the distillation, and the higher the value of T is, the smoother the output probability distribution, the more frequently the method negatively labels corresponding information, and the more attention is paid to negative labels. When the value of T is 1, the traditional Softmax function is used. In this way, training the model requires a smaller sample size, and the trained model has better generalizability. \n\nIn this paper, the model constructed via the above approach is regarded as the teacher model, and the student model is trained. The realization process is as follows: \n\n(1) The teacher model used in this study is trained by transfer learning; \n\n(2) To train the student model, the loss is divided into distillation loss and student loss: \n\nthe former uses the output of the teacher model as the label input loss value, and the latter uses the real label value to calculate the loss. A weighted sum of the two losses is calculated to obtain the total loss of the student model to train the student model. \n\nThe total loss of the student model is calculated as follows: \n\nwhere \u03bb is a super parameter.",
            "score": 0.5280422517451813,
            "section_title": ") Model compression using a knowledge distillation compression model",
            "char_start_offset": 35467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2369
                },
                {
                    "start": 2372,
                    "end": 2433
                },
                {
                    "start": 2436,
                    "end": 2465
                }
            ],
            "ref_mentions": [
                {
                    "start": 1160,
                    "end": 1164,
                    "matchedPaperCorpusId": "2512242"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430419921875
        },
        {
            "corpus_id": "54445469",
            "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks",
            "text": "In knowledge distillation [10], we use a teacher model to help train the current model, which is called the student model. The teacher model is often a pre-trained model with higher accuracy, so by imitation, the student model is able to improve its own accuracy while keeping the model complexity the same. One example is using a ResNet-152 as the teacher model to help training ResNet-50.\n\nDuring training, we add a distillation loss to penalize the difference between the softmax outputs from the teacher model and the learner model. Given an input, assume p is the true probability distribution, and z and r are outputs of the last fully-connected layer of the student model and the teacher model, respectively. Remember previously we use a negative cross entropy loss (p, softmax(z)) to measure the difference between p and z, here we use the same loss again for the distillation. Therefore, the loss is changed to (p, softmax(z)) + T 2 (softmax(r/T ), softmax(z/T )), where T is the temperature hyper-parameter to make the softmax outputs smoother thus distill the knowledge of label distribution from teacher's prediction.",
            "score": 0.5278885659689816,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 20003,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385009765625
        },
        {
            "corpus_id": "265445809",
            "title": "Cosine Similarity Knowledge Distillation for Individual Class Information Transfer",
            "text": "This section covers the specifics of our knowledge distillation approach, which includes Cosine Similarity Knowledge Distillation (CSKD) and Cosine Similarity Weighted Temperature scaling (CSWT). These methods effectively transfer knowledge from the teacher to the student model.",
            "score": 0.5278701533676236,
            "section_title": "METHODOLOGY",
            "char_start_offset": 7162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 279
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.022369384765625
        },
        {
            "corpus_id": "277940190",
            "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
            "text": "The concept of knowledge distillation (KD) has gained significant attention in recent years, with numerous studies focusing on improving the effectiveness of this technique (Ku et al., 2020;Zhang et al., 2021;Gao, 2023). The architecture of the knowledge distillation framework can be visualized, as shown in Figure 2, which illustrates the key components and their interactions in the distillation process. This framework typically involves a teacher model, which is pretrained on a large dataset, and a student model that learns from the teacher's output. This process allows the student to mimic the teacher's behavior and transfer knowledge in a more efficient manner, especially for applications where computational resources are limited. \n\nKnowledge distillation (KD) involves transferring knowledge from a pre-trained teacher model to a student model, typically with a smaller capacity, to enhance student performance (Ruffy and Chahal, 2019). This is typically achieved by optimizing a combined loss function that blends hard-target supervision with a softened probability distribution from the teacher model. \n\n(1) where L CE is the standard cross-entropy loss, L KL is the Kullback-Leibler divergence (Hinton et al., 2015), T is the temperature, \u03bb balances the two terms, and \u03c3(\u2022) denotes the softmax function. \n\nThis process has been shown to be highly effective in various applications including image classification, natural language processing, and speech recognition. Recent studies have introduced new variants of knowledge distillation (KD), such as teaching-assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of KD by introducing additional components or modifying the learning process (Yu et al., 2024;Zhang et al., 2024c). Furthermore, novel frameworks, such as partial-to-whole knowledge distillation (PWKD) and Correlation-Aware Knowledge Distillation (CAKD) have been proposed to improve the efficacy of KD by decomposing knowledge and prioritizing influential distillation components Fig. 2: Foundations of Knowledge Distillation.",
            "score": 0.5271751119268768,
            "section_title": "II. BACKGROUND AND FOUNDATIONS",
            "char_start_offset": 3801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 1787,
                    "end": 1804,
                    "matchedPaperCorpusId": "273811396"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279541015625
        },
        {
            "corpus_id": "256901005",
            "title": "Learning From Biased Soft Labels",
            "text": "Recently, knowledge distillation Bucilu\u01ce et al. (2006); Ba and Caruana (2014); Hinton et al. (2015) has engendered remarkable achievements in a wide range of applications. Although it was firstly proposed for model compression by distilling knowledge from the big model (teacher) to the small model (student), considerable efforts have been devoted to figuring out the dark knowledge hidden in the outputs of the teacher model. The dark knowledge is compatibly utilized for transfer learning Vapnik et al. (2015); Zagoruyko and Komodakis (2016a); Noroozi et al. (2018). \n\nIn practice, the student loss is defined as the tradeoff between imitating the ground-truth label and imitating the output of the teacher model. Many studies Rusu et al. (2015); Furlanello et al. (2018) have demonstrated that learning from the teacher model can be more effective than the ground-truth labels. This seems counterintuitive since it challenges the correctness of the groundtruth labels. Apart from knowledge distillation, label smoothing Szegedy et al. (2016); Zoph et al. (2018) also softens the labels by incorporating uniform noise, which is a useful trick to improve generalization. Knowledge distillation and label smoothing are often analyzed together, and Yuan et al. (2020) elucidates that they can be unified as learning from soft labels. The essence of both is why the soft labels are effective. \n\nIn this paper, we mainly focus on the effectiveness of these soft labels. To be clarified, this paper investigates when the soft labels are effective, rather than when the soft labels are superior to ground-truth labels. It is apparent that, when the soft labels are close to the ground-truth labels, the student model will have an adequate performance. A straightforward question is, \"whether the large-bias soft labels are still effective?\". 2020) empirically demonstrates the poorly-trained teacher model can also improve the student model. However, it sets the tradeoff \u03b1 = 0.9 and temperature \u03c4 = 20, which means the defective soft labels are still close to the ground-truth label.",
            "score": 0.5271678086412881,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 54,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 56,
                    "end": 77,
                    "matchedPaperCorpusId": "11536917"
                },
                {
                    "start": 492,
                    "end": 512,
                    "matchedPaperCorpusId": "12874183"
                },
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "4525685"
                },
                {
                    "start": 750,
                    "end": 774,
                    "matchedPaperCorpusId": "4110009"
                },
                {
                    "start": 1024,
                    "end": 1045,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 1047,
                    "end": 1065,
                    "matchedPaperCorpusId": "12227989"
                },
                {
                    "start": 1249,
                    "end": 1267,
                    "matchedPaperCorpusId": "219962714"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03704833984375
        },
        {
            "corpus_id": "266939439",
            "title": "DSP-KD: Dual-Stage Progressive Knowledge Distillation for Skin Disease Classification",
            "text": "In the vanilla knowledge distillation method, given  as the number of classes and a vector of logits  = [ ,  , \u2026 ,  , \u2026 ,  \u2208  \u00d7 as the outputs of the last fully connected layer of a deep model, the final prediction vector  = [ ,  , \u2026 ,  , \u2026 ,  \u2208  \u00d7 can be expressed as [9]: \n\nwhere the temperature factor  is introduced to control the importance of each soft target. In most prior works,  was set as a fixed single value for the entire distillation process. Chi et al. conducted a sequence of experiments enabling the student to distill knowledge from labels with varying degrees of softening [11]. The results indicated a positive correlation between distillation performance and the frequency of temperatures within an appropriate range. To enable the student model to extract knowledge more comprehensively from each sample, the authors proposed Normalized Knowledge Distillation (NormKD), introducing a normalization operation before the SoftMax layer that approximates the logit distribution as a normal distribution with a scale parameter \u0431. The final prediction vector  = [ ,  , \u2026 ,  , \u2026  \u2208  \u00d7 after normalization can be expressed as: \n\nwhere  serves as an adaptive hyperparameter for scaling the normal distribution, derived from the distribution of logit-based losses under different temperatures. This approach incurs minimal additional computational or storage costs and can be readily applied to other logit-based KD methods. It addresses, to some extent, the unevenness issue associated with a single temperature, thereby enhancing the overall performance of the system. \n\nAfter undergoing Stage 1 in our experiments, the student model acquires updated weight parameters. At this point, the student has effectively reconstructed the feature behaviors present in the teacher. Building upon the narrowed gap between the teacher and student, the student model enters a second round of normalized logit distillation with the teacher model. This process aims to enrich the information in predicting labels and \n\nwhere T Norm serves as an adaptive hyperparameter for scaling the normal distribution, derived from the distribution of logit-based losses under different temperatures. This approach incurs minimal additional computational or storage costs and can be readily applied to other logit-based KD methods. It addresses, to some extent, the unevenness issue associated with a single temperature, thereby enhancing the overall performance of the system.",
            "score": 0.527128907974407,
            "section_title": "Normalized Logits",
            "char_start_offset": 32731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2465
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 272,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2437744140625
        },
        {
            "corpus_id": "259274976",
            "title": "Mitigating Accuracy-Robustness Trade-Off via Balanced Multi-Teacher Adversarial Distillation",
            "text": "Due to the different model architectures and training methods, the teachers may have different knowledge scales. Eliminating the accuracy-robustness trade-off will become unrealistic if the student learns from the two teachers with no equal knowledge scales. Then we want to know how to measure the knowledge scale and further balance it. \n\nIn the knowledge distillation process, the part that teachers applied to guide students is the predicted distribution for the samples. So we can reasonably believe that the teacher's knowledge exists in the predicted distribution for the samples, which actually represents the information understood by the teacher, e.g., the samples' classes, the samples' difficulty, or the relationship between the classes. In knowledge distillation framework [20], the teacher's predicted distribution can be defined as where the predicted possibility p k (x) of k-th class can be formulated as follows: \n\nwhere z k (x) denotes the k-th dimension output logits of the model before the softmax layer, \u03c4 denotes the temperature applied in the training process of knowledge distillation. \n\nAlthough the knowledge described above exists in the teacher's predicted distributions, how to quantify them is not easy. Here we try to consider it from the optimization perspective. Intuitively speaking, a well-trained network model is trained from a network model with randomly initialized weights. The model with randomly initialized weights can be considered as a general model I without any knowledge; After training, the model can be considered to possess the knowledge brought by the optimization processes. Based on information theory, relative entropy (also known as Kullback-Leibler divergence [36]) represents the information cost required from one distribution to another distribution. So the obtained knowledge scale can be denoted as the relative entropy from the predicted distribution of an initialized model I to a well-trained model. Here we further define the teacher's knowledge scale K T in mathematical form as follows: \n\nthen we try to further simplify the knowledge scale K T and provide Theorem 1. And the corresponding proof can be found in the Appendix A (Proof A.1.). Theorem 1. The teacher's knowledge scale K T is negatively related to the information entropy H(P T ) of teacher's predicted distribution and have a relationship as follows:",
            "score": 0.527095660448277,
            "section_title": "Measuring Knowledge Scale",
            "char_start_offset": 17206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2057
                },
                {
                    "start": 2060,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2211
                },
                {
                    "start": 2212,
                    "end": 2222
                },
                {
                    "start": 2223,
                    "end": 2385
                }
            ],
            "ref_mentions": [
                {
                    "start": 1720,
                    "end": 1724,
                    "matchedPaperCorpusId": "5747983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060302734375
        },
        {
            "corpus_id": "273704712",
            "title": "Lightweight and efficient deep learning models for fruit detection in orchards",
            "text": "Figure 11 illustrates how knowledge distillation (KD) employs a more intricate teaching network to educate a more streamlined and lightweight learner network. This enables the smaller network to obtain a greater quantity of useful knowledge without imposing an additional burden on the network. The teacher network, as outlined in this paper, comprises a fine-tuned EGSS module and an extended ELD network, designated as ELD-Teacher. The fine-tuned EGSS module is referred to as EGSS-Teacher, which serves to enhance the feature. By incorporating depth-separable 3\u00c3-3 convolutions and expanding the depth and width of the network, the representation and abstraction capabilities of the teacher network can be enhanced. This allows for the extraction of more complex high-level features, which can be employed to educate the student network. It is unlikely that the teacher and student networks will exhibit significant structural differences. If the networks are too dissimilar, it will be challenging for the student network to acquire useful knowledge, even if the teacher network demonstrates robust learning capabilities and feature information. This process can be interpreted as the student's learning capacity being constrained, rendering it impossible to learn new knowledge even if this limit is surpassed. \n\nThe pivotal stage in this process is the dynamic alteration of the softmax probability distribution through the modification of the temperature coefficient T, as illustrated in Eq. (7). \n\nwhere T=1 corresponds to the original softmax function,T<1 steepens the probability distribution, and T>1 makes the probability distribution smoother. During the process of knowledge distillation, the output results of Soft labels and soft predictions are generated when the teacher and student networks make predictions following a softmax warm-up. The discrepancy between the soft labels and the soft predictions is calculated in order to guarantee that the predictions of the student network are analogous to those of the teacher network. The calculation process is illustrated in Fig. 12.",
            "score": 0.5265919144215364,
            "section_title": "Knowledge distillation",
            "char_start_offset": 23718,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1855
                },
                {
                    "start": 1856,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "256793071",
            "title": "Nemesis: Neural Mean Teacher Learning-Based Emotion-Centric Speaker",
            "text": "The latent knowledge encapsulated within a larger network is often referred to as \"dark knowledge\" [22]. Knowledge distillation (KD) [23] is the self-supervised process of transferring dark knowledge from a bigger model to a smaller one, which has been shown to be utilized effectively in various vision-language tasks. The same process is called self-distillation when the models have equal sizes. In our case, the teacher model supplies an extra supervision signal to the student model to improve in the imitation of its behavior by providing predicted soft labels [24].",
            "score": 0.5263347170450219,
            "section_title": "Knowledge Distillation",
            "char_start_offset": 7623,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 572
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "219636179"
                },
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "7200347"
                },
                {
                    "start": 567,
                    "end": 571,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004505157470703125
        },
        {
            "corpus_id": "278091583",
            "title": "A review of lightweight convolutional neural networks for ultrasound signal classification",
            "text": "Knowledge Distillation (Hinton, 2015) is an another model compression technique whose goal is to transfer knowledge from a larger, better performing 'teacher model' to a smaller 'student model' , thus allowing the student model to achieve performance close to that of the teacher model with fewer computational resources and memory usage. \n\nKnowledge distillation can reduce the size of the model regardless of the structural differences between the teacher and student models. When training the student model, the softmax output probability distribution of the teacher model is used as the training target, and a method is proposed to control the output probability distribution with a 'temperature' parameter, which can make the target 'soft' . Given the logits z of the network, the category probability p of an image is calculated as Equation 1. \n\nwhere T is the temperature parameter. When T = 1, the standard softmax function is obtained. As T increases, the probability distribution produced by the softmax function becomes softer, thus providing more information. As shown in Figure 5, knowledge distillation can be categorized into logit-based distillation and feature-based distillation based on the location of the knowledge in the teacher model (Chen et al., 2024) (Micucci and Iula, 2022). Through distillation, the student model can learn the noise-resistant properties of the teacher model and improve the robustness of ultrasound signal classification. However, if the teacher model itself is sensitive to noise, it can be combined with adversarial training to enhance the robustness of the student model to noise.",
            "score": 0.5262696495332498,
            "section_title": "Knowledge distillation",
            "char_start_offset": 20911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 1257,
                    "end": 1276,
                    "matchedPaperCorpusId": "265557267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "204838340",
            "title": "Contrastive Representation Distillation",
            "text": "The seminal work of Bucilu\u01ce et al. (2006) and Hinton et al. (2015) introduced the idea of knowledge distillation between large, cumbersome models into smaller, faster models without losing too much generalization power. The general motivation was that at training time, the availability of computation allows \"slop\" in model size, and potentially faster learning. But computation and memory constraints at inference time necessitate the use of smaller models. Bucilu\u01ce et al. (2006) achieve this by matching output logits; Hinton et al. (2015) introduced the idea of temperature in the softmax outputs to better represent smaller probabilities in the output of a single sample. These smaller probabilities provide useful information about the learned representation of the teacher model; some tradeoff between large temperatures (which increase entropy) or small temperatures tend to provide the highest transfer of knowledge between student and teacher. The method in (Li et al., 2014) was also closely related to (Hinton et al., 2015). \n\nAttention transfer (Zagoruyko & Komodakis, 2016a) focuses on the features maps of the network as opposed to the output logits. Here the idea is to elicit similar response patterns in the teacher and student feature maps (called \"attention\"). However, only feature maps with the same spatial \u2020 Average relative improvement = 1 \n\n, where Acc i crd , Acc i kd , and Acc i van represent the accuracies of CRD, KD, and vanilla training of the i-th student model, respectively. resolution can be combined in this approach, which is a significant limitation since it requires student and teacher networks with very similar architectures. This technique achieves state of the art results for distillation (as measured by the generalization of the student network). FitNets (Romero et al., 2014) also deal with intermediate representations by using regressions to guide the feature activations of the student network. Since Zagoruyko & Komodakis (2016a) do a weighted form of this regression, they tend to perform better.",
            "score": 0.5261673885897831,
            "section_title": "RELATED WORK",
            "char_start_offset": 5073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2051
                }
            ],
            "ref_mentions": [
                {
                    "start": 20,
                    "end": 41,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 460,
                    "end": 481,
                    "matchedPaperCorpusId": "11253972"
                },
                {
                    "start": 968,
                    "end": 985,
                    "matchedPaperCorpusId": "704391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "244920778",
            "title": "Safe Distillation Box",
            "text": "main (x) is calculated by the mean squared error:\n\nwhere \u03c3(\u00b7) denotes the softmax function, the temperature T dis is in the same setting as in L dis and z T (x) is the raw logits output of the pre-trained network T . We directly minimize the distance of the soft labels, which keeps mostly knowledge features in the proxy stream. For the purpose of knowledge augmentation, we propose L aug (x) to encourage the proxy stream of the SDB model T to effectively search for more knowledge. The knowledge augmentation loss L aug (x) is therefore formulated as:\n\nwhere z T0 (x) is the raw output logits of T 0 , T aug is the temperature hyperparameter. Minimizing the augmentation loss L aug (x) can be treated as a search for more useful knowledge. The former item of Eq. 10 forces the network to produce new knowledge different from the basic one, and the latter one constrains the knowledge searching space.\n\nChoosing T aug . Note that the knowledge with lower temperature largely ignores the impact of the negative logits, which are essential for the complete knowledge of the teacher network. In real applications, the temperature is set to a middle value for distillation, considering that the compact student network is unable to take over the whole knowledge from the teacher (Hinton, Vinyals, and Dean 2015). But when we do the knowledge augmentation, we focus on the case of T aug \u2192 +\u221e, which enlarges the whole knowledge capacity and is beneficial for establishing a knowledgeable teacher. Moreover, in the proxy stream, we train the knowledgeable teacher, rather than an accurate predictor, which lessens the label information in the soft labels. Thus, the proxy stream focuses on the knowledge augmentation while the original stream gives more accurate predictions.\n\nWhen the temperature T aug \u2192 +\u221e, the back propagation via L aug (x) is equal to compute the following gradient:\n\nwhere we directly match the raw logits. The corresponding proof is given in the supplement.",
            "score": 0.5256735098099711,
            "section_title": "Safe Distillation Box",
            "char_start_offset": 16530,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1277,
                    "end": 1309,
                    "matchedPaperCorpusId": "7200347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31787109375
        },
        {
            "paperId": "3f15d1caa39a6d408669a3ea8206b84dbb11b6b7",
            "corpusId": 274241902,
            "title": "Simplified Knowledge Distillation for Deep Neural Networks Bridging the Performance Gap with a Novel Teacher\u2013Student Architecture",
            "venue": "Electronics",
            "year": 2024,
            "referenceCount": 18,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13224530?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13224530, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240683787",
                    "name": "Sabina Umirzakova"
                },
                {
                    "authorId": "2323119268",
                    "name": "Mirjamol Abdullaev"
                },
                {
                    "authorId": "2267680163",
                    "name": "Sevara Mardieva"
                },
                {
                    "authorId": "2140789040",
                    "name": "Nodira Latipova"
                },
                {
                    "authorId": "2102629657",
                    "name": "Shakhnoza Muksimova"
                }
            ],
            "abstract": "The rapid evolution of deep learning has led to significant achievements in computer vision, primarily driven by complex convolutional neural networks (CNNs). However, the increasing depth and parameter count of these networks often result in overfitting and elevated computational demands. Knowledge distillation (KD) has emerged as a promising technique to address these issues by transferring knowledge from a large, well-trained teacher model to a more compact student model. This paper introduces a novel knowledge distillation method that simplifies the distillation process and narrows the performance gap between teacher and student models without relying on intricate knowledge representations. Our approach leverages a unique teacher network architecture designed to enhance the efficiency and effectiveness of knowledge transfer. Additionally, we introduce a streamlined teacher network architecture that transfers knowledge effectively through a simplified distillation process, enabling the student model to achieve high accuracy with reduced computational demands. Comprehensive experiments conducted on the CIFAR-10 dataset demonstrate that our proposed model achieves superior performance compared to traditional KD methods and established architectures such as ResNet and VGG networks. The proposed method not only maintains high accuracy but also significantly reduces training and validation losses. Key findings highlight the optimal hyperparameter settings (temperature T = 15.0 and smoothing factor \u03b1 = 0.7), which yield the highest validation accuracy and lowest loss values. This research contributes to the theoretical and practical advancements in knowledge distillation, providing a robust framework for future applications and research in neural network compression and optimization. The simplicity and efficiency of our approach pave the way for more accessible and scalable solutions in deep learning model deployment.",
            "corpus_id": "274241902",
            "text": "The rapid evolution of deep learning has led to significant achievements in computer vision, primarily driven by complex convolutional neural networks (CNNs). However, the increasing depth and parameter count of these networks often result in overfitting and elevated computational demands. Knowledge distillation (KD) has emerged as a promising technique to address these issues by transferring knowledge from a large, well-trained teacher model to a more compact student model. This paper introduces a novel knowledge distillation method that simplifies the distillation process and narrows the performance gap between teacher and student models without relying on intricate knowledge representations. Our approach leverages a unique teacher network architecture designed to enhance the efficiency and effectiveness of knowledge transfer. Additionally, we introduce a streamlined teacher network architecture that transfers knowledge effectively through a simplified distillation process, enabling the student model to achieve high accuracy with reduced computational demands. Comprehensive experiments conducted on the CIFAR-10 dataset demonstrate that our proposed model achieves superior performance compared to traditional KD methods and established architectures such as ResNet and VGG networks. The proposed method not only maintains high accuracy but also significantly reduces training and validation losses. Key findings highlight the optimal hyperparameter settings (temperature T = 15.0 and smoothing factor \u03b1 = 0.7), which yield the highest validation accuracy and lowest loss values. This research contributes to the theoretical and practical advancements in knowledge distillation, providing a robust framework for future applications and research in neural network compression and optimization. The simplicity and efficiency of our approach pave the way for more accessible and scalable solutions in deep learning model deployment.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.062103271484375
        },
        {
            "paperId": "655a2cbec97a455f00040b0b8d4b9682c1e304bc",
            "corpusId": 267751335,
            "title": "Knowledge Distillation Based on Transformed Teacher Matching",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 47,
            "citationCount": 21,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11148, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2240165697",
                    "name": "Kaixiang Zheng"
                },
                {
                    "authorId": "2240092055",
                    "name": "En-Hui Yang"
                }
            ],
            "abstract": "As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel distillation approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at https://github.com/zkxufo/TTM.",
            "corpus_id": "267751335",
            "text": "As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed probability distribution, we introduce a sample-adaptive weighting coefficient into TTM, yielding a novel distillation approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments, that although WTTM is simple, it is effective, improves upon TTM, and achieves state-of-the-art accuracy performance. Our source code is available at https://github.com/zkxufo/TTM.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2349853515625
        },
        {
            "paperId": "a73c6afca6fba53134e806e8f44792b6e364d9be",
            "corpusId": 277305874,
            "title": "Tailored Temperature for Student in Knowledge Distillation",
            "venue": "2024 IEEE Smart World Congress (SWC)",
            "year": 2024,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SWC62898.2024.00132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SWC62898.2024.00132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352002989",
                    "name": "Hongjin Chen"
                },
                {
                    "authorId": "2341361336",
                    "name": "Tiantian Zeng"
                },
                {
                    "authorId": "2341354501",
                    "name": "Kai Xiao"
                },
                {
                    "authorId": "2314077537",
                    "name": "Shaozhi Wu"
                },
                {
                    "authorId": "2314557103",
                    "name": "Xingang Liu"
                },
                {
                    "authorId": "2314116794",
                    "name": "Han Su"
                }
            ],
            "abstract": "In knowledge distillation, the temperature controls the shape of the model\u2019s prediction probability distribution, thereby influencing the transfer of knowledge from the teacher. Most knowledge distillation methods simply fix the temperature, while a few studies employ a dynamic temperature improperly, both of which constrain the further progress of the distilled student model. In this paper, to address this issue, we present a method dubbed TTSD to distill with the temperature tailored for the student, which achieves label smoothing effectively according to the student\u2019s performance. The tailored temperature consists of two parts: a learnable temperature (LT) and an adaptive temperature (AT), each functioning at different stages of student development (junior and senior). LT learns alongside the junior to find the optimal temperature, whereas in the senior stage, AT compensates for LT\u2019s deficiencies through adaptation. Extensive experiments on CIFAR-100 ImageNet-2012, and Describable Textures Dataset (DTD) demonstrate that TTSD can effectively improve existing knowledge distillation frameworks.",
            "corpus_id": "277305874",
            "text": "In knowledge distillation, the temperature controls the shape of the model\u2019s prediction probability distribution, thereby influencing the transfer of knowledge from the teacher. Most knowledge distillation methods simply fix the temperature, while a few studies employ a dynamic temperature improperly, both of which constrain the further progress of the distilled student model. In this paper, to address this issue, we present a method dubbed TTSD to distill with the temperature tailored for the student, which achieves label smoothing effectively according to the student\u2019s performance. The tailored temperature consists of two parts: a learnable temperature (LT) and an adaptive temperature (AT), each functioning at different stages of student development (junior and senior). LT learns alongside the junior to find the optimal temperature, whereas in the senior stage, AT compensates for LT\u2019s deficiencies through adaptation. Extensive experiments on CIFAR-100 ImageNet-2012, and Describable Textures Dataset (DTD) demonstrate that TTSD can effectively improve existing knowledge distillation frameworks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06610107421875
        },
        {
            "paperId": "e98c7ed92c907d3d24da0e0eb4f53b52863660be",
            "corpusId": 273375236,
            "title": "Optimizing YOLOv5s Object Detection through Knowledge Distillation algorithm",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 22,
            "citationCount": 14,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12259, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326809501",
                    "name": "Guanming Huang"
                },
                {
                    "authorId": "2326115422",
                    "name": "Aoran Shen"
                },
                {
                    "authorId": "2326247732",
                    "name": "Yuxiang Hu"
                },
                {
                    "authorId": "2322453010",
                    "name": "Junliang Du"
                },
                {
                    "authorId": "2322686991",
                    "name": "Jiacheng Hu"
                },
                {
                    "authorId": "2326212101",
                    "name": "Yingbin Liang"
                }
            ],
            "abstract": "This paper explores the application of knowledge distillation technology in target detection tasks, especially the impact of different distillation temperatures on the performance of student models. By using YOLOv5l as the teacher network and a smaller YOLOv5s as the student network, we found that with the increase of distillation temperature, the student's detection accuracy gradually improved, and finally achieved mAP50 and mAP50-95 indicators that were better than the original YOLOv5s model at a specific temperature. Experimental results show that appropriate knowledge distillation strategies can not only improve the accuracy of the model but also help improve the reliability and stability of the model in practical applications. This paper also records in detail the accuracy curve and loss function descent curve during the model training process and shows that the model converges to a stable state after 150 training cycles. These findings provide a theoretical basis and technical reference for further optimizing target detection algorithms.",
            "corpus_id": "273375236",
            "text": "This paper explores the application of knowledge distillation technology in target detection tasks, especially the impact of different distillation temperatures on the performance of student models. By using YOLOv5l as the teacher network and a smaller YOLOv5s as the student network, we found that with the increase of distillation temperature, the student's detection accuracy gradually improved, and finally achieved mAP50 and mAP50-95 indicators that were better than the original YOLOv5s model at a specific temperature. Experimental results show that appropriate knowledge distillation strategies can not only improve the accuracy of the model but also help improve the reliability and stability of the model in practical applications. This paper also records in detail the accuracy curve and loss function descent curve during the model training process and shows that the model converges to a stable state after 150 training cycles. These findings provide a theoretical basis and technical reference for further optimizing target detection algorithms.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.043609619140625
        },
        {
            "paperId": "490a5f909f20a90b021bce1429342717225c94a6",
            "corpusId": 269149054,
            "title": "ReffAKD: Resource-efficient Autoencoder-based Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09886, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2163625937",
                    "name": "Divyang Doshi"
                },
                {
                    "authorId": "2296722975",
                    "name": "Jung-Eun Kim"
                }
            ],
            "abstract": "In this research, we propose an innovative method to boost Knowledge Distillation efficiency without the need for resource-heavy teacher models. Knowledge Distillation trains a smaller ``student'' model with guidance from a larger ``teacher'' model, which is computationally costly. However, the main benefit comes from the soft labels provided by the teacher, helping the student grasp nuanced class similarities. In our work, we propose an efficient method for generating these soft labels, thereby eliminating the need for a large teacher model. We employ a compact autoencoder to extract essential features and calculate similarity scores between different classes. Afterward, we apply the softmax function to these similarity scores to obtain a soft probability vector. This vector serves as valuable guidance during the training of the student model. Our extensive experiments on various datasets, including CIFAR-100, Tiny Imagenet, and Fashion MNIST, demonstrate the superior resource efficiency of our approach compared to traditional knowledge distillation methods that rely on large teacher models. Importantly, our approach consistently achieves similar or even superior performance in terms of model accuracy. We also perform a comparative study with various techniques recently developed for knowledge distillation showing our approach achieves competitive performance with using significantly less resources. We also show that our approach can be easily added to any logit based knowledge distillation method. This research contributes to making knowledge distillation more accessible and cost-effective for practical applications, making it a promising avenue for improving the efficiency of model training. The code for this work is available at, https://github.com/JEKimLab/ReffAKD.",
            "corpus_id": "269149054",
            "text": "In this research, we propose an innovative method to boost Knowledge Distillation efficiency without the need for resource-heavy teacher models. Knowledge Distillation trains a smaller ``student'' model with guidance from a larger ``teacher'' model, which is computationally costly. However, the main benefit comes from the soft labels provided by the teacher, helping the student grasp nuanced class similarities. In our work, we propose an efficient method for generating these soft labels, thereby eliminating the need for a large teacher model. We employ a compact autoencoder to extract essential features and calculate similarity scores between different classes. Afterward, we apply the softmax function to these similarity scores to obtain a soft probability vector. This vector serves as valuable guidance during the training of the student model. Our extensive experiments on various datasets, including CIFAR-100, Tiny Imagenet, and Fashion MNIST, demonstrate the superior resource efficiency of our approach compared to traditional knowledge distillation methods that rely on large teacher models. Importantly, our approach consistently achieves similar or even superior performance in terms of model accuracy. We also perform a comparative study with various techniques recently developed for knowledge distillation showing our approach achieves competitive performance with using significantly less resources. We also show that our approach can be easily added to any logit based knowledge distillation method. This research contributes to making knowledge distillation more accessible and cost-effective for practical applications, making it a promising avenue for improving the efficiency of model training. The code for this work is available at, https://github.com/JEKimLab/ReffAKD.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2010498046875
        },
        {
            "paperId": "42b19e092e81594bd38cfbf22915c9efcc62bedc",
            "corpusId": 261599193,
            "title": "A Deep Transfer Learning Framework Using Teacher\u2013Student Structure for Land Cover Classification of Remote-Sensing Imagery",
            "venue": "IEEE Geoscience and Remote Sensing Letters",
            "year": 2024,
            "referenceCount": 18,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LGRS.2023.3312591?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LGRS.2023.3312591, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108451996",
                    "name": "Xiaodong Zhang"
                },
                {
                    "authorId": "2238406363",
                    "name": "Xianwei Li"
                },
                {
                    "authorId": "8771500",
                    "name": "Guanzhou Chen"
                },
                {
                    "authorId": "50728828",
                    "name": "Puyun Liao"
                },
                {
                    "authorId": "2864434",
                    "name": "Tong Wang"
                },
                {
                    "authorId": "2257353048",
                    "name": "Haobo Yang"
                },
                {
                    "authorId": "2181708007",
                    "name": "Chanjuan He"
                },
                {
                    "authorId": "2238392741",
                    "name": "Wenlin Zhou"
                },
                {
                    "authorId": "2238394571",
                    "name": "Yufeng Sun"
                }
            ],
            "abstract": "Deep-learning techniques are widely used for land cover classification in remote-sensing, primarily because they can effectively extract complex features from imagery data, which is essential for accurate land cover classification. However, the heterogeneity of data can pose a challenge to the generalizability of deep models. To address this issue, we propose a novel transfer learning framework for land cover classification using teacher\u2013student structure. The proposed framework utilizes the knowledge acquired by the teacher model from large datasets to facilitate fine-tuning of the student model on small datasets, which prevents problems such as overfitting resulting from training large models on small datasets and inferior performance arising from the limited learning capacity of small models. To achieve this goal, we design a loss function based on central moment discrepancy (CMD) and high-temperature softmax. In addition, we conducted experiments on three distinct pairs of datasets and found that our proposed framework outperforms both training the student model from scratch on the target domain and simply fine-tuning the teacher and student models from the source domain to the target domain. Specifically, we observed an average increase in mean intersection over union (mIoU) of 9.9%, 2.1%, and 4.3%. These results demonstrate the effectiveness and generalizability of our proposed framework for remote-sensing land cover classification.",
            "corpus_id": "261599193",
            "text": "Deep-learning techniques are widely used for land cover classification in remote-sensing, primarily because they can effectively extract complex features from imagery data, which is essential for accurate land cover classification. However, the heterogeneity of data can pose a challenge to the generalizability of deep models. To address this issue, we propose a novel transfer learning framework for land cover classification using teacher\u2013student structure. The proposed framework utilizes the knowledge acquired by the teacher model from large datasets to facilitate fine-tuning of the student model on small datasets, which prevents problems such as overfitting resulting from training large models on small datasets and inferior performance arising from the limited learning capacity of small models. To achieve this goal, we design a loss function based on central moment discrepancy (CMD) and high-temperature softmax. In addition, we conducted experiments on three distinct pairs of datasets and found that our proposed framework outperforms both training the student model from scratch on the target domain and simply fine-tuning the teacher and student models from the source domain to the target domain. Specifically, we observed an average increase in mean intersection over union (mIoU) of 9.9%, 2.1%, and 4.3%. These results demonstrate the effectiveness and generalizability of our proposed framework for remote-sensing land cover classification.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09332275390625
        },
        {
            "paperId": "2ada410fa1eaeb4c437bb3b054a4d9ba217e7684",
            "corpusId": 278502136,
            "title": "Robust&Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal",
            "venue": "",
            "year": 2025,
            "referenceCount": 47,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.06381, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351822853",
                    "name": "Saif Ur Rehman Khan"
                },
                {
                    "authorId": "35637737",
                    "name": "M. Asim"
                },
                {
                    "authorId": "2335450606",
                    "name": "Sebastian Vollmer"
                },
                {
                    "authorId": "2269305492",
                    "name": "Andreas Dengel"
                }
            ],
            "abstract": "Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).",
            "corpus_id": "278502136",
            "text": "Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0321044921875
        },
        {
            "paperId": "6002b2c04e8bf59f6ec99b1061d4bb1be6a84db1",
            "corpusId": 267044757,
            "title": "KDProg: A Knowledge distillation graph neural network for cancer prognosis prediction and analysis",
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine",
            "year": 2023,
            "referenceCount": 26,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BIBM58861.2023.10386037?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BIBM58861.2023.10386037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148507963",
                    "name": "Shuwen Xiong"
                },
                {
                    "authorId": "2108400793",
                    "name": "Zixuan Wang"
                },
                {
                    "authorId": "2210949385",
                    "name": "Junming Zhang"
                },
                {
                    "authorId": "2279931271",
                    "name": "Hong Luo"
                },
                {
                    "authorId": "2108214131",
                    "name": "Yongqing Zhang"
                },
                {
                    "authorId": "2256506003",
                    "name": "Quan Zou"
                }
            ],
            "abstract": "Accurately predicting cancer prognosis remains challenging, owing to the combination of computational and practical challenges. This study proposes KDProg, a knowledge distillation-based graph learning framework for predicting cancer prognosis and exploring downstream tasks. The framework includes a novel feature distillation paradigm that compresses a multi-layer complex teacher model to a single-layer simple student by using the teacher model\u2019s middle-layer feature representations and outputs as supervision information to improve the student model\u2019s performance. In addition, instead of introducing a unified temperature hyperparameter, KDProg adopts a novel strategy to parameterize the distillation temperature and combine it with the Cox partial log-likelihood function. So the model can learn the appropriate temperature. Furthermore, considering multi-omics data of patients are often complex to obtain in practical cancer prognosis, this paper uses different input data for the teacher and student models, respectively. The input data for the teacher model are multi-omics data (mRNA, CNV, and DNA methylation), clinical data, and KEGG pathways. The input data for the student model are mRNA, clinical data, and KEGG pathways. Extensive experiments on 15 real-world datasets from TCGA demonstrated the effectiveness and efficiency of the proposed method in predicting cancer prognosis. The results suggest that the proposed model can guide clinical decision-making.",
            "corpus_id": "267044757",
            "text": "Accurately predicting cancer prognosis remains challenging, owing to the combination of computational and practical challenges. This study proposes KDProg, a knowledge distillation-based graph learning framework for predicting cancer prognosis and exploring downstream tasks. The framework includes a novel feature distillation paradigm that compresses a multi-layer complex teacher model to a single-layer simple student by using the teacher model\u2019s middle-layer feature representations and outputs as supervision information to improve the student model\u2019s performance. In addition, instead of introducing a unified temperature hyperparameter, KDProg adopts a novel strategy to parameterize the distillation temperature and combine it with the Cox partial log-likelihood function. So the model can learn the appropriate temperature. Furthermore, considering multi-omics data of patients are often complex to obtain in practical cancer prognosis, this paper uses different input data for the teacher and student models, respectively. The input data for the teacher model are multi-omics data (mRNA, CNV, and DNA methylation), clinical data, and KEGG pathways. The input data for the student model are mRNA, clinical data, and KEGG pathways. Extensive experiments on 15 real-world datasets from TCGA demonstrated the effectiveness and efficiency of the proposed method in predicting cancer prognosis. The results suggest that the proposed model can guide clinical decision-making.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0125732421875
        },
        {
            "paperId": "84cc5b5c7be3c069bc510737af05fa2fffeefa6d",
            "corpusId": 270245756,
            "title": "NTCE-KD: Non-Target-Class-Enhanced Knowledge Distillation",
            "venue": "Italian National Conference on Sensors",
            "year": 2024,
            "referenceCount": 46,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1424-8220/24/11/3617/pdf?version=1717425973",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11175301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304851293",
                    "name": "Chuan Li"
                },
                {
                    "authorId": "2061528299",
                    "name": "Xiao Teng"
                },
                {
                    "authorId": "2304714565",
                    "name": "Yan Ding"
                },
                {
                    "authorId": "2248489187",
                    "name": "Long Lan"
                }
            ],
            "abstract": "Most logit-based knowledge distillation methods transfer soft labels from the teacher model to the student model via Kullback\u2013Leibler divergence based on softmax, an exponential normalization function. However, this exponential nature of softmax tends to prioritize the largest class (target class) while neglecting smaller ones (non-target classes), leading to an oversight of the non-target classes\u2019s significance. To address this issue, we propose Non-Target-Class-Enhanced Knowledge Distillation (NTCE-KD) to amplify the role of non-target classes both in terms of magnitude and diversity. Specifically, we present a magnitude-enhanced Kullback\u2013Leibler (MKL) divergence multi-shrinking the target class to enhance the impact of non-target classes in terms of magnitude. Additionally, to enrich the diversity of non-target classes, we introduce a diversity-based data augmentation strategy (DDA), further enhancing overall performance. Extensive experimental results on the CIFAR-100 and ImageNet-1k datasets demonstrate that non-target classes are of great significance and that our method achieves state-of-the-art performance across a wide range of teacher\u2013student pairs.",
            "corpus_id": "270245756",
            "text": "Most logit-based knowledge distillation methods transfer soft labels from the teacher model to the student model via Kullback\u2013Leibler divergence based on softmax, an exponential normalization function. However, this exponential nature of softmax tends to prioritize the largest class (target class) while neglecting smaller ones (non-target classes), leading to an oversight of the non-target classes\u2019s significance. To address this issue, we propose Non-Target-Class-Enhanced Knowledge Distillation (NTCE-KD) to amplify the role of non-target classes both in terms of magnitude and diversity. Specifically, we present a magnitude-enhanced Kullback\u2013Leibler (MKL) divergence multi-shrinking the target class to enhance the impact of non-target classes in terms of magnitude. Additionally, to enrich the diversity of non-target classes, we introduce a diversity-based data augmentation strategy (DDA), further enhancing overall performance. Extensive experimental results on the CIFAR-100 and ImageNet-1k datasets demonstrate that non-target classes are of great significance and that our method achieves state-of-the-art performance across a wide range of teacher\u2013student pairs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1873779296875
        },
        {
            "paperId": "480ab2a04b5db92acde5dbf31ff3c4c2114ce71d",
            "corpusId": 258564700,
            "title": "Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 34,
            "citationCount": 20,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.05010",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.05010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46752897",
                    "name": "Rongzhi Zhang"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "2115346248",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "2155378946",
                    "name": "Jia-Ling Liu"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                },
                {
                    "authorId": "1398342639",
                    "name": "Marc Najork"
                },
                {
                    "authorId": "2152737229",
                    "name": "Chao Zhang"
                }
            ],
            "abstract": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this\"distribution closeness\"and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales.",
            "corpus_id": "258564700",
            "text": "Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this\"distribution closeness\"and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on five datasets demonstrate PTLoss can significantly improve the distillation effectiveness for teachers of various scales.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.009521484375
        },
        {
            "paperId": "9f74ec62fd06bd69e62abeded920addc3fd40af2",
            "corpusId": 276427678,
            "title": "Leveraging Knowledge Distillation in Vision Transformers for Binary Classification of Kidney Tumors from CT Radiography Images",
            "venue": "Automation, Control, and Information Technology",
            "year": 2024,
            "referenceCount": 17,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACIT62805.2024.10877056?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACIT62805.2024.10877056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292331572",
                    "name": "Mouhamed Laid Abimouloud"
                },
                {
                    "authorId": "2292333440",
                    "name": "Khaled Bensid"
                },
                {
                    "authorId": "2065367778",
                    "name": "M. Elleuch"
                },
                {
                    "authorId": "29833207",
                    "name": "Oussama Aiadi"
                },
                {
                    "authorId": "2139481",
                    "name": "M. Kherallah"
                }
            ],
            "abstract": "Kidney tumors are commonly diagnosed through computed tomography (CT) imaging, making accurate classification into normal or Cyst categories essential for effective treatment planning. In this study, we propose kidney tumor classification using knowledge distillation between a Token vision transformer (teacher model) and a Vision transformer (student model). The teacher model employs a token-based learning mechanism to capture spatially important regions of the CT images, which are then distilled into the student model to enhance its performance while maintaining computational efficiency. Our method processes CT radiography images by partitioning them into non-overlapping patches that are fed into transformer encoder layers. The teacher model, equipped with a Token Learner module, extracts salient tokens, while the student model is trained using both true labels and softened teacher predictions through temperature-scaled softmax. This knowledge distillation framework is tested on a binary classification task to distinguish normal from cyst kidney tumors. The results indicate that the student model achieves comparable accuracy to the teacher model, despite having fewer parameters and faster inference times. Specifically, the student model achieves a peak accuracy of 95.96% and a recall of 95.56% at \u03b1 =0.1 and temperature T =9. These findings show that knowledge distillation allows the student model to perform efficiently, delivering high classification accuracy with reduced computational resources. This approach presents a viable solution for real-time medical image classification in resource-constrained settings and demonstrates potential applicability to other medical imaging tasks.",
            "corpus_id": "276427678",
            "text": "Kidney tumors are commonly diagnosed through computed tomography (CT) imaging, making accurate classification into normal or Cyst categories essential for effective treatment planning. In this study, we propose kidney tumor classification using knowledge distillation between a Token vision transformer (teacher model) and a Vision transformer (student model). The teacher model employs a token-based learning mechanism to capture spatially important regions of the CT images, which are then distilled into the student model to enhance its performance while maintaining computational efficiency. Our method processes CT radiography images by partitioning them into non-overlapping patches that are fed into transformer encoder layers. The teacher model, equipped with a Token Learner module, extracts salient tokens, while the student model is trained using both true labels and softened teacher predictions through temperature-scaled softmax. This knowledge distillation framework is tested on a binary classification task to distinguish normal from cyst kidney tumors. The results indicate that the student model achieves comparable accuracy to the teacher model, despite having fewer parameters and faster inference times. Specifically, the student model achieves a peak accuracy of 95.96% and a recall of 95.56% at \u03b1 =0.1 and temperature T =9. These findings show that knowledge distillation allows the student model to perform efficiently, delivering high classification accuracy with reduced computational resources. This approach presents a viable solution for real-time medical image classification in resource-constrained settings and demonstrates potential applicability to other medical imaging tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.427001953125
        },
        {
            "paperId": "d634b750db99355d90e0b8b897b1526526b36e4f",
            "corpusId": 268633083,
            "title": "Target-Embedding Autoencoder With Knowledge Distillation for Multi-Label Classification",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
            "year": 2024,
            "referenceCount": 41,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TETCI.2024.3372693?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TETCI.2024.3372693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2272249633",
                    "name": "Ying Ma"
                },
                {
                    "authorId": "2218731200",
                    "name": "Xiaoyan Zou"
                },
                {
                    "authorId": "2204818294",
                    "name": "Qizheng Pan"
                },
                {
                    "authorId": "2274920116",
                    "name": "Ming Yan"
                },
                {
                    "authorId": "2158399349",
                    "name": "Guoqi Li"
                }
            ],
            "abstract": "In the task of multi-label classification, it is a key challenge to determine the correlation between labels. One solution to this is the Target Embedding Autoencoder (TEA), but most TEA-based frameworks have numerous parameters, large models, and high complexity, which makes it difficult to deal with the problem of large-scale learning. To address this issue, we provide a Target Embedding Autoencoder framework based on Knowledge Distillation (KD-TEA) that compresses a Teacher model with large parameters into a small Student model through knowledge distillation. Specifically, KD-TEA transfers the dark knowledge learned from the Teacher model to the Student model. The dark knowledge can provide effective regularization to alleviate the over-fitting problem in the training process, thereby enhancing the generalization ability of the Student model, and better completing the multi-label task. In order to make the Student model learn the knowledge of the Teacher model directly, we improve the distillation loss: KD-TEA uses MSE loss instead of KL divergence loss to improve the performance of the model in multi-label tasks. Experiments on multiple datasets show that our KD-TEA framework is superior to the most advanced multi-label classification methods in both performance and efficiency.",
            "corpus_id": "268633083",
            "text": "In the task of multi-label classification, it is a key challenge to determine the correlation between labels. One solution to this is the Target Embedding Autoencoder (TEA), but most TEA-based frameworks have numerous parameters, large models, and high complexity, which makes it difficult to deal with the problem of large-scale learning. To address this issue, we provide a Target Embedding Autoencoder framework based on Knowledge Distillation (KD-TEA) that compresses a Teacher model with large parameters into a small Student model through knowledge distillation. Specifically, KD-TEA transfers the dark knowledge learned from the Teacher model to the Student model. The dark knowledge can provide effective regularization to alleviate the over-fitting problem in the training process, thereby enhancing the generalization ability of the Student model, and better completing the multi-label task. In order to make the Student model learn the knowledge of the Teacher model directly, we improve the distillation loss: KD-TEA uses MSE loss instead of KL divergence loss to improve the performance of the model in multi-label tasks. Experiments on multiple datasets show that our KD-TEA framework is superior to the most advanced multi-label classification methods in both performance and efficiency.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0836181640625
        },
        {
            "paperId": "191bd8f008f80883bba1fa38908e2c836a5f7bbe",
            "corpusId": 259370551,
            "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 37,
            "citationCount": 13,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.acl-short.97.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2023.acl-short.97, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2047971184",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "2042620448",
                    "name": "Aashiq Muhamed"
                },
                {
                    "authorId": "2078216680",
                    "name": "Aditya Anantharaman"
                },
                {
                    "authorId": "2107926840",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "1752041",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "144474630",
                    "name": "Kai Zhong"
                },
                {
                    "authorId": "2057040752",
                    "name": "Qi Cui"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                },
                {
                    "authorId": "2213153798",
                    "name": "Yiran Chen"
                }
            ],
            "abstract": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models.Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher\u2019s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
            "corpus_id": "259370551",
            "text": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models.Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher\u2019s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06231689453125
        },
        {
            "paperId": "f1ab9e74e7b46804cb16d54e32af9ef24d6b2392",
            "corpusId": 275117509,
            "title": "Leveraging logit uncertainty for better knowledge distillation",
            "venue": "Scientific Reports",
            "year": 2024,
            "referenceCount": 7,
            "citationCount": 1,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC11682123, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303981889",
                    "name": "Zhenkai Guo"
                },
                {
                    "authorId": "2337674692",
                    "name": "Dong Wang"
                },
                {
                    "authorId": "2210876822",
                    "name": "Qiang He"
                },
                {
                    "authorId": "2297246012",
                    "name": "Pengzhou Zhang"
                }
            ],
            "abstract": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
            "corpus_id": "275117509",
            "text": "Knowledge distillation improves student model performance. However, using a larger teacher model does not necessarily result in better distillation gains due to significant architecture and output gaps with smaller student networks. To address this issue, we reconsider teacher outputs and find that categories with strong teacher confidence benefit distillation more, while those with weaker certainty contribute less. Thus, we propose Logits Uncertainty Distillation (LUD) to bridge this gap. We introduce category uncertainty weighting to consider the uncertainty in the teacher model\u2019s predictions. A confidence threshold, based on the teacher\u2019s predictions, helps construct a mask that discounts uncertain classes during distillation. Furthermore, we incorporate two Spearman correlation loss functions to align the logits of the teacher and student models. These loss functions measure the discrepancy between the models\u2019 outputs at the category and sample levels. We also introduce adaptive dynamic temperature factors to optimize the distillation process. By combining these techniques, we enhance knowledge distillation results and facilitate effective knowledge transfer between teacher and student models, even when architectural differences exist. Extensive experiments on multiple datasets demonstrate the effectiveness of our method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0170440673828125
        },
        {
            "paperId": "32402c3c1df0b0cfaa535396d03d0c4cd6cd35f6",
            "corpusId": 274620303,
            "title": "Non\u2010Autoregressive Translation Algorithm Based on LLM Knowledge Distillation in English Corpus",
            "venue": "Engineering Reports",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/eng2.13077?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/eng2.13077, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334629103",
                    "name": "Fang Ju"
                },
                {
                    "authorId": "2334598524",
                    "name": "Weihui Wang"
                }
            ],
            "abstract": "Although significant advancements have been made in the quality of machine translation by large\u2010scale language models, their high computational costs and resource consumption have hindered their widespread adoption in practical applications. So this research introduces an English corpus\u2010based machine translation algorithm that leverages knowledge distillation from large language model, with the goal of enhancing translation quality and reducing the computational demands of the model. Initially, we conducted a thorough analysis of the English corpus to identify prevalent language patterns and structures. Following this, we developed a knowledge distillation approach that transfers the translation expertise of a large teacher model to a smaller student model, thereby achieving increased translation accuracy and efficiency. We designed a dynamic temperature hyperparameter distillation strategy that effectively enhances the precision of translations. In the experimental phase, we utilized several standard English corpora to train and assess our algorithm. The findings indicate that, compared to current machine translation systems, our method significantly reduces the need for computational resources while preserving translation quality.",
            "corpus_id": "274620303",
            "text": "Although significant advancements have been made in the quality of machine translation by large\u2010scale language models, their high computational costs and resource consumption have hindered their widespread adoption in practical applications. So this research introduces an English corpus\u2010based machine translation algorithm that leverages knowledge distillation from large language model, with the goal of enhancing translation quality and reducing the computational demands of the model. Initially, we conducted a thorough analysis of the English corpus to identify prevalent language patterns and structures. Following this, we developed a knowledge distillation approach that transfers the translation expertise of a large teacher model to a smaller student model, thereby achieving increased translation accuracy and efficiency. We designed a dynamic temperature hyperparameter distillation strategy that effectively enhances the precision of translations. In the experimental phase, we utilized several standard English corpora to train and assess our algorithm. The findings indicate that, compared to current machine translation systems, our method significantly reduces the need for computational resources while preserving translation quality.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.01094818115234375
        },
        {
            "paperId": "cbe20bb5e40c4fa12d312001906b96a8ba400e18",
            "corpusId": 270206047,
            "title": "Lightweight Method for Adaptive Power Grid Operation Safety Control Model Based on Knowledge Distillation",
            "venue": "CVDL",
            "year": 2024,
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3653804.3655993?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3653804.3655993, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2149193442",
                    "name": "Wei-Wei Wei"
                },
                {
                    "authorId": "2296752039",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2152122353",
                    "name": "Zexu Du"
                },
                {
                    "authorId": "2296276405",
                    "name": "Zongbo Chu"
                },
                {
                    "authorId": "2116019811",
                    "name": "Guoliang Zhang"
                }
            ],
            "abstract": "Safety control is one of the most important tasks for power grid operation. However, safety control and risk warning models with high accuracy always have multitudinous parameters and need scaled resources for calculation. So as to sort out this problem, we proposed a novel and adaptive model lightweight method based on knowledge distillation. In our method, the intermediate features of the teacher network is utilized to guide the learning of the student network and a regressor is introduced behind the selected middle layer of teacher network to realize dimension match. The prediction confidence of teacher network is mapped to the feature space for the regressor initialization. Besides, the prediction confidence is used to construct the distillation temperature to improve the adaptability of information transfer. Experiments on ImageNet, CIFAR 100 and Grid-specific datasets demonstrate the lightweight model obtained by our method possesses powerful and adaptive feature extraction and representation ability.",
            "corpus_id": "270206047",
            "text": "Safety control is one of the most important tasks for power grid operation. However, safety control and risk warning models with high accuracy always have multitudinous parameters and need scaled resources for calculation. So as to sort out this problem, we proposed a novel and adaptive model lightweight method based on knowledge distillation. In our method, the intermediate features of the teacher network is utilized to guide the learning of the student network and a regressor is introduced behind the selected middle layer of teacher network to realize dimension match. The prediction confidence of teacher network is mapped to the feature space for the regressor initialization. Besides, the prediction confidence is used to construct the distillation temperature to improve the adaptability of information transfer. Experiments on ImageNet, CIFAR 100 and Grid-specific datasets demonstrate the lightweight model obtained by our method possesses powerful and adaptive feature extraction and representation ability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00864410400390625
        },
        {
            "paperId": "2b9a19d46a123ad4c7cecf92590f0a8d7fd035e1",
            "corpusId": 276993655,
            "title": "Mobile-Friendly Plant Disease Detection and Classification using Metadata and Asymmetric Knowledge Distillation",
            "venue": "2024 2nd International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT)",
            "year": 2024,
            "referenceCount": 19,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAICCIT64383.2024.10912090?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAICCIT64383.2024.10912090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2508504",
                    "name": "Balika. J. Chelliah"
                },
                {
                    "authorId": "2350012274",
                    "name": "S.Jasper Sofi"
                },
                {
                    "authorId": "2350011366",
                    "name": "K. S. S. SreeKrishna"
                },
                {
                    "authorId": "2350011808",
                    "name": "Sai Vishaal"
                },
                {
                    "authorId": "2280482333",
                    "name": "A. Senthilselvi"
                },
                {
                    "authorId": "2277669477",
                    "name": "S. Senthil Pandi"
                }
            ],
            "abstract": "The increasing interest in the mobile applications of real-time plant disease detection systems has given rise to new difficulties in building computational models that can perform with high accuracy with limited resources. This paper presents an inventive concept that leverages metadata and asymmetric knowledge distillation to improve the use of mobile applications for detecting and classifying plant diseases. It is reported that a teacher model (ResNet \u2013 50) trained on environmental factors (e.g. temperature, humidity, and soil) enriched dataset was converted to a student model (MobileNet) which is mobile deployable. The knowledge embedded in soft labels of the teacher model was distilled and taught to the student model allowing it to practice high accuracy data classification with less computations. Adding metadata further gives evidence to the assertion and helps improve the overall detection accuracy. The results of the experiments indicate the effective use of knowledge distillation in a mobile-based plant disease classification where a lightweight MobileNet model performs comparably with the ResNet-50 model. This structure offers a practical and scalable solution for real-time disease monitoring and identification, allowing farmers to make timely decisions with a smartphone.",
            "corpus_id": "276993655",
            "text": "The increasing interest in the mobile applications of real-time plant disease detection systems has given rise to new difficulties in building computational models that can perform with high accuracy with limited resources. This paper presents an inventive concept that leverages metadata and asymmetric knowledge distillation to improve the use of mobile applications for detecting and classifying plant diseases. It is reported that a teacher model (ResNet \u2013 50) trained on environmental factors (e.g. temperature, humidity, and soil) enriched dataset was converted to a student model (MobileNet) which is mobile deployable. The knowledge embedded in soft labels of the teacher model was distilled and taught to the student model allowing it to practice high accuracy data classification with less computations. Adding metadata further gives evidence to the assertion and helps improve the overall detection accuracy. The results of the experiments indicate the effective use of knowledge distillation in a mobile-based plant disease classification where a lightweight MobileNet model performs comparably with the ResNet-50 model. This structure offers a practical and scalable solution for real-time disease monitoring and identification, allowing farmers to make timely decisions with a smartphone.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.017578125
        }
    ],
    "quotes": {
        "cost": 0.255819,
        "quotes": [
            {
                "idx": 0,
                "key": "[201660247 | Kumar et al. | 2019 | Citations: 5]",
                "snippets": "[42] scale the final logits by a temperature parameter to obtain soft thresholds and these are used to train the smaller model. They also show that matching the logits as done by (Bucila et al., 2006), is a special case of the technique they propose...Precisely, [42] propose a much general methodology for distillation of knowledge from a larger teacher network to a drastically smaller student network by using a parameter termed as the temperature. Essentially, the output logits of teacher network are normalized with respect to the temperature parameter and subsequently the softmax operation is carried out on them in order to obtain a softer threshold which leads to an improved and more general knowledge transfer to the student network.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 1022,
                        "end": 1254,
                        "sentence_offsets": [
                            {
                                "start": 1022,
                                "end": 1149
                            },
                            {
                                "start": 1150,
                                "end": 1255
                            }
                        ],
                        "ref_mentions": [
                            "11253972"
                        ],
                        "quote": "[42] scale the final logits by a temperature parameter to obtain soft thresholds and these are used to train the smaller model. They also show that matching the logits as done by (Bucila et al., 2006), is a special case of the technique they propose"
                    },
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 1565,
                        "end": 2059,
                        "sentence_offsets": [
                            {
                                "start": 1565,
                                "end": 1764
                            },
                            {
                                "start": 1765,
                                "end": 2058
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Precisely, [42] propose a much general methodology for distillation of knowledge from a larger teacher network to a drastically smaller student network by using a parameter termed as the temperature. Essentially, the output logits of teacher network are normalized with respect to the temperature parameter and subsequently the softmax operation is carried out on them in order to obtain a softer threshold which leads to an improved and more general knowledge transfer to the student network."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[212644537 | Deng et al. | 2020 | Citations: 95]",
                "snippets": "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 519,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 73
                            },
                            {
                                "start": 74,
                                "end": 200
                            },
                            {
                                "start": 201,
                                "end": 395
                            },
                            {
                                "start": 396,
                                "end": 519
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[220280430 | Pouransari et al. | 2020 | Citations: 5]",
                "snippets": "In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation with Temperature Scaling",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 275,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 275
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling)."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[221284805 | Jang et al. | 2020 | Citations: 31]",
                "snippets": "In the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space (Rusu et al., 2015). If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[1923568 | Rusu et al. | 2015 | Citations: 696]": "Abstract: Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent."
                },
                "metadata": [
                    {
                        "section_title": "C. KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 506,
                        "end": 1339,
                        "sentence_offsets": [
                            {
                                "start": 506,
                                "end": 693
                            },
                            {
                                "start": 694,
                                "end": 923
                            },
                            {
                                "start": 924,
                                "end": 988
                            },
                            {
                                "start": 989,
                                "end": 1089
                            },
                            {
                                "start": 1090,
                                "end": 1249
                            },
                            {
                                "start": 1250,
                                "end": 1339
                            }
                        ],
                        "ref_mentions": [
                            "1923568"
                        ],
                        "quote": "In the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space (Rusu et al., 2015). If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[225397480 | Liu et al. | 2020 | Citations: 1]",
                "snippets": "By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  .",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Training Process",
                        "pdf_hash": "",
                        "start": 112,
                        "end": 720,
                        "sentence_offsets": [
                            {
                                "start": 112,
                                "end": 208
                            },
                            {
                                "start": 209,
                                "end": 319
                            },
                            {
                                "start": 320,
                                "end": 375
                            },
                            {
                                "start": 378,
                                "end": 450
                            },
                            {
                                "start": 451,
                                "end": 531
                            },
                            {
                                "start": 532,
                                "end": 720
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  ."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[227228186 | Liu et al. | 2020 | Citations: 123]",
                "snippets": "To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.[2]).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1169,
                        "end": 1416,
                        "sentence_offsets": [
                            {
                                "start": 1169,
                                "end": 1317
                            },
                            {
                                "start": 1318,
                                "end": 1399
                            },
                            {
                                "start": 1400,
                                "end": 1416
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.[2])."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[233714221 | Kim et al. | 2020 | Citations: 182]",
                "snippets": "By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation as Softening Targets",
                        "pdf_hash": "",
                        "start": 872,
                        "end": 1241,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "By scaling the softmax output P T (x) of the teacher as well as P S (x) of the student, the student is trained with the loss function L KD , given by:\n\nwhere H is a cross-entropy loss and \u03b1 is a hyperparameter. Note that when the temperature \u03c4 is set to 1, Eq. (2) is equivalent to the cross-entropy of P S (x) to the soft target, a linear combination of y and P T (x):"
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[233777216 | Zhou et al. | 2021 | Citations: 52]",
                "snippets": "An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[198179476 | Tung et al. | 2019 | Citations: 981]": "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 709,
                        "end": 1754,
                        "sentence_offsets": [
                            {
                                "start": 709,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 956
                            },
                            {
                                "start": 959,
                                "end": 1077
                            },
                            {
                                "start": 1078,
                                "end": 1185
                            },
                            {
                                "start": 1186,
                                "end": 1436
                            },
                            {
                                "start": 1437,
                                "end": 1625
                            },
                            {
                                "start": 1626,
                                "end": 1754
                            }
                        ],
                        "ref_mentions": [
                            "198179476"
                        ],
                        "quote": "An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[234336288 | Jaiswal et al. | 2021 | Citations: 0]",
                "snippets": "For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "C. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1717,
                        "end": 2069,
                        "sentence_offsets": [
                            {
                                "start": 1717,
                                "end": 1788
                            },
                            {
                                "start": 1789,
                                "end": 1968
                            },
                            {
                                "start": 1969,
                                "end": 2069
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[234353710 | Liu et al. | 2019 | Citations: 32]",
                "snippets": "Hinton et al. (Hinton et al., 2015) apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Recently, Qiao et al. propose deep Co-Training",
                        "pdf_hash": "",
                        "start": 486,
                        "end": 954,
                        "sentence_offsets": [
                            {
                                "start": 486,
                                "end": 707
                            },
                            {
                                "start": 708,
                                "end": 954
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Hinton et al. (Hinton et al., 2015) apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[235421655 | Lee et al. | 2021 | Citations: 12]",
                "snippets": "Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 545,
                        "end": 1025,
                        "sentence_offsets": [
                            {
                                "start": 545,
                                "end": 659
                            },
                            {
                                "start": 660,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 910
                            },
                            {
                                "start": 911,
                                "end": 1025
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[237002008 | Lin et al. | 2021 | Citations: 0]",
                "snippets": "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "2) SOFTMAX TEMPERATURE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1085,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 120
                            },
                            {
                                "start": 121,
                                "end": 496
                            },
                            {
                                "start": 497,
                                "end": 724
                            },
                            {
                                "start": 727,
                                "end": 764
                            },
                            {
                                "start": 765,
                                "end": 827
                            },
                            {
                                "start": 828,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1085
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[239616535 | Jin et al. | 2021 | Citations: 6]",
                "snippets": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions....The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 ....The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conventional Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 460,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions"
                    },
                    {
                        "section_title": "Conventional Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 881,
                        "end": 978,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": ".The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4 "
                    },
                    {
                        "section_title": "Conventional Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1050,
                        "end": 1190,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": ".The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels)."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[243832889 | Eyono et al. | 2021 | Citations: 3]",
                "snippets": "Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 760,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 760,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[244130170 | Jang | 2021 | Citations: 3]",
                "snippets": "Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature \u03c4 in the softmax function as follows:...The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function:...In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 422,
                        "end": 654,
                        "sentence_offsets": [
                            {
                                "start": 278,
                                "end": 508
                            },
                            {
                                "start": 509,
                                "end": 631
                            },
                            {
                                "start": 632,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation transfers softened versions of the target probabilities provided by the pretrained teacher network. The soft target probabilities are defined by introducing a temperature \u03c4 in the softmax function as follows:"
                    },
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 875,
                        "end": 1161,
                        "sentence_offsets": [
                            {
                                "start": 875,
                                "end": 1059
                            },
                            {
                                "start": 1060,
                                "end": 1159
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The same temperature scaling technique is applied to the student network when it learns known training samples, yielding temperature-scaled posterior probabilities p \u03c4 ic and p u\u03c4 ic . The student network is then trained to minimize the following knowledge distillation loss function:"
                    },
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 1618,
                        "end": 1817,
                        "sentence_offsets": [
                            {
                                "start": 1618,
                                "end": 1816
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In this way, the student can have a chance to produce high probabilities for the synthetic unknown classes when evaluating an unfamiliar sample by reducing the risk of overfitting for known classes."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[247486748 | Oh et al. | 2022 | Citations: 1]",
                "snippets": "The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1343,
                        "end": 1544,
                        "sentence_offsets": [
                            {
                                "start": 1343,
                                "end": 1544
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[249151858 | Lu et al. | 2022 | Citations: 29]",
                "snippets": "Temperature-scaled softmax (Li et al., 2016) is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[4853851 | Li et al. | 2016 | Citations: 4431]": "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance."
                },
                "metadata": [
                    {
                        "section_title": "Teacher-Student Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 628,
                        "end": 1057,
                        "sentence_offsets": [
                            {
                                "start": 628,
                                "end": 740
                            },
                            {
                                "start": 741,
                                "end": 851
                            },
                            {
                                "start": 854,
                                "end": 887
                            },
                            {
                                "start": 888,
                                "end": 1057
                            }
                        ],
                        "ref_mentions": [
                            "4853851"
                        ],
                        "quote": "Temperature-scaled softmax (Li et al., 2016) is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[251040920 | Liu et al. | 2022 | Citations: 8]",
                "snippets": "The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Models",
                        "pdf_hash": "",
                        "start": 249,
                        "end": 1101,
                        "sentence_offsets": [
                            {
                                "start": 249,
                                "end": 375
                            },
                            {
                                "start": 376,
                                "end": 432
                            },
                            {
                                "start": 435,
                                "end": 624
                            },
                            {
                                "start": 625,
                                "end": 674
                            },
                            {
                                "start": 675,
                                "end": 756
                            },
                            {
                                "start": 757,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 1103
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The soft labels can be obtained using softmax with temperature scaling, providing more inter-class and intraclass information. The distillation loss for soft logits can be denoted as: \n\nwhere D (\u2022) is the distance function (e.g., Kullback-Leibler divergence) to minimize the difference between the logits produced by the teacher model and those produced by the student model. z i is the corresponding logit of the i-th class. \u03c4 is a temperature hyperparameter intended to smooth outputs from teacher models. As shown in Figure 2, the teacher and student models use the softmax function with \u03c4 after their respective MLP layer to obtain p \u03c4 t i and p \u03c4 s i . A larger \u03c4 leads to a smoother probability distribution over classes, promoting the student model to learn more smoothing information about which classes are more similar to the predicted class."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[252596141 | Nguyen et al. | 2022 | Citations: 2]",
                "snippets": "The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A.2 KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 540,
                        "end": 1089,
                        "sentence_offsets": [
                            {
                                "start": 465,
                                "end": 562
                            },
                            {
                                "start": 565,
                                "end": 675
                            },
                            {
                                "start": 676,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 916
                            },
                            {
                                "start": 917,
                                "end": 1045
                            },
                            {
                                "start": 1046,
                                "end": 1148
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[252968120 | Thapa | 2022 | Citations: 1]",
                "snippets": "The temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 235,
                        "end": 1850,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[253655030 | Borza et al. | 2022 | Citations: 2]",
                "snippets": "In the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1025,
                        "end": 1195,
                        "sentence_offsets": [
                            {
                                "start": 1025,
                                "end": 1195
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[254910794 | Amin et al. | 2023 | Citations: 2]",
                "snippets": "When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "G. COMBINING MODELS BY KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 653,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 48
                            },
                            {
                                "start": 49,
                                "end": 197
                            },
                            {
                                "start": 198,
                                "end": 242
                            },
                            {
                                "start": 243,
                                "end": 311
                            },
                            {
                                "start": 312,
                                "end": 458
                            },
                            {
                                "start": 459,
                                "end": 546
                            },
                            {
                                "start": 547,
                                "end": 653
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[256900863 | Zhang et al. | 2023 | Citations: 1]",
                "snippets": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "B. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 617,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 209
                            },
                            {
                                "start": 210,
                                "end": 617
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[258156357 | Ullah et al. | 2023 | Citations: 3]",
                "snippets": "Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Spatio-Temporal Knowledge Distillation from Teacher to Student Model",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1006,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 128
                            },
                            {
                                "start": 129,
                                "end": 255
                            },
                            {
                                "start": 256,
                                "end": 427
                            },
                            {
                                "start": 428,
                                "end": 620
                            },
                            {
                                "start": 621,
                                "end": 746
                            },
                            {
                                "start": 747,
                                "end": 891
                            },
                            {
                                "start": 892,
                                "end": 1006
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[260447668 | Cui et al. | 2022 | Citations: 3]",
                "snippets": "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Key Idea and Training Framework",
                        "pdf_hash": "",
                        "start": 190,
                        "end": 2175,
                        "sentence_offsets": [
                            {
                                "start": 190,
                                "end": 402
                            },
                            {
                                "start": 403,
                                "end": 462
                            },
                            {
                                "start": 463,
                                "end": 524
                            },
                            {
                                "start": 525,
                                "end": 643
                            },
                            {
                                "start": 644,
                                "end": 897
                            },
                            {
                                "start": 898,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1205
                            },
                            {
                                "start": 1208,
                                "end": 1382
                            },
                            {
                                "start": 1383,
                                "end": 1468
                            },
                            {
                                "start": 1469,
                                "end": 1543
                            },
                            {
                                "start": 1546,
                                "end": 1711
                            },
                            {
                                "start": 1712,
                                "end": 1912
                            },
                            {
                                "start": 1913,
                                "end": 2061
                            },
                            {
                                "start": 2062,
                                "end": 2175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[265384964 | Xie et al. | 2023 | Citations: 6]",
                "snippets": "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[265444951 | Su et al. | 2023 | Citations: 2]",
                "snippets": "In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by where p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 111,
                        "end": 619,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 139
                            },
                            {
                                "start": 140,
                                "end": 258
                            },
                            {
                                "start": 261,
                                "end": 436
                            },
                            {
                                "start": 437,
                                "end": 481
                            },
                            {
                                "start": 482,
                                "end": 650
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by where p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[267335043 | Ralambomihanta et al. | 2024 | Citations: 3]",
                "snippets": "Distillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Distillation",
                        "pdf_hash": "",
                        "start": 328,
                        "end": 707,
                        "sentence_offsets": [
                            {
                                "start": 328,
                                "end": 443
                            },
                            {
                                "start": 444,
                                "end": 597
                            },
                            {
                                "start": 598,
                                "end": 707
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Distillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[267535483 | Al-Ahmadi | 2024 | Citations: 1]",
                "snippets": "In higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. TEACHER MODEL",
                        "pdf_hash": "",
                        "start": 183,
                        "end": 1221,
                        "sentence_offsets": [
                            {
                                "start": 183,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 388
                            },
                            {
                                "start": 389,
                                "end": 561
                            },
                            {
                                "start": 562,
                                "end": 745
                            },
                            {
                                "start": 746,
                                "end": 870
                            },
                            {
                                "start": 871,
                                "end": 996
                            },
                            {
                                "start": 997,
                                "end": 1090
                            },
                            {
                                "start": 1091,
                                "end": 1221
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[268060109 | Cao et al. | 2024 | Citations: 46]",
                "snippets": "The choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Softmax loss calculation:",
                        "pdf_hash": "",
                        "start": 845,
                        "end": 1320,
                        "sentence_offsets": [
                            {
                                "start": 845,
                                "end": 908
                            },
                            {
                                "start": 909,
                                "end": 1052
                            },
                            {
                                "start": 1053,
                                "end": 1161
                            },
                            {
                                "start": 1162,
                                "end": 1320
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[268820185 | Kaleem et al. | 2024 | Citations: 13]",
                "snippets": "The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation. This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function. The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION TO KNOWLEDGE DISTILLATION",
                        "pdf_hash": "",
                        "start": 1675,
                        "end": 2061,
                        "sentence_offsets": [
                            {
                                "start": 1604,
                                "end": 1826
                            },
                            {
                                "start": 1828,
                                "end": 2050
                            },
                            {
                                "start": 2050,
                                "end": 2191
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation. This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function. The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[269635406 | Hu et al. | 2024 | Citations: 0]",
                "snippets": "Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 263,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 211
                            },
                            {
                                "start": 211,
                                "end": 263
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[270257779 | Chen et al. | 2024 | Citations: 4]",
                "snippets": "When T =1, q i is the standard softmax function. In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation is a compression technique designed to transfer knowledge or information",
                        "pdf_hash": "",
                        "start": 552,
                        "end": 794,
                        "sentence_offsets": [
                            {
                                "start": 482,
                                "end": 606
                            },
                            {
                                "start": 608,
                                "end": 678
                            },
                            {
                                "start": 678,
                                "end": 726
                            },
                            {
                                "start": 726,
                                "end": 919
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When T =1, q i is the standard softmax function. In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[270377282 | Liu et al. | 2024 | Citations: 2]",
                "snippets": "By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels...The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,(Chen et al., 2023) . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Fermented grains surface temperature detection based on YOLO v5n",
                        "pdf_hash": "",
                        "start": 326,
                        "end": 490,
                        "sentence_offsets": [
                            {
                                "start": 196,
                                "end": 491
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels"
                    },
                    {
                        "section_title": "Fermented grains surface temperature detection based on YOLO v5n",
                        "pdf_hash": "",
                        "start": 895,
                        "end": 1626,
                        "sentence_offsets": [
                            {
                                "start": 895,
                                "end": 993
                            },
                            {
                                "start": 994,
                                "end": 1163
                            },
                            {
                                "start": 1164,
                                "end": 1253
                            },
                            {
                                "start": 1254,
                                "end": 1296
                            },
                            {
                                "start": 1297,
                                "end": 1466
                            },
                            {
                                "start": 1467,
                                "end": 1625
                            }
                        ],
                        "ref_mentions": [
                            "264462123"
                        ],
                        "quote": "The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,(Chen et al., 2023) . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[271218098 | Giakoumoglou et al. | 2024 | Citations: 0]",
                "snippets": "The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 155,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 155
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[272341595 | Sarikaya et al. | 2024 | Citations: 0]",
                "snippets": "T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "2) KNOWLEDGE DISTILLATION LOSS",
                        "pdf_hash": "",
                        "start": 1255,
                        "end": 1445,
                        "sentence_offsets": [
                            {
                                "start": 1253,
                                "end": 1445
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[272969060 | Huang et al. | 2024 | Citations: 0]",
                "snippets": "In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions:",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preliminaries",
                        "pdf_hash": "",
                        "start": 129,
                        "end": 623,
                        "sentence_offsets": [
                            {
                                "start": 129,
                                "end": 356
                            },
                            {
                                "start": 357,
                                "end": 415
                            },
                            {
                                "start": 418,
                                "end": 509
                            },
                            {
                                "start": 510,
                                "end": 623
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions:"
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[273549238 | Dubey et al. | 2024 | Citations: 3]",
                "snippets": "By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 622,
                        "end": 1525,
                        "sentence_offsets": [
                            {
                                "start": 622,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 878
                            },
                            {
                                "start": 881,
                                "end": 976
                            },
                            {
                                "start": 977,
                                "end": 1168
                            },
                            {
                                "start": 1169,
                                "end": 1301
                            },
                            {
                                "start": 1302,
                                "end": 1383
                            },
                            {
                                "start": 1384,
                                "end": 1525
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[274269913 | Rizk et al. | 2024 | Citations: 2]",
                "snippets": "A key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 1157,
                        "end": 1781,
                        "sentence_offsets": [
                            {
                                "start": 1157,
                                "end": 1327
                            },
                            {
                                "start": 1328,
                                "end": 1401
                            },
                            {
                                "start": 1404,
                                "end": 1471
                            },
                            {
                                "start": 1472,
                                "end": 1544
                            },
                            {
                                "start": 1545,
                                "end": 1649
                            },
                            {
                                "start": 1650,
                                "end": 1781
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[274436184 | Li et al. | 2024 | Citations: 0]",
                "snippets": "In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Learning node-specific temperatures",
                        "pdf_hash": "",
                        "start": 560,
                        "end": 1205,
                        "sentence_offsets": [
                            {
                                "start": 560,
                                "end": 668
                            },
                            {
                                "start": 669,
                                "end": 814
                            },
                            {
                                "start": 815,
                                "end": 883
                            },
                            {
                                "start": 886,
                                "end": 971
                            },
                            {
                                "start": 972,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1205
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge."
                    }
                ]
            },
            {
                "idx": 40,
                "key": "[274776589 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "The core principle involves softening the Softmax output by a temperature parameter to enhance the similarity between categories and improve the generalisation ability of the student model. The underlying formula is q i = exp(z i /T) (Ej) exp(z j /T) , where z i represents the teacher model output and q i denotes the temperature-smoothed output.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge distillation",
                        "pdf_hash": "",
                        "start": 250,
                        "end": 597,
                        "sentence_offsets": [
                            {
                                "start": 250,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 597
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The core principle involves softening the Softmax output by a temperature parameter to enhance the similarity between categories and improve the generalisation ability of the student model. The underlying formula is q i = exp(z i /T) (Ej) exp(z j /T) , where z i represents the teacher model output and q i denotes the temperature-smoothed output."
                    }
                ]
            },
            {
                "idx": 41,
                "key": "[275993890 | Murtada et al. | 2025 | Citations: 1]",
                "snippets": "In the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Our Knowledge Distillation Method:",
                        "pdf_hash": "",
                        "start": 377,
                        "end": 1022,
                        "sentence_offsets": [
                            {
                                "start": 377,
                                "end": 594
                            },
                            {
                                "start": 595,
                                "end": 697
                            },
                            {
                                "start": 698,
                                "end": 859
                            },
                            {
                                "start": 860,
                                "end": 1022
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training."
                    }
                ]
            },
            {
                "idx": 42,
                "key": "[276725399 | Xuan et al. | 2025 | Citations: 4]",
                "snippets": "Knowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORKS",
                        "pdf_hash": "",
                        "start": 383,
                        "end": 1173,
                        "sentence_offsets": [
                            {
                                "start": 383,
                                "end": 506
                            },
                            {
                                "start": 507,
                                "end": 602
                            },
                            {
                                "start": 603,
                                "end": 735
                            },
                            {
                                "start": 736,
                                "end": 876
                            },
                            {
                                "start": 877,
                                "end": 1053
                            },
                            {
                                "start": 1054,
                                "end": 1173
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Knowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model."
                    }
                ]
            },
            {
                "idx": 43,
                "key": "[277104482 | Li et al. | 2025 | Citations: 0]",
                "snippets": "The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model...The softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively...where T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Distilling Transformers into Hybrid Models",
                        "pdf_hash": "",
                        "start": 840,
                        "end": 1023,
                        "sentence_offsets": [
                            {
                                "start": 840,
                                "end": 1024
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model"
                    },
                    {
                        "section_title": "Knowledge Distilling Transformers into Hybrid Models",
                        "pdf_hash": "",
                        "start": 1068,
                        "end": 1242,
                        "sentence_offsets": [
                            {
                                "start": 1068,
                                "end": 1242
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively"
                    },
                    {
                        "section_title": "Knowledge Distilling Transformers into Hybrid Models",
                        "pdf_hash": "",
                        "start": 1245,
                        "end": 1459,
                        "sentence_offsets": [
                            {
                                "start": 1245,
                                "end": 1458
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "where T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model."
                    }
                ]
            },
            {
                "idx": 44,
                "key": "[277632659 | Khan et al. | 2025 | Citations: 0]",
                "snippets": "Hinton et al. (Hinton et al., 2015) introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[7200347 | Hinton et al. | 2015 | Citations: 19742]": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 222,
                        "end": 975,
                        "sentence_offsets": [
                            {
                                "start": 222,
                                "end": 628
                            },
                            {
                                "start": 629,
                                "end": 850
                            },
                            {
                                "start": 851,
                                "end": 975
                            }
                        ],
                        "ref_mentions": [
                            "7200347"
                        ],
                        "quote": "Hinton et al. (Hinton et al., 2015) introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case."
                    }
                ]
            },
            {
                "idx": 45,
                "key": "[277994315 | Hasegawa et al. | 2025 | Citations: 0]",
                "snippets": "In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Temperature parameter",
                        "pdf_hash": "",
                        "start": 370,
                        "end": 508,
                        "sentence_offsets": [
                            {
                                "start": 370,
                                "end": 508
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance."
                    }
                ]
            },
            {
                "idx": 46,
                "key": "[85528498 | Liu et al. | 2019 | Citations: 7]",
                "snippets": "Knowledge distillation, also known as model compression, aims to learn a small or shallow neural network (normally called the student model, denoted as S(\u00b7)) with limited training examples and computational power by transferring the generalization ability from a large well-trained neural network (called the teacher model, denoted as T (\u00b7)), as shown in Fig.2 and Fig.4. During training, the student model will be guided by the teacher. The student attempts to match its softened softmax outputs with that of the teacher, and its hard softmax outputs with the ground truth. Given the output z i before the last layer of a neural network, usually called logits, the softmax transforms z i to a probability P i using the following equation.\n\nNormal softmax tends to set the probability for one class to one and that for the rest to zero. This makes it hard to distill hidden knowledge to the student with the teacher as the source. To improve the generalization ability of student model and efficiently use the hidden knowledge, Hinton et. al. [27] proposed high temperature softmax function in lieu of using the normal softmax (temperature=1 in this case).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D. Knowledge Distillation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1156,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge distillation, also known as model compression, aims to learn a small or shallow neural network (normally called the student model, denoted as S(\u00b7)) with limited training examples and computational power by transferring the generalization ability from a large well-trained neural network (called the teacher model, denoted as T (\u00b7)), as shown in Fig.2 and Fig.4. During training, the student model will be guided by the teacher. The student attempts to match its softened softmax outputs with that of the teacher, and its hard softmax outputs with the ground truth. Given the output z i before the last layer of a neural network, usually called logits, the softmax transforms z i to a probability P i using the following equation.\n\nNormal softmax tends to set the probability for one class to one and that for the rest to zero. This makes it hard to distill hidden knowledge to the student with the teacher as the source. To improve the generalization ability of student model and efficiently use the hidden knowledge, Hinton et. al. [27] proposed high temperature softmax function in lieu of using the normal softmax (temperature=1 in this case)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.033669,
        "cot": "Let me plan the dimensions for clustering the quotes to answer the query about how temperature scaling in the softmax function facilitates the transfer of dark knowledge during knowledge distillation.\n\nThe query is specifically asking about the mechanism by which temperature scaling helps transfer \"dark knowledge\" from teacher to student models. Looking at the available quotes, I need to organize them into meaningful sections that will help build a comprehensive answer.\n\nHere's my plan for clustering the quotes into dimensions:\n\n1. \"Introduction to Knowledge Distillation and Dark Knowledge\" - This should be a synthesis paragraph explaining the basic concept of knowledge distillation, what dark knowledge is, and why it's important. This provides necessary background for understanding the rest of the answer.\n\n2. \"Temperature Scaling Mechanism in Softmax\" - This should be a synthesis paragraph explaining the technical details of how temperature scaling modifies the softmax function. This is fundamental to understanding the mechanism being asked about.\n\n3. \"How Temperature Affects Probability Distributions\" - This should be a synthesis paragraph examining how different temperature values impact the softness/sharpness of probability distributions and why this matters for knowledge transfer.\n\n4. \"Dark Knowledge Transfer Process\" - This should be a synthesis paragraph explaining how the modified probability distributions actually facilitate the transfer of dark knowledge from teacher to student models.\n\n5. \"Practical Considerations for Temperature Values\" - This should be a synthesis section covering empirical findings and recommendations about temperature parameter values in practice.\n\nAll dimensions should use the synthesis format because the query is asking for an explanation of a mechanism, not a list of items. Each section builds upon the previous one to create a coherent explanation of how temperature scaling facilitates dark knowledge transfer.",
        "plan": {
            "Introduction to Knowledge Distillation and Dark Knowledge (synthesis)": [
                1,
                8,
                9,
                10,
                11,
                12,
                22,
                24,
                26,
                27
            ],
            "Temperature Scaling Mechanism in Softmax (synthesis)": [
                0,
                4,
                6,
                7,
                14,
                15,
                16,
                17,
                18,
                23,
                30,
                31,
                32,
                35,
                36,
                37,
                38,
                40,
                43,
                44,
                46
            ],
            "How Temperature Affects Probability Distributions (synthesis)": [
                3,
                5,
                13,
                19,
                25,
                28,
                33,
                39,
                41,
                42
            ],
            "Dark Knowledge Transfer Process (synthesis)": [
                2,
                20,
                21,
                29,
                34
            ],
            "Practical Considerations for Temperature Values (synthesis)": [
                45
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Knowledge Distillation and Dark Knowledge",
                "tldr": "Knowledge distillation transfers knowledge from a larger teacher model to a smaller student model using soft probability distributions. The \"dark knowledge\" embedded in these soft targets reveals inter-class relationships that aren't captured by hard labels alone. (11 sources)",
                "text": "\nKnowledge distillation, introduced by Hinton et al., is a technique for transferring knowledge from a complex teacher model to a simpler student model while preserving generalization ability <Paper corpusId=\"234353710\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper> <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This approach addresses the challenge of deploying large ensemble models, which despite their superior performance, can be computationally expensive and cumbersome for widespread use <Paper corpusId=\"234353710\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nAt the core of knowledge distillation is the concept of \"dark knowledge,\" which refers to the information embedded in the probability distributions produced by the teacher model <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. Unlike traditional training methods that rely solely on hard labels (one-hot encodings), knowledge distillation leverages soft targets\u2014probability distributions that reveal how the teacher model relates different classes to each other <Paper corpusId=\"212644537\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper>. These soft targets contain valuable information about inter-class relationships that aren't captured by one-hot labels alone <Paper corpusId=\"212644537\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper> <Paper corpusId=\"235421655\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper>.\n\nThe distillation process typically involves training the student model using a weighted sum of two losses: (1) a standard cross-entropy loss with hard labels and (2) a cross-entropy loss with soft labels produced by the teacher model <Paper corpusId=\"239616535\" paperTitle=\"(Jin et al., 2021)\" isShortName></Paper>. This dual optimization approach allows the student to both learn the correct classifications and mimic the rich inter-class knowledge captured by the teacher <Paper corpusId=\"267335043\" paperTitle=\"(Ralambomihanta et al., 2024)\" isShortName></Paper>.\n\nA key challenge in knowledge distillation is that well-trained models tend to produce very high probabilities for the correct class and near-zero probabilities for others, making the soft labels nearly identical to hard labels and limiting the transfer of dark knowledge <Paper corpusId=\"237002008\" paperTitle=\"(Lin et al., 2021)\" isShortName></Paper>. To address this issue, knowledge distillation employs temperature scaling in the softmax function, which controls the \"softness\" of the probability distributions and reveals the dark knowledge embedded in the teacher model <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nBeyond classification tasks, dark knowledge also serves important functions in other domains. For example, in communication systems involving lossy compression, the teacher model's output can provide a feasible sub-optimal solution that is more beneficial for training than learning directly from ground-truth data that represents an infeasible perfect solution <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "Hinton et al. (Hinton et al., 2015) apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label."
                        ],
                        "paper": {
                            "corpus_id": 234353710,
                            "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning",
                            "authors": [
                                {
                                    "authorId": "2145288039",
                                    "name": "Lu Liu"
                                },
                                {
                                    "authorId": "1726720",
                                    "name": "R. Tan"
                                }
                            ],
                            "year": 2019,
                            "venue": "Pattern Recognition",
                            "n_citations": 32
                        },
                        "score": 0.6943359375
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."
                        ],
                        "paper": {
                            "corpus_id": 256900863,
                            "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK",
                            "authors": [
                                {
                                    "authorId": "2135847074",
                                    "name": "Xiongtao Zhang"
                                },
                                {
                                    "authorId": "2206403890",
                                    "name": "Zezong Yin"
                                },
                                {
                                    "authorId": "3247526",
                                    "name": "Yunliang Jiang"
                                },
                                {
                                    "authorId": "1390650781",
                                    "name": "Yizhang Jiang"
                                },
                                {
                                    "authorId": "2340356",
                                    "name": "Da-Song Sun"
                                },
                                {
                                    "authorId": "2189281",
                                    "name": "Yong Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.783203125
                    },
                    {
                        "id": "(Jaiswal et al., 2021)",
                        "snippets": [
                            "For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."
                        ],
                        "paper": {
                            "corpus_id": 234336288,
                            "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification",
                            "authors": [
                                {
                                    "authorId": "2139643077",
                                    "name": "Bhavesh Jaiswal"
                                },
                                {
                                    "authorId": "32136431",
                                    "name": "Nagendra Gajjar"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Journal of Advanced Computer Science and Applications",
                            "n_citations": 0
                        },
                        "score": 0.70556640625
                    },
                    {
                        "id": "(Deng et al., 2020)",
                        "snippets": [
                            "Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels."
                        ],
                        "paper": {
                            "corpus_id": 212644537,
                            "title": "Multitask Emotion Recognition with Incomplete Labels",
                            "authors": [
                                {
                                    "authorId": "46203817",
                                    "name": "Didan Deng"
                                },
                                {
                                    "authorId": "2808872",
                                    "name": "Zhaokang Chen"
                                },
                                {
                                    "authorId": "2075335081",
                                    "name": "Bertram E. Shi"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
                            "n_citations": 95
                        },
                        "score": 0.87353515625
                    },
                    {
                        "id": "(Lee et al., 2021)",
                        "snippets": [
                            "Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."
                        ],
                        "paper": {
                            "corpus_id": 235421655,
                            "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks",
                            "authors": [
                                {
                                    "authorId": "2109519891",
                                    "name": "Dongjin Lee"
                                },
                                {
                                    "authorId": "120425550",
                                    "name": "Seongsik Park"
                                },
                                {
                                    "authorId": "2157129258",
                                    "name": "Jongwan Kim"
                                },
                                {
                                    "authorId": "2111845679",
                                    "name": "Wuhyeong Doh"
                                },
                                {
                                    "authorId": "2999019",
                                    "name": "Sungroh Yoon"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.68798828125
                    },
                    {
                        "id": "(Jin et al., 2021)",
                        "snippets": [
                            "In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions",
                            ".The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4",
                            ".The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels)."
                        ],
                        "paper": {
                            "corpus_id": 239616535,
                            "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding",
                            "authors": [
                                {
                                    "authorId": "8844876",
                                    "name": "Woojeong Jin"
                                },
                                {
                                    "authorId": "2095979",
                                    "name": "Maziar Sanjabi"
                                },
                                {
                                    "authorId": "35557488",
                                    "name": "Shaoliang Nie"
                                },
                                {
                                    "authorId": "48327785",
                                    "name": "L Tan"
                                },
                                {
                                    "authorId": "1384550891",
                                    "name": "Xiang Ren"
                                },
                                {
                                    "authorId": "22593971",
                                    "name": "Hamed Firooz"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 6
                        },
                        "score": 0.6875
                    },
                    {
                        "id": "(Ralambomihanta et al., 2024)",
                        "snippets": [
                            "Distillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning."
                        ],
                        "paper": {
                            "corpus_id": 267335043,
                            "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
                            "authors": [
                                {
                                    "authorId": "2281945399",
                                    "name": "Tokiniaina Raharison Ralambomihanta"
                                },
                                {
                                    "authorId": "2281945133",
                                    "name": "Shahrad Mohammadzadeh"
                                },
                                {
                                    "authorId": "2282084764",
                                    "name": "Mohammad Sami Nur Islam"
                                },
                                {
                                    "authorId": "2281945352",
                                    "name": "Wassim Jabbour"
                                },
                                {
                                    "authorId": "2282098962",
                                    "name": "Laurence Liang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6435546875
                    },
                    {
                        "id": "(Lin et al., 2021)",
                        "snippets": [
                            "During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 237002008,
                            "title": "In-Network Flow Classification With Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "145446632",
                                    "name": "K. Lin"
                                },
                                {
                                    "authorId": "2124901704",
                                    "name": "Chen-yang Li"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.86865234375
                    },
                    {
                        "id": "(Su et al., 2023)",
                        "snippets": [
                            "In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by where p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher."
                        ],
                        "paper": {
                            "corpus_id": 265444951,
                            "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268314644",
                                    "name": "Chi-Ping Su"
                                },
                                {
                                    "authorId": "1569686364",
                                    "name": "Ching-Hsun Tseng"
                                },
                                {
                                    "authorId": "2338265886",
                                    "name": "Bin Pu"
                                },
                                {
                                    "authorId": "2338506240",
                                    "name": "Lei Zhao"
                                },
                                {
                                    "authorId": "2328588941",
                                    "name": "Zhuangzhuang Chen"
                                },
                                {
                                    "authorId": "2116351339",
                                    "name": "Shin-Jye Lee"
                                }
                            ],
                            "year": 2023,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.95361328125
                    },
                    {
                        "id": "(Cui et al., 2022)",
                        "snippets": [
                            "Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part."
                        ],
                        "paper": {
                            "corpus_id": 260447668,
                            "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback",
                            "authors": [
                                {
                                    "authorId": "2172485573",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "47093519",
                                    "name": "Jiajia Guo"
                                },
                                {
                                    "authorId": "2113999930",
                                    "name": "Zheng Cao"
                                },
                                {
                                    "authorId": "120710335",
                                    "name": "Huaze Tang"
                                },
                                {
                                    "authorId": "2257212132",
                                    "name": "Chao-Kai Wen"
                                },
                                {
                                    "authorId": "2227268421",
                                    "name": "Shi Jin"
                                },
                                {
                                    "authorId": "2288090155",
                                    "name": "Xin Wang"
                                },
                                {
                                    "authorId": "2240356164",
                                    "name": "Xiaolin Hou"
                                }
                            ],
                            "year": 2022,
                            "venue": "IEEE Transactions on Communications",
                            "n_citations": 3
                        },
                        "score": 0.66455078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Temperature Scaling Mechanism in Softmax",
                "tldr": "Temperature scaling modifies the standard softmax function by dividing logits by a temperature parameter T, which controls the \"softness\" of probability distributions. Higher temperature values produce smoother distributions that better reveal inter-class relationships, enabling more effective transfer of dark knowledge. (18 sources)",
                "text": "\nTemperature scaling is a fundamental mechanism in knowledge distillation that modifies the standard softmax function to reveal the rich inter-class relationships learned by the teacher model. The conventional softmax function, which converts logits to probabilities, is reformulated by introducing a temperature parameter T that divides each logit before applying the exponential function <Paper corpusId=\"268820185\" paperTitle=\"(Kaleem et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269635406\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper>. The modified softmax formula is expressed as:\n\n$$p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$\n\nwhere $p_i$ represents the probability for class $i$, $z_i$ is the corresponding logit, and $T$ is the temperature parameter <Paper corpusId=\"274269913\" paperTitle=\"(Rizk et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273549238\" paperTitle=\"(Dubey et al., 2024)\" isShortName></Paper>.\n\nWhen T=1, this formula reverts to the standard softmax function <Paper corpusId=\"225397480\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"270257779\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, as T increases, the probability distribution becomes smoother, spreading probabilities more evenly across classes <Paper corpusId=\"274269913\" paperTitle=\"(Rizk et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277104482\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This scaling mechanism effectively adjusts the slope of the softmax function\u2014higher T values decrease the slope significantly, allowing the output values to carry considerably more information <Paper corpusId=\"252596141\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nThe mathematical basis for temperature scaling lies in its ability to normalize probability vectors. The standard softmax function typically produces non-uniform probability distributions that can vary significantly between different models, even for the same class. By dividing logits by T, both teacher and student models generate more uniformly distributed probability vectors, making it feasible to compute meaningful loss values between them <Paper corpusId=\"258156357\" paperTitle=\"(Ullah et al., 2023)\" isShortName></Paper>.\n\nDuring the knowledge distillation process, the same temperature parameter is applied to both the teacher and student models <Paper corpusId=\"277632659\" paperTitle=\"(Khan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This ensures that the student learns to mimic the teacher's softened distributions, capturing not just the correct classification but also the subtle relationships between classes <Paper corpusId=\"249151858\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4853851\" paperTitle=\"(Li et al., 2016)\" isShortName></Paper>. \n\nThe temperature parameter is a crucial hyperparameter that can be tuned to optimize knowledge transfer <Paper corpusId=\"258156357\" paperTitle=\"(Ullah et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272341595\" paperTitle=\"(Sarikaya et al., 2024)\" isShortName></Paper>. It serves as the key mechanism that enables the teacher to effectively transfer its \"dark knowledge\" to the student, revealing patterns of similarity among classes that would otherwise remain hidden in standard training approaches <Paper corpusId=\"247486748\" paperTitle=\"(Oh et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233777216\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"198179476\" paperTitle=\"(Tung et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Kaleem et al., 2024)",
                        "snippets": [
                            "The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation. This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function. The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions."
                        ],
                        "paper": {
                            "corpus_id": 268820185,
                            "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision",
                            "authors": [
                                {
                                    "authorId": "2294363351",
                                    "name": "Sheikh Musa Kaleem"
                                },
                                {
                                    "authorId": "2294363576",
                                    "name": "Tufail Rouf"
                                },
                                {
                                    "authorId": "2016381637",
                                    "name": "Gousia Habib"
                                },
                                {
                                    "authorId": "1468566373",
                                    "name": "T. Saleem"
                                },
                                {
                                    "authorId": "143632380",
                                    "name": "B. Lall"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.71728515625
                    },
                    {
                        "id": "(Hu et al., 2024)",
                        "snippets": [
                            "Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution."
                        ],
                        "paper": {
                            "corpus_id": 269635406,
                            "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management",
                            "authors": [
                                {
                                    "authorId": "2301005855",
                                    "name": "Gang Hu"
                                },
                                {
                                    "authorId": "2300397509",
                                    "name": "Ming Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6298828125
                    },
                    {
                        "id": "(Rizk et al., 2024)",
                        "snippets": [
                            "A key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from."
                        ],
                        "paper": {
                            "corpus_id": 274269913,
                            "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2311724069",
                                    "name": "Hamada Rizk"
                                },
                                {
                                    "authorId": "2677103",
                                    "name": "Ahmed M. Elmogy"
                                },
                                {
                                    "authorId": "145195785",
                                    "name": "Mohamed Rihan"
                                },
                                {
                                    "authorId": "2171271984",
                                    "name": "Hirozumi Yamaguchi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Italian National Conference on Sensors",
                            "n_citations": 2
                        },
                        "score": 0.6533203125
                    },
                    {
                        "id": "(Huang et al., 2024)",
                        "snippets": [
                            "In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions:"
                        ],
                        "paper": {
                            "corpus_id": 272969060,
                            "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation",
                            "authors": [
                                {
                                    "authorId": "2180087917",
                                    "name": "Yaomin Huang"
                                },
                                {
                                    "authorId": "2257133494",
                                    "name": "Zaoming Yan"
                                },
                                {
                                    "authorId": "2242115637",
                                    "name": "Chaomin Shen"
                                },
                                {
                                    "authorId": "152786529",
                                    "name": "Faming Fang"
                                },
                                {
                                    "authorId": "2323414943",
                                    "name": "Guixu Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Computer Vision",
                            "n_citations": 0
                        },
                        "score": 0.625
                    },
                    {
                        "id": "(Dubey et al., 2024)",
                        "snippets": [
                            "By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 273549238,
                            "title": "AI Readiness in Healthcare through Storytelling XAI",
                            "authors": [
                                {
                                    "authorId": "2282540984",
                                    "name": "Akshat Dubey"
                                },
                                {
                                    "authorId": "2282577952",
                                    "name": "Zewen Yang"
                                },
                                {
                                    "authorId": "2282533557",
                                    "name": "Georges Hattab"
                                }
                            ],
                            "year": 2024,
                            "venue": "EXPLIMED@ECAI",
                            "n_citations": 3
                        },
                        "score": 0.75244140625
                    },
                    {
                        "id": "(Liu et al., 2020)",
                        "snippets": [
                            "By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  ."
                        ],
                        "paper": {
                            "corpus_id": 225397480,
                            "title": "A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning",
                            "authors": [
                                {
                                    "authorId": "2145253722",
                                    "name": "Zihan Liu"
                                },
                                {
                                    "authorId": "2110398805",
                                    "name": "Zhi-yuan Shi"
                                }
                            ],
                            "year": 2020,
                            "venue": "Journal of Physics: Conference Series",
                            "n_citations": 1
                        },
                        "score": 0.712890625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "When T =1, q i is the standard softmax function. In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor."
                        ],
                        "paper": {
                            "corpus_id": 270257779,
                            "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model",
                            "authors": [
                                {
                                    "authorId": "2283187602",
                                    "name": "Jinyin Chen"
                                },
                                {
                                    "authorId": "2289841303",
                                    "name": "Xiao-Ming Zhao"
                                },
                                {
                                    "authorId": "1796810",
                                    "name": "Haibin Zheng"
                                },
                                {
                                    "authorId": "2304969928",
                                    "name": "Xiao Li"
                                },
                                {
                                    "authorId": "2291571475",
                                    "name": "Sheng Xiang"
                                },
                                {
                                    "authorId": "2304823063",
                                    "name": "Haifeng Guo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Soft Computing",
                            "n_citations": 4
                        },
                        "score": 0.7236328125
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model",
                            "The softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively",
                            "where T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model."
                        ],
                        "paper": {
                            "corpus_id": 277104482,
                            "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling",
                            "authors": [
                                {
                                    "authorId": "2320013374",
                                    "name": "Yingyue Li"
                                },
                                {
                                    "authorId": "2060439659",
                                    "name": "Bencheng Liao"
                                },
                                {
                                    "authorId": "2313037989",
                                    "name": "Wenyu Liu"
                                },
                                {
                                    "authorId": "2266175736",
                                    "name": "Xinggang Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.82421875
                    },
                    {
                        "id": "(Nguyen et al., 2022)",
                        "snippets": [
                            "The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective."
                        ],
                        "paper": {
                            "corpus_id": 252596141,
                            "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data",
                            "authors": [
                                {
                                    "authorId": "2162621330",
                                    "name": "Minh-Duong Nguyen"
                                },
                                {
                                    "authorId": "145436642",
                                    "name": "Viet Quoc Pham"
                                },
                                {
                                    "authorId": "2233724",
                                    "name": "D. Hoang"
                                },
                                {
                                    "authorId": "1389575160",
                                    "name": "Long Tran-Thanh"
                                },
                                {
                                    "authorId": "30479205",
                                    "name": "Diep N. Nguyen"
                                },
                                {
                                    "authorId": "2054135428",
                                    "name": "W. Hwang"
                                }
                            ],
                            "year": 2022,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.921875
                    },
                    {
                        "id": "(Ullah et al., 2023)",
                        "snippets": [
                            "Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy."
                        ],
                        "paper": {
                            "corpus_id": 258156357,
                            "title": "A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition",
                            "authors": [
                                {
                                    "authorId": "1491632645",
                                    "name": "Hayat Ullah"
                                },
                                {
                                    "authorId": "1748235",
                                    "name": "Arslan Munir"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of Imaging",
                            "n_citations": 3
                        },
                        "score": 0.6474609375
                    },
                    {
                        "id": "(Khan et al., 2025)",
                        "snippets": [
                            "Hinton et al. (Hinton et al., 2015) introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case."
                        ],
                        "paper": {
                            "corpus_id": 277632659,
                            "title": "Crowd counting at the edge using weighted knowledge distillation",
                            "authors": [
                                {
                                    "authorId": "2115771635",
                                    "name": "Muhammad Asif Khan"
                                },
                                {
                                    "authorId": "2330247142",
                                    "name": "Hamid Menouar"
                                },
                                {
                                    "authorId": "2311725227",
                                    "name": "Ridha Hamila"
                                },
                                {
                                    "authorId": "2354319497",
                                    "name": "Adnan Abu-Dayya"
                                }
                            ],
                            "year": 2025,
                            "venue": "Scientific Reports",
                            "n_citations": 0
                        },
                        "score": 0.658203125
                    },
                    {
                        "id": "(Hinton et al., 2015)",
                        "snippets": [
                            "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
                        ],
                        "paper": {
                            "corpus_id": 7200347,
                            "title": "Distilling the Knowledge in a Neural Network",
                            "authors": [
                                {
                                    "authorId": "1695689",
                                    "name": "Geoffrey E. Hinton"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "49959210",
                                    "name": "J. Dean"
                                }
                            ],
                            "year": 2015,
                            "venue": "arXiv.org",
                            "n_citations": 19742
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Temperature-scaled softmax (Li et al., 2016) is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 249151858,
                            "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation",
                            "authors": [
                                {
                                    "authorId": "2064914349",
                                    "name": "Bin Lu"
                                },
                                {
                                    "authorId": "1693639",
                                    "name": "Xiaoying Gan"
                                },
                                {
                                    "authorId": "2145501350",
                                    "name": "Lina Yang"
                                },
                                {
                                    "authorId": "2108309275",
                                    "name": "Weinan Zhang"
                                },
                                {
                                    "authorId": "1922573",
                                    "name": "Luoyi Fu"
                                },
                                {
                                    "authorId": "2107937507",
                                    "name": "Xinbing Wang"
                                }
                            ],
                            "year": 2022,
                            "venue": "Knowledge Discovery and Data Mining",
                            "n_citations": 29
                        },
                        "score": 0.72265625
                    },
                    {
                        "id": "(Li et al., 2016)",
                        "snippets": [
                            "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance."
                        ],
                        "paper": {
                            "corpus_id": 4853851,
                            "title": "Learning without Forgetting",
                            "authors": [
                                {
                                    "authorId": "49969902",
                                    "name": "Zhizhong Li"
                                },
                                {
                                    "authorId": "2433269",
                                    "name": "Derek Hoiem"
                                }
                            ],
                            "year": 2016,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 4431
                        },
                        "score": 0
                    },
                    {
                        "id": "(Sarikaya et al., 2024)",
                        "snippets": [
                            "T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training."
                        ],
                        "paper": {
                            "corpus_id": 272341595,
                            "title": "Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "152648598",
                                    "name": "M. Sarikaya"
                                },
                                {
                                    "authorId": "2146435812",
                                    "name": "G\u00f6khan Ince"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 0
                        },
                        "score": 0.80517578125
                    },
                    {
                        "id": "(Oh et al., 2022)",
                        "snippets": [
                            "The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 247486748,
                            "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing",
                            "authors": [
                                {
                                    "authorId": "2121943561",
                                    "name": "Seokju Oh"
                                },
                                {
                                    "authorId": "2145183886",
                                    "name": "Donghyun Kim"
                                },
                                {
                                    "authorId": "2109513069",
                                    "name": "Chae-Suk Lee"
                                },
                                {
                                    "authorId": "3453343",
                                    "name": "Jongpil Jeong"
                                }
                            ],
                            "year": 2022,
                            "venue": "Electronics",
                            "n_citations": 1
                        },
                        "score": 0.6630859375
                    },
                    {
                        "id": "(Zhou et al., 2021)",
                        "snippets": [
                            "An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student."
                        ],
                        "paper": {
                            "corpus_id": 233777216,
                            "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective",
                            "authors": [
                                {
                                    "authorId": "1993679419",
                                    "name": "Qihua Zhou"
                                },
                                {
                                    "authorId": "2034348867",
                                    "name": "Zhihao Qu"
                                },
                                {
                                    "authorId": "144123438",
                                    "name": "Song Guo"
                                },
                                {
                                    "authorId": "2150444053",
                                    "name": "Boyuan Luo"
                                },
                                {
                                    "authorId": "8572939",
                                    "name": "Jingcai Guo"
                                },
                                {
                                    "authorId": "150358457",
                                    "name": "Zhenda Xu"
                                },
                                {
                                    "authorId": "1854500",
                                    "name": "R. Akerkar"
                                }
                            ],
                            "year": 2021,
                            "venue": "IEEE Internet of Things Journal",
                            "n_citations": 52
                        },
                        "score": 0.873046875
                    },
                    {
                        "id": "(Tung et al., 2019)",
                        "snippets": [
                            "Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."
                        ],
                        "paper": {
                            "corpus_id": 198179476,
                            "title": "Similarity-Preserving Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2065573607",
                                    "name": "Frederick Tung"
                                },
                                {
                                    "authorId": "10771328",
                                    "name": "Greg Mori"
                                }
                            ],
                            "year": 2019,
                            "venue": "IEEE International Conference on Computer Vision",
                            "n_citations": 981
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "How Temperature Affects Probability Distributions",
                "tldr": "Temperature scaling fundamentally alters probability distributions by controlling their \"softness\" - higher temperatures produce more uniform distributions that reveal subtle inter-class relationships, while lower temperatures create sharper distributions that emphasize dominant classes. (11 sources)",
                "text": "\nTemperature scaling in the softmax function directly manipulates how probability mass is distributed across classes, which is crucial for effective knowledge transfer in distillation. When using standard softmax (T=1), well-trained models typically produce very peaked distributions with high confidence in the predicted class <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>. However, increasing the temperature parameter T>1 creates \"softer\" probability distributions that spread more evenly across classes, revealing the teacher model's uncertainty and confidence levels across all categories <Paper corpusId=\"227228186\" paperTitle=\"(Liu et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"243832889\" paperTitle=\"(Eyono et al., 2021)\" isShortName></Paper>.\n\nThis softening effect occurs because temperature scaling penalizes larger logit values more than smaller ones <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>. As temperature increases, the probability distribution becomes smoother, conveying more nuanced information about the interrelationships between different categories according to the teacher model <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>. For example, a truck image might have hard labels with 0.99 probability for the truck class and near-zero for others, but with higher temperatures, it might show softer logits like 0.65 for truck, 0.25 for car, and smaller values for other classes - revealing that trucks and cars share similarities <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>.\n\nConversely, lowering the temperature (T<1) creates \"sharper\" distributions that push the maximum probability closer to 1 and others closer to 0, resulting in more deterministic outputs <Paper corpusId=\"267535483\" paperTitle=\"(Al-Ahmadi, 2024)\" isShortName></Paper>. This can be beneficial in reinforcement learning contexts where the teacher's output represents expected returns for different actions, and a sharper distribution provides more unambiguous information for action selection <Paper corpusId=\"221284805\" paperTitle=\"(Jang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"1923568\" paperTitle=\"(Rusu et al., 2015)\" isShortName></Paper>.\n\nThe temperature parameter effectively controls the balance between true label knowledge and dark knowledge <Paper corpusId=\"274436184\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Lower temperatures make the distillation assign less weight to logits that are significantly smaller than the average, while larger temperatures pay more attention to the seemingly unimportant parts of the logit distribution <Paper corpusId=\"276725399\" paperTitle=\"(Xuan et al., 2025)\" isShortName></Paper>. This focus on the \"unimportant\" parts is actually valuable because the hard-target term in distillation already ensures the dominant part of the logit is correct, so capturing the fine-grained information in the remaining logits allows the student model to learn more subtle patterns from the teacher <Paper corpusId=\"276725399\" paperTitle=\"(Xuan et al., 2025)\" isShortName></Paper>.\n\nBy adjusting temperature, the entropy of the softmax output probability distribution can be increased, amplifying the information carried by negative labels <Paper corpusId=\"270377282\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. When both teacher and student models use the same temperature value during training, the student can effectively learn from the teacher's relative confidences across classes rather than just the hard predictions <Paper corpusId=\"267535483\" paperTitle=\"(Al-Ahmadi, 2024)\" isShortName></Paper>. This temperature-based smoothing enhances the efficacy of knowledge distillation by enabling the transfer of comprehensive information regarding class relationships, simplifying the learning process and potentially accelerating training <Paper corpusId=\"275993890\" paperTitle=\"(Murtada et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Thapa, 2022)",
                        "snippets": [
                            "The temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures."
                        ],
                        "paper": {
                            "corpus_id": 252968120,
                            "title": "On effects of Knowledge Distillation on Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2093329661",
                                    "name": "Sushil Thapa"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9638671875
                    },
                    {
                        "id": "(Liu et al._1, 2020)",
                        "snippets": [
                            "To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.[2])."
                        ],
                        "paper": {
                            "corpus_id": 227228186,
                            "title": "Bringing AI To Edge: From Deep Learning's Perspective",
                            "authors": [
                                {
                                    "authorId": "2115296404",
                                    "name": "Di Liu"
                                },
                                {
                                    "authorId": "2815982",
                                    "name": "Hao Kong"
                                },
                                {
                                    "authorId": "2041283357",
                                    "name": "Xiangzhong Luo"
                                },
                                {
                                    "authorId": "2248636",
                                    "name": "Weichen Liu"
                                },
                                {
                                    "authorId": "2062739126",
                                    "name": "Ravi Subramaniam"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neurocomputing",
                            "n_citations": 123
                        },
                        "score": 0.70458984375
                    },
                    {
                        "id": "(Eyono et al., 2021)",
                        "snippets": [
                            "Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1."
                        ],
                        "paper": {
                            "corpus_id": 243832889,
                            "title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family",
                            "authors": [
                                {
                                    "authorId": "35369096",
                                    "name": "Roy Henha Eyono"
                                },
                                {
                                    "authorId": "2264326",
                                    "name": "Fabio Maria Carlucci"
                                },
                                {
                                    "authorId": "27245978",
                                    "name": "P. Esperan\u00e7a"
                                },
                                {
                                    "authorId": "88739736",
                                    "name": "Binxin Ru"
                                },
                                {
                                    "authorId": "73007356",
                                    "name": "Phillip Torr"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.64404296875
                    },
                    {
                        "id": "(Xie et al., 2023)",
                        "snippets": [
                            "With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization."
                        ],
                        "paper": {
                            "corpus_id": 265384964,
                            "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2268047558",
                                    "name": "Jinzhou Xie"
                                },
                                {
                                    "authorId": "2268031861",
                                    "name": "Hongmin Zhao"
                                }
                            ],
                            "year": 2023,
                            "venue": "Fire",
                            "n_citations": 6
                        },
                        "score": 0.8193359375
                    },
                    {
                        "id": "(Al-Ahmadi, 2024)",
                        "snippets": [
                            "In higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better."
                        ],
                        "paper": {
                            "corpus_id": 267535483,
                            "title": "Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces",
                            "authors": [
                                {
                                    "authorId": "2283215857",
                                    "name": "Abdullah Al-Ahmadi"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.82568359375
                    },
                    {
                        "id": "(Jang et al., 2020)",
                        "snippets": [
                            "In the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space (Rusu et al., 2015). If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated."
                        ],
                        "paper": {
                            "corpus_id": 221284805,
                            "title": "Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems",
                            "authors": [
                                {
                                    "authorId": "2941497",
                                    "name": "Ingook Jang"
                                },
                                {
                                    "authorId": "2118020583",
                                    "name": "Hyunseok Kim"
                                },
                                {
                                    "authorId": "2115475643",
                                    "name": "Donghun Lee"
                                },
                                {
                                    "authorId": "120595891",
                                    "name": "Young-Sung Son"
                                },
                                {
                                    "authorId": "4328349",
                                    "name": "Seonghyun Kim"
                                }
                            ],
                            "year": 2020,
                            "venue": "IEEE Access",
                            "n_citations": 31
                        },
                        "score": 0.67529296875
                    },
                    {
                        "id": "(Rusu et al., 2015)",
                        "snippets": [
                            "Abstract: Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent."
                        ],
                        "paper": {
                            "corpus_id": 1923568,
                            "title": "Policy Distillation",
                            "authors": [
                                {
                                    "authorId": "2228824",
                                    "name": "Andrei A. Rusu"
                                },
                                {
                                    "authorId": "2016840",
                                    "name": "Sergio Gomez Colmenarejo"
                                },
                                {
                                    "authorId": "1854385",
                                    "name": "\u00c7aglar G\u00fcl\u00e7ehre"
                                },
                                {
                                    "authorId": "2755582",
                                    "name": "Guillaume Desjardins"
                                },
                                {
                                    "authorId": "2066516991",
                                    "name": "J. Kirkpatrick"
                                },
                                {
                                    "authorId": "1996134",
                                    "name": "Razvan Pascanu"
                                },
                                {
                                    "authorId": "3255983",
                                    "name": "Volodymyr Mnih"
                                },
                                {
                                    "authorId": "2645384",
                                    "name": "K. Kavukcuoglu"
                                },
                                {
                                    "authorId": "2315504",
                                    "name": "R. Hadsell"
                                }
                            ],
                            "year": 2015,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 696
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge."
                        ],
                        "paper": {
                            "corpus_id": 274436184,
                            "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation",
                            "authors": [
                                {
                                    "authorId": "2327960212",
                                    "name": "Chengyu Li"
                                },
                                {
                                    "authorId": "2320341",
                                    "name": "Debo Cheng"
                                },
                                {
                                    "authorId": "2152233401",
                                    "name": "Guixian Zhang"
                                },
                                {
                                    "authorId": "2316406036",
                                    "name": "Yi Li"
                                },
                                {
                                    "authorId": "2186753918",
                                    "name": "Shichao Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.88623046875
                    },
                    {
                        "id": "(Xuan et al., 2025)",
                        "snippets": [
                            "Knowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model."
                        ],
                        "paper": {
                            "corpus_id": 276725399,
                            "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
                            "authors": [
                                {
                                    "authorId": "2235060849",
                                    "name": "Hao Xuan"
                                },
                                {
                                    "authorId": "2348692399",
                                    "name": "Bokai Yang"
                                },
                                {
                                    "authorId": "2348481539",
                                    "name": "Xingyu Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.97607421875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels",
                            "The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,(Chen et al., 2023) . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function."
                        ],
                        "paper": {
                            "corpus_id": 270377282,
                            "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm",
                            "authors": [
                                {
                                    "authorId": "2305667661",
                                    "name": "Xiaolian Liu"
                                },
                                {
                                    "authorId": "2305658388",
                                    "name": "Shaopeng Gong"
                                },
                                {
                                    "authorId": "2305657615",
                                    "name": "Xiangxu Hua"
                                },
                                {
                                    "authorId": "2305821873",
                                    "name": "Taotao Chen"
                                },
                                {
                                    "authorId": "2306058720",
                                    "name": "Chunjiang Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 2
                        },
                        "score": 0.76171875
                    },
                    {
                        "id": "(Murtada et al., 2025)",
                        "snippets": [
                            "In the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training."
                        ],
                        "paper": {
                            "corpus_id": 275993890,
                            "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design",
                            "authors": [
                                {
                                    "authorId": "2342923579",
                                    "name": "Amna Murtada"
                                },
                                {
                                    "authorId": "88856585",
                                    "name": "Omnia Abdelrhman"
                                },
                                {
                                    "authorId": "30665494",
                                    "name": "T. Attia"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.654296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Dark Knowledge Transfer Process",
                "tldr": "The knowledge distillation process transfers dark knowledge from teacher to student through temperature-scaled softmax distributions and Kullback-Leibler divergence loss. This mechanism enables the student to learn not just correct classifications but also the nuanced inter-class relationships captured by the teacher. (5 sources)",
                "text": "\nThe dark knowledge transfer process begins with applying temperature scaling to both the teacher and student models' logits before computing softmax probabilities. This scaling mechanism is essential for revealing the hidden patterns and relationships the teacher model has learned during its training <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>. The temperature parameter \u03c4 (or T) effectively controls how much of this dark knowledge is exposed and transferred to the student model\u2014higher values produce softer distributions that better reveal which classes the teacher model considers similar to the predicted class <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nTo facilitate this knowledge transfer, the student model is trained to mimic the teacher's softened predictions through a Kullback-Leibler divergence loss term <Paper corpusId=\"253655030\" paperTitle=\"(Borza et al., 2022)\" isShortName></Paper>. This approach ensures that the student not only learns to predict the correct class but also internalizes the nuanced relationships between classes that the teacher has captured. Importantly, research indicates that using the same temperature value for both teacher and student models typically yields optimal results, creating a consistent framework for knowledge transfer <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of temperature scaling in the knowledge distillation process has been empirically demonstrated, with studies showing that this smoothing strategy can improve knowledge distillation accuracy by approximately 1.2% compared to distillation without temperature scaling <Paper corpusId=\"220280430\" paperTitle=\"(Pouransari et al., 2020)\" isShortName></Paper>. This improvement underscores the value of properly revealing and transferring dark knowledge through carefully controlled temperature parameters.\n\nThe temperature parameter selection is crucial for effective knowledge transfer. While higher temperatures produce softer probability distributions that encourage the student to learn nuanced relationships, excessively high values can result in overly smoothed distributions that may be less informative <Paper corpusId=\"268060109\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>. This creates a balancing act in the distillation process, where the temperature must be carefully tuned to maximize the transfer of meaningful dark knowledge while maintaining the informativeness of the distributions. Empirical experiments are typically conducted to determine the optimal temperature value, with research suggesting that effective values can range from 1 to 20 depending on the specific task and model architectures <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nSince its introduction in Hinton's original knowledge distillation work, this method of transferring knowledge through temperature-scaled softmax outputs has become a fundamental approach in model compression and knowledge transfer <Paper corpusId=\"271218098\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper>. The process allows smaller student models to benefit from the rich internal representations learned by larger teacher models, making advanced AI capabilities more accessible for deployment in resource-constrained environments.",
                "citations": [
                    {
                        "id": "(Amin et al., 2023)",
                        "snippets": [
                            "When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results."
                        ],
                        "paper": {
                            "corpus_id": 254910794,
                            "title": "Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation",
                            "authors": [
                                {
                                    "authorId": "2164150522",
                                    "name": "Saddam Al Amin"
                                },
                                {
                                    "authorId": "1806836",
                                    "name": "Md. Saddam Hossain Mukta"
                                },
                                {
                                    "authorId": "2197620015",
                                    "name": "Md. Sezan Mahmud Saikat"
                                },
                                {
                                    "authorId": "2197241772",
                                    "name": "Md. Ismail Hossain"
                                },
                                {
                                    "authorId": "7484275",
                                    "name": "Md. Adnanul Islam"
                                },
                                {
                                    "authorId": "2115615362",
                                    "name": "Mohiuddin Ahmed"
                                },
                                {
                                    "authorId": "25447315",
                                    "name": "S. Azam"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 2
                        },
                        "score": 0.61328125
                    },
                    {
                        "id": "(Borza et al., 2022)",
                        "snippets": [
                            "In the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions."
                        ],
                        "paper": {
                            "corpus_id": 253655030,
                            "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling",
                            "authors": [
                                {
                                    "authorId": "144583200",
                                    "name": "D. Borza"
                                },
                                {
                                    "authorId": "1821352",
                                    "name": "A. Darabant"
                                },
                                {
                                    "authorId": "74810447",
                                    "name": "Tudor Alexandru Ileni"
                                },
                                {
                                    "authorId": "150020994",
                                    "name": "Alexandru-Ion Marinescu"
                                }
                            ],
                            "year": 2022,
                            "venue": "Mathematics",
                            "n_citations": 2
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Pouransari et al., 2020)",
                        "snippets": [
                            "In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling)."
                        ],
                        "paper": {
                            "corpus_id": 220280430,
                            "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution",
                            "authors": [
                                {
                                    "authorId": "1842915",
                                    "name": "H. Pouransari"
                                },
                                {
                                    "authorId": "2577513",
                                    "name": "Oncel Tuzel"
                                }
                            ],
                            "year": 2020,
                            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                            "n_citations": 5
                        },
                        "score": 0.66650390625
                    },
                    {
                        "id": "(Cao et al., 2024)",
                        "snippets": [
                            "The choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer."
                        ],
                        "paper": {
                            "corpus_id": 268060109,
                            "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems",
                            "authors": [
                                {
                                    "authorId": "2259997326",
                                    "name": "Kangjie Cao"
                                },
                                {
                                    "authorId": "2288621104",
                                    "name": "Ting Zhang"
                                },
                                {
                                    "authorId": "2269063771",
                                    "name": "Jueqiao Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Scientific Reports",
                            "n_citations": 46
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Giakoumoglou et al., 2024)",
                        "snippets": [
                            "The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax."
                        ],
                        "paper": {
                            "corpus_id": 271218098,
                            "title": "Discriminative and Consistent Representation Distillation",
                            "authors": [
                                {
                                    "authorId": "2196360101",
                                    "name": "Nikolaos Giakoumoglou"
                                },
                                {
                                    "authorId": "2292259667",
                                    "name": "Tania Stathaki"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.7919921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Considerations for Temperature Values",
                "tldr": "Selecting appropriate temperature values is crucial for effective knowledge distillation, with empirical evidence suggesting optimal ranges between 2-20 depending on the specific task and model. While conventional knowledge distillation often uses T=4 as a starting point, practitioners should experiment with different values to maximize knowledge transfer. (1 source)",
                "text": "\nWhen implementing temperature scaling in knowledge distillation, choosing an appropriate temperature value is critical for maximizing the effectiveness of dark knowledge transfer. While there is no universal optimal temperature that works across all scenarios, empirical evidence suggests that certain ranges tend to be more effective than others. In conventional knowledge distillation approaches, setting the softmax temperature to approximately T = 4 has been shown to improve performance <Paper corpusId=\"277994315\" paperTitle=\"(Hasegawa et al., 2025)\" isShortName></Paper>.\n\nThe optimal temperature value typically depends on several factors, including the complexity of the task, the architecture of both teacher and student models, and the dataset characteristics <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Temperature values that are too low (close to 1) may not sufficiently soften the probability distributions to reveal meaningful dark knowledge, while excessively high values might over-smooth the distributions, diluting the useful information contained within them <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nPractitioners should adopt an experimental approach when determining the optimal temperature for their specific knowledge distillation task. This typically involves training student models with various temperature values (often ranging from 2 to 20) and comparing their performance on validation datasets <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Some researchers recommend starting with the conventional T = 4 setting and then exploring higher or lower values based on initial results <Paper corpusId=\"277994315\" paperTitle=\"(Hasegawa et al., 2025)\" isShortName></Paper>.\n\nIt's also worth noting that the temperature parameter used during the training phase of knowledge distillation does not affect the inference phase\u2014once training is complete, the student model typically uses the standard softmax function (T = 1) for making predictions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This ensures that the final model maintains appropriate confidence levels in its predictions while still benefiting from the rich inter-class relationships learned during temperature-scaled training.\n\nFor more complex distillation scenarios, some advanced approaches dynamically adjust the temperature parameter throughout the training process or use different temperatures for different layers or components of the model <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. However, for most practical applications, a carefully selected static temperature value is sufficient to achieve effective knowledge transfer from teacher to student models.",
                "citations": [
                    {
                        "id": "(Hasegawa et al., 2025)",
                        "snippets": [
                            "In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance."
                        ],
                        "paper": {
                            "corpus_id": 277994315,
                            "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification",
                            "authors": [
                                {
                                    "authorId": "2354312339",
                                    "name": "Tatsuhito Hasegawa"
                                },
                                {
                                    "authorId": "2348382817",
                                    "name": "Shunsuke Sakai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.83154296875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.119829
    }
}