{
    "query": "Does alignment training (such as SFT or RLHF) impart any new domain knowledge or reasoning abilities to large language models, or does it solely influence their output formats and response styles? Please include empirical studies (e.g., token distribution analyses, small SFT dataset experiments) that explicitly test this distinction.",
    "user_id": "lib_user",
    "task_id": "adaf32c0-60de-459c-9b5e-453c3efff54d",
    "timestamp": "2025-06-23T21:47:48.467419",
    "n_retrieval": 256,
    "n_retrieved": 256,
    "n_candidates": 35,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.329562,
    "decomposed_query": {
        "rewritten_query": "Effects of alignment training (SFT or RLHF) on domain knowledge or reasoning abilities versus output formats and response styles in large language models, with focus on empirical studies such as token distribution analyses and small SFT dataset experiments.",
        "keyword_query": "alignment training SFT RLHF domain knowledge reasoning abilities output formats response styles large language models empirical studies token distribution analyses small SFT dataset experiments",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010698,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 32,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.04072, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47447649",
                    "name": "Geyang Guo"
                },
                {
                    "authorId": "2266167155",
                    "name": "Ranchi Zhao"
                },
                {
                    "authorId": "1997234792",
                    "name": "Tianyi Tang"
                },
                {
                    "authorId": "2257376413",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ],
            "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
            "corpus_id": 265043685,
            "sentences": [
                {
                    "corpus_id": "265043685",
                    "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
                    "text": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
                    "score": 0.49751685922597244,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8515625
                },
                {
                    "corpus_id": "265043685",
                    "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
                    "text": "In this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. \n\nReinforcement learning from human feedback Large-scale pre-training empowers large language models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020;Kojima et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences. Consequently, this gives rise to unexpected behaviors like harmful or inaccurate information, and emphasizes the necessity to align language models with human preferences. The current mainstream approaches (Ouyang et al., 2022) to better harness the capabilities of LLMs include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To be specific, this involves three stages: firstly, using SFT to enable the model to better follow human instructions; subsequently, training a reward model (RM) using human preference data; and ultimately, tune the model to maximize the reward through the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm. Furthermore, there are works exploring enhancement for this process (Ramamurthy et al., 2022;Lightman et al., 2023;Lee et al., 2023). However, RLHF presents challenges due to complex coding and hyper-parameters selecting. Besides, it requires loading three to four models simultaneously, resulting in high memory usage. These challenges propel researchers to explore alternative approaches to align language models with human feedback. \n\nAlignment without reinforcement learning Several studies are based on the rationale that language models have already acquired comprehensive knowledge during the pre-training, and only high-quality supervised fine-tuning data is required for further tuning (Zhou et al., 2023).",
                    "score": 0.42391337713307947,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4413,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 162,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 744
                        },
                        {
                            "start": 745,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1485
                        },
                        {
                            "start": 1486,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1699
                        },
                        {
                            "start": 1702,
                            "end": 1979
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 361,
                            "end": 381,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 381,
                            "end": 401,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 779,
                            "end": 800,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1332,
                            "end": 1357,
                            "matchedPaperCorpusId": "252693405"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5322265625
                },
                {
                    "corpus_id": "265043685",
                    "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
                    "text": "Pre-trained large language models (LLMs) such as LLaMA (Touvron et al., 2023a) have shown remarkable potentials to solve various downstream tasks by mastering the universal pre-training task of next-token prediction. While after large-scale pre-training, it often needs subsequent tuning for enhancing and regulating the behaviors of LLMs. Two typical approaches are supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), which can largely improve LLMs in both task solving capacity and human alignment (Ouyang et al., 2022). \n\nDespite widely explored, SFT and RLHF have their own strengths and weaknesses. On the one hand, SFT is easy to implement and can effectively boost the general task solving abilities by instruction based eliciting (Wei et al., 2021;Ouyang et al., 2022;Chung et al., 2022), while it mainly imitates the behaviors of experts (essentially doing behavior clone (Wiseman & Rush, 2016)), which are demonstrated by the human annotators or powerful LLMs such as ChatGPT. Therefore, the SFT performance highly relies on high-quality demonstration data (Zhou et al., 2023), and might suffer from the huge distribution shifts between its outputs and imitated outputs (Zhang et al., 2019;Schulman, 2023;Zhao et al., 2023a). On the other hand, RLHF can better explore the semantic space of LLMs, and identify the optimal policy by encouraging good behaviors and discouraging bad behaviors during learning. However, it is very complicated to effectively implement, often suffering from training instability issues such as reward collapse (Song et al., 2023;Wolf et al., 2023). \n\nTo leverage the benefits of SFT and RLHF, several recent studies propose to develop alignment approaches without reinforcement learning (RL). These studies typically construct refined instruction data using methods such as quantile ranking (Lu et al., 2022) and rejection-sampling (Touvron et al., 2023b), and then follow or slightly modify the original SFT loss.",
                    "score": 0.5599054175763164,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 553
                        },
                        {
                            "start": 556,
                            "end": 634
                        },
                        {
                            "start": 635,
                            "end": 1017
                        },
                        {
                            "start": 1018,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1617
                        },
                        {
                            "start": 1620,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1983
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 531,
                            "end": 552,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 769,
                            "end": 787,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 787,
                            "end": 807,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 912,
                            "end": 934,
                            "matchedPaperCorpusId": "2783746"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52001953125
                }
            ],
            "relevance_judgement": 0.8515625,
            "relevance_judgment_input_expanded": "# Title: Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment\n# Venue: International Conference on Learning Representations\n# Authors: Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, Ji-rong Wen\n## Abstract\nAlignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.\n## INTRODUCTION\nPre-trained large language models (LLMs) such as LLaMA (Touvron et al., 2023a) have shown remarkable potentials to solve various downstream tasks by mastering the universal pre-training task of next-token prediction. While after large-scale pre-training, it often needs subsequent tuning for enhancing and regulating the behaviors of LLMs. Two typical approaches are supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), which can largely improve LLMs in both task solving capacity and human alignment (Ouyang et al., 2022). \n\nDespite widely explored, SFT and RLHF have their own strengths and weaknesses. On the one hand, SFT is easy to implement and can effectively boost the general task solving abilities by instruction based eliciting (Wei et al., 2021;Ouyang et al., 2022;Chung et al., 2022), while it mainly imitates the behaviors of experts (essentially doing behavior clone (Wiseman & Rush, 2016)), which are demonstrated by the human annotators or powerful LLMs such as ChatGPT. Therefore, the SFT performance highly relies on high-quality demonstration data (Zhou et al., 2023), and might suffer from the huge distribution shifts between its outputs and imitated outputs (Zhang et al., 2019;Schulman, 2023;Zhao et al., 2023a). On the other hand, RLHF can better explore the semantic space of LLMs, and identify the optimal policy by encouraging good behaviors and discouraging bad behaviors during learning. However, it is very complicated to effectively implement, often suffering from training instability issues such as reward collapse (Song et al., 2023;Wolf et al., 2023). \n\nTo leverage the benefits of SFT and RLHF, several recent studies propose to develop alignment approaches without reinforcement learning (RL). These studies typically construct refined instruction data using methods such as quantile ranking (Lu et al., 2022) and rejection-sampling (Touvron et al., 2023b), and then follow or slightly modify the original SFT loss.\n\n## RELATED WORK\nIn this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. \n\nReinforcement learning from human feedback Large-scale pre-training empowers large language models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020;Kojima et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences. Consequently, this gives rise to unexpected behaviors like harmful or inaccurate information, and emphasizes the necessity to align language models with human preferences. The current mainstream approaches (Ouyang et al., 2022) to better harness the capabilities of LLMs include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To be specific, this involves three stages: firstly, using SFT to enable the model to better follow human instructions; subsequently, training a reward model (RM) using human preference data; and ultimately, tune the model to maximize the reward through the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm. Furthermore, there are works exploring enhancement for this process (Ramamurthy et al., 2022;Lightman et al., 2023;Lee et al., 2023). However, RLHF presents challenges due to complex coding and hyper-parameters selecting. Besides, it requires loading three to four models simultaneously, resulting in high memory usage. These challenges propel researchers to explore alternative approaches to align language models with human feedback. \n\nAlignment without reinforcement learning Several studies are based on the rationale that language models have already acquired comprehensive knowledge during the pre-training, and only high-quality supervised fine-tuning data is required for further tuning (Zhou et al., 2023).",
            "reference_string": "[265043685 | Guo et al. | 2023 | Citations: 32]"
        },
        {
            "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 63,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.11407, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112075893",
                    "name": "Xikang Yang"
                },
                {
                    "authorId": "102993876",
                    "name": "Xuehai Tang"
                },
                {
                    "authorId": "2126086577",
                    "name": "Jizhong Han"
                },
                {
                    "authorId": "2241727804",
                    "name": "Songlin Hu"
                }
            ],
            "abstract": "The widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with human values. Existing jailbreak techniques reveal how this alignment can be compromised through specific prompts or adversarial suffixes. In this study, we introduce a new threat: LLMs' bias toward authority. While this inherent bias can improve the quality of outputs generated by LLMs, it also introduces a potential vulnerability, increasing the risk of producing harmful content. Notably, the biases in LLMs is the varying levels of trust given to different types of authoritative information in harmful queries. For example, malware development often favors trust GitHub. To better reveal the risks with LLM, we propose DarkCite, an adaptive authority citation matcher and generator designed for a black-box setting. DarkCite matches optimal citation types to specific risk types and generates authoritative citations relevant to harmful instructions, enabling more effective jailbreak attacks on aligned LLMs.Our experiments show that DarkCite achieves a higher attack success rate (e.g., LLama-2 at 76% versus 68%) than previous methods. To counter this risk, we propose an authenticity and harm verification defense strategy, raising the average defense pass rate (DPR) from 11% to 74%. More importantly, the ability to link citations to the content they encompass has become a foundational function in LLMs, amplifying the influence of LLMs' bias toward authority.",
            "corpus_id": 274131023,
            "sentences": [
                {
                    "corpus_id": "274131023",
                    "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models",
                    "text": "A large language model(LLM) is a typical autoregressive model designed to predict the probability of the next word in a vocabulary V, given a specific context x. Formally, the probability of generating a subsequent sentence can be expressed as: \n\nHere, P (y|x) represents the probability of the predicted sentence y given the context x. The context x = {x 1 , x 2 , . . . , x n } (where each x i \u2208 V) is the initial input prompt, while the sentence y = {y 1 , y 2 , . . . , y m } (where each y i \u2208 V) denotes the model's generated response. \n\nAligned LLMs have attracted significant attention as researchers strive to make their outputs align with human expectations, values, and safety guidelines. Foundational studies [7], [8] underscore the critical role of alignment in preventing harmful or undesirable behaviors, spurring the advancement of various alignment techniques. To enhance the safety of these models, two primary mechanisms have been established: harmfulness filtering and alignment training. \n\nHarmfulness Filter. This mechanism consists of filters designed to assess the safety of user inputs and the responses generated by LLMs. Commercial LLMs currently incorporate these filters to intercept potentially risky or harmful content in both user instructions and model outputs. Examples of such LLMs include ChatGPT [28], Gemini [29], Claude [30], and Bing AI [31], all of which have implemented these safety barriers. Numerous additional guardrail projects [32], [33], [34] focus on hazard detection, continuously working to enhance the safety and reliability of LLM systems. \n\nAlignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) [9]. SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks.",
                    "score": 0.4431914125237046,
                    "section_title": "Aligned LLMs",
                    "char_start_offset": 8359,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 244
                        },
                        {
                            "start": 247,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 373
                        },
                        {
                            "start": 374,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 540
                        },
                        {
                            "start": 543,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 876
                        },
                        {
                            "start": 877,
                            "end": 1007
                        },
                        {
                            "start": 1010,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1293
                        },
                        {
                            "start": 1294,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1592
                        },
                        {
                            "start": 1595,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1865
                        },
                        {
                            "start": 1866,
                            "end": 2036
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1480,
                            "end": 1484,
                            "matchedPaperCorpusId": "269101741"
                        },
                        {
                            "start": 1861,
                            "end": 1864,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79541015625
                }
            ],
            "relevance_judgement": 0.79541015625,
            "relevance_judgment_input_expanded": "# Title: The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models\n# Venue: arXiv.org\n# Authors: Xikang Yang, Xuehai Tang, Jizhong Han, Songlin Hu\n## Abstract\nThe widespread deployment of large language models (LLMs) across various domains has showcased their immense potential while exposing significant safety vulnerabilities. A major concern is ensuring that LLM-generated content aligns with human values. Existing jailbreak techniques reveal how this alignment can be compromised through specific prompts or adversarial suffixes. In this study, we introduce a new threat: LLMs' bias toward authority. While this inherent bias can improve the quality of outputs generated by LLMs, it also introduces a potential vulnerability, increasing the risk of producing harmful content. Notably, the biases in LLMs is the varying levels of trust given to different types of authoritative information in harmful queries. For example, malware development often favors trust GitHub. To better reveal the risks with LLM, we propose DarkCite, an adaptive authority citation matcher and generator designed for a black-box setting. DarkCite matches optimal citation types to specific risk types and generates authoritative citations relevant to harmful instructions, enabling more effective jailbreak attacks on aligned LLMs.Our experiments show that DarkCite achieves a higher attack success rate (e.g., LLama-2 at 76% versus 68%) than previous methods. To counter this risk, we propose an authenticity and harm verification defense strategy, raising the average defense pass rate (DPR) from 11% to 74%. More importantly, the ability to link citations to the content they encompass has become a foundational function in LLMs, amplifying the influence of LLMs' bias toward authority.\n## Aligned LLMs\nA large language model(LLM) is a typical autoregressive model designed to predict the probability of the next word in a vocabulary V, given a specific context x. Formally, the probability of generating a subsequent sentence can be expressed as: \n\nHere, P (y|x) represents the probability of the predicted sentence y given the context x. The context x = {x 1 , x 2 , . . . , x n } (where each x i \u2208 V) is the initial input prompt, while the sentence y = {y 1 , y 2 , . . . , y m } (where each y i \u2208 V) denotes the model's generated response. \n\nAligned LLMs have attracted significant attention as researchers strive to make their outputs align with human expectations, values, and safety guidelines. Foundational studies [7], [8] underscore the critical role of alignment in preventing harmful or undesirable behaviors, spurring the advancement of various alignment techniques. To enhance the safety of these models, two primary mechanisms have been established: harmfulness filtering and alignment training. \n\nHarmfulness Filter. This mechanism consists of filters designed to assess the safety of user inputs and the responses generated by LLMs. Commercial LLMs currently incorporate these filters to intercept potentially risky or harmful content in both user instructions and model outputs. Examples of such LLMs include ChatGPT [28], Gemini [29], Claude [30], and Bing AI [31], all of which have implemented these safety barriers. Numerous additional guardrail projects [32], [33], [34] focus on hazard detection, continuously working to enhance the safety and reliability of LLM systems. \n\nAlignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) [9]. SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks.",
            "reference_string": "[274131023 | Yang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Aligning Large Language Models through Synthetic Feedback",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 70,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.13735",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.13735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2829848",
                    "name": "Sungdong Kim"
                },
                {
                    "authorId": "152846184",
                    "name": "Sanghwan Bae"
                },
                {
                    "authorId": "51228826",
                    "name": "Jamin Shin"
                },
                {
                    "authorId": "2149085001",
                    "name": "Soyoung Kang"
                },
                {
                    "authorId": "10469987",
                    "name": "Donghyun Kwak"
                },
                {
                    "authorId": "31760501",
                    "name": "Kang Min Yoo"
                },
                {
                    "authorId": "4418074",
                    "name": "Minjoon Seo"
                }
            ],
            "abstract": "Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost",
            "corpus_id": 258841835,
            "sentences": [
                {
                    "corpus_id": "258841835",
                    "title": "Aligning Large Language Models through Synthetic Feedback",
                    "text": "Alignment learning has been an essential learning scheme to align the behaviors of large language models (LLMs) with human values like safety and truthfulness while following the intention of users accurately (Ouyang et al., 2022). Vanilla LLMsthose not aligned yet -could misunderstand user intentions or produce unsafe and inaccurate responses. Desirable human values such as helpfulness, harmlessness, or honesty can be defined, and human demonstrations with these values are then used for the alignment learning (Askell et al., 2021;Bai et al., 2022a). 1 The code is available at github.com/naver-ai/almost. Figure 1: A procedure of reward modeling through synthetic feedback. We assume that the response from a larger LLM with more and better demonstrations might be better overall. We train a reward model with synthetic comparisons generated top on the assumption. \n\nTypically, alignment learning consists of three stages: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a). \n\nHowever, the three-stage training recipe requires significant human effort, especially in the first two stages. More specifically, both the SFT and RM training stages must be provided with an abundance of high-quality human demonstrations and ranking datasets for obtaining models to facilitate RLHF. For instance, Ouyang et al. (2022) prepare and utilize 13k human demonstrations and 33k comparisons. \n\nOn the other hand, Self-Instruct (Wang et al., 2022) attempts to generate synthetic self-generated instruction datasets using in-context learning with a few seed demonstrations. Meanwhile, the release of LLaMA (Touvron et al., 2023) brings upon many open-sourced aligned LLMs trained on the outputs of proprietary LLMs or human-annotated instructions.",
                    "score": 0.43597683602808035,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 231
                        },
                        {
                            "start": 232,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 871
                        },
                        {
                            "start": 874,
                            "end": 1076
                        },
                        {
                            "start": 1079,
                            "end": 1190
                        },
                        {
                            "start": 1191,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1480
                        },
                        {
                            "start": 1483,
                            "end": 1660
                        },
                        {
                            "start": 1661,
                            "end": 1834
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76708984375
                }
            ],
            "relevance_judgement": 0.76708984375,
            "relevance_judgment_input_expanded": "# Title: Aligning Large Language Models through Synthetic Feedback\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, Minjoon Seo\n## Abstract\nAligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs. However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT. In this work, we propose a novel alignment learning framework with synthetic feedback not dependent on extensive human annotations and proprietary LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM to simulate high-quality demonstrations to train a supervised policy and further optimize the model with reinforcement learning. Our resulting model, Aligned Language Model with Synthetic Training dataset (ALMoST), outperforms recent open-sourced models, which are trained on the outputs of InstructGPT or human-annotated demonstrations, in alignment benchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2, 55.0% and 58.5% of the time, respectively. Further analyses demonstrate the efficacy and importance of synthetic feedback in our framework. The code is available at https://github.com/naver-ai/almost\n## Introduction\nAlignment learning has been an essential learning scheme to align the behaviors of large language models (LLMs) with human values like safety and truthfulness while following the intention of users accurately (Ouyang et al., 2022). Vanilla LLMsthose not aligned yet -could misunderstand user intentions or produce unsafe and inaccurate responses. Desirable human values such as helpfulness, harmlessness, or honesty can be defined, and human demonstrations with these values are then used for the alignment learning (Askell et al., 2021;Bai et al., 2022a). 1 The code is available at github.com/naver-ai/almost. Figure 1: A procedure of reward modeling through synthetic feedback. We assume that the response from a larger LLM with more and better demonstrations might be better overall. We train a reward model with synthetic comparisons generated top on the assumption. \n\nTypically, alignment learning consists of three stages: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a). \n\nHowever, the three-stage training recipe requires significant human effort, especially in the first two stages. More specifically, both the SFT and RM training stages must be provided with an abundance of high-quality human demonstrations and ranking datasets for obtaining models to facilitate RLHF. For instance, Ouyang et al. (2022) prepare and utilize 13k human demonstrations and 33k comparisons. \n\nOn the other hand, Self-Instruct (Wang et al., 2022) attempts to generate synthetic self-generated instruction datasets using in-context learning with a few seed demonstrations. Meanwhile, the release of LLaMA (Touvron et al., 2023) brings upon many open-sourced aligned LLMs trained on the outputs of proprietary LLMs or human-annotated instructions.",
            "reference_string": "[258841835 | Kim et al. | 2023 | Citations: 70]"
        },
        {
            "title": "Latent Distance Guided Alignment Training for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 16,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.06390, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2295750681",
                    "name": "Haotian Luo"
                }
            ],
            "abstract": "Ensuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.",
            "corpus_id": 269009422,
            "sentences": [
                {
                    "corpus_id": "269009422",
                    "title": "Latent Distance Guided Alignment Training for Large Language Models",
                    "text": "Over the past two years, LLMs have demonstrated strong performance.LLMs have shown remarkable performance in various fields of NLP, such as mathematical problem solving, summarization generation, reading comprehension, and open-ended question answering, achieving notable results.In order to align the behavior of LLMs with human expectations, such as adhering to facts and avoiding biases, and to better elicit their capabilities, such as mathematical reasoning, researchers have proposed alignment training methods, which typically involve a process requiring extensive manual annotation of data.Aligment training is typically employed after Supervised Fine-tune(SFT), with the most commonly used mainstream methods being Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO) (Rafailov et al., 2023).\n\nSince mainstream alignment training methods typically require extensive manual annotation, which is expensive to obtain, the pursuit of an alignment method that does not necessitate human annotation is becoming increasingly popular.To solve this challenging problem, there are currently some efforts aimed at avoiding manual annotation in alignment tasks.RLAIF (Lee et al., 2023) utilizes large language models to generate preference labels instead of human annotators and explores the direct utilization of language models to generate reward scores.SPIN (Chen et al., 2024) iteratively train a LLM to align on the SFT datasets through a self-play mechanism which shares a similar motivation with GAN (Goodfellow et al., 2020).Also, (Yuan et al., 2024) have studied Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training.\n\nIn the present study, we introduce a DPO-based novel approach termed LD-Align (Latent Distance Guided Alignment Training), aimed at iteratively aligning a fine-tuned LLM with a given highquality SFT dataset without any additional human annotation or reliance on a more powerful LLM for support.Within this framework, we consider samples sourced from the SFT dataset as golden labels, contrasting with those generated by the model, which we categorize as dispreferred samples.",
                    "score": 0.5615104627382355,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 67,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 598
                        },
                        {
                            "start": 598,
                            "end": 839
                        },
                        {
                            "start": 841,
                            "end": 1073
                        },
                        {
                            "start": 1073,
                            "end": 1196
                        },
                        {
                            "start": 1196,
                            "end": 1391
                        },
                        {
                            "start": 1391,
                            "end": 1568
                        },
                        {
                            "start": 1568,
                            "end": 1751
                        },
                        {
                            "start": 1753,
                            "end": 2047
                        },
                        {
                            "start": 2047,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 815,
                            "end": 838,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 1542,
                            "end": 1567,
                            "matchedPaperCorpusId": "1033682"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.712890625
                }
            ],
            "relevance_judgement": 0.712890625,
            "relevance_judgment_input_expanded": "# Title: Latent Distance Guided Alignment Training for Large Language Models\n# Venue: arXiv.org\n# Authors: Haotian Luo\n## Abstract\nEnsuring alignment with human preferences is a crucial characteristic of large language models (LLMs). Presently, the primary alignment methods, RLHF and DPO, require extensive human annotation, which is expensive despite their efficacy. The significant expenses associated with current alignment techniques motivate researchers to investigate the development of annotation-free alignment training methods. In pursuit of improved alignment without relying on external annotation, we introduce Latent Distance Guided Alignment Training (LD-Align). This approach seeks to align the model with a high-quality supervised fine-tune dataset using guidance from a latent space. The latent space is generated through sample reconstruction, akin to auto-encoding. Consequently, we utilize the distance between sample pairs in the latent space to guide DPO-based alignment training. Extensive experimentation and evaluation show the efficacy of our proposed method in achieving notable alignment.\n## Introduction\nOver the past two years, LLMs have demonstrated strong performance.LLMs have shown remarkable performance in various fields of NLP, such as mathematical problem solving, summarization generation, reading comprehension, and open-ended question answering, achieving notable results.In order to align the behavior of LLMs with human expectations, such as adhering to facts and avoiding biases, and to better elicit their capabilities, such as mathematical reasoning, researchers have proposed alignment training methods, which typically involve a process requiring extensive manual annotation of data.Aligment training is typically employed after Supervised Fine-tune(SFT), with the most commonly used mainstream methods being Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO) (Rafailov et al., 2023).\n\nSince mainstream alignment training methods typically require extensive manual annotation, which is expensive to obtain, the pursuit of an alignment method that does not necessitate human annotation is becoming increasingly popular.To solve this challenging problem, there are currently some efforts aimed at avoiding manual annotation in alignment tasks.RLAIF (Lee et al., 2023) utilizes large language models to generate preference labels instead of human annotators and explores the direct utilization of language models to generate reward scores.SPIN (Chen et al., 2024) iteratively train a LLM to align on the SFT datasets through a self-play mechanism which shares a similar motivation with GAN (Goodfellow et al., 2020).Also, (Yuan et al., 2024) have studied Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training.\n\nIn the present study, we introduce a DPO-based novel approach termed LD-Align (Latent Distance Guided Alignment Training), aimed at iteratively aligning a fine-tuned LLM with a given highquality SFT dataset without any additional human annotation or reliance on a more powerful LLM for support.Within this framework, we consider samples sourced from the SFT dataset as golden labels, contrasting with those generated by the model, which we categorize as dispreferred samples.",
            "reference_string": "[269009422 | Luo | 2024 | Citations: 0]"
        },
        {
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03717, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345822162",
                    "name": "Mohit Raghavendra"
                },
                {
                    "authorId": "2151210591",
                    "name": "Vaskar Nath"
                },
                {
                    "authorId": "2265402399",
                    "name": "Sean M. Hendryx"
                }
            ],
            "abstract": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
            "corpus_id": 273186633,
            "sentences": [
                {
                    "corpus_id": "273186633",
                    "title": "Revisiting the Superficial Alignment Hypothesis",
                    "text": "Large Language Models (LLMs) based on the Transformer architecture have achieved state-of-the-art performance on tasks that involve instruction following, problem-solving, and reasoning (Achiam et al., 2023;Dubey et al., 2024;Vaswani et al., 2017). The standard pipeline for building LLMs powered applications involves unsupervised training of a model on a giant corpus of data to gain general language understanding capability, referred to as pre-training (Brown et al., 2020;Radford et al., 2019). The model is further improved using post-training, which involves finetuning it to excel at a particular domain or behave like a helpful chatbot. This process is also referred to as alignment. The predominant way to do this is through Supervised Finetuning (SFT) where the language model is provided with a prompt, and the model is finetuned to respond to the task (Wei et al., 2022). An additional step is Reinforcement Learning through Human Feedback (RLHF) where a model is trained using reinforcement learning to generate human-preferred responses, by being rewarded for good responses and penalized for bad responses (Ouyang et al., 2022). \n\nTo achieve the post-training goal of responding appropriately to various user queries, LLMs need to develop several task-specific capabilities, like mathematics, reasoning, utilizing knowledge, and tool use. To teach a model these capabilities, model builders collect human-annotated or synthetically generated data and finetune the model to obtain the desired behavior. Since data collection at scale is labor and cost-intensive, it is essential to understand the qualitative and quantitative value of obtaining additional post-training data. Studies like LIMA (Zhou et al., 2024) have hypothesized that post-training alignment is all about learning the style and format of the desired behavior. Specifically, it puts forward the Superficial Alignment Hypothesis, whose claims are: \n\n\u2022 C1: A model's knowledge is learned entirely during pre-training. \n\n\u2022 C3: Post-training is largely about style and doesn't does not teach a model new capabilities. \n\n\u2022 C2: A small number of examples can saturate a model's performance for a given task.",
                    "score": 0.4840911339453103,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 499
                        },
                        {
                            "start": 500,
                            "end": 645
                        },
                        {
                            "start": 646,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 884
                        },
                        {
                            "start": 885,
                            "end": 1144
                        },
                        {
                            "start": 1147,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1843
                        },
                        {
                            "start": 1844,
                            "end": 1929
                        },
                        {
                            "start": 1932,
                            "end": 1998
                        },
                        {
                            "start": 2001,
                            "end": 2096
                        },
                        {
                            "start": 2099,
                            "end": 2184
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 226,
                            "end": 247,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 457,
                            "end": 477,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 477,
                            "end": 498,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 865,
                            "end": 883,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 1122,
                            "end": 1143,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68896484375
                }
            ],
            "relevance_judgement": 0.68896484375,
            "relevance_judgment_input_expanded": "# Title: Revisiting the Superficial Alignment Hypothesis\n# Venue: arXiv.org\n# Authors: Mohit Raghavendra, Vaskar Nath, Sean M. Hendryx\n## Abstract\nThe Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.\n## Introduction\nLarge Language Models (LLMs) based on the Transformer architecture have achieved state-of-the-art performance on tasks that involve instruction following, problem-solving, and reasoning (Achiam et al., 2023;Dubey et al., 2024;Vaswani et al., 2017). The standard pipeline for building LLMs powered applications involves unsupervised training of a model on a giant corpus of data to gain general language understanding capability, referred to as pre-training (Brown et al., 2020;Radford et al., 2019). The model is further improved using post-training, which involves finetuning it to excel at a particular domain or behave like a helpful chatbot. This process is also referred to as alignment. The predominant way to do this is through Supervised Finetuning (SFT) where the language model is provided with a prompt, and the model is finetuned to respond to the task (Wei et al., 2022). An additional step is Reinforcement Learning through Human Feedback (RLHF) where a model is trained using reinforcement learning to generate human-preferred responses, by being rewarded for good responses and penalized for bad responses (Ouyang et al., 2022). \n\nTo achieve the post-training goal of responding appropriately to various user queries, LLMs need to develop several task-specific capabilities, like mathematics, reasoning, utilizing knowledge, and tool use. To teach a model these capabilities, model builders collect human-annotated or synthetically generated data and finetune the model to obtain the desired behavior. Since data collection at scale is labor and cost-intensive, it is essential to understand the qualitative and quantitative value of obtaining additional post-training data. Studies like LIMA (Zhou et al., 2024) have hypothesized that post-training alignment is all about learning the style and format of the desired behavior. Specifically, it puts forward the Superficial Alignment Hypothesis, whose claims are: \n\n\u2022 C1: A model's knowledge is learned entirely during pre-training. \n\n\u2022 C3: Post-training is largely about style and doesn't does not teach a model new capabilities. \n\n\u2022 C2: A small number of examples can saturate a model's performance for a given task.",
            "reference_string": "[273186633 | Raghavendra et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 95,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323786965",
                    "name": "\u00c1ngela L\u00f3pez-Cardona"
                },
                {
                    "authorId": "2309479586",
                    "name": "Carlos Segura"
                },
                {
                    "authorId": "2257295049",
                    "name": "Alexandros Karatzoglou"
                },
                {
                    "authorId": "2330844467",
                    "name": "Sergi Abadal"
                },
                {
                    "authorId": "2309480030",
                    "name": "Ioannis Arapakis"
                }
            ],
            "abstract": "Advancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.",
            "corpus_id": 273026214,
            "sentences": [
                {
                    "corpus_id": "273026214",
                    "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
                    "text": "2.1 Large Language Models-Human Alignment \n\nLLMs-Human Alignment typically involves training LLMs3 on datasets curated by humans (learning from human feedback data) [Ouyang et al., 2024]. This can be achieved through Supervised Fine-Tuning (SFT), where the model is trained on pairs of prompts (x) and corresponding human-generated responses (y) [Liu et al., 2024]. Alternatively, alignment can be pursued via preference optimization, using a human preference dataset that differentiates between a better response (y w ) and a worse one (y l ) for the same prompt (x): \n\nw , y \n\nTo this day, RLHF [Ouyang et al., 2024] remains the most popular technique used in state-of-the-art LLMs like GPT-4 [OpenAI, 2023], Claude [Bai et al., 2022a], Bard [Google, 2023], and Llama 2-Chat [Touvron et al., 2023]. Different implementations of RLHF can vary in terms of data collection, training processes, and choice of RL algorithms. Typically, RLHF [Ouyang et al., 2024] involves three main steps: (1) collecting feedback, (2) training a RM based on that feedback, and (3) optimising the LLMs using RL techniques, such as Proximal Policy Optimization (PPO) Schulman et al. [2017]. Since RLHF was first introduced, several advancements have been made, including fine-grained reward systems [Bai et al., 2022a, Wu et al., 2023a, Dong et al., 2023a, Wang et al., 2023c, 2024b], or replaced the original PPO algorithm with other RL techniques [Wu et al., 2023b]. \n\nAn alternative to RLHF is DPO [Rafailov et al., 2023], which employs an offline RL approach to optimize language models based on preference data, without the need for a separate RM.",
                    "score": 0.47173132798796935,
                    "section_title": "Preliminaries",
                    "char_start_offset": 5680,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 41
                        },
                        {
                            "start": 44,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 365
                        },
                        {
                            "start": 366,
                            "end": 568
                        },
                        {
                            "start": 571,
                            "end": 576
                        },
                        {
                            "start": 579,
                            "end": 800
                        },
                        {
                            "start": 801,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1169
                        },
                        {
                            "start": 1170,
                            "end": 1447
                        },
                        {
                            "start": 1450,
                            "end": 1631
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 165,
                            "end": 186,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 597,
                            "end": 618,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 938,
                            "end": 959,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68603515625
                }
            ],
            "relevance_judgement": 0.68603515625,
            "relevance_judgment_input_expanded": "# Title: Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: \u00c1ngela L\u00f3pez-Cardona, Carlos Segura, Alexandros Karatzoglou, Sergi Abadal, Ioannis Arapakis\n## Abstract\nAdvancements in Natural Language Processing (NLP), have led to the emergence of Large Language Models (LLMs) such as GPT, Llama, Claude, and Gemini, which excel across a range of tasks but require extensive fine-tuning to align their outputs with human expectations. A widely used method for achieving this alignment is Reinforcement Learning from Human Feedback (RLHF), which, despite its success, faces challenges in accurately modelling human preferences. In this paper, we introduce GazeReward, a novel framework that integrates implicit feedback -- and specifically eye-tracking (ET) data -- into the Reward Model (RM). In addition, we explore how ET-based features can provide insights into user preferences. Through ablation studies we test our framework with different integration methods, LLMs, and ET generator models, demonstrating that our approach significantly improves the accuracy of the RM on established human preference datasets. This work advances the ongoing discussion on optimizing AI alignment with human values, exploring the potential of cognitive data for shaping future NLP research.\n## Preliminaries\n2.1 Large Language Models-Human Alignment \n\nLLMs-Human Alignment typically involves training LLMs3 on datasets curated by humans (learning from human feedback data) [Ouyang et al., 2024]. This can be achieved through Supervised Fine-Tuning (SFT), where the model is trained on pairs of prompts (x) and corresponding human-generated responses (y) [Liu et al., 2024]. Alternatively, alignment can be pursued via preference optimization, using a human preference dataset that differentiates between a better response (y w ) and a worse one (y l ) for the same prompt (x): \n\nw , y \n\nTo this day, RLHF [Ouyang et al., 2024] remains the most popular technique used in state-of-the-art LLMs like GPT-4 [OpenAI, 2023], Claude [Bai et al., 2022a], Bard [Google, 2023], and Llama 2-Chat [Touvron et al., 2023]. Different implementations of RLHF can vary in terms of data collection, training processes, and choice of RL algorithms. Typically, RLHF [Ouyang et al., 2024] involves three main steps: (1) collecting feedback, (2) training a RM based on that feedback, and (3) optimising the LLMs using RL techniques, such as Proximal Policy Optimization (PPO) Schulman et al. [2017]. Since RLHF was first introduced, several advancements have been made, including fine-grained reward systems [Bai et al., 2022a, Wu et al., 2023a, Dong et al., 2023a, Wang et al., 2023c, 2024b], or replaced the original PPO algorithm with other RL techniques [Wu et al., 2023b]. \n\nAn alternative to RLHF is DPO [Rafailov et al., 2023], which employs an offline RL approach to optimize language models based on preference data, without the need for a separate RM.",
            "reference_string": "[273026214 | Lopez-Cardona et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding",
            "venue": "",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09300, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2121536275",
                    "name": "Yu Fei"
                },
                {
                    "authorId": "1899492908",
                    "name": "Yasaman Razeghi"
                },
                {
                    "authorId": "2299171638",
                    "name": "Sameer Singh"
                }
            ],
            "abstract": "Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .",
            "corpus_id": 273346831,
            "sentences": [
                {
                    "corpus_id": "273346831",
                    "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding",
                    "text": "Large language models (LLMs) pre-trained on massive text corpora possess broad general knowledge, yet they often struggle to produce responses aligned with user instructions without additional fine-tuning. As a result, alignment1 , such as instruction tuning (Wei et al., 2022a) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a), have become essential for developing useful LLMs like GPT-4 (Hurst et al., 2024). However, the current training pipelines require separate alignment tuning for every model size within each model family. \n\nIn practice, aligning the largest models leads to substantial computational overhead (e.g., the RLHF stage of Tulu 3 405B (Lambert et al., 2025) takes 11,776 H100 GPU hours), impeding the rapid iteration and deployment of new model families. \n\nRecent studies (Zhou et al., 2024;Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, Lin et al. (2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment. These findings raise a natural question: If the aligned models differ from the base models only at a few, select tokens, is it necessary to train large aligned models? \n\nIn this work, we propose NUDGING, a simple, training-free guided decoding algorithm that aligns any base model at inference time by injecting a few alignment tokens from a small aligned model. Our key insight is that base models show high uncertainty on alignment-related tokens-i.e., places where base and aligned models disagree. Leveraging this observation, NUDGING employs a small aligned model to generate nudging tokens that guide a large base model's output toward desired directions whenever the base model's top-1 token probability is below a certain threshold.",
                    "score": 0.446102763551339,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 205
                        },
                        {
                            "start": 206,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 576
                        },
                        {
                            "start": 579,
                            "end": 820
                        },
                        {
                            "start": 823,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1361
                        },
                        {
                            "start": 1364,
                            "end": 1556
                        },
                        {
                            "start": 1557,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1934
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 333,
                            "end": 354,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 838,
                            "end": 857,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 1073,
                            "end": 1090,
                            "matchedPaperCorpusId": "265608902"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68310546875
                }
            ],
            "relevance_judgement": 0.68310546875,
            "relevance_judgment_input_expanded": "# Title: Nudging: Inference-time Alignment of LLMs via Guided Decoding\n# Venue: \n# Authors: Yu Fei, Yasaman Razeghi, Sameer Singh\n## Abstract\nLarge language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .\n## Introduction\nLarge language models (LLMs) pre-trained on massive text corpora possess broad general knowledge, yet they often struggle to produce responses aligned with user instructions without additional fine-tuning. As a result, alignment1 , such as instruction tuning (Wei et al., 2022a) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a), have become essential for developing useful LLMs like GPT-4 (Hurst et al., 2024). However, the current training pipelines require separate alignment tuning for every model size within each model family. \n\nIn practice, aligning the largest models leads to substantial computational overhead (e.g., the RLHF stage of Tulu 3 405B (Lambert et al., 2025) takes 11,776 H100 GPU hours), impeding the rapid iteration and deployment of new model families. \n\nRecent studies (Zhou et al., 2024;Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, Lin et al. (2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment. These findings raise a natural question: If the aligned models differ from the base models only at a few, select tokens, is it necessary to train large aligned models? \n\nIn this work, we propose NUDGING, a simple, training-free guided decoding algorithm that aligns any base model at inference time by injecting a few alignment tokens from a small aligned model. Our key insight is that base models show high uncertainty on alignment-related tokens-i.e., places where base and aligned models disagree. Leveraging this observation, NUDGING employs a small aligned model to generate nudging tokens that guide a large base model's output toward desired directions whenever the base model's top-1 token probability is below a certain threshold.",
            "reference_string": "[273346831 | Fei et al. | 2024 | Citations: 0]"
        },
        {
            "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303638132",
                    "name": "Yuzi Yan"
                },
                {
                    "authorId": "2188993538",
                    "name": "Yibo Miao"
                },
                {
                    "authorId": "47785961",
                    "name": "J. Li"
                },
                {
                    "authorId": "2303417197",
                    "name": "Yipin Zhang"
                },
                {
                    "authorId": "2305675512",
                    "name": "Jian Xie"
                },
                {
                    "authorId": "2295008708",
                    "name": "Zhijie Deng"
                },
                {
                    "authorId": "2303454212",
                    "name": "Dong Yan"
                }
            ],
            "abstract": "Aligning large language models (LLMs) with human preferences has gained significant attention, with Proximal Policy Optimization (PPO) as a standard yet computationally expensive method and Direct Preference Optimization (DPO) as a more efficient alternative. While DPO offers simplicity, it remains underutilized in state-of-the-art LLMs, suggesting potential limitations. In this work, we revisit DPO, analyzing its theoretical foundations and empirical performance to bridge this gap. We identify three key properties, termed 3D properties, that emerge from DPO's learning process: Drastic drop in rejected response likelihood, Degradation into response suppression, and Dispersion effect on unseen responses. We show that these issues arise from DPO's optimization dynamics, where the interaction between chosen and rejected response gradients leads to instability. Our findings are supported by experiments on both a controlled toy model and real-world LLM tasks, including mathematical problem-solving and instruction following. To address these challenges, we propose simple regularization techniques that improve training stability and performance. Additionally, we examine how preference data distribution impacts DPO's effectiveness, offering insights into how alignment models handle out-of-domain (OOD) data. Our work connects these observations to broader research and provides a theoretical explanation for DPO's limitations. We hope these insights will guide future advancements in reward-model-free preference learning, bringing it closer to reward-model-based approaches.",
            "corpus_id": 270380285,
            "sentences": [
                {
                    "corpus_id": "270380285",
                    "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward",
                    "text": "Large language models (LLMs) trained on extensive datasets have shown outstanding performance across diverse tasks and domains [28,10,16,35].Various techniques for fine-tuning LLMs have been developed, including the well-known Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) [1,28].SFT focuses on tailoring the LLMs' responses for specific tasks directly with labeled data, whereas RLHF improves LLMs through feedback data reflecting human preferences.In particular, RLHF is pushing the application boundaries of both closedsource [20,3,27] and open-source LLMs [28,33], due to the necessity for polishing the value, fairness, and helpfulness of LLMs in practical scenarios [38,25,21].\n\nExisting RLHF methods can be majorly categorized into two classes based on whether the reward signal is explicitly modeled.Reward-based alignment pioneered by OpenAI [21,1,28] first trains a reward model from user preferences, typically through Maximum Likelihood Estimation (MLE), and then leverages actor-critic algorithms such as Proximal Policy Optimization (PPO) [24] to tune the SFT model to realize alignment.This approach often requires substantial computational resources and suffers from sample inefficiency [9].Conversely, another class of methods, known as reward-free alignment, such as Direct Preference Optimization (DPO) [23], Identity Preference Optimization (IPO) [4], and Sequence Likelihood Calibration (SLiC) [36], do not rely on an extra reward model.These approaches offer a more resource-efficient alternative by optimizing the policy directly from preferences, therefore attracting much attention from the academic society.In this work, we commence our analysis with the vanilla DP as a case study, subsequently extending our findings to encompass broader reward-free alignment strategies.\n\nDespite the simplicity and promise of DPO, a variety of phenomena that cannot be clearly understood or explained have been observed and reported in practice.",
                    "score": 0.4340337045178865,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 141,
                            "end": 317
                        },
                        {
                            "start": 317,
                            "end": 487
                        },
                        {
                            "start": 487,
                            "end": 720
                        },
                        {
                            "start": 722,
                            "end": 845
                        },
                        {
                            "start": 845,
                            "end": 1138
                        },
                        {
                            "start": 1138,
                            "end": 1244
                        },
                        {
                            "start": 1244,
                            "end": 1495
                        },
                        {
                            "start": 1495,
                            "end": 1670
                        },
                        {
                            "start": 1670,
                            "end": 1836
                        },
                        {
                            "start": 1838,
                            "end": 1995
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 134,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 713,
                            "end": 716,
                            "matchedPaperCorpusId": "221665105"
                        },
                        {
                            "start": 716,
                            "end": 719,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 888,
                            "end": 892,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 1359,
                            "end": 1363,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 1404,
                            "end": 1407,
                            "matchedPaperCorpusId": "264288854"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67431640625
                }
            ],
            "relevance_judgement": 0.67431640625,
            "relevance_judgment_input_expanded": "# Title: 3D-Properties: Identifying Challenges in DPO and Charting a Path Forward\n# Venue: International Conference on Learning Representations\n# Authors: Yuzi Yan, Yibo Miao, J. Li, Yipin Zhang, Jian Xie, Zhijie Deng, Dong Yan\n## Abstract\nAligning large language models (LLMs) with human preferences has gained significant attention, with Proximal Policy Optimization (PPO) as a standard yet computationally expensive method and Direct Preference Optimization (DPO) as a more efficient alternative. While DPO offers simplicity, it remains underutilized in state-of-the-art LLMs, suggesting potential limitations. In this work, we revisit DPO, analyzing its theoretical foundations and empirical performance to bridge this gap. We identify three key properties, termed 3D properties, that emerge from DPO's learning process: Drastic drop in rejected response likelihood, Degradation into response suppression, and Dispersion effect on unseen responses. We show that these issues arise from DPO's optimization dynamics, where the interaction between chosen and rejected response gradients leads to instability. Our findings are supported by experiments on both a controlled toy model and real-world LLM tasks, including mathematical problem-solving and instruction following. To address these challenges, we propose simple regularization techniques that improve training stability and performance. Additionally, we examine how preference data distribution impacts DPO's effectiveness, offering insights into how alignment models handle out-of-domain (OOD) data. Our work connects these observations to broader research and provides a theoretical explanation for DPO's limitations. We hope these insights will guide future advancements in reward-model-free preference learning, bringing it closer to reward-model-based approaches.\n## Introduction\nLarge language models (LLMs) trained on extensive datasets have shown outstanding performance across diverse tasks and domains [28,10,16,35].Various techniques for fine-tuning LLMs have been developed, including the well-known Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) [1,28].SFT focuses on tailoring the LLMs' responses for specific tasks directly with labeled data, whereas RLHF improves LLMs through feedback data reflecting human preferences.In particular, RLHF is pushing the application boundaries of both closedsource [20,3,27] and open-source LLMs [28,33], due to the necessity for polishing the value, fairness, and helpfulness of LLMs in practical scenarios [38,25,21].\n\nExisting RLHF methods can be majorly categorized into two classes based on whether the reward signal is explicitly modeled.Reward-based alignment pioneered by OpenAI [21,1,28] first trains a reward model from user preferences, typically through Maximum Likelihood Estimation (MLE), and then leverages actor-critic algorithms such as Proximal Policy Optimization (PPO) [24] to tune the SFT model to realize alignment.This approach often requires substantial computational resources and suffers from sample inefficiency [9].Conversely, another class of methods, known as reward-free alignment, such as Direct Preference Optimization (DPO) [23], Identity Preference Optimization (IPO) [4], and Sequence Likelihood Calibration (SLiC) [36], do not rely on an extra reward model.These approaches offer a more resource-efficient alternative by optimizing the policy directly from preferences, therefore attracting much attention from the academic society.In this work, we commence our analysis with the vanilla DP as a case study, subsequently extending our findings to encompass broader reward-free alignment strategies.\n\nDespite the simplicity and promise of DPO, a variety of phenomena that cannot be clearly understood or explained have been observed and reported in practice.",
            "reference_string": "[270380285 | Yan et al. | 2024 | Citations: 13]"
        },
        {
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.01525, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "122045993",
                    "name": "Sheng-Chieh Lin"
                },
                {
                    "authorId": "2299485255",
                    "name": "Luyu Gao"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "2266752758",
                    "name": "Wenhan Xiong"
                },
                {
                    "authorId": "2273564585",
                    "name": "Jimmy Lin"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "2292024725",
                    "name": "Xilun Chen"
                }
            ],
            "abstract": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
            "corpus_id": 269502676,
            "sentences": [
                {
                    "corpus_id": "269502676",
                    "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
                    "text": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
                    "score": 0.4591106771718589,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67138671875
                }
            ],
            "relevance_judgement": 0.67138671875,
            "relevance_judgment_input_expanded": "# Title: FLAME: Factuality-Aware Alignment for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Sheng-Chieh Lin, Luyu Gao, Barlas O\u011fuz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, Xilun Chen\n## Abstract\nAlignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.\n",
            "reference_string": "[269502676 | Lin et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 99,
            "citation_count": 68,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.09283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284178258",
                    "name": "Zhichen Dong"
                },
                {
                    "authorId": "2254279326",
                    "name": "Zhanhui Zhou"
                },
                {
                    "authorId": "2268678836",
                    "name": "Chao Yang"
                },
                {
                    "authorId": "2254280929",
                    "name": "Jing Shao"
                },
                {
                    "authorId": "2268675804",
                    "name": "Yu Qiao"
                }
            ],
            "abstract": "Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.",
            "corpus_id": 267658120,
            "sentences": [
                {
                    "corpus_id": "267658120",
                    "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
                    "text": "At the core of defenses lies alignment, which involves fine-tuning pre-trained models to enhance their internal safety capabilities. In this section, we introduce various alignment algorithms and emphasize the data specifically designed to align models for improved safety. \n\nAlignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations. RLHF (Stiennon et al., 2020;Ouyang et al., 2022) utilizes human feedback and preferences to enhance the capabilities of LLMs, and DPO (Rafailov et al., 2023) simplifies the training process of RLHF by avoiding the need for a reward model. Methods like RLHF and DPO typically optimize a homogeneous and static objective based on human feedback, which is often a weighted combination of different objectives. To achieve joint optimization of multiple objectives (e.g., safety, helpfulness, and honesty) with customization according to specific scenarios, Multi-Objective RLHF (Dai et al., 2023;Ji et al., 2023;Wu et al., 2023c) extends RLHF by introducing fine-grained objective functions to enable trade-offs between safety and other goals such as helpfulness. Meanwhile, MODPO (Zhou et al., 2023b) builds upon RL-free DPO and enables joint optimization of multiple objectives. \n\nAlignment data. Based on the type of data used, data utilization can be divided into two categories: demonstration data for SFT and preference data for preference optimization approaches like DPO. As mentioned above, SFT utilizes high-quality demonstration data, where each question is associated with a single answer. Considering that SFT aims to maximize or minimize the generation probability on this data, selecting appropriate data becomes crucial.",
                    "score": 0.41890191626764206,
                    "section_title": "LLM Safety Alignment",
                    "char_start_offset": 16767,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 132
                        },
                        {
                            "start": 133,
                            "end": 273
                        },
                        {
                            "start": 276,
                            "end": 297
                        },
                        {
                            "start": 298,
                            "end": 420
                        },
                        {
                            "start": 421,
                            "end": 633
                        },
                        {
                            "start": 634,
                            "end": 743
                        },
                        {
                            "start": 744,
                            "end": 982
                        },
                        {
                            "start": 983,
                            "end": 1150
                        },
                        {
                            "start": 1151,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1620
                        },
                        {
                            "start": 1623,
                            "end": 1638
                        },
                        {
                            "start": 1639,
                            "end": 1819
                        },
                        {
                            "start": 1820,
                            "end": 1941
                        },
                        {
                            "start": 1942,
                            "end": 2076
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 749,
                            "end": 772,
                            "matchedPaperCorpusId": "221665105"
                        },
                        {
                            "start": 772,
                            "end": 792,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65576171875
                }
            ],
            "relevance_judgement": 0.65576171875,
            "relevance_judgment_input_expanded": "# Title: Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao\n## Abstract\nLarge Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.\n## LLM Safety Alignment\nAt the core of defenses lies alignment, which involves fine-tuning pre-trained models to enhance their internal safety capabilities. In this section, we introduce various alignment algorithms and emphasize the data specifically designed to align models for improved safety. \n\nAlignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations. RLHF (Stiennon et al., 2020;Ouyang et al., 2022) utilizes human feedback and preferences to enhance the capabilities of LLMs, and DPO (Rafailov et al., 2023) simplifies the training process of RLHF by avoiding the need for a reward model. Methods like RLHF and DPO typically optimize a homogeneous and static objective based on human feedback, which is often a weighted combination of different objectives. To achieve joint optimization of multiple objectives (e.g., safety, helpfulness, and honesty) with customization according to specific scenarios, Multi-Objective RLHF (Dai et al., 2023;Ji et al., 2023;Wu et al., 2023c) extends RLHF by introducing fine-grained objective functions to enable trade-offs between safety and other goals such as helpfulness. Meanwhile, MODPO (Zhou et al., 2023b) builds upon RL-free DPO and enables joint optimization of multiple objectives. \n\nAlignment data. Based on the type of data used, data utilization can be divided into two categories: demonstration data for SFT and preference data for preference optimization approaches like DPO. As mentioned above, SFT utilizes high-quality demonstration data, where each question is associated with a single answer. Considering that SFT aims to maximize or minimize the generation probability on this data, selecting appropriate data becomes crucial.",
            "reference_string": "[267658120 | Dong et al. | 2024 | Citations: 68]"
        },
        {
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04879, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2264977662",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2303408438",
                    "name": "Quentin Fournier"
                },
                {
                    "authorId": "2305480521",
                    "name": "Matthew Riemer"
                },
                {
                    "authorId": "2305538500",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2301579793",
                    "name": "Amal Zouaq"
                },
                {
                    "authorId": "2283308757",
                    "name": "Payel Das"
                },
                {
                    "authorId": "123607932",
                    "name": "Sarath Chandar"
                }
            ],
            "abstract": "Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.",
            "corpus_id": 270357323,
            "sentences": [
                {
                    "corpus_id": "270357323",
                    "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
                    "text": "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023).3. Fine-tune an LLM with RL using PPO (Schulman et al., 2017) and the reward model.RLHF is the most commonly used method for preference alignment but often requires a lot of computation and steps for alignment.Various variants of RLHF have been proposed, such as using pure RL for training LLMs with human feedback in an online manner (Bai et al., 2022) and modifying the reward modeling with adversarial probing (Glaese et al., 2022).",
                    "score": 0.5869886070044211,
                    "section_title": "F Background and Related Work F.1 Alignment Methods",
                    "char_start_offset": 34017,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 110,
                            "end": 206
                        },
                        {
                            "start": 206,
                            "end": 429
                        },
                        {
                            "start": 429,
                            "end": 555
                        },
                        {
                            "start": 555,
                            "end": 705
                        },
                        {
                            "start": 705,
                            "end": 788
                        },
                        {
                            "start": 788,
                            "end": 915
                        },
                        {
                            "start": 915,
                            "end": 1140
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6416015625
                }
            ],
            "relevance_judgement": 0.6416015625,
            "relevance_judgment_input_expanded": "# Title: A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Megh Thakkar, Quentin Fournier, Matthew Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar\n## Abstract\nLarge language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.\n## F Background and Related Work F.1 Alignment Methods\nAlignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023).3. Fine-tune an LLM with RL using PPO (Schulman et al., 2017) and the reward model.RLHF is the most commonly used method for preference alignment but often requires a lot of computation and steps for alignment.Various variants of RLHF have been proposed, such as using pure RL for training LLMs with human feedback in an online manner (Bai et al., 2022) and modifying the reward modeling with adversarial probing (Glaese et al., 2022).",
            "reference_string": "[270357323 | Thakkar et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling",
            "venue": "",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.06176, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116804111",
                    "name": "Z. Yu"
                },
                {
                    "authorId": "2216725593",
                    "name": "Lau Jia Jaw"
                },
                {
                    "authorId": "2214035971",
                    "name": "Zhang Hui"
                },
                {
                    "authorId": "2256997696",
                    "name": "Bryan Kian Hsiang Low"
                }
            ],
            "abstract": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values through instruction tuning. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. A response to this downside is to fall back to supervised fine-tuning (SFT) with additional carefully selected expert demonstrations. However, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. In this study, we propose another alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative adversarial training style to enable the LLMs to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. Our preliminary findings indicate that RLGAF can help align LLMs outputs with competitive performance against RLHF and SFT, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating AI alignment.",
            "corpus_id": 263831784,
            "sentences": [
                {
                    "corpus_id": "263831784",
                    "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling",
                    "text": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values through instruction tuning. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. A response to this downside is to fall back to supervised fine-tuning (SFT) with additional carefully selected expert demonstrations. However, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. In this study, we propose another alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative adversarial training style to enable the LLMs to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. Our preliminary findings indicate that RLGAF can help align LLMs outputs with competitive performance against RLHF and SFT, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating AI alignment.",
                    "score": 0.42859646098210713,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63720703125
                }
            ],
            "relevance_judgement": 0.63720703125,
            "relevance_judgment_input_expanded": "# Title: Fine-tuning Language Models with Generative Adversarial Reward Modelling\n# Venue: \n# Authors: Z. Yu, Lau Jia Jaw, Zhang Hui, Bryan Kian Hsiang Low\n## Abstract\nReinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values through instruction tuning. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. A response to this downside is to fall back to supervised fine-tuning (SFT) with additional carefully selected expert demonstrations. However, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. In this study, we propose another alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative adversarial training style to enable the LLMs to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. Our preliminary findings indicate that RLGAF can help align LLMs outputs with competitive performance against RLHF and SFT, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating AI alignment.\n",
            "reference_string": "[263831784 | Yu et al. | 2023 | Citations: 4]"
        },
        {
            "title": "CBF-LLM: Safe Control for LLM Alignment",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.15625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239106560",
                    "name": "Yuya Miyaoka"
                },
                {
                    "authorId": "2239109456",
                    "name": "Masaki Inoue"
                }
            ],
            "abstract": "This paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.",
            "corpus_id": 271974342,
            "sentences": [
                {
                    "corpus_id": "271974342",
                    "title": "CBF-LLM: Safe Control for LLM Alignment",
                    "text": "While large language models (LLMs) are known to have strong language understanding and generation abilities, they can also generate harmful, biased, and toxic content [1] [2]. Alignment of LLMs ensures that they generate content that is \"desirable\" for the user, typically meaning content that is safe and ethical. Various approaches for LLM alignment have been presented ( [1], [2], [3] and reference therein). \n\nThe major approach to the alignment is reinforcement learning from human feedback (RLHF) [4], where a reward model is constructed by human feedback and used for the training of LLMs. Variants of RLHF architectures are also proposed, such as Safe-RLHF [5], SENSEI [6], and f-DPG [7], and their implementations are presented, such as training pre-trained LLMs [8] [9], and applications like information-seeking chatbot [10]. Collecting human feedback with data is time-consuming and expensive. To overcome the drawback, RL from AI Feedback (RLAIF) is presented in [11] instead of using human labels. In addition, the method to construct the training data automatically is proposed in [12]. Furthermore, to reduce the computational cost, direct preference optimization (DPO) is proposed [13], where the training data is directly used for training LLMs without accessing the reward model. Supervised fine-tuning (SFT) is a different approach for alignment from RLHF, as studied in [14]. A common feature of alignment methods like RLHF and SFT is that they modify LLMs' model parameters. \n\nAn alternative approach for LLM alignment is to directly intervene in the input prompt or output of LLMs, rather than modifying the model parameters. In-context learning (ICL) [15] is a major approach for intervening in the input prompt. In ICL, a few demonstrations are provided in prompt to instruct the LLMs on the task, including few-shot learning [16] [17].",
                    "score": 0.5012981105959466,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 314
                        },
                        {
                            "start": 315,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1298
                        },
                        {
                            "start": 1299,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1496
                        },
                        {
                            "start": 1499,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1861
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 503,
                            "end": 506,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 665,
                            "end": 668,
                            "matchedPaperCorpusId": "264306078"
                        },
                        {
                            "start": 677,
                            "end": 680,
                            "matchedPaperCorpusId": "250562745"
                        },
                        {
                            "start": 776,
                            "end": 779,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 1198,
                            "end": 1202,
                            "matchedPaperCorpusId": "258959321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6318359375
                }
            ],
            "relevance_judgement": 0.6318359375,
            "relevance_judgment_input_expanded": "# Title: CBF-LLM: Safe Control for LLM Alignment\n# Venue: arXiv.org\n# Authors: Yuya Miyaoka, Masaki Inoue\n## Abstract\nThis paper proposes a control-based framework for aligning large language models (LLMs) by leveraging a control barrier function (CBF) to ensure user-desirable text generation. The presented framework applies the safety filter, designed based on the CBF, to the output generation of the baseline LLM, i.e., the sequence of the token, with the aim of intervening in the generated text. The overall text-generation system is implemented with Llama 3 and a RoBERTa model, and the source code is available at https://github.com/Mya-Mya/CBF-LLM. The experiment demonstrates its control ability and effectiveness in reducing the number of interventions needed for user-specified alignment tasks.\n## Introduction\nWhile large language models (LLMs) are known to have strong language understanding and generation abilities, they can also generate harmful, biased, and toxic content [1] [2]. Alignment of LLMs ensures that they generate content that is \"desirable\" for the user, typically meaning content that is safe and ethical. Various approaches for LLM alignment have been presented ( [1], [2], [3] and reference therein). \n\nThe major approach to the alignment is reinforcement learning from human feedback (RLHF) [4], where a reward model is constructed by human feedback and used for the training of LLMs. Variants of RLHF architectures are also proposed, such as Safe-RLHF [5], SENSEI [6], and f-DPG [7], and their implementations are presented, such as training pre-trained LLMs [8] [9], and applications like information-seeking chatbot [10]. Collecting human feedback with data is time-consuming and expensive. To overcome the drawback, RL from AI Feedback (RLAIF) is presented in [11] instead of using human labels. In addition, the method to construct the training data automatically is proposed in [12]. Furthermore, to reduce the computational cost, direct preference optimization (DPO) is proposed [13], where the training data is directly used for training LLMs without accessing the reward model. Supervised fine-tuning (SFT) is a different approach for alignment from RLHF, as studied in [14]. A common feature of alignment methods like RLHF and SFT is that they modify LLMs' model parameters. \n\nAn alternative approach for LLM alignment is to directly intervene in the input prompt or output of LLMs, rather than modifying the model parameters. In-context learning (ICL) [15] is a major approach for intervening in the input prompt. In ICL, a few demonstrations are provided in prompt to instruct the LLMs on the task, including few-shot learning [16] [17].",
            "reference_string": "[271974342 | Miyaoka et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.14238, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "79733119",
                    "name": "Kyungjae Lee"
                },
                {
                    "authorId": "1474356736",
                    "name": "Dasol Hwang"
                },
                {
                    "authorId": "2282197642",
                    "name": "Sunghyun Park"
                },
                {
                    "authorId": "2288604723",
                    "name": "Youngsoo Jang"
                },
                {
                    "authorId": "2269856969",
                    "name": "Moontae Lee"
                }
            ],
            "abstract": "Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.",
            "corpus_id": 268553509,
            "sentences": [
                {
                    "corpus_id": "268553509",
                    "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
                    "text": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a crucial framework for aligning large language models (LLMs) with human preferences.To facilitate preference alignment, existing approaches such as InstructGPT (Ouyang et al., 2022a), Sparrow (Glaese et al., 2022), Llama-2 (Touvron et al., 2023) commonly train a reward model with preferential human feedback.This reward model assesses the overall quality of model outputs as a scalar value.Then training LLMs with the reward signals encourages the models to generate more favorable responses better aligned with human preferences.\n\nDespite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in * Equally contributed to this work.NLP tasks.Previous works (Zhou et al., 2023;Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities.Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users.Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens.However, enhancing the capabilities of LLMs is more critical than adjusting their interaction styles or formats to better match human preferences.\n\nTo address the superficial nature of preference alignment, we first investigate why the current RLHF often leads surface-level alignment.We tackle factuality and mathematical reasoning because the stylistic adjustment rarely contributes to downstream performance.Observing preferencebased reward models is notably deficient in evaluating mathematical reasoning, we hypothesize that preference-based reward models may cause superficial alignment.As a solution, we leverage finegrained LLM feedback that incorporates both verbal response and numeric score adhering to detailed criteria.",
                    "score": 0.43031550058335877,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 150,
                            "end": 375
                        },
                        {
                            "start": 375,
                            "end": 457
                        },
                        {
                            "start": 457,
                            "end": 597
                        },
                        {
                            "start": 599,
                            "end": 817
                        },
                        {
                            "start": 817,
                            "end": 827
                        },
                        {
                            "start": 827,
                            "end": 989
                        },
                        {
                            "start": 989,
                            "end": 1160
                        },
                        {
                            "start": 1160,
                            "end": 1303
                        },
                        {
                            "start": 1303,
                            "end": 1449
                        },
                        {
                            "start": 1451,
                            "end": 1588
                        },
                        {
                            "start": 1588,
                            "end": 1714
                        },
                        {
                            "start": 1714,
                            "end": 1896
                        },
                        {
                            "start": 1896,
                            "end": 2035
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 226,
                            "end": 248,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62744140625
                }
            ],
            "relevance_judgement": 0.62744140625,
            "relevance_judgment_input_expanded": "# Title: Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection\n# Venue: arXiv.org\n# Authors: Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee\n## Abstract\nDespite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.\n## Introduction\nReinforcement Learning from Human Feedback (RLHF) has emerged as a crucial framework for aligning large language models (LLMs) with human preferences.To facilitate preference alignment, existing approaches such as InstructGPT (Ouyang et al., 2022a), Sparrow (Glaese et al., 2022), Llama-2 (Touvron et al., 2023) commonly train a reward model with preferential human feedback.This reward model assesses the overall quality of model outputs as a scalar value.Then training LLMs with the reward signals encourages the models to generate more favorable responses better aligned with human preferences.\n\nDespite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in * Equally contributed to this work.NLP tasks.Previous works (Zhou et al., 2023;Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities.Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users.Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens.However, enhancing the capabilities of LLMs is more critical than adjusting their interaction styles or formats to better match human preferences.\n\nTo address the superficial nature of preference alignment, we first investigate why the current RLHF often leads surface-level alignment.We tackle factuality and mathematical reasoning because the stylistic adjustment rarely contributes to downstream performance.Observing preferencebased reward models is notably deficient in evaluating mathematical reasoning, we hypothesize that preference-based reward models may cause superficial alignment.As a solution, we leverage finegrained LLM feedback that incorporates both verbal response and numeric score adhering to detailed criteria.",
            "reference_string": "[268553509 | Lee et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Superficial Safety Alignment Hypothesis",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 82,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326007326",
                    "name": "Jianwei Li"
                },
                {
                    "authorId": "2326001415",
                    "name": "Jung-Eun Kim"
                }
            ],
            "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.",
            "corpus_id": 273350763,
            "sentences": [
                {
                    "corpus_id": "273350763",
                    "title": "Superficial Safety Alignment Hypothesis",
                    "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
                    "score": 0.49388998936365924,
                    "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                    "char_start_offset": 31564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 274
                        },
                        {
                            "start": 275,
                            "end": 516
                        },
                        {
                            "start": 517,
                            "end": 763
                        },
                        {
                            "start": 764,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1127
                        },
                        {
                            "start": 1130,
                            "end": 1262
                        },
                        {
                            "start": 1263,
                            "end": 1453
                        },
                        {
                            "start": 1456,
                            "end": 1647
                        },
                        {
                            "start": 1648,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1931
                        },
                        {
                            "start": 1934,
                            "end": 1972
                        },
                        {
                            "start": 1973,
                            "end": 2151
                        },
                        {
                            "start": 2152,
                            "end": 2350
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6259765625
                }
            ],
            "relevance_judgement": 0.6259765625,
            "relevance_judgment_input_expanded": "# Title: Superficial Safety Alignment Hypothesis\n# Venue: arXiv.org\n# Authors: Jianwei Li, Jung-Eun Kim\n## Abstract\nAs large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components 7.5\\% during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units 20\\% in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.\n## DISCUSSION, LIMITATION, AND CONCLUSION\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
            "reference_string": "[273350763 | Li et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2411.08733",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.08733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2179326599",
                    "name": "Somanshu Singla"
                },
                {
                    "authorId": "2261683280",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2115347044",
                    "name": "Tianyang Liu"
                },
                {
                    "authorId": "2329737016",
                    "name": "Abdullah Ashfaq"
                },
                {
                    "authorId": "2295863002",
                    "name": "Zhiting Hu"
                },
                {
                    "authorId": "2259944106",
                    "name": "Eric P. Xing"
                }
            ],
            "abstract": "Aligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving alignment without these extensive tuning costs and expensive annotations, we present a novel, tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO\u2019s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.",
            "corpus_id": 273901354,
            "sentences": [
                {
                    "corpus_id": "273901354",
                    "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
                    "text": "To marry the strengths of both paradigms, in this paper, we propose DRPO, Dynamic Rewarding with Prompt Optimization, a novel tuning-free approach for LLM self-alignment. DRPO draws inspiration from two key insights from recent alignment research. First, the superficial alignment hypothesis (Zhou et al., 2024) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2024a;Zhao et al., 2024). Second, reward models in RLHF often generalize poorly to out-of-distribution samples (Burns et al., 2023), whereas LLMs, well-known for their superior generalization capabilities, can provide more effective rewards and feedback for alignment. Building on these insights, DRPO is constructed atop a search-based prompt optimization (PO) framework (Pryzant et al., 2023;Hao et al., 2023;Wang et al., 2023), allowing LLMs to selfcorrect and automatically craft detailed alignment instruction. This steers model behavior more effectively, without relying on any use of human preferences or model training. \n\nThe core novelty of DRPO lies in its dynamic rewarding mechanism, integrated with the optimization framework. This mechanism enables LLMbased rewards to be adjusted on the fly based on specific queries, helping to identify and rectify the model's alignment blind spots. For example, if an LLM with outdated knowledge pretends to answer a question requiring the latest news, its \"knowledge limitation\" reward will be low, and the alignment prompt will be updated accordingly. We apply this novel method to automatically craft both the system prompt and responses in ICL examples, which have proven highly effective in improving alignment. \n\nWe conducted comprehensive experiments on 8 recent LLMs using the standard alignment benchmark, just-eval-instruct, composed of questions from multiple alignment datasets. Our results show that DRPO can effectively align both base and SFT/RLHF tuned models. Notably, DRPO significantly enhances base models, enabling them to outperform their SFT/RLHF-tuned counterparts. \n\nFigure 2: Comparing DRPO with other alignment methods, such as RLHF and URIAL (Lin et al., 2024a).",
                    "score": 0.43442998438149555,
                    "section_title": "Introduction",
                    "char_start_offset": 1805,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 681
                        },
                        {
                            "start": 682,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1040
                        },
                        {
                            "start": 1043,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1312
                        },
                        {
                            "start": 1313,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1680
                        },
                        {
                            "start": 1683,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2053
                        },
                        {
                            "start": 2056,
                            "end": 2154
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 292,
                            "end": 311,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 400,
                            "end": 419,
                            "matchedPaperCorpusId": "265608902"
                        },
                        {
                            "start": 824,
                            "end": 842,
                            "matchedPaperCorpusId": "264451925"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61962890625
                }
            ],
            "relevance_judgement": 0.61962890625,
            "relevance_judgment_input_expanded": "# Title: Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing\n## Abstract\nAligning Large Language Models (LLMs) traditionally relies on complex and costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To address the challenge of achieving alignment without these extensive tuning costs and expensive annotations, we present a novel, tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO\u2019s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.\n## Introduction\nTo marry the strengths of both paradigms, in this paper, we propose DRPO, Dynamic Rewarding with Prompt Optimization, a novel tuning-free approach for LLM self-alignment. DRPO draws inspiration from two key insights from recent alignment research. First, the superficial alignment hypothesis (Zhou et al., 2024) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2024a;Zhao et al., 2024). Second, reward models in RLHF often generalize poorly to out-of-distribution samples (Burns et al., 2023), whereas LLMs, well-known for their superior generalization capabilities, can provide more effective rewards and feedback for alignment. Building on these insights, DRPO is constructed atop a search-based prompt optimization (PO) framework (Pryzant et al., 2023;Hao et al., 2023;Wang et al., 2023), allowing LLMs to selfcorrect and automatically craft detailed alignment instruction. This steers model behavior more effectively, without relying on any use of human preferences or model training. \n\nThe core novelty of DRPO lies in its dynamic rewarding mechanism, integrated with the optimization framework. This mechanism enables LLMbased rewards to be adjusted on the fly based on specific queries, helping to identify and rectify the model's alignment blind spots. For example, if an LLM with outdated knowledge pretends to answer a question requiring the latest news, its \"knowledge limitation\" reward will be low, and the alignment prompt will be updated accordingly. We apply this novel method to automatically craft both the system prompt and responses in ICL examples, which have proven highly effective in improving alignment. \n\nWe conducted comprehensive experiments on 8 recent LLMs using the standard alignment benchmark, just-eval-instruct, composed of questions from multiple alignment datasets. Our results show that DRPO can effectively align both base and SFT/RLHF tuned models. Notably, DRPO significantly enhances base models, enabling them to outperform their SFT/RLHF-tuned counterparts. \n\nFigure 2: Comparing DRPO with other alignment methods, such as RLHF and URIAL (Lin et al., 2024a).",
            "reference_string": "[273901354 | Singla et al. | 2024 | Citations: 2]"
        },
        {
            "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 43,
            "citation_count": 75,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.07074",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.07074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1515662094",
                    "name": "K. Lu"
                },
                {
                    "authorId": "2114128654",
                    "name": "Hongyi Yuan"
                },
                {
                    "authorId": "2112340945",
                    "name": "Zheng Yuan"
                },
                {
                    "authorId": "2167032295",
                    "name": "Runji Lin"
                },
                {
                    "authorId": "35996608",
                    "name": "Junyang Lin"
                },
                {
                    "authorId": "2111727840",
                    "name": "Chuanqi Tan"
                },
                {
                    "authorId": "2192678144",
                    "name": "Chang Zhou"
                }
            ],
            "abstract": "Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.",
            "corpus_id": 260887200,
            "sentences": [
                {
                    "corpus_id": "260887200",
                    "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
                    "text": "The rise of contemporary chatbots, including GPT-4 (OpenAI, 2023), has brought to the forefront of generative artificial intelligence that is based on large language models (LLMs) to tackle a variety of real-world tasks. Well-aligned LLMs with human expectations can precisely recognize human intentions and properly formalize responses expressed in natural languages (Wang et al., 2023d). Achieving such a level of alignment typically necessitates fine-tuning processes, such as supervised fine-tuning (SFT) (Taori et al., 2023;Chiang et al., 2023;Touvron et al., 2023b), response ranking (Yuan et al., 2023b;Song et al., 2023;Rafailov et al., 2023), and reinforcement learning with human feedback (RLHF) (Bai et al., 2022a;Ouyang et al., 2022;Touvron et al., 2023b), to enable LLMs to comprehend and execute diverse instructions effectively. \n\nA broad range of training data covering various semantics and specialties is crucial for achieving alignment with human preference through SFT, which is typically gathered through crowd-sourcing (Ouyang et al., 2022;Bai et al., 2022a;Touvron et al., 2023b) or by distilling from other LLMs (Taori et al., 2023;Ding et al., 2023). The SFT data for alignment is generally formalized in a multi-turn utterance manner, and each turn is composed of a human query and a corresponding response expected to generate by well-aligned chatbots. Recent research indicates that the training dataset for alignment should be diverse and complex, covering various domains, tasks, and formats (Xu et al., 2023a;Mukherjee et al., 2023;Wang et al., 2023b). Such diversity and complexity are mainly determined by query formation. Various methods are proposed and claimed to improve the diversity and complexity of the queries and advance the alignment of LLMs (Wang et al. 2023c;Xu The contributions of this work are mainly three-fold.",
                    "score": 0.42123017866894263,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 843
                        },
                        {
                            "start": 846,
                            "end": 1175
                        },
                        {
                            "start": 1176,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1861
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6142578125
                }
            ],
            "relevance_judgement": 0.6142578125,
            "relevance_judgment_input_expanded": "# Title: #InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: K. Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou\n## Abstract\nFoundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.\n## INTRODUCTION\nThe rise of contemporary chatbots, including GPT-4 (OpenAI, 2023), has brought to the forefront of generative artificial intelligence that is based on large language models (LLMs) to tackle a variety of real-world tasks. Well-aligned LLMs with human expectations can precisely recognize human intentions and properly formalize responses expressed in natural languages (Wang et al., 2023d). Achieving such a level of alignment typically necessitates fine-tuning processes, such as supervised fine-tuning (SFT) (Taori et al., 2023;Chiang et al., 2023;Touvron et al., 2023b), response ranking (Yuan et al., 2023b;Song et al., 2023;Rafailov et al., 2023), and reinforcement learning with human feedback (RLHF) (Bai et al., 2022a;Ouyang et al., 2022;Touvron et al., 2023b), to enable LLMs to comprehend and execute diverse instructions effectively. \n\nA broad range of training data covering various semantics and specialties is crucial for achieving alignment with human preference through SFT, which is typically gathered through crowd-sourcing (Ouyang et al., 2022;Bai et al., 2022a;Touvron et al., 2023b) or by distilling from other LLMs (Taori et al., 2023;Ding et al., 2023). The SFT data for alignment is generally formalized in a multi-turn utterance manner, and each turn is composed of a human query and a corresponding response expected to generate by well-aligned chatbots. Recent research indicates that the training dataset for alignment should be diverse and complex, covering various domains, tasks, and formats (Xu et al., 2023a;Mukherjee et al., 2023;Wang et al., 2023b). Such diversity and complexity are mainly determined by query formation. Various methods are proposed and claimed to improve the diversity and complexity of the queries and advance the alignment of LLMs (Wang et al. 2023c;Xu The contributions of this work are mainly three-fold.",
            "reference_string": "[260887200 | Lu et al. | 2023 | Citations: 75]"
        },
        {
            "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325904632",
                    "name": "Teng Xiao"
                },
                {
                    "authorId": "2325912099",
                    "name": "Mingxiao Li"
                },
                {
                    "authorId": "2191690098",
                    "name": "Yige Yuan"
                },
                {
                    "authorId": "2290912804",
                    "name": "Huaisheng Zhu"
                },
                {
                    "authorId": "2291070605",
                    "name": "Chao Cui"
                },
                {
                    "authorId": "2320465874",
                    "name": "V.G. Honavar"
                }
            ],
            "abstract": "This paper introduces a novel generalized self-imitation learning GSIL framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop GSIL by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. GSIL eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, GSIL encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that GSIL consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench). Code is public available at https://github.com/tengxiao1/GSIL.",
            "corpus_id": 273345418,
            "sentences": [
                {
                    "corpus_id": "273345418",
                    "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
                    "text": "Pre-training endows large language models (LLMs) with extensive knowledge about the world. However, it does not behave in accordance with some task-dependent requirements. To achieve the desired performance on certain tasks, a post-training process known as alignment or fine-tuning is essential. Alignment has emerged as a pivotal approach to improve the following performance of pre-trained language models, especially in complex instruction-following tasks: commonsense reasoning, coding, summarization, and math problemsolving (Bai et al., 2022;Ouyang et al., 2022;Stiennon et al., 2020;Rafailov et al., 2024b). \n\nThe current alignment methods can be broadly categorized into groups: (i) supervised fine-tuning (SFT) based on demonstration data, aligning an input prompt and a human response. (ii) Preference fine-tuning (Tajwar et al., 2024) with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Christiano et al., 2017) or direct preference optimization (DPO) (Rafailov et al., 2024b;Zhao et al., 2023;Azar et al., 2024;Tang et al., 2024;Ethayarajh et al., 2024) based on preference data containing preferred and dis-preferred responses to prompts. Although RLHF and DPO have achieved promising results (Rafailov et al., 2024b;Tunstall et al., 2023), they require expensive human preference labels on several candidate demonstrations of a query to be used as feedback, limiting their applicability to language model alignment in settings where there is a lack of preference feedback. Furthermore, preference fine-tuning may suffer from reward overoptimization (also known as reward hacking), as shown by (Rafailov et al., 2024a;Gao et al., 2023a). Recent work (Sharma et al., 2024) also shows that simply performing SFT on demonstrations can result in a better model than preference fine-tuning with AI feedback.",
                    "score": 0.44199827966590177,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 90
                        },
                        {
                            "start": 91,
                            "end": 171
                        },
                        {
                            "start": 172,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 615
                        },
                        {
                            "start": 618,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1511
                        },
                        {
                            "start": 1512,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1840
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 549,
                            "end": 569,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 569,
                            "end": 591,
                            "matchedPaperCorpusId": "221665105"
                        },
                        {
                            "start": 591,
                            "end": 614,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 902,
                            "end": 923,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 988,
                            "end": 1012,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 1030,
                            "end": 1048,
                            "matchedPaperCorpusId": "264288854"
                        },
                        {
                            "start": 1231,
                            "end": 1255,
                            "matchedPaperCorpusId": "258959321"
                        },
                        {
                            "start": 1656,
                            "end": 1674,
                            "matchedPaperCorpusId": "252992904"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61279296875
                }
            ],
            "relevance_judgement": 0.61279296875,
            "relevance_judgment_input_expanded": "# Title: How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Teng Xiao, Mingxiao Li, Yige Yuan, Huaisheng Zhu, Chao Cui, V.G. Honavar\n## Abstract\nThis paper introduces a novel generalized self-imitation learning GSIL framework, which effectively and efficiently aligns large language models with offline demonstration data. We develop GSIL by deriving a surrogate objective of imitation learning with density ratio estimates, facilitating the use of self-generated data and optimizing the imitation learning objective with simple classification losses. GSIL eliminates the need for complex adversarial training in standard imitation learning, achieving lightweight and efficient fine-tuning for large language models. In addition, GSIL encompasses a family of offline losses parameterized by a general class of convex functions for density ratio estimation and enables a unified view for alignment with demonstration data. Extensive experiments show that GSIL consistently and significantly outperforms baselines in many challenging benchmarks, such as coding (HuamnEval), mathematical reasoning (GSM8K) and instruction-following benchmark (MT-Bench). Code is public available at https://github.com/tengxiao1/GSIL.\n## Introduction\nPre-training endows large language models (LLMs) with extensive knowledge about the world. However, it does not behave in accordance with some task-dependent requirements. To achieve the desired performance on certain tasks, a post-training process known as alignment or fine-tuning is essential. Alignment has emerged as a pivotal approach to improve the following performance of pre-trained language models, especially in complex instruction-following tasks: commonsense reasoning, coding, summarization, and math problemsolving (Bai et al., 2022;Ouyang et al., 2022;Stiennon et al., 2020;Rafailov et al., 2024b). \n\nThe current alignment methods can be broadly categorized into groups: (i) supervised fine-tuning (SFT) based on demonstration data, aligning an input prompt and a human response. (ii) Preference fine-tuning (Tajwar et al., 2024) with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Christiano et al., 2017) or direct preference optimization (DPO) (Rafailov et al., 2024b;Zhao et al., 2023;Azar et al., 2024;Tang et al., 2024;Ethayarajh et al., 2024) based on preference data containing preferred and dis-preferred responses to prompts. Although RLHF and DPO have achieved promising results (Rafailov et al., 2024b;Tunstall et al., 2023), they require expensive human preference labels on several candidate demonstrations of a query to be used as feedback, limiting their applicability to language model alignment in settings where there is a lack of preference feedback. Furthermore, preference fine-tuning may suffer from reward overoptimization (also known as reward hacking), as shown by (Rafailov et al., 2024a;Gao et al., 2023a). Recent work (Sharma et al., 2024) also shows that simply performing SFT on demonstrations can result in a better model than preference fine-tuning with AI feedback.",
            "reference_string": "[273345418 | Xiao et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 77,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334583384",
                    "name": "Melody Y. Guan"
                },
                {
                    "authorId": "2185778",
                    "name": "Manas R. Joglekar"
                },
                {
                    "authorId": "2297774080",
                    "name": "Eric Wallace"
                },
                {
                    "authorId": "2329885104",
                    "name": "Saachi Jain"
                },
                {
                    "authorId": "2286382037",
                    "name": "Boaz Barak"
                },
                {
                    "authorId": "2219550562",
                    "name": "Alec Helyar"
                },
                {
                    "authorId": "2328288671",
                    "name": "Rachel Dias"
                },
                {
                    "authorId": "2275244586",
                    "name": "Andrea Vallone"
                },
                {
                    "authorId": "2282935403",
                    "name": "Hongyu Ren"
                },
                {
                    "authorId": "2329627448",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "2275839391",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "2336876158",
                    "name": "Sam Toyer"
                },
                {
                    "authorId": "2151087994",
                    "name": "Johannes Heidecke"
                },
                {
                    "authorId": "2297773170",
                    "name": "Alex Beutel"
                },
                {
                    "authorId": "2105840001",
                    "name": "Amelia Glaese"
                }
            ],
            "abstract": "As large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.",
            "corpus_id": 274982908,
            "sentences": [
                {
                    "corpus_id": "274982908",
                    "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
                    "text": "To study the impact that the SFT and RL stages of deliberative alignment have on model performance, we conduct ablation experiments where we drop safety data from one or both stages. Specifically, we compare the following four settings (see Figure 14 As expected, the \"Safety in SFT & RL\" performs much better than the \"No safety training\" run in terms of disallowed content, response style, and jailbreaks, although in this specific ablation setup the safety training also increases overrefusals. The key finding is that the \"Safety in SFT only\" and \"Safety in RL only\" runs attain intermediate results, showing that both SFT and RL training play critical roles in deliberative alignment training. We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL. \n\nIn Figure 14, we also compare these ablations to a baseline where we do not perform any safety training, but we provide the entire spec to the model at inference time in the system message. Because we would not know what safety category is relevant for prompts received at deployment time, the spec we provide is not tailored to any safety category but instead has the summarized versions of all the content policies (see Section 2.2). Note that it is infeasible to include the detailed versions of the content policies for all safety categories, because each one spans 5-10K tokens and would altogether exceed the model's context window. \n\nDespite having access to the full spec, this baseline appears to learn less safety behavior than the model trained with deliberative alignment (and in many cases, even the model only trained with safety in the SFT stage). This baseline particularly struggles to adhere to response style guidelines. These results indicate that embedding these policies during training is more reliable than providing all of the policies at deployment time.",
                    "score": 0.4204884951692138,
                    "section_title": "Ablations for different components of the method",
                    "char_start_offset": 35683,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 182
                        },
                        {
                            "start": 183,
                            "end": 497
                        },
                        {
                            "start": 498,
                            "end": 698
                        },
                        {
                            "start": 699,
                            "end": 836
                        },
                        {
                            "start": 839,
                            "end": 1028
                        },
                        {
                            "start": 1029,
                            "end": 1274
                        },
                        {
                            "start": 1275,
                            "end": 1477
                        },
                        {
                            "start": 1480,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1778
                        },
                        {
                            "start": 1779,
                            "end": 1919
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61279296875
                }
            ],
            "relevance_judgement": 0.61279296875,
            "relevance_judgment_input_expanded": "# Title: Deliberative Alignment: Reasoning Enables Safer Language Models\n# Venue: arXiv.org\n# Authors: Melody Y. Guan, Manas R. Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese\n## Abstract\nAs large-scale language models increasingly impact safety-critical domains, ensuring their reliable adherence to well-defined principles remains a fundamental challenge. We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering. We used this approach to align OpenAI's o-series models, and achieved highly precise adherence to OpenAI's safety policies, without requiring human-written chain-of-thoughts or answers. Deliberative Alignment pushes the Pareto frontier by simultaneously increasing robustness to jailbreaks while decreasing overrefusal rates, and also improves out-of-distribution generalization. We demonstrate that reasoning over explicitly specified policies enables more scalable, trustworthy, and interpretable alignment.\n## Ablations for different components of the method\nTo study the impact that the SFT and RL stages of deliberative alignment have on model performance, we conduct ablation experiments where we drop safety data from one or both stages. Specifically, we compare the following four settings (see Figure 14 As expected, the \"Safety in SFT & RL\" performs much better than the \"No safety training\" run in terms of disallowed content, response style, and jailbreaks, although in this specific ablation setup the safety training also increases overrefusals. The key finding is that the \"Safety in SFT only\" and \"Safety in RL only\" runs attain intermediate results, showing that both SFT and RL training play critical roles in deliberative alignment training. We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL. \n\nIn Figure 14, we also compare these ablations to a baseline where we do not perform any safety training, but we provide the entire spec to the model at inference time in the system message. Because we would not know what safety category is relevant for prompts received at deployment time, the spec we provide is not tailored to any safety category but instead has the summarized versions of all the content policies (see Section 2.2). Note that it is infeasible to include the detailed versions of the content policies for all safety categories, because each one spans 5-10K tokens and would altogether exceed the model's context window. \n\nDespite having access to the full spec, this baseline appears to learn less safety behavior than the model trained with deliberative alignment (and in many cases, even the model only trained with safety in the SFT stage). This baseline particularly struggles to adhere to response style guidelines. These results indicate that embedding these policies during training is more reliable than providing all of the policies at deployment time.",
            "reference_string": "[274982908 | Guan et al. | 2024 | Citations: 77]"
        },
        {
            "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 13,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19874, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2146232823",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2294566328",
                    "name": "Maksym Andriushchenko"
                },
                {
                    "authorId": "2294568278",
                    "name": "Francesco Croce"
                },
                {
                    "authorId": "2288272044",
                    "name": "Nicolas Flammarion"
                }
            ],
            "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on the established benchmark MT-Bench, especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding high-quality, potentially carefully selected via greedy search, demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime, where ICL can be a viable alternative to IFT. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT. We provide our code at https://github.com/tml-epfl/icl-alignment.",
            "corpus_id": 270123077,
            "sentences": [
                {
                    "corpus_id": "270123077",
                    "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
                    "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities (Bubeck et al., 2023). However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Traditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Wei et al., 2021;Chung et al., 2022) or preference data (DPO, KTO, IPO, and ORPO, Rafailov et al., 2023;Ethayarajh et al., 2024;Azar et al., 2024;Hong et al., 2024), and reinforcement learning (RLHF and RLAIF, Ouyang et al., 2022;Bai et al., 2022). These approaches typically involve multiple rounds of refinement of the instructed model (Touvron et al., 2023), i.e. high computational cost, and need a large amount of annotated data like human preferences, which might be difficult to collect. However, a line of work (Taori et al., 2023;Chen et al., 2023;Zhou et al., 2023;Lee et al., 2023;Zhao et al., 2024;Kaur et al., 2024) has suggested that IFT on a small amount of high quality instructionfollowing examples, even only 1000, can be sufficient to match the performance of more complex alignment mechanisms. In particular, Zhou et al. (2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023). Using such small instruction datasets for IFT has the clear advantage of significantly reducing the cost of model alignment.",
                    "score": 0.4476754274056283,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 294
                        },
                        {
                            "start": 295,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1501
                        },
                        {
                            "start": 1502,
                            "end": 1626
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 520,
                            "end": 538,
                            "matchedPaperCorpusId": "264288854"
                        },
                        {
                            "start": 949,
                            "end": 967,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 984,
                            "end": 1002,
                            "matchedPaperCorpusId": "267522812"
                        },
                        {
                            "start": 1221,
                            "end": 1239,
                            "matchedPaperCorpusId": "258822910"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60302734375
                }
            ],
            "relevance_judgement": 0.60302734375,
            "relevance_judgment_input_expanded": "# Title: Is In-Context Learning Sufficient for Instruction Following in LLMs?\n# Venue: arXiv.org\n# Authors: Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion\n## Abstract\nIn-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on the established benchmark MT-Bench, especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding high-quality, potentially carefully selected via greedy search, demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime, where ICL can be a viable alternative to IFT. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT. We provide our code at https://github.com/tml-epfl/icl-alignment.\n## INTRODUCTION\nThe large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities (Bubeck et al., 2023). However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Traditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Wei et al., 2021;Chung et al., 2022) or preference data (DPO, KTO, IPO, and ORPO, Rafailov et al., 2023;Ethayarajh et al., 2024;Azar et al., 2024;Hong et al., 2024), and reinforcement learning (RLHF and RLAIF, Ouyang et al., 2022;Bai et al., 2022). These approaches typically involve multiple rounds of refinement of the instructed model (Touvron et al., 2023), i.e. high computational cost, and need a large amount of annotated data like human preferences, which might be difficult to collect. However, a line of work (Taori et al., 2023;Chen et al., 2023;Zhou et al., 2023;Lee et al., 2023;Zhao et al., 2024;Kaur et al., 2024) has suggested that IFT on a small amount of high quality instructionfollowing examples, even only 1000, can be sufficient to match the performance of more complex alignment mechanisms. In particular, Zhou et al. (2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023). Using such small instruction datasets for IFT has the clear advantage of significantly reducing the cost of model alignment.",
            "reference_string": "[270123077 | Zhao et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Aligning Large Language Models by On-Policy Self-Judgment",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 35,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284690114",
                    "name": "Sangkyu Lee"
                },
                {
                    "authorId": "2829848",
                    "name": "Sungdong Kim"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2266468609",
                    "name": "Minjoon Seo"
                },
                {
                    "authorId": "31760501",
                    "name": "Kang Min Yoo"
                },
                {
                    "authorId": "2284703785",
                    "name": "Youngjae Yu"
                }
            ],
            "abstract": "Existing approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.",
            "corpus_id": 267751124,
            "sentences": [
                {
                    "corpus_id": "267751124",
                    "title": "Aligning Large Language Models by On-Policy Self-Judgment",
                    "text": "Learning from Preference Scores There are several approaches utilizing an RM for alignment learning. RLHF (Ziegler et al., 2020) utilizes an RM for on-policy reinforcement learning. RRHF (Yuan et al., 2023) maximizes the margin of log-likelihood by the rank of responses determined by the score from RM and human annotators. RAFT (Dong et al., 2023) and ReST (Gulcehre et al., 2023) apply rejection sampling on sampled responses through the RM to perform self-imitation learning. SALMON (Sun et al., 2023) trains LLMs to generate scores for responses through principledriven synthetic preference data utilizing the SFT model. However, all these approaches require a  separate RM for the alignment procedure. \n\nOptimizing on Preference Orders From preference orders in the static dataset, DPO (Rafailov et al., 2023) optimizes LLMs by implicit rewards without a separated RM. IPO (Azar et al., 2023) proposes a modified objective using an unbounded preference mapping function to mitigate overfitting on deterministic preferences in the dataset. PCO (Xu et al., 2023) utilizes cringe loss for optimization, which considers the token-level likelihood of rejected samples as contrastive training. SPIN (Chen et al., 2024) performs iterative training considering the distribution gap between SFT datasets and the model's responses as preference orders. Self-Rewarding Language Models (Yuan et al., 2024) trains LLMs to generate scores for a given response by chain-of-thought reasoning to construct preference datasets by self-generated responses. All these approaches differ from our work in that they do not perform on-policy learning. \n\nGenerative Pairwise Evaluator The generative pairwise evaluator, which we refer to as JM, has been utilized in previous approaches to alignment learning. ILF (Scheurer et al., 2023) selects the response that reflects human-requested feedback through JM. SLiC-HF (Zhao et al., 2023) constructs a static preference dataset with responses obtained from the SFT model ordered by JM.",
                    "score": 0.43246272777553213,
                    "section_title": "Related Work",
                    "char_start_offset": 22042,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 181
                        },
                        {
                            "start": 182,
                            "end": 324
                        },
                        {
                            "start": 325,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 625
                        },
                        {
                            "start": 626,
                            "end": 707
                        },
                        {
                            "start": 710,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1348
                        },
                        {
                            "start": 1349,
                            "end": 1543
                        },
                        {
                            "start": 1544,
                            "end": 1633
                        },
                        {
                            "start": 1636,
                            "end": 1789
                        },
                        {
                            "start": 1790,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 2014
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.599609375
                }
            ],
            "relevance_judgement": 0.599609375,
            "relevance_judgment_input_expanded": "# Title: Aligning Large Language Models by On-Policy Self-Judgment\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu\n## Abstract\nExisting approaches for aligning large language models with human preferences face a trade-off that requires a separate reward model (RM) for on-policy learning. In this paper, we present a novel alignment framework, SELF-JUDGE that (1) does on-policy learning and 2) is parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge. Specifically, we view the pairwise judgment task, choosing the better response from a response pair, as a special case of the instruction-following task. The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines in preference benchmarks. We also show that the rejecting sampling by itself can improve performance further without an additional evaluator.\n## Related Work\nLearning from Preference Scores There are several approaches utilizing an RM for alignment learning. RLHF (Ziegler et al., 2020) utilizes an RM for on-policy reinforcement learning. RRHF (Yuan et al., 2023) maximizes the margin of log-likelihood by the rank of responses determined by the score from RM and human annotators. RAFT (Dong et al., 2023) and ReST (Gulcehre et al., 2023) apply rejection sampling on sampled responses through the RM to perform self-imitation learning. SALMON (Sun et al., 2023) trains LLMs to generate scores for responses through principledriven synthetic preference data utilizing the SFT model. However, all these approaches require a  separate RM for the alignment procedure. \n\nOptimizing on Preference Orders From preference orders in the static dataset, DPO (Rafailov et al., 2023) optimizes LLMs by implicit rewards without a separated RM. IPO (Azar et al., 2023) proposes a modified objective using an unbounded preference mapping function to mitigate overfitting on deterministic preferences in the dataset. PCO (Xu et al., 2023) utilizes cringe loss for optimization, which considers the token-level likelihood of rejected samples as contrastive training. SPIN (Chen et al., 2024) performs iterative training considering the distribution gap between SFT datasets and the model's responses as preference orders. Self-Rewarding Language Models (Yuan et al., 2024) trains LLMs to generate scores for a given response by chain-of-thought reasoning to construct preference datasets by self-generated responses. All these approaches differ from our work in that they do not perform on-policy learning. \n\nGenerative Pairwise Evaluator The generative pairwise evaluator, which we refer to as JM, has been utilized in previous approaches to alignment learning. ILF (Scheurer et al., 2023) selects the response that reflects human-requested feedback through JM. SLiC-HF (Zhao et al., 2023) constructs a static preference dataset with responses obtained from the SFT model ordered by JM.",
            "reference_string": "[267751124 | Lee et al. | 2024 | Citations: 11]"
        },
        {
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 51,
            "citation_count": 198,
            "influential_citation_count": 27,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.01552, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51583409",
                    "name": "Bill Yuchen Lin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "50085131",
                    "name": "Ximing Lu"
                },
                {
                    "authorId": "46217681",
                    "name": "Nouha Dziri"
                },
                {
                    "authorId": "1947172233",
                    "name": "Melanie Sclar"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                }
            ],
            "abstract": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
            "corpus_id": 265608902,
            "sentences": [
                {
                    "corpus_id": "265608902",
                    "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                    "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
                    "score": 0.4499773640343987,
                    "section_title": "Preprint",
                    "char_start_offset": 993,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 629
                        },
                        {
                            "start": 632,
                            "end": 816
                        },
                        {
                            "start": 817,
                            "end": 969
                        },
                        {
                            "start": 970,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1651
                        },
                        {
                            "start": 1652,
                            "end": 1901
                        },
                        {
                            "start": 1904,
                            "end": 2114
                        },
                        {
                            "start": 2115,
                            "end": 2302
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59033203125
                }
            ],
            "relevance_judgement": 0.59033203125,
            "relevance_judgment_input_expanded": "# Title: The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning\n# Venue: arXiv.org\n# Authors: Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Raghavi Chandu, Chandra Bhagavatula, Yejin Choi\n## Abstract\nThe alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.\n## Preprint\nOn the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
            "reference_string": "[265608902 | Lin et al. | 2023 | Citations: 198]"
        },
        {
            "title": "Progressively Label Enhancement for Large Language Model Alignment",
            "venue": "",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.02599, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2246701947",
                    "name": "Biao Liu"
                },
                {
                    "authorId": "2314833623",
                    "name": "Ning Xu"
                },
                {
                    "authorId": "2314830191",
                    "name": "Xin Geng"
                }
            ],
            "abstract": "Large Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns. In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment. Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often rely on large high-quality datasets. Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data. To deal with this problem, we propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model's training process based on the evolving quality of the generated data. Specifically, we prompt the model to generate responses for both the original query and the query guided by a set of carefully designed principles, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores. Experimental results demonstrate the effectiveness of PLE compared to existing LLM alignment methods.",
            "corpus_id": 271709991,
            "sentences": [
                {
                    "corpus_id": "271709991",
                    "title": "Progressively Label Enhancement for Large Language Model Alignment",
                    "text": "For instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF. RAFT (Dong et al., 2023) expands the SFT dataset by generating additional samples and selecting those with high reward scores to enhance the SFT dataset. RRHF (Yuan et al., 2023) simplifies the RLHF process by integrating the subsequent RL steps into the SFT phase as a regularization term. \n\nHowever, these methods are highly dependent on large amounts of high-quality data, which is impractical in certain applications, such as the medical field (Yang et al., 2024;Li et al., 2023) or chip design (Liu et al., 2023). Additionally, even though some methods generate extra data to expand the training set to alleviate the problem. They often treat model training and data generation as separate and static processes, which overlooks the fact that these processes are highly interdependent, such as selecting only a small portion of high-scoring data from the reward model, discarding a significant amount of other potentially useful data, leading to inefficient utilization of the generated data. Therefore, we consider designing an efficient framework that couples the data generation and model training processes, allowing them to work synergistically, thus ensuring that all generated data, including potentially useful lower-scoring data, is effectively utilized, thereby improving training efficiency. \n\nMotivated by the above consideration, we propose a novel framework named PLE, i.e., Progressively Label Enhancement for Language Model Alignment. Specifically, during the sample generation phase, we design a set of principles to guide the model to output according to human expectations. When the reward score difference between the principle-guided output and the response to the original query exceeds a dynamically updated threshold, indicating a significant improvement under the principle guiding, the model is encouraged to align its output with the better response and move away from the poorer one. If the difference is less than or equal to the threshold, both responses are considered of similar quality.",
                    "score": 0.5553771175943076,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 1626,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 525
                        },
                        {
                            "start": 528,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 865
                        },
                        {
                            "start": 866,
                            "end": 1231
                        },
                        {
                            "start": 1232,
                            "end": 1541
                        },
                        {
                            "start": 1544,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2150
                        },
                        {
                            "start": 2151,
                            "end": 2258
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 19,
                            "end": 38,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 240,
                            "end": 259,
                            "matchedPaperCorpusId": "258170300"
                        },
                        {
                            "start": 394,
                            "end": 413,
                            "matchedPaperCorpusId": "258059818"
                        },
                        {
                            "start": 683,
                            "end": 702,
                            "matchedPaperCorpusId": "260681932"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58837890625
                }
            ],
            "relevance_judgement": 0.58837890625,
            "relevance_judgment_input_expanded": "# Title: Progressively Label Enhancement for Large Language Model Alignment\n# Venue: \n# Authors: Biao Liu, Ning Xu, Xin Geng\n## Abstract\nLarge Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns. In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment. Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often rely on large high-quality datasets. Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data. To deal with this problem, we propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model's training process based on the evolving quality of the generated data. Specifically, we prompt the model to generate responses for both the original query and the query guided by a set of carefully designed principles, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores. Experimental results demonstrate the effectiveness of PLE compared to existing LLM alignment methods.\n## INTRODUCTION\nFor instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF. RAFT (Dong et al., 2023) expands the SFT dataset by generating additional samples and selecting those with high reward scores to enhance the SFT dataset. RRHF (Yuan et al., 2023) simplifies the RLHF process by integrating the subsequent RL steps into the SFT phase as a regularization term. \n\nHowever, these methods are highly dependent on large amounts of high-quality data, which is impractical in certain applications, such as the medical field (Yang et al., 2024;Li et al., 2023) or chip design (Liu et al., 2023). Additionally, even though some methods generate extra data to expand the training set to alleviate the problem. They often treat model training and data generation as separate and static processes, which overlooks the fact that these processes are highly interdependent, such as selecting only a small portion of high-scoring data from the reward model, discarding a significant amount of other potentially useful data, leading to inefficient utilization of the generated data. Therefore, we consider designing an efficient framework that couples the data generation and model training processes, allowing them to work synergistically, thus ensuring that all generated data, including potentially useful lower-scoring data, is effectively utilized, thereby improving training efficiency. \n\nMotivated by the above consideration, we propose a novel framework named PLE, i.e., Progressively Label Enhancement for Language Model Alignment. Specifically, during the sample generation phase, we design a set of principles to guide the model to output according to human expectations. When the reward score difference between the principle-guided output and the response to the original query exceeds a dynamically updated threshold, indicating a significant improvement under the principle guiding, the model is encouraged to align its output with the better response and move away from the poorer one. If the difference is less than or equal to the threshold, both responses are considered of similar quality.",
            "reference_string": "[271709991 | Liu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.03199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256985123",
                    "name": "Ziyun Cui"
                },
                {
                    "authorId": "2304966381",
                    "name": "Ziyang Zhang"
                },
                {
                    "authorId": "2257370821",
                    "name": "Wen Wu"
                },
                {
                    "authorId": "2107310187",
                    "name": "Guangzhi Sun"
                },
                {
                    "authorId": "2305503381",
                    "name": "Chao Zhang"
                }
            ],
            "abstract": "Advances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.",
            "corpus_id": 270258300,
            "sentences": [
                {
                    "corpus_id": "270258300",
                    "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
                    "text": "With the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019;Bai et al., 2022a;Ouyang et al., 2022;Nakano et al., 2021;Askell et al., 2021). \n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong. \n\n\u2022 We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation. \n\n\u2022 When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.",
                    "score": 0.4248514079682154,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 370
                        },
                        {
                            "start": 371,
                            "end": 544
                        },
                        {
                            "start": 545,
                            "end": 611
                        },
                        {
                            "start": 612,
                            "end": 858
                        },
                        {
                            "start": 861,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1225
                        },
                        {
                            "start": 1226,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1503
                        },
                        {
                            "start": 1504,
                            "end": 1723
                        },
                        {
                            "start": 1724,
                            "end": 1810
                        },
                        {
                            "start": 1813,
                            "end": 1994
                        },
                        {
                            "start": 1997,
                            "end": 2130
                        },
                        {
                            "start": 2131,
                            "end": 2266
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 525,
                            "end": 543,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 797,
                            "end": 817,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.58203125
                }
            ],
            "relevance_judgement": 0.58203125,
            "relevance_judgment_input_expanded": "# Title: Bayesian WeakS-to-Strong from Text Classification to Generation\n# Venue: International Conference on Learning Representations\n# Authors: Ziyun Cui, Ziyang Zhang, Wen Wu, Guangzhi Sun, Chao Zhang\n## Abstract\nAdvances in large language models raise the question of how alignment techniques will adapt as models become increasingly complex and humans will only be able to supervise them weakly. Weak-to-Strong mimics such a scenario where weak model supervision attempts to harness the full capabilities of a much stronger model. This work extends Weak-to-Strong to WeakS-to-Strong by exploring an ensemble of weak models which simulate the variability in human opinions. Confidence scores are estimated using a Bayesian approach to guide the WeakS-to-Strong generalization. Furthermore, we extend the application of WeakS-to-Strong from text classification tasks to text generation tasks where more advanced strategies are investigated for supervision. Moreover, direct preference optimization is applied to advance the student model's preference learning, beyond the basic learning framework of teacher forcing. Results demonstrate the effectiveness of the proposed approach for the reliability of a strong student model, showing potential for superalignment.\n## INTRODUCTION\nWith the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019;Bai et al., 2022a;Ouyang et al., 2022;Nakano et al., 2021;Askell et al., 2021). \n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong. \n\n\u2022 We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation. \n\n\u2022 When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.",
            "reference_string": "[270258300 | Cui et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Reinforcement Learning Enhanced LLMs: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 109,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.10400, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2109514219",
                    "name": "Shuhe Wang"
                },
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2257786599",
                    "name": "Jie Zhang"
                },
                {
                    "authorId": "2232782142",
                    "name": "Runyi Hu"
                },
                {
                    "authorId": "48570150",
                    "name": "Xiaoya Li"
                },
                {
                    "authorId": "2237773313",
                    "name": "Tianwei Zhang"
                },
                {
                    "authorId": "2276484424",
                    "name": "Jiwei Li"
                },
                {
                    "authorId": "2257430459",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2220704972",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "2285877067",
                    "name": "Eduard H. Hovy"
                }
            ],
            "abstract": "Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
            "corpus_id": 274776492,
            "sentences": [
                {
                    "corpus_id": "274776492",
                    "title": "Reinforcement Learning Enhanced LLMs: A Survey",
                    "text": "ChatGLM (GLM et al., 2024), developed by Zhipu AI, represents an evolving series of large language models. The latest version in this series is GLM-4, which includes variants such as GLM-4, GLM-4-Air, and GLM-4-9B. These models are pre-trained on a dataset of over 10 trillion tokens, predominantly in Chinese and English, and are subsequently post-trained through a combination of supervised fine-tuning (SFT) and RLHF to achieve advanced alignment quality. Evaluation results indicate that GLM-4 rivals or even surpasses GPT-4 (OpenAI, 2023) on general benchmarks like MMLU, and demonstrates superior performance in Chinese-specific alignments as measured by Align-Bench (Liu et al., 2023b). \n\nThe reinforcement learning phase involves the ChatGLM-RLHF (Hou et al., 2024) pipeline, which enhances alignment with human preferences. This pipeline comprises three primary components: gathering human preference data, training a re-ward model, and optimizing policy models. To support large-scale training, ChatGLM-RLHF includes methods to reduce reward variance for stable training, leverages model parallelism with fused gradient descent, and applies regularization constraints to prevent catastrophic forgetting in large language models. Experimental results confirm that ChatGLM-RLHF yields substantial improvements in alignment-focused tasks compared to the supervised fine-tuned version of ChatGLM.",
                    "score": 0.433260409137268,
                    "section_title": "ChatGLM",
                    "char_start_offset": 26854,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 106
                        },
                        {
                            "start": 107,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 458
                        },
                        {
                            "start": 459,
                            "end": 693
                        },
                        {
                            "start": 696,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1402
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.580078125
                }
            ],
            "relevance_judgement": 0.580078125,
            "relevance_judgment_input_expanded": "# Title: Reinforcement Learning Enhanced LLMs: A Survey\n# Venue: arXiv.org\n# Authors: Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li, Tianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, Eduard H. Hovy\n## Abstract\nReinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements. In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.\n## ChatGLM\nChatGLM (GLM et al., 2024), developed by Zhipu AI, represents an evolving series of large language models. The latest version in this series is GLM-4, which includes variants such as GLM-4, GLM-4-Air, and GLM-4-9B. These models are pre-trained on a dataset of over 10 trillion tokens, predominantly in Chinese and English, and are subsequently post-trained through a combination of supervised fine-tuning (SFT) and RLHF to achieve advanced alignment quality. Evaluation results indicate that GLM-4 rivals or even surpasses GPT-4 (OpenAI, 2023) on general benchmarks like MMLU, and demonstrates superior performance in Chinese-specific alignments as measured by Align-Bench (Liu et al., 2023b). \n\nThe reinforcement learning phase involves the ChatGLM-RLHF (Hou et al., 2024) pipeline, which enhances alignment with human preferences. This pipeline comprises three primary components: gathering human preference data, training a re-ward model, and optimizing policy models. To support large-scale training, ChatGLM-RLHF includes methods to reduce reward variance for stable training, leverages model parallelism with fused gradient descent, and applies regularization constraints to prevent catastrophic forgetting in large language models. Experimental results confirm that ChatGLM-RLHF yields substantial improvements in alignment-focused tasks compared to the supervised fine-tuned version of ChatGLM.",
            "reference_string": "[274776492 | Wang et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 46,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.20053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2139984073",
                    "name": "Avelina Asada Hadji-Kyriacou"
                },
                {
                    "authorId": "46837178",
                    "name": "Ognjen Arandjelov\u00edc"
                }
            ],
            "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",
            "corpus_id": 270123676,
            "sentences": [
                {
                    "corpus_id": "270123676",
                    "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
                    "text": "Reinforcement Learning from Human Feedback (RLHF) is a technique that can be used to align an agent -such as a Large Language Model (LLM) -to human preferences and lead to more truthful, more helpful, less harmful and more preferred outputs [31].Proximal Policy Optimization (PPO) [38] and Direct Preference Optimization (DPO) [33] are two such aligment techniques which have been extensively used to improve the quality of LLM outputs, leading to instruction following agents or chat assistants which are quickly approaching human-baselines in a variety of knowledge and reasoning tasks [5,11,43,20,26,37,12].\n\nHowever, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it.One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters.Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone.\n\nTo combat the the risk of RLHF compromising the abilities of an LLM in favor of producing preferable outputs we introduce Direct Preference Heads (DPH), a novel feature based approach that optimises a reward score produced by the LLM rather than optimising the logits produced by language modelling head.DPH can be used in combination with (or without) existing alignment techniques to allow language models to self-evaluate outputs sampled at inference time and select the highest scoring candidate.\n\nWe evaluate the performance of DPH using an efficient 551M parameter LM on a variety of commonsense reasoning and Natural Language Understanding (NLU) tasks.All code used to train our models is available on GitHub and we release our model weights on Hugging Face.",
                    "score": 0.5036600552789388,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 246
                        },
                        {
                            "start": 246,
                            "end": 610
                        },
                        {
                            "start": 612,
                            "end": 729
                        },
                        {
                            "start": 729,
                            "end": 1058
                        },
                        {
                            "start": 1058,
                            "end": 1234
                        },
                        {
                            "start": 1236,
                            "end": 1540
                        },
                        {
                            "start": 1540,
                            "end": 1736
                        },
                        {
                            "start": 1738,
                            "end": 1895
                        },
                        {
                            "start": 1895,
                            "end": 2001
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.568359375
                }
            ],
            "relevance_judgement": 0.568359375,
            "relevance_judgment_input_expanded": "# Title: Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads\n# Venue: Neural Information Processing Systems\n# Authors: Avelina Asada Hadji-Kyriacou, Ognjen Arandjelov\u00edc\n## Abstract\nPre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.\n## Introduction\nReinforcement Learning from Human Feedback (RLHF) is a technique that can be used to align an agent -such as a Large Language Model (LLM) -to human preferences and lead to more truthful, more helpful, less harmful and more preferred outputs [31].Proximal Policy Optimization (PPO) [38] and Direct Preference Optimization (DPO) [33] are two such aligment techniques which have been extensively used to improve the quality of LLM outputs, leading to instruction following agents or chat assistants which are quickly approaching human-baselines in a variety of knowledge and reasoning tasks [5,11,43,20,26,37,12].\n\nHowever, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it.One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters.Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone.\n\nTo combat the the risk of RLHF compromising the abilities of an LLM in favor of producing preferable outputs we introduce Direct Preference Heads (DPH), a novel feature based approach that optimises a reward score produced by the LLM rather than optimising the logits produced by language modelling head.DPH can be used in combination with (or without) existing alignment techniques to allow language models to self-evaluate outputs sampled at inference time and select the highest scoring candidate.\n\nWe evaluate the performance of DPH using an efficient 551M parameter LM on a variety of commonsense reasoning and Natural Language Understanding (NLU) tasks.All code used to train our models is available on GitHub and we release our model weights on Hugging Face.",
            "reference_string": "[270123676 | Hadji-Kyriacou et al. | 2024 | Citations: 1]"
        },
        {
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 25,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.12057",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.12057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2173701426",
                    "name": "Akide Liu"
                }
            ],
            "abstract": "Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 \\cite{brown2020language, leiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to this approach is the establishment of a tiered policy for \\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in \\cref{fig:overall}, where the level of support rendered is modulated across entry, intermediate, and advanced tiers based on the complexity of the query, with adjustments made in response to human feedback. A comprehensive evaluation of this methodology is conducted using multiple LLMs and the results indicate state-of-the-art performance in \\cref{comparison} , surpassing existing solutions including ChatPDF.com. Moreover, the paper emphasizes that this approach is more efficient compared to the direct processing of full text by LLMs. The source code is publicly available at: \\url{https://github.com/AkideLiu/ANLP}.",
            "corpus_id": 260125946,
            "sentences": [
                {
                    "corpus_id": "260125946",
                    "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
                    "text": "The main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment. So the key takeaway is that for alignment, \"less is more\" -focused optimization of a small, diverse dataset can be more effective than simply scaling up data quantity. Response from MPT-30B-Chat 1 The authors of the paper propose a novel method for generating diverse and high-quality responses to open-ended prompts using a pretrained language model called LIMA. 2 They demonstrate that fine-tuning a strong pretrained language model on a small number of carefully curated examples can produce competitive results on a wide range of prompts. 3 They also find that the mental effort required to construct such examples is a key factor in achieving alignment and that less is often more when it comes to the amount of data needed for instruction tuning. 4 They compare LIMA to other state-of-",
                    "score": 0.4216194480107571,
                    "section_title": "21",
                    "char_start_offset": 61924,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5673828125
                }
            ],
            "relevance_judgement": 0.5673828125,
            "relevance_judgment_input_expanded": "# Title: External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback\n# Venue: arXiv.org\n# Authors: Akide Liu\n## Abstract\nMemory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 \\cite{brown2020language, leiter2023chatgpt, zaitsu2023distinguishing, OpenAI2023GPT4TR} , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to this approach is the establishment of a tiered policy for \\textbf{External Reasoning based on Multiple LLM Interchange Assistance} in \\cref{fig:overall}, where the level of support rendered is modulated across entry, intermediate, and advanced tiers based on the complexity of the query, with adjustments made in response to human feedback. A comprehensive evaluation of this methodology is conducted using multiple LLMs and the results indicate state-of-the-art performance in \\cref{comparison} , surpassing existing solutions including ChatPDF.com. Moreover, the paper emphasizes that this approach is more efficient compared to the direct processing of full text by LLMs. The source code is publicly available at: \\url{https://github.com/AkideLiu/ANLP}.\n## 21\nThe main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment. So the key takeaway is that for alignment, \"less is more\" -focused optimization of a small, diverse dataset can be more effective than simply scaling up data quantity. Response from MPT-30B-Chat 1 The authors of the paper propose a novel method for generating diverse and high-quality responses to open-ended prompts using a pretrained language model called LIMA. 2 They demonstrate that fine-tuning a strong pretrained language model on a small number of carefully curated examples can produce competitive results on a wide range of prompts. 3 They also find that the mental effort required to construct such examples is a key factor in achieving alignment and that less is often more when it comes to the amount of data needed for instruction tuning. 4 They compare LIMA to other state-of-",
            "reference_string": "[260125946 | Liu | 2023 | Citations: 1]"
        },
        {
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 276,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328012601",
                    "name": "Luping Wang"
                },
                {
                    "authorId": "2328756685",
                    "name": "Sheng Chen"
                },
                {
                    "authorId": "2328023793",
                    "name": "Linnan Jiang"
                },
                {
                    "authorId": "2330945343",
                    "name": "Shu Pan"
                },
                {
                    "authorId": "2328007357",
                    "name": "Runze Cai"
                },
                {
                    "authorId": "2328645120",
                    "name": "Sen Yang"
                },
                {
                    "authorId": "2328276373",
                    "name": "Fei Yang"
                }
            ],
            "abstract": "The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.",
            "corpus_id": 273653892,
            "sentences": [
                {
                    "corpus_id": "273653892",
                    "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
                    "text": "Then human labelers rank these output pairs based on their preferences. Given human predictions, the reward model is trained to predict these rankings, effectively learning human preferences. Notably, [102] proposes an approach, namely Reinforcement Learning from AI Feedback (RLAIF), the annotation of preference on response pairs can be generated by an AI agent, increasing the automatic ability of the reinforcement process. \u201a Reinforcement Learning Fine-Tuning: The final step involves formalizing the alignment process as a reinforcement learning problem. Here, the pre-trained language model acts as a policy generating text, with the reward model providing feedback scores. To prevent the model from deviating too far from its initial state, a penalty term is often included in the reward function. The language model is then optimized using algorithms like SARSA [103], DQN [104], PPO [105], DPO [106], and GRPO [63], iteratively improving its performance based on human-aligned rewards. 3) Datasets for LLM: A critical component of the development and deployment of LLM is the datasets used at various stages of their lifecycle, which significantly influence their capabilities and performance. In this section, we delve into the datasets that are instrumental in the Pre-training, SFT, and RLHF. The Pre-training phase is where an LLM absorbs the foundational knowledge from a diverse array of textual data. This stage is pivotal, as it sets the stage for the model's general understanding of language. The datasets used in Pretraining are vast and varied, encompassing everything from the sprawling expanse of the internet to curated collections of literature and encyclopedias. SFT is the process where the LLM is fine-tuned on specific tasks or domains. This phase refines the model's abilities, enabling it to perform with greater precision and relevance in targeted applications. SFT datasets are often more specialized and may include annotated examples that guide the model towards desired behaviors and outputs. RLHF is the stage where the LLM is further optimized based on human feedback. This phase enhances the model's alignment with human preferences and values, ensuring that its outputs are more aligned with user expectations.",
                    "score": 0.4167238424391366,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 35412,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 71
                        },
                        {
                            "start": 72,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 427
                        },
                        {
                            "start": 428,
                            "end": 560
                        },
                        {
                            "start": 561,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1203
                        },
                        {
                            "start": 1204,
                            "end": 1305
                        },
                        {
                            "start": 1306,
                            "end": 1417
                        },
                        {
                            "start": 1418,
                            "end": 1512
                        },
                        {
                            "start": 1513,
                            "end": 1689
                        },
                        {
                            "start": 1690,
                            "end": 1766
                        },
                        {
                            "start": 1767,
                            "end": 1894
                        },
                        {
                            "start": 1895,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2107
                        },
                        {
                            "start": 2108,
                            "end": 2251
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 871,
                            "end": 876,
                            "matchedPaperCorpusId": "10253791"
                        },
                        {
                            "start": 882,
                            "end": 887,
                            "matchedPaperCorpusId": "57373825"
                        },
                        {
                            "start": 904,
                            "end": 909,
                            "matchedPaperCorpusId": "258959321"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55078125
                }
            ],
            "relevance_judgement": 0.55078125,
            "relevance_judgment_input_expanded": "# Title: Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies\n# Venue: arXiv.org\n# Authors: Luping Wang, Sheng Chen, Linnan Jiang, Shu Pan, Runze Cai, Sen Yang, Fei Yang\n## Abstract\nThe large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.\n## I. INTRODUCTION\nThen human labelers rank these output pairs based on their preferences. Given human predictions, the reward model is trained to predict these rankings, effectively learning human preferences. Notably, [102] proposes an approach, namely Reinforcement Learning from AI Feedback (RLAIF), the annotation of preference on response pairs can be generated by an AI agent, increasing the automatic ability of the reinforcement process. \u201a Reinforcement Learning Fine-Tuning: The final step involves formalizing the alignment process as a reinforcement learning problem. Here, the pre-trained language model acts as a policy generating text, with the reward model providing feedback scores. To prevent the model from deviating too far from its initial state, a penalty term is often included in the reward function. The language model is then optimized using algorithms like SARSA [103], DQN [104], PPO [105], DPO [106], and GRPO [63], iteratively improving its performance based on human-aligned rewards. 3) Datasets for LLM: A critical component of the development and deployment of LLM is the datasets used at various stages of their lifecycle, which significantly influence their capabilities and performance. In this section, we delve into the datasets that are instrumental in the Pre-training, SFT, and RLHF. The Pre-training phase is where an LLM absorbs the foundational knowledge from a diverse array of textual data. This stage is pivotal, as it sets the stage for the model's general understanding of language. The datasets used in Pretraining are vast and varied, encompassing everything from the sprawling expanse of the internet to curated collections of literature and encyclopedias. SFT is the process where the LLM is fine-tuned on specific tasks or domains. This phase refines the model's abilities, enabling it to perform with greater precision and relevance in targeted applications. SFT datasets are often more specialized and may include annotated examples that guide the model towards desired behaviors and outputs. RLHF is the stage where the LLM is further optimized based on human feedback. This phase enhances the model's alignment with human preferences and values, ensuring that its outputs are more aligned with user expectations.",
            "reference_string": "[273653892 | Wang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Chain of Hindsight Aligns Language Models with Feedback",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 91,
            "citation_count": 124,
            "influential_citation_count": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2302.02676",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.02676, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2143855835",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "47218071",
                    "name": "Carmelo Sferrazza"
                },
                {
                    "authorId": "1689992",
                    "name": "P. Abbeel"
                }
            ],
            "abstract": "Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.",
            "corpus_id": 257038005,
            "sentences": [
                {
                    "corpus_id": "257038005",
                    "title": "Chain of Hindsight Aligns Language Models with Feedback",
                    "text": "The main techniques behind them can be categorized as supervised finetuning (SFT) or training on filtered human annotations and learning a reward function from human feedback for reinforcement learning, which is often dubbed as RLHF [10,32,27,50] and has been used to train RL agents without the need for hand-designed rewards. Ouyang et al. [35] demonstrates improved language model alignment performance by training models with SFT and RLHF using human feedback. Our work belongs to the category of SFT, and differs from SFT in that our method conditions on feedback and can learn from examples without positive ratings. Our method is complementary to RLHF and can be directly combined together for further improvement. \n\nUsing instructions to provide models with human preference and desired behaviors is demonstrated in Bai et al. [5], where models are prompted with a set of statements/principles and are trained with RLHF. In our work, we provide models with a sequence of model outputs and their feedback and train models to generate desired outputs conditioned on feedback/control tokens. \n\nInstruction finetuning and conditional training. Finetuning on chain of hindsight using human feedback is akin to instruction finetuning. Driven by the impressive in-context learning ability of large language models, finetuning pretrained models on instructions has been shown to improve language models in many benchmarks [see e.g. [52] are widely considered as instructions in prior works [11,51], specifically in the form of step by step explanations written by humans. In relation to these, our chain of hindsight consists of human written hindsight feedback and ranked model outputs. Conditional training [21,13,25,8,12,31] explores conditioning the model on some control tokens for controllable generations. In relation to it, CoH generalizes to condition on a sequence of control tokens instead of one control token. By doing so, CoH enables the model to understand the differences between control tokens and their corresponding outputs. Our work suggests a promising direction of using hindsight feedback to construct instructions from model outputs, and can be combined with prior instruction finetuning and conditional training works for further improvements.",
                    "score": 0.44752776742608125,
                    "section_title": "Related Work",
                    "char_start_offset": 21995,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 327
                        },
                        {
                            "start": 328,
                            "end": 464
                        },
                        {
                            "start": 465,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 721
                        },
                        {
                            "start": 724,
                            "end": 928
                        },
                        {
                            "start": 929,
                            "end": 1096
                        },
                        {
                            "start": 1099,
                            "end": 1147
                        },
                        {
                            "start": 1148,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1571
                        },
                        {
                            "start": 1572,
                            "end": 1687
                        },
                        {
                            "start": 1688,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 1922
                        },
                        {
                            "start": 1923,
                            "end": 2043
                        },
                        {
                            "start": 2044,
                            "end": 2268
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 233,
                            "end": 237,
                            "matchedPaperCorpusId": "4787508"
                        },
                        {
                            "start": 237,
                            "end": 240,
                            "matchedPaperCorpusId": "8818528"
                        },
                        {
                            "start": 240,
                            "end": 243,
                            "matchedPaperCorpusId": "235377145"
                        },
                        {
                            "start": 243,
                            "end": 246,
                            "matchedPaperCorpusId": "4130751"
                        },
                        {
                            "start": 1719,
                            "end": 1721,
                            "matchedPaperCorpusId": "235294299"
                        },
                        {
                            "start": 1724,
                            "end": 1727,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53662109375
                }
            ],
            "relevance_judgement": 0.53662109375,
            "relevance_judgment_input_expanded": "# Title: Chain of Hindsight Aligns Language Models with Feedback\n# Venue: International Conference on Learning Representations\n# Authors: Hao Liu, Carmelo Sferrazza, P. Abbeel\n## Abstract\nLearning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.\n## Related Work\nThe main techniques behind them can be categorized as supervised finetuning (SFT) or training on filtered human annotations and learning a reward function from human feedback for reinforcement learning, which is often dubbed as RLHF [10,32,27,50] and has been used to train RL agents without the need for hand-designed rewards. Ouyang et al. [35] demonstrates improved language model alignment performance by training models with SFT and RLHF using human feedback. Our work belongs to the category of SFT, and differs from SFT in that our method conditions on feedback and can learn from examples without positive ratings. Our method is complementary to RLHF and can be directly combined together for further improvement. \n\nUsing instructions to provide models with human preference and desired behaviors is demonstrated in Bai et al. [5], where models are prompted with a set of statements/principles and are trained with RLHF. In our work, we provide models with a sequence of model outputs and their feedback and train models to generate desired outputs conditioned on feedback/control tokens. \n\nInstruction finetuning and conditional training. Finetuning on chain of hindsight using human feedback is akin to instruction finetuning. Driven by the impressive in-context learning ability of large language models, finetuning pretrained models on instructions has been shown to improve language models in many benchmarks [see e.g. [52] are widely considered as instructions in prior works [11,51], specifically in the form of step by step explanations written by humans. In relation to these, our chain of hindsight consists of human written hindsight feedback and ranked model outputs. Conditional training [21,13,25,8,12,31] explores conditioning the model on some control tokens for controllable generations. In relation to it, CoH generalizes to condition on a sequence of control tokens instead of one control token. By doing so, CoH enables the model to understand the differences between control tokens and their corresponding outputs. Our work suggests a promising direction of using hindsight feedback to construct instructions from model outputs, and can be combined with prior instruction finetuning and conditional training works for further improvements.",
            "reference_string": "[257038005 | Liu et al. | 2023 | Citations: 124]"
        },
        {
            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 118,
            "influential_citation_count": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.07124",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.07124, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2192674200",
                    "name": "Yuhui Li"
                },
                {
                    "authorId": "2239197291",
                    "name": "Fangyun Wei"
                },
                {
                    "authorId": "2256929424",
                    "name": "Jinjing Zhao"
                },
                {
                    "authorId": "2256776221",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "40975176",
                    "name": "Hongyang Zhang"
                }
            ],
            "abstract": "Large language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.",
            "corpus_id": 261705563,
            "sentences": [
                {
                    "corpus_id": "261705563",
                    "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
                    "text": "Pre-trained large language models (LLMs) exhibit a remarkable capacity to address human queries, aid in coding tasks, and more. Nonetheless, the generated outputs of these models can sometimes diverge from preferred human values and even pose potential risks. To make pre-trained LLMs more user-friendly and safe, numerous alignment methods have been proposed, such as RLHF (Casper et al., 2023), RLAIF (Bai et al., 2022b), RRHF (Yuan et al., 2023), RAFT (Dong et al., 2023), and DPO (Rafailov et al., 2023). These methods, however, necessitate the finetuning of pre-trained LLMs and demand considerable amounts of meticulously human-annotated data and computational resources. Take RLHF as an example, this comprehensive approach encompasses three primary phases: supervised finetuning (SFT), reward modeling (RM), and reinforcement learning (RL), together with the necessity to manage four separate models or heads-policy, value, reward, and reference models-each of which has at least billions of parameters. Efficiently operating these models requires significant GPU memory, and the act of updating their parameters poses a threat of overwriting the knowledge retained from the initial pre-training. Additionally, it is worth noting that training larger models is often met with heightened instability and requires significant engineering expertise. Hence, aligning frozen LLMs presents a more appealing option to the community. \n\nThis work shows that fixed LLMs are alignable using a novel inference method without finetuning and data. To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used. Logically, the action of \"selecting a sub-distribution\" should not mandate modifications to model parameters. Reject sampling is a working example of inference-time alignment. However, the method is sample-inefficient (as tested by our experiments).",
                    "score": 0.469622249609457,
                    "section_title": "Introduction",
                    "char_start_offset": 435,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 259
                        },
                        {
                            "start": 260,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1354
                        },
                        {
                            "start": 1355,
                            "end": 1433
                        },
                        {
                            "start": 1436,
                            "end": 1541
                        },
                        {
                            "start": 1542,
                            "end": 1827
                        },
                        {
                            "start": 1828,
                            "end": 1937
                        },
                        {
                            "start": 1938,
                            "end": 2003
                        },
                        {
                            "start": 2004,
                            "end": 2077
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53515625
                }
            ],
            "relevance_judgement": 0.53515625,
            "relevance_judgment_input_expanded": "# Title: RAIN: Your Language Models Can Align Themselves without Finetuning\n# Venue: International Conference on Learning Representations\n# Authors: Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang\n## Abstract\nLarge language models (LLMs) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned the pre-trained models using reinforcement learning or instruction tuning, a.k.a. the finetuning step. In contrast, aligning frozen LLMs without requiring alignment data is more appealing. This work explores the potential of the latter setting. We discover that by integrating self-evaluation and rewind mechanisms, unaligned LLMs can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method, Rewindable Auto-regressive INference (RAIN), that allows pre-trained LLMs to evaluate their own generation and use the evaluation results to guide rewind and generation for AI safety. Notably, RAIN operates without the need of extra data for model alignment and abstains from any training, gradient computation, or parameter updates. Experimental results evaluated by GPT-4 and humans demonstrate the effectiveness of RAIN: on the HH dataset, RAIN improves the harmlessness rate of LLaMA 30B from 82% of vanilla inference to 97%, while maintaining the helpfulness rate. On the TruthfulQA dataset, RAIN improves the truthfulness of the already-well-aligned LLaMA-2-chat 13B model by 5%.\n## Introduction\nPre-trained large language models (LLMs) exhibit a remarkable capacity to address human queries, aid in coding tasks, and more. Nonetheless, the generated outputs of these models can sometimes diverge from preferred human values and even pose potential risks. To make pre-trained LLMs more user-friendly and safe, numerous alignment methods have been proposed, such as RLHF (Casper et al., 2023), RLAIF (Bai et al., 2022b), RRHF (Yuan et al., 2023), RAFT (Dong et al., 2023), and DPO (Rafailov et al., 2023). These methods, however, necessitate the finetuning of pre-trained LLMs and demand considerable amounts of meticulously human-annotated data and computational resources. Take RLHF as an example, this comprehensive approach encompasses three primary phases: supervised finetuning (SFT), reward modeling (RM), and reinforcement learning (RL), together with the necessity to manage four separate models or heads-policy, value, reward, and reference models-each of which has at least billions of parameters. Efficiently operating these models requires significant GPU memory, and the act of updating their parameters poses a threat of overwriting the knowledge retained from the initial pre-training. Additionally, it is worth noting that training larger models is often met with heightened instability and requires significant engineering expertise. Hence, aligning frozen LLMs presents a more appealing option to the community. \n\nThis work shows that fixed LLMs are alignable using a novel inference method without finetuning and data. To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used. Logically, the action of \"selecting a sub-distribution\" should not mandate modifications to model parameters. Reject sampling is a working example of inference-time alignment. However, the method is sample-inefficient (as tested by our experiments).",
            "reference_string": "[261705563 | Li et al. | 2023 | Citations: 118]"
        },
        {
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.10804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118240359",
                    "name": "Jinhao Jiang"
                },
                {
                    "authorId": "2018027",
                    "name": "Junyi Li"
                },
                {
                    "authorId": "2290240238",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2157992507",
                    "name": "Yang Song"
                },
                {
                    "authorId": "2146341464",
                    "name": "Tao Zhang"
                },
                {
                    "authorId": "2274218622",
                    "name": "Ji-Rong Wen"
                }
            ],
            "abstract": "Adapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge memorization, followed by training to apply this knowledge following human instructions and preferences. However, this method may result in inefficient knowledge memorization due to a lack of awareness of knowledge utilization and imposes substantial demands on LLMs to simultaneously learn knowledge utilization and format alignment with limited training samples. To facilitate the domain adaptation of LLM, we revise this process and propose a new domain adaptation framework including domain knowledge learning and general format alignment, called Mix-CPT. Specifically, we first conduct a knowledge mixture continual pre-training that concurrently focuses on knowledge memorization and utilization, allowing for mutual reinforcement. To avoid catastrophic forgetting during the continual pre-training process, we further incorporate a logit swap self-distillation constraint. Subsequently, leveraging the knowledge and capabilities acquired during continual pre-training, we efficiently perform instruction tuning and alignment with a few general training samples to achieve format alignment. Extensive experiments demonstrate that our proposed Mix-CPT framework can simultaneously improve the task-solving capabilities of LLMs on the target and general domains compared to the traditional adaptation methods.",
            "corpus_id": 271213062,
            "sentences": [
                {
                    "corpus_id": "271213062",
                    "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
                    "text": "Instruction Tuning and Alignment.Instruction tuning (also known as supervised fine-tuning) employs human-annotated instructions (Sanh et al., 2022;Mishra et al., 2022;K\u00f6pf et al., 2023;Sun et al., 2023) or synthetic instructions by proprietary models (Taori et al., 2023;Chiang et al., 2023;Wang et al., 2023b) to fine-tune LLMs.Besides, alignment with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a) or direct preference optimization (DPO) (Rafailov et al., 2023a) aims to align LLMs with human preference.Both instruction tuning and alignment are able to elicit knowledge from LLMs and improve their capabilities to solve downstream tasks.\n\nRecent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks.Inspired by these studies, we propose to expose knowledge memorization and capability elicitation from instruction tuning and alignment.Unlike these studies which typically focused on instruction tuning or alignment, we differ in that we unify the three stages of training LLMs (i.e., continual pre-training, instruction tuning, and alignment) and conduct a knowledge mixture pre-training to mainly focus on learning new domain knowledge while maintaining general knowledge.",
                    "score": 0.4184022341562816,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 32983,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 33
                        },
                        {
                            "start": 33,
                            "end": 329
                        },
                        {
                            "start": 329,
                            "end": 532
                        },
                        {
                            "start": 532,
                            "end": 666
                        },
                        {
                            "start": 668,
                            "end": 936
                        },
                        {
                            "start": 936,
                            "end": 1085
                        },
                        {
                            "start": 1085,
                            "end": 1419
                        },
                        {
                            "start": 1419,
                            "end": 1555
                        },
                        {
                            "start": 1555,
                            "end": 1893
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 167,
                            "matchedPaperCorpusId": "237421373"
                        },
                        {
                            "start": 167,
                            "end": 185,
                            "matchedPaperCorpusId": "258179434"
                        },
                        {
                            "start": 185,
                            "end": 202,
                            "matchedPaperCorpusId": "258479665"
                        },
                        {
                            "start": 680,
                            "end": 699,
                            "matchedPaperCorpusId": "258822910"
                        },
                        {
                            "start": 1065,
                            "end": 1084,
                            "matchedPaperCorpusId": "258822910"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53076171875
                }
            ],
            "relevance_judgement": 0.53076171875,
            "relevance_judgment_input_expanded": "# Title: Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment\n# Venue: arXiv.org\n# Authors: Jinhao Jiang, Junyi Li, Wayne Xin Zhao, Yang Song, Tao Zhang, Ji-Rong Wen\n## Abstract\nAdapting general large language models (LLMs) to specialized domains presents great challenges due to varied data distributions. This adaptation typically requires continual pre-training on massive domain-specific corpora to facilitate knowledge memorization, followed by training to apply this knowledge following human instructions and preferences. However, this method may result in inefficient knowledge memorization due to a lack of awareness of knowledge utilization and imposes substantial demands on LLMs to simultaneously learn knowledge utilization and format alignment with limited training samples. To facilitate the domain adaptation of LLM, we revise this process and propose a new domain adaptation framework including domain knowledge learning and general format alignment, called Mix-CPT. Specifically, we first conduct a knowledge mixture continual pre-training that concurrently focuses on knowledge memorization and utilization, allowing for mutual reinforcement. To avoid catastrophic forgetting during the continual pre-training process, we further incorporate a logit swap self-distillation constraint. Subsequently, leveraging the knowledge and capabilities acquired during continual pre-training, we efficiently perform instruction tuning and alignment with a few general training samples to achieve format alignment. Extensive experiments demonstrate that our proposed Mix-CPT framework can simultaneously improve the task-solving capabilities of LLMs on the target and general domains compared to the traditional adaptation methods.\n## RELATED WORK\nInstruction Tuning and Alignment.Instruction tuning (also known as supervised fine-tuning) employs human-annotated instructions (Sanh et al., 2022;Mishra et al., 2022;K\u00f6pf et al., 2023;Sun et al., 2023) or synthetic instructions by proprietary models (Taori et al., 2023;Chiang et al., 2023;Wang et al., 2023b) to fine-tune LLMs.Besides, alignment with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a) or direct preference optimization (DPO) (Rafailov et al., 2023a) aims to align LLMs with human preference.Both instruction tuning and alignment are able to elicit knowledge from LLMs and improve their capabilities to solve downstream tasks.\n\nRecent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks.Inspired by these studies, we propose to expose knowledge memorization and capability elicitation from instruction tuning and alignment.Unlike these studies which typically focused on instruction tuning or alignment, we differ in that we unify the three stages of training LLMs (i.e., continual pre-training, instruction tuning, and alignment) and conduct a knowledge mixture pre-training to mainly focus on learning new domain knowledge while maintaining general knowledge.",
            "reference_string": "[271213062 | Jiang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Anchored Alignment for Self-Explanations Enhancement",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304958073",
                    "name": "Luis-Felipe Villa-Arenas"
                },
                {
                    "authorId": "2304424699",
                    "name": "Ata Nizamoglu"
                },
                {
                    "authorId": "2257126685",
                    "name": "Qianli Wang"
                },
                {
                    "authorId": "2280334474",
                    "name": "Sebastian Moller"
                },
                {
                    "authorId": "2114572998",
                    "name": "Vera Schmitt"
                }
            ],
            "abstract": "In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",
            "corpus_id": 273403465,
            "sentences": [
                {
                    "corpus_id": "273403465",
                    "title": "Anchored Alignment for Self-Explanations Enhancement",
                    "text": "In this section, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning-self-explanation-even in the absence of annotated rationale explanations. However, we assume access to human-annotated data in the form of classification datasets for domain-specific adaptation, reflecting a common constraint in realworld applications, where comprehensive explanation data is often scarce or prohibitively expensive compared to classification datasets. \n\nBuilding on prior work (Bai et al., 2022;Wang et al., 2023;Yuan et al., 2024;Wu et al., 2024), our alignment methodology incorporates familiar components such as self-instruction dataset generation, human-free evaluation of candidate responses using LLM-as-Judge, preference pair selection, and model alignment. \n\nHowever, our approach differs from previous methods in two key ways: First, for the assessment of candidate responses, we use the evaluation explanation quality framework introduced in Sections 2.1 and 2.2. Second, we propose a novel technique, Alignment with Anchor Preference Pairs, which improves preference pair selection by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of DPO. \n\nThe steps of the methodology are as follows: \n\n1. Supervised fine-tuning of the base model M Base specifically on a target classification task, resulting in M SFT . \n\n2. Instruct M SFT to generate multiple explanation-prediction pairs for each prompt, and evaluate the quality of these self-explanations using the methodology outlined in Sections 2.1 and 2.2. During alignment, the base model M Base acts as the judge M Judge , ensuring the process remains self-contained. \n\n3. Construct an alignment dataset by selecting preference pairs using an anchor-based strategy (see Section 3.3). \n\n4. Align M SFT via DPO with the dataset created in the third step, producing the aligned model M Anchor .",
                    "score": 0.4297518083081697,
                    "section_title": "SELF-EXPLANATION ALIGNMENT WITH ANCHOR PREFERENCE PAIRS",
                    "char_start_offset": 6078,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 225
                        },
                        {
                            "start": 226,
                            "end": 521
                        },
                        {
                            "start": 524,
                            "end": 835
                        },
                        {
                            "start": 838,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1359
                        },
                        {
                            "start": 1362,
                            "end": 1406
                        },
                        {
                            "start": 1409,
                            "end": 1526
                        },
                        {
                            "start": 1529,
                            "end": 1721
                        },
                        {
                            "start": 1722,
                            "end": 1834
                        },
                        {
                            "start": 1837,
                            "end": 1950
                        },
                        {
                            "start": 1953,
                            "end": 2058
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 583,
                            "end": 601,
                            "matchedPaperCorpusId": "267035293"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52587890625
                }
            ],
            "relevance_judgement": 0.52587890625,
            "relevance_judgment_input_expanded": "# Title: Anchored Alignment for Self-Explanations Enhancement\n# Venue: arXiv.org\n# Authors: Luis-Felipe Villa-Arenas, Ata Nizamoglu, Qianli Wang, Sebastian Moller, Vera Schmitt\n## Abstract\nIn this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.\n## SELF-EXPLANATION ALIGNMENT WITH ANCHOR PREFERENCE PAIRS\nIn this section, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning-self-explanation-even in the absence of annotated rationale explanations. However, we assume access to human-annotated data in the form of classification datasets for domain-specific adaptation, reflecting a common constraint in realworld applications, where comprehensive explanation data is often scarce or prohibitively expensive compared to classification datasets. \n\nBuilding on prior work (Bai et al., 2022;Wang et al., 2023;Yuan et al., 2024;Wu et al., 2024), our alignment methodology incorporates familiar components such as self-instruction dataset generation, human-free evaluation of candidate responses using LLM-as-Judge, preference pair selection, and model alignment. \n\nHowever, our approach differs from previous methods in two key ways: First, for the assessment of candidate responses, we use the evaluation explanation quality framework introduced in Sections 2.1 and 2.2. Second, we propose a novel technique, Alignment with Anchor Preference Pairs, which improves preference pair selection by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of DPO. \n\nThe steps of the methodology are as follows: \n\n1. Supervised fine-tuning of the base model M Base specifically on a target classification task, resulting in M SFT . \n\n2. Instruct M SFT to generate multiple explanation-prediction pairs for each prompt, and evaluate the quality of these self-explanations using the methodology outlined in Sections 2.1 and 2.2. During alignment, the base model M Base acts as the judge M Judge , ensuring the process remains self-contained. \n\n3. Construct an alignment dataset by selecting preference pairs using an anchor-based strategy (see Section 3.3). \n\n4. Align M SFT via DPO with the dataset created in the third step, producing the aligned model M Anchor .",
            "reference_string": "[273403465 | Villa-Arenas et al. | 2024 | Citations: 0]"
        },
        {
            "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 60,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49081354",
                    "name": "Souradip Chakraborty"
                },
                {
                    "authorId": "2279349525",
                    "name": "Jiahao Qiu"
                },
                {
                    "authorId": "2279340788",
                    "name": "Hui Yuan"
                },
                {
                    "authorId": "2063871",
                    "name": "Alec Koppel"
                },
                {
                    "authorId": "2261325066",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "3387859",
                    "name": "A. S. Bedi"
                },
                {
                    "authorId": "2268167170",
                    "name": "Mengdi Wang"
                }
            ],
            "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.",
            "corpus_id": 267657954,
            "sentences": [
                {
                    "corpus_id": "267657954",
                    "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
                    "text": "In this section, we present a comprehensive empirical evaluation of the alignment impossibilities and our proposed solutions for language models, structured into two distinct subsections: Small This figure shows the performance of our proposed MaxMin RLHF algorithm for the preference dataset described in Figure 2. The task is to align a language model to generate positive sentiment responses that are concise (of shorter token length) in nature. The top row shows the sentiment score (higher is better) distribution of the language model before alignment (left) and after alignment (right). The bottom row shows the conciseness score (lower is better) distribution of the language model before alignment (left) and after alignment (right). We note that MaxMin-RLHF aligned language model can generate highly positive sentiment sentences and satisfy the conciseness criteria. This shows alignment with both the majority and minority preferences. \n\nScale experiments (Sec. 5.1) for initial proof of concept, and Large Scale experiments (Sec. 5.2) for broader validation. We first demonstrate the practical challenges of alignment (cf. Theorem 1), followed by showcasing the efficacy of our MaxMin-RLHF strategy. This approach illustrates that, with a focus on social welfare objectives, alignment across diverse human preferences is attainable.",
                    "score": 0.4429957641247676,
                    "section_title": "Experimental Results",
                    "char_start_offset": 31330,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 742
                        },
                        {
                            "start": 743,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 947
                        },
                        {
                            "start": 950,
                            "end": 973
                        },
                        {
                            "start": 974,
                            "end": 1042
                        },
                        {
                            "start": 1043,
                            "end": 1071
                        },
                        {
                            "start": 1072,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1212
                        },
                        {
                            "start": 1213,
                            "end": 1345
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51806640625
                }
            ],
            "relevance_judgement": 0.51806640625,
            "relevance_judgment_input_expanded": "# Title: MaxMin-RLHF: Alignment with Diverse Human Preferences\n# Venue: International Conference on Machine Learning\n# Authors: Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, A. S. Bedi, Mengdi Wang\n## Abstract\nReinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language models (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional RLHF algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and fairness of our approach. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.\n## Experimental Results\nIn this section, we present a comprehensive empirical evaluation of the alignment impossibilities and our proposed solutions for language models, structured into two distinct subsections: Small This figure shows the performance of our proposed MaxMin RLHF algorithm for the preference dataset described in Figure 2. The task is to align a language model to generate positive sentiment responses that are concise (of shorter token length) in nature. The top row shows the sentiment score (higher is better) distribution of the language model before alignment (left) and after alignment (right). The bottom row shows the conciseness score (lower is better) distribution of the language model before alignment (left) and after alignment (right). We note that MaxMin-RLHF aligned language model can generate highly positive sentiment sentences and satisfy the conciseness criteria. This shows alignment with both the majority and minority preferences. \n\nScale experiments (Sec. 5.1) for initial proof of concept, and Large Scale experiments (Sec. 5.2) for broader validation. We first demonstrate the practical challenges of alignment (cf. Theorem 1), followed by showcasing the efficacy of our MaxMin-RLHF strategy. This approach illustrates that, with a focus on social welfare objectives, alignment across diverse human preferences is attainable.",
            "reference_string": "[267657954 | Chakraborty et al. | 2024 | Citations: 60]"
        },
        {
            "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.01798, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2188780004",
                    "name": "Atoosa Malemir Chegini"
                },
                {
                    "authorId": "2329098249",
                    "name": "Hamid Kazemi"
                },
                {
                    "authorId": "2256998308",
                    "name": "Iman Mirzadeh"
                },
                {
                    "authorId": "2329100656",
                    "name": "Dong Yin"
                },
                {
                    "authorId": "2321176651",
                    "name": "Maxwell Horton"
                },
                {
                    "authorId": "2279751489",
                    "name": "Moin Nabi"
                },
                {
                    "authorId": "1682124",
                    "name": "Mehrdad Farajtabar"
                },
                {
                    "authorId": "1398372702",
                    "name": "Keivan Alizadeh-Vahid"
                }
            ],
            "abstract": "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.",
            "corpus_id": 273811395,
            "sentences": [
                {
                    "corpus_id": "273811395",
                    "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
                    "text": "In our main experiments, we focus on reward-based RLHF, in particular Proximal Policy Optimization (PPO). The conventional framework for reward-based RLHF consists of several key stages as follows. \n\nSupervised Fine-Tuning (SFT). The initial stage of alignment involves supervised fine-tuning, where a pre-trained language model is refined using a high-quality instruction dataset. \n\nReward Model. Reward-based RLHF involves training a reward model, which is typically initialized from the SFT model. In this process, the logit layer of the SFT model is replaced by a new linear layer. This linear layer takes the embedding of the last token and outputs a scalar reward. Higher rewards indicate better samples. For a given prompt x and a pair of responses y w (chosen) and y l (rejected), the loss function is optimized as: \n\nwhere R \u03b8 (\u2022, \u2022) is the reward model, \u03b8 denotes its parameters, \u03c3 indicates the sigmoid function. \n\nPolicy Training. The last phase of RLHF is dedicated to training the policy model, which is initialized from a reference model, typically the SFT model. Based on a recent study [Xu et al., 2024], PPO performs better in distribution shifts and results in superior alignment with human preferences across challenging tasks like code generation and dialogue systems. Therefore, we selected PPO as the training algorithm. The goal is to optimize the policy model to maximize the reward for a given prompt x and its generated response y, while also minimizing the KL divergence between the policy model and the reference model. The overall loss function for this stage is given by: \n\nwhere \u03c0 \u03b8 is the policy model, \u03c0 ref is the reference model, and R(., .) is the trained reward model.",
                    "score": 0.47488818945465217,
                    "section_title": "RLHF",
                    "char_start_offset": 9527,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 197
                        },
                        {
                            "start": 200,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 381
                        },
                        {
                            "start": 384,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 670
                        },
                        {
                            "start": 671,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 823
                        },
                        {
                            "start": 826,
                            "end": 923
                        },
                        {
                            "start": 926,
                            "end": 942
                        },
                        {
                            "start": 943,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1289
                        },
                        {
                            "start": 1290,
                            "end": 1343
                        },
                        {
                            "start": 1344,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1602
                        },
                        {
                            "start": 1605,
                            "end": 1706
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51416015625
                }
            ],
            "relevance_judgement": 0.51416015625,
            "relevance_judgment_input_expanded": "# Title: SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF\n# Venue: arXiv.org\n# Authors: Atoosa Malemir Chegini, Hamid Kazemi, Iman Mirzadeh, Dong Yin, Maxwell Horton, Moin Nabi, Mehrdad Farajtabar, Keivan Alizadeh-Vahid\n## Abstract\nIn Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model's ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-based Alignment Learning for Stronger Adaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama2-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.\n## RLHF\nIn our main experiments, we focus on reward-based RLHF, in particular Proximal Policy Optimization (PPO). The conventional framework for reward-based RLHF consists of several key stages as follows. \n\nSupervised Fine-Tuning (SFT). The initial stage of alignment involves supervised fine-tuning, where a pre-trained language model is refined using a high-quality instruction dataset. \n\nReward Model. Reward-based RLHF involves training a reward model, which is typically initialized from the SFT model. In this process, the logit layer of the SFT model is replaced by a new linear layer. This linear layer takes the embedding of the last token and outputs a scalar reward. Higher rewards indicate better samples. For a given prompt x and a pair of responses y w (chosen) and y l (rejected), the loss function is optimized as: \n\nwhere R \u03b8 (\u2022, \u2022) is the reward model, \u03b8 denotes its parameters, \u03c3 indicates the sigmoid function. \n\nPolicy Training. The last phase of RLHF is dedicated to training the policy model, which is initialized from a reference model, typically the SFT model. Based on a recent study [Xu et al., 2024], PPO performs better in distribution shifts and results in superior alignment with human preferences across challenging tasks like code generation and dialogue systems. Therefore, we selected PPO as the training algorithm. The goal is to optimize the policy model to maximize the reward for a given prompt x and its generated response y, while also minimizing the KL divergence between the policy model and the reference model. The overall loss function for this stage is given by: \n\nwhere \u03c0 \u03b8 is the policy model, \u03c0 ref is the reference model, and R(., .) is the trained reward model.",
            "reference_string": "[273811395 | Chegini et al. | 2024 | Citations: 1]"
        },
        {
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.21438, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308348037",
                    "name": "Zhichao Wang"
                },
                {
                    "authorId": "2308275618",
                    "name": "Bin Bi"
                },
                {
                    "authorId": "2312678367",
                    "name": "Zixu Zhu"
                },
                {
                    "authorId": "2308660934",
                    "name": "Xiang-Bo Mao"
                },
                {
                    "authorId": "2328302000",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2328258020",
                    "name": "Shiyu Wang"
                }
            ],
            "abstract": "By pretraining on trillions of tokens, an LLM gains the capability of text generation. However, to enhance its utility and reduce potential harm, SFT and alignment are applied sequentially to the pretrained model. Due to the differing nature and objective functions of SFT and alignment, catastrophic forgetting has become a significant issue. To address this, we introduce Unified Fine-Tuning (UFT), which integrates SFT and alignment into a single training stage using the same objective and loss functions through an implicit reward function. Our experimental results demonstrate that UFT outperforms SFT on instruction-tuning data alone. Moreover, when combining instruction-tuning data with alignment data, UFT effectively prevents catastrophic forgetting across these two stages and shows a clear advantage over sequentially applying SFT and alignment. This is evident in the significant improvements observed in the \\textbf{ifeval} task for instruction-following and the \\textbf{truthful-qa} task for factuality. The proposed general fine-tuning framework UFT establishes an effective and efficient pretraining-UFT paradigm for LLM training.",
            "corpus_id": 273662392,
            "sentences": [
                {
                    "corpus_id": "273662392",
                    "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
                    "text": "To enable large language models (LLMs) to understand and generate natural language, they are constructed with billions of parameters and pretrained on datasets containing trillions of tokens [OAA + 24]. However, several challenges arise after the pretraining stage of LLMs [WBP + 24]. One major issue is that pretrained LLMs can only continue generation based on the previous context and often struggle to accurately answer user questions. To address this, supervised fine-tuning (SFT) is introduced, using pairs of questions and answers. For example, in models like Mistral, preset instructions such as ' [INST]' and '[/INST]' are used to frame a question as a prompt [JSM + 23]. The corresponding answer is then used as the target output. The model's probability of generating the correct answer is maximized through next-token prediction, employing the cross-entropy loss function to classify tokens across the entire token space. \n\nThe next challenge for LLMs lies in ethical concerns, where LLMs may inadvertently teach humans to engage in unethical activities, such as robbing banks [OWJ + 22]. To address this issue, various alignment methodologies have been proposed, including Reinforcement Learning from Human Feedback (RLHF) [OWJ + 22, BJN + 22] with Proximal Policy Optimization (PPO) [SWD + 17], Direct Preference Optimization (DPO) [RSM + 23], Kahneman & Tversky Optimization (KTO) [EXM + 24], and UNified Alignment (UNA) [WBH + 24]. The core idea of alignment is to equip LLMs with the ability to reject harmful requests by learning from human feedback. \n\nFor pretrained LLMs, SFT and alignment are traditionally performed in sequence. However, this staged approach often leads to performance degradation, where the model loses capabilities acquired in earlier phases. This paper seeks to address and mitigate this degradation. \n\nFigure 1: UFT integrates SFT and alignment through a generalized implicit reward function. It likens pre-training and fine-tuning of LLMs to Chinese proveb \"Read ten thousand books, travel ten thousand miles\".",
                    "score": 0.43637307028789896,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 284
                        },
                        {
                            "start": 285,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 538
                        },
                        {
                            "start": 539,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 740
                        },
                        {
                            "start": 741,
                            "end": 933
                        },
                        {
                            "start": 936,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1568
                        },
                        {
                            "start": 1571,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1783
                        },
                        {
                            "start": 1784,
                            "end": 1842
                        },
                        {
                            "start": 1845,
                            "end": 1935
                        },
                        {
                            "start": 1936,
                            "end": 2054
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50830078125
                }
            ],
            "relevance_judgement": 0.50830078125,
            "relevance_judgment_input_expanded": "# Title: UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function\n# Venue: arXiv.org\n# Authors: Zhichao Wang, Bin Bi, Zixu Zhu, Xiang-Bo Mao, Jun Wang, Shiyu Wang\n## Abstract\nBy pretraining on trillions of tokens, an LLM gains the capability of text generation. However, to enhance its utility and reduce potential harm, SFT and alignment are applied sequentially to the pretrained model. Due to the differing nature and objective functions of SFT and alignment, catastrophic forgetting has become a significant issue. To address this, we introduce Unified Fine-Tuning (UFT), which integrates SFT and alignment into a single training stage using the same objective and loss functions through an implicit reward function. Our experimental results demonstrate that UFT outperforms SFT on instruction-tuning data alone. Moreover, when combining instruction-tuning data with alignment data, UFT effectively prevents catastrophic forgetting across these two stages and shows a clear advantage over sequentially applying SFT and alignment. This is evident in the significant improvements observed in the \\textbf{ifeval} task for instruction-following and the \\textbf{truthful-qa} task for factuality. The proposed general fine-tuning framework UFT establishes an effective and efficient pretraining-UFT paradigm for LLM training.\n## Introduction\nTo enable large language models (LLMs) to understand and generate natural language, they are constructed with billions of parameters and pretrained on datasets containing trillions of tokens [OAA + 24]. However, several challenges arise after the pretraining stage of LLMs [WBP + 24]. One major issue is that pretrained LLMs can only continue generation based on the previous context and often struggle to accurately answer user questions. To address this, supervised fine-tuning (SFT) is introduced, using pairs of questions and answers. For example, in models like Mistral, preset instructions such as ' [INST]' and '[/INST]' are used to frame a question as a prompt [JSM + 23]. The corresponding answer is then used as the target output. The model's probability of generating the correct answer is maximized through next-token prediction, employing the cross-entropy loss function to classify tokens across the entire token space. \n\nThe next challenge for LLMs lies in ethical concerns, where LLMs may inadvertently teach humans to engage in unethical activities, such as robbing banks [OWJ + 22]. To address this issue, various alignment methodologies have been proposed, including Reinforcement Learning from Human Feedback (RLHF) [OWJ + 22, BJN + 22] with Proximal Policy Optimization (PPO) [SWD + 17], Direct Preference Optimization (DPO) [RSM + 23], Kahneman & Tversky Optimization (KTO) [EXM + 24], and UNified Alignment (UNA) [WBH + 24]. The core idea of alignment is to equip LLMs with the ability to reject harmful requests by learning from human feedback. \n\nFor pretrained LLMs, SFT and alignment are traditionally performed in sequence. However, this staged approach often leads to performance degradation, where the model loses capabilities acquired in earlier phases. This paper seeks to address and mitigate this degradation. \n\nFigure 1: UFT integrates SFT and alignment through a generalized implicit reward function. It likens pre-training and fine-tuning of LLMs to Chinese proveb \"Read ten thousand books, travel ten thousand miles\".",
            "reference_string": "[273662392 | Wang et al. | 2024 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271162009",
            "title": "Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing",
            "text": "Alignment Algorithms.Aligning LLMs towards human-desired objectives is a problem that has been significantly noticed.Common methods for model alignment usually involve SFT and RLHF.SFT (Brown et al., 2020;Wang et al., 2022) finetunes a pre-trained model on task-specific data which contains instructional commands and human-annotated expected outcome (Chiang et al., 2023;Taori et al., 2023).RLHF is a technique that fine-tunes language models using human preferences to align their outputs with desired behaviors.Glaese et al. (2022); Rafailov et al. (2024) use RLHF to improve LLM safety when facing malicious questions.However, successfully training models using SFT or RLHF is challenging.The quality and quantity of training data are crucial for good training results and effectiveness (Zhou et al., 2024;Wang et al., 2024;Taori et al., 2023;Achiam et al., 2023;Touvron et al., 2023), requiring extensive data collection, cleaning, computational resources, and time.Besides, researchers have also discovered that during the training process of SFT or RLHF, the reasoning and understanding capabilities of models may decrease (Ouyang et al., 2022;Lu et al., 2024;Yue et al., 2024).This phenomenon may be caused by overestimating the model to overfit to the reward model or training data distribution (Noukhovitch et al., 2023;Rita et al., 2024), deviating from the original model and losing general capabilities.\n\nModification of LLM Parameters and forward process.Prior studies have explored modifying the forward propagation process or directly altering model parameters.Meng et al. (2022Meng et al. ( , 2023) ) propose model editing methods to update or insert specific knowledge without affecting other basic knowledge.Geva et al. (2022) hypothesize the existence of word vectors in MLP layers strongly correlating with specific tokens and propose setting activations of selected word vectors to a constant for detoxification.",
            "score": 0.657754650193859,
            "section_title": "Related Works",
            "char_start_offset": 5344,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 181
                },
                {
                    "start": 181,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1416
                },
                {
                    "start": 1418,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 536,
                    "end": 558,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 791,
                    "end": 810,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 810,
                    "end": 828,
                    "matchedPaperCorpusId": "259108263"
                },
                {
                    "start": 1130,
                    "end": 1151,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1167,
                    "end": 1184,
                    "matchedPaperCorpusId": "263831589"
                },
                {
                    "start": 1304,
                    "end": 1330,
                    "matchedPaperCorpusId": "266151093"
                },
                {
                    "start": 1577,
                    "end": 1594,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34716796875
        },
        {
            "corpus_id": "268856712",
            "title": "HyperCLOVA X Technical Report",
            "text": "The first phase in alignment learning is SFT, in which HyperCLOVA, the pretrained LLM, is trained to maximize the likelihood of a completion given each prompt.This phase improves the model's ability to follow instructions and solve problems such as coding and creative writing.Furthermore, it allows the model to leverage knowledge from data across various domains, ranging from commonsense to humanities, sciences, and ethics.\n\nIn our SFT dataset, we define three special tokens: '<|user|>', '<|assistant|>', and '<|endofturn|>' to distinguish between the user's and the assistant's turns.Even if a token corresponding to a special token is part of the user input, it is processed as a regular token, thereby ensuring that each role in the context remains distinct from the user's instruction.For training on multi-turn samples, we apply loss masking on all text except for the assistant's turns.\n\nFor SFT training, we use an efficient batching strategy that groups sequences with similar lengths, in order to minimize padding within mini-batches and increase GPU utilization.The actual mini-batch size depends on the average length of sequences in each mini-batch, but the maximum number of tokens for each mini-batch is kept the same.",
            "score": 0.6262484752240849,
            "section_title": "Supervised Fine-tuning (SFT)",
            "char_start_offset": 8776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 427
                },
                {
                    "start": 429,
                    "end": 590
                },
                {
                    "start": 590,
                    "end": 794
                },
                {
                    "start": 794,
                    "end": 897
                },
                {
                    "start": 899,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2176513671875
        },
        {
            "corpus_id": "268531548",
            "title": "OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",
            "text": "The upper part of Figure 2 shows the results from the evaluated open-source LLMs for each dimension (averaged over all tasks in the corresponding evaluation dimension).Generally, SFT/RLHF can help LLMs better leverage the knowledge acquired during pre-training and improve their ability to follow instructions.As a result, most SFT/RLHFtrained LLMs can handle general questions reasonably well.However, many LLMs, regardless of their size, still struggle with more complex tasks like commonsense reasoning and certain NLP tasks.This suggests that the training data in SFT/RLHF may lack diversity in instructions, leading to improvements only in specific tasks similar to the SFT/RLHF data style.\n\nQwen-72B-Chat is the largest open-source LLM in our experiments, excelling all other open-source LLMs in mathematical reasoning.However, it falls short compared to Yi-34B-Chat in disciplinary knowledge.Interestingly, the top LLMs in NLP tasks evaluation are InternLM-Chat-7B and InternLM-Chat-7B-v1.1, both based on InternLM, and they outperform larger LLMs like Qwen-72B-Chat and Yi-34B-Chat.Moreover, the leading models in alignment evaluation are Baichuan2-7B-Chat and Baichuan2-13B-Chat, both built on Baichuan2.This suggests that the quality of pretrained LLMs significantly impacts subsequent performance.Our evaluation results also suggest which dimensions are focused on for improvement through pre-training and SFT/RLHF in the assessed LLMs.For instance, Baichuan2 prioritizes alignment, leading to competitive performance in the alignment evaluation of OpenEval.BELLE-7B-2M and MOSS-SFT-16B appear less impressive as they have been released earlier than other evaluated open-source LLMs.Furthermore, these two LLMs demonstrate strong performance in safety, probably due to inverse scaling law (Perez et al., 2023).",
            "score": 0.6110147880970063,
            "section_title": "Results from Open-source LLMs",
            "char_start_offset": 15907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 168,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 695
                },
                {
                    "start": 697,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1213
                },
                {
                    "start": 1213,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1447
                },
                {
                    "start": 1447,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321044921875
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "According to this hypothesis, the alignment process, primarily through supervised fine-tuning (SFT), does not inject new knowledge or improve inherent abilities but rather adjusts the output response format. This implies that the strong mathematical reasoning ability may not be significantly improved by a large amount of synthetic SFT data. \n\nIn this paper, we re-examine these two common beliefs mentioned above regarding mathematical reasoning abilities of LLMs. For the first belief, we introduce the Skywork-Math model series, which are supervised fine-tuned (SFT) on common 7B pre-trained LLM models without employing other complex alignment techniques such as RLHF (Bai et al., 2022;Casper et al., 2023) and DPO (Rafailov et al., 2024). Skywork-Math 7B models have achieved impressive accuracies of 51.2% on the competition-level MATH (Hendrycks et al., 2021) benchmark and 83.9% on the GSM8K (Cobbe et al., 2021) benchmark, notably outperforming an early version of GPT-4 on MATH. Our empirical findings, consistent with the conclusions in Li et al. (2024), suggest that strong mathematical reasoning ability can indeed exist in common 7B language models. Moreover, scaling up synthetic SFT data can further enhance the mathematical reasoning ability of Skywork-Math 7B models. \n\nFor the second belief, we propose Skywork-MathQA high-quality SFT dataset containing 2.5 million instances, which is much larger than open-sourced dataset of its kind to date, such as MetaMathQA (Yu et al., 2024) containing 395K samples. We empirically observe that the scaling law curve on the SFT alignment for mathematical reasoning in modern LLMs is far from being saturated (ref. Figure 5). We have carefully scaled the Skywork-MathQA SFT dataset with diverse and high-quality samples specifically within the mathematical domain to enhance the model's capability in understanding and solving mathematical problems.",
            "score": 0.6027615243015156,
            "section_title": "Introduction",
            "char_start_offset": 2127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 743,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 843,
                    "end": 867,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1484,
                    "end": 1501,
                    "matchedPaperCorpusId": "262084051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.495849609375
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "Alignment in LLMs. Large Language Models (LLMs) have recently transformed Natural Language Processing (NLP) (Achiam et al., 2023;Anil et al., 2023;Anthropic, 2024;Touvron et al., 2023), excelling in tasks such as automated summarization (Scao et al., 2022) and machine translation (Almazrouei et al., 2023). Alignment in LLMs refers to the process of ensuring that the model's outputs adhere to user preferences (Shen et al., 2023). Various techniques contribute to achieving alignment, including supervised fine-tuning (SFT) (Taori et al., 2023), reinforcement learning from human feedback (RLHF) (Bai et al., 2022), and direct policy optimization (DPO) (Rafailov et al., 2024). Among these techniques, SFT is typically an indispensable method for aligning LLMs and has achieved highly competitive performance across various tasks (Chiang et al., 2023), particularly in mathematical reasoning (Li et al., 2024). SFT involves fine-tuning a pre-trained large model using annotated data, making the model's performance more accurate for downstream tasks. Our work aims to deeply explore the performance boundaries of common 7B pre-trained LLMs using only the SFT alignment technique. \n\nQuantity and Quality of SFT Data. Data is the fuel that powers the performance of LLMs. This ongoing discussion about whether the quantity or quality of SFT data is more important highlights their significance in enhancing the SFT performance of LLMs. (1) Quantity. Many recent research demonstrates the scaling properties in LLM fine-tuning (Kaplan et al., 2020;Li et al., 2024). The size of the fine-tuning dataset is a crucial factor affecting the LLMs' performance. However, the optimal fine-tuning data size is highly task-dependent (Zhang et al., 2024). ( 2) Quality.",
            "score": 0.5953212868785338,
            "section_title": "Related Work",
            "char_start_offset": 7725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 655,
                    "end": 678,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1722,
                    "end": 1742,
                    "matchedPaperCorpusId": "268032247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47607421875
        },
        {
            "corpus_id": "270357323",
            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
            "text": "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023).3. Fine-tune an LLM with RL using PPO (Schulman et al., 2017) and the reward model.RLHF is the most commonly used method for preference alignment but often requires a lot of computation and steps for alignment.Various variants of RLHF have been proposed, such as using pure RL for training LLMs with human feedback in an online manner (Bai et al., 2022) and modifying the reward modeling with adversarial probing (Glaese et al., 2022).",
            "score": 0.5869886070044211,
            "section_title": "F Background and Related Work F.1 Alignment Methods",
            "char_start_offset": 34017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 206
                },
                {
                    "start": 206,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6416015625
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "Supervised fine-tuning (SFT) refines large language models (LLMs) using task-specific instruction data to enhance their capability to follow instructions (Touvron et al., 2023;Peng et al., 2023) and to align their outputs with human preferences and safety considerations (Ouyang et al., 2022;Rafailov et al., 2023;Dong et al., 2023b;Yuan et al., 2023).This process is often termed \"alignment\", signifying the tailoring of model outputs to conform to specific downstream requirements.Nevertheless, current research casts doubt on the necessity and potential adverse impacts of SFT.But the alignment achieved through SFT is often considered to be \"superficial\", with the process potentially repurposing pre-existing knowledge from pre-training to merely reshape outputs to meet specific criteria (Zhou et al., 2023;Lin et al., 2023).It has been observed that even a small-scale SFT training dataset can produce significant alignment effects (Liu et al., 2023;Xia et al., 2024).On the other hand, recent empirical studies (Luo et al., 2023;Dong et al., 2023a) have raised concerns that SFT might hurt the knowledge acquired during its pre-training phase, leading to serious consequences like catastrophic forgetting.\n\nNot only is there no definitive consensus on the necessity of SFT, but the majority of these studies also focus on monolingual tasks.LLMs still encounter challenges in handling complex crosslingual generation tasks (Schioppa et al., 2023;Wang et al., 2023).Current research on crosslingual alignment primarily seeks to extrapolate or align English capabilities to other languages using the SFT paradigm (Zhang et al., 2023;Chai et al., 2024;Xu et al., 2024), yet there remains a gap in exploring the specific impacts of SFT-based cross-lingual alignment.Furthermore, given the potential risk of SFT leading to the forgetting of pre-training knowledge, the question of how to achieve cross-lingual alignment without training remains underexplored.",
            "score": 0.586826663519264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 483
                },
                {
                    "start": 483,
                    "end": 580
                },
                {
                    "start": 580,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1213
                },
                {
                    "start": 1215,
                    "end": 1348
                },
                {
                    "start": 1348,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 292,
                    "end": 314,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 314,
                    "end": 333,
                    "matchedPaperCorpusId": "258170300"
                },
                {
                    "start": 333,
                    "end": 351,
                    "matchedPaperCorpusId": "258059818"
                },
                {
                    "start": 939,
                    "end": 957,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 1019,
                    "end": 1037,
                    "matchedPaperCorpusId": "261031244"
                },
                {
                    "start": 1638,
                    "end": 1656,
                    "matchedPaperCorpusId": "266999425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "271328240",
            "title": "ALLaM: Large Language Models for Arabic and English",
            "text": "To extrinsically evaluate the impact of higher quality SFT data, we trained two 13B models using our v1 and v2 SFT datasets.Even though v2 has half as many samples and v1, both versions performed equally well on English and Arabic evaluation benchmarks, as shown in Table 3.This reduction in data volume led to faster training times and reduced costs without compromising performance.It also clearly demonstrates the value of quality filtering for alignment.\n\nTraining Details We fine-tune our base model, which was trained on 3.2 trillion (2T Llama-2 + 1.2T ALLaM) tokens, for 3 epochs using Ultra-Instinct-v2 with a learning rate of 5 \u00d7 10 \u22126 and a batch size of 1024.The model is not trained to generate the prompt, as we mask out our prompt tokens when calculating the loss.Ultra-Instinct-v2 contains a substantial number of multiturn conversations.To train on these multi-turn conversations, we performed turn-augmentation.\n\nFigure 9 visually explains the process of turn augmentation.\n\nWhile training the SFT model, we encountered tokenization issues.Specifically, Llama-2's tokenizer was trained using sentencepiece8 , which breaks the beginning and end of sequence tokens into multiple tokens and adversely affects long multi-turn conversations.To address this issue, we patched sentencepiece using the HuggingFace LlamaTokenizer wrapper (Wolf et al., 2020).Over many iterations of training, we saw that even having 1% noisy samples (e.g., empty responses or formatting issues) in alignment data can noticeably affect model quality.",
            "score": 0.5803849898049382,
            "section_title": "Quality Filtering",
            "char_start_offset": 20657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 458
                },
                {
                    "start": 460,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 928
                },
                {
                    "start": 930,
                    "end": 990
                },
                {
                    "start": 992,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1366
                },
                {
                    "start": 1366,
                    "end": 1540
                }
            ],
            "ref_mentions": [
                {
                    "start": 1346,
                    "end": 1365,
                    "matchedPaperCorpusId": "269498086"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2054443359375
        },
        {
            "corpus_id": "270870458",
            "title": "When Search Engine Services Meet Large Language Models: Visions and Challenges",
            "text": "Following the broad-based learning in the pre-training stage, LLMs undergo Supervised Fine-tuning (SFT) to enhance their capabilities for particular applications.Later, model alignments, such as Reinforcement Learning from Human Feedback (RLHF), would be carried out to adjust the model's outputs to closely match human expectations and norms, thereby improving the model's efficacy, accuracy, and even ethical considerations in its applications [5].\n\nSFT is a powerful technique for optimizing LLMs to perform specific tasks with enhanced accuracy and performance.This process involves leveraging the pre-existing knowledge of the LLM, gained through pre-training on extensive datasets, and adapting it to excel in targeted applications.\n\n\u2022 Data Preparation: The process begins by selecting taskspecific, labeled datasets that align with the intended application of the LLM.This data could range from specialized corpora in sectors like healthcare or finance to structured question-answer pairs for tasks such as question-answering (QA) [113].\n\n\u2022 Training Procedure: SFT capitalizes token sequence prediction tasks (e.g., question-answering), enhancing the model's adaptability and accuracy without sacrificing generalizability [113].\n\nRLHF involves several steps designed to align the model's outputs with qualitative judgments or desired behaviors as determined by human feedback [5], [96].Key components include:  [133].3) Fine-tuning with Human Feedback: Continual finetuning using human feedback on new samples to refine both the LLM and the reward model, enhancing model alignment with human expectations [5].\n\nBy integrating model SFT and alignments, LLMs achieve superior performance, ethical soundness, and practical value in applications [13], [20].",
            "score": 0.5751542014118896,
            "section_title": "Supervised Fine-tuning (SFT) and Alignments",
            "char_start_offset": 17233,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 450
                },
                {
                    "start": 452,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 738
                },
                {
                    "start": 740,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1044
                },
                {
                    "start": 1046,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1616
                },
                {
                    "start": 1618,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 1038,
                    "end": 1043,
                    "matchedPaperCorpusId": "237440234"
                },
                {
                    "start": 1229,
                    "end": 1234,
                    "matchedPaperCorpusId": "237440234"
                },
                {
                    "start": 1388,
                    "end": 1392,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483154296875
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "Furthermore, given the potential risk of SFT leading to the forgetting of pre-training knowledge, the question of how to achieve cross-lingual alignment without training remains underexplored.\n\nTo bridge these gaps, our study conducts an indepth examination of the impact of SFT on crosslingual generation.We investigate the influence of SFT on the decoding patterns of foundation models in cross-lingual contexts, hypothesizing that the success of SFT largely hinges on the selection of initial prior tokens that are critical for eliciting taskspecific generation in the target language.Furthermore, the observed decoding similarities between Instruction: Translate the following sentence from English to Ukrainian: \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.foundation and SFT models support the extension of the superficial alignment hypothesis to crosslingual scenarios.Responding to these insights, we introduce a training-free alignment method named \"PRETTY\" for cross-lingual and non-English tasks.\n\nThe Prefix TexTs act as a Yarn (PRETTY) linking the foundation LLM and the SFT LLM, eliciting the foundation LLM to exhibit near-SFT performance levels.Specifically, we augment the original input with a few tokens that serve as decoding priors, and then prompt the foundation LLM to resume decoding based on this modified input.In most cases, only one or two task-related prior tokens are needed, and the method for constructing these prior tokens is flexible across various kinds of language resources, fostering the democratization of multilingual LLMs.\n\nWe conducted experiments on machine translation (Goyal et al., 2022), cross-lingual summarization (Bhattacharjee et al., 2023) and non-English part-of-speech (POS) tagging (Liang et al., 2020) tasks across eight languages.These tasks exemplify cross-lingual generation and multilingual language understanding, and they provide ample non-English test data to evaluate effectiveness across varying levels of resource availability.",
            "score": 0.56909159142684,
            "section_title": "Introduction",
            "char_start_offset": 1784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 194,
                    "end": 306
                },
                {
                    "start": 306,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 919
                },
                {
                    "start": 919,
                    "end": 1050
                },
                {
                    "start": 1052,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1607
                },
                {
                    "start": 1609,
                    "end": 1831
                },
                {
                    "start": 1831,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 1707,
                    "end": 1735,
                    "matchedPaperCorpusId": "258947845"
                },
                {
                    "start": 1781,
                    "end": 1801,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.271240234375
        },
        {
            "corpus_id": "267412276",
            "title": "Aligner: Efficient Alignment by Learning to Correct",
            "text": "Preliminary: Supervised Fine-Tuning (SFT) SFT aims to finetune the pretrained LLM to generate target answers using supervised learning -specifically, maximum likelihood estimation -on a curated high-quality dataset D SFT = {x (i) , y (i) } ( \n\nSimilarly, illustrated in Figure 1, Aligner improves alignment between the model and human intentions by redistributing the model's answers through conditional generation. In practical implementation, Aligner only needs to make a minor adjustment to the SFT training code (only need to change one line of code), as detailed in Appendix E. \n\nOverall, the whole pipeline of Aligner training can be summarized as follows: Based on a preference dataset, the model is fine-tuned to learn the correction residuals between preferred and non-preferred responses. After a single training session, this model can be deployed on any model to achieve corrected alignment. \n\nModel Training Based on the above procedures, we have constructed the dataset M = {x (i) , y \n\no , y \n\nc } N i=1 , which x represents the user's query, y o is the original answer, and y c is the corrected answer according to established principles. The model training process is relatively straightforward. We train the Aligner, a conditional seq2seq model \u00b5 \u03d5 (y c |y o , x) parameterized by \u03d5, to redistribute the preliminary answers y o to the aligned answer y c . Demonstrated in Figure 1, the composed answer generation process for aligned answers based on the upstream LLM \u03c0 \u03b8 is: \n\nwhere y k is a possible answer generated by upstream LLM \u03c0 \u03b8 . By calculating empirical loss on the whole dataset M, we can get equation (3) from equation (2): \n\nThe second term in equation ( 3) is not related to the Aligner parameter and the training objective for Aligner can be derived as equation ( 4): \n\nBy optimizing this objective, we actually optimize the upper bound of the SFT training objective, which ensures that y c is effectively learned. It is worth noting that Aligner does not require access to the parameters of the upstream LLM \u03c0 \u03b8 during both training and inference phases.",
            "score": 0.5666040908369278,
            "section_title": "Aligner",
            "char_start_offset": 4416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1654
                },
                {
                    "start": 1657,
                    "end": 1801
                },
                {
                    "start": 1804,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2089
                }
            ],
            "ref_mentions": [
                {
                    "start": 1650,
                    "end": 1653,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27587890625
        },
        {
            "corpus_id": "269009422",
            "title": "Latent Distance Guided Alignment Training for Large Language Models",
            "text": "Over the past two years, LLMs have demonstrated strong performance.LLMs have shown remarkable performance in various fields of NLP, such as mathematical problem solving, summarization generation, reading comprehension, and open-ended question answering, achieving notable results.In order to align the behavior of LLMs with human expectations, such as adhering to facts and avoiding biases, and to better elicit their capabilities, such as mathematical reasoning, researchers have proposed alignment training methods, which typically involve a process requiring extensive manual annotation of data.Aligment training is typically employed after Supervised Fine-tune(SFT), with the most commonly used mainstream methods being Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO) (Rafailov et al., 2023).\n\nSince mainstream alignment training methods typically require extensive manual annotation, which is expensive to obtain, the pursuit of an alignment method that does not necessitate human annotation is becoming increasingly popular.To solve this challenging problem, there are currently some efforts aimed at avoiding manual annotation in alignment tasks.RLAIF (Lee et al., 2023) utilizes large language models to generate preference labels instead of human annotators and explores the direct utilization of language models to generate reward scores.SPIN (Chen et al., 2024) iteratively train a LLM to align on the SFT datasets through a self-play mechanism which shares a similar motivation with GAN (Goodfellow et al., 2020).Also, (Yuan et al., 2024) have studied Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training.\n\nIn the present study, we introduce a DPO-based novel approach termed LD-Align (Latent Distance Guided Alignment Training), aimed at iteratively aligning a fine-tuned LLM with a given highquality SFT dataset without any additional human annotation or reliance on a more powerful LLM for support.Within this framework, we consider samples sourced from the SFT dataset as golden labels, contrasting with those generated by the model, which we categorize as dispreferred samples.",
            "score": 0.5615104627382355,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 67,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 839
                },
                {
                    "start": 841,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1391
                },
                {
                    "start": 1391,
                    "end": 1568
                },
                {
                    "start": 1568,
                    "end": 1751
                },
                {
                    "start": 1753,
                    "end": 2047
                },
                {
                    "start": 2047,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 838,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1542,
                    "end": 1567,
                    "matchedPaperCorpusId": "1033682"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "265043685",
            "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
            "text": "Pre-trained large language models (LLMs) such as LLaMA (Touvron et al., 2023a) have shown remarkable potentials to solve various downstream tasks by mastering the universal pre-training task of next-token prediction. While after large-scale pre-training, it often needs subsequent tuning for enhancing and regulating the behaviors of LLMs. Two typical approaches are supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF), which can largely improve LLMs in both task solving capacity and human alignment (Ouyang et al., 2022). \n\nDespite widely explored, SFT and RLHF have their own strengths and weaknesses. On the one hand, SFT is easy to implement and can effectively boost the general task solving abilities by instruction based eliciting (Wei et al., 2021;Ouyang et al., 2022;Chung et al., 2022), while it mainly imitates the behaviors of experts (essentially doing behavior clone (Wiseman & Rush, 2016)), which are demonstrated by the human annotators or powerful LLMs such as ChatGPT. Therefore, the SFT performance highly relies on high-quality demonstration data (Zhou et al., 2023), and might suffer from the huge distribution shifts between its outputs and imitated outputs (Zhang et al., 2019;Schulman, 2023;Zhao et al., 2023a). On the other hand, RLHF can better explore the semantic space of LLMs, and identify the optimal policy by encouraging good behaviors and discouraging bad behaviors during learning. However, it is very complicated to effectively implement, often suffering from training instability issues such as reward collapse (Song et al., 2023;Wolf et al., 2023). \n\nTo leverage the benefits of SFT and RLHF, several recent studies propose to develop alignment approaches without reinforcement learning (RL). These studies typically construct refined instruction data using methods such as quantile ranking (Lu et al., 2022) and rejection-sampling (Touvron et al., 2023b), and then follow or slightly modify the original SFT loss.",
            "score": 0.5599054175763164,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 531,
                    "end": 552,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 769,
                    "end": 787,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 787,
                    "end": 807,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 912,
                    "end": 934,
                    "matchedPaperCorpusId": "2783746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52001953125
        },
        {
            "corpus_id": "271709991",
            "title": "Progressively Label Enhancement for Large Language Model Alignment",
            "text": "For instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF. RAFT (Dong et al., 2023) expands the SFT dataset by generating additional samples and selecting those with high reward scores to enhance the SFT dataset. RRHF (Yuan et al., 2023) simplifies the RLHF process by integrating the subsequent RL steps into the SFT phase as a regularization term. \n\nHowever, these methods are highly dependent on large amounts of high-quality data, which is impractical in certain applications, such as the medical field (Yang et al., 2024;Li et al., 2023) or chip design (Liu et al., 2023). Additionally, even though some methods generate extra data to expand the training set to alleviate the problem. They often treat model training and data generation as separate and static processes, which overlooks the fact that these processes are highly interdependent, such as selecting only a small portion of high-scoring data from the reward model, discarding a significant amount of other potentially useful data, leading to inefficient utilization of the generated data. Therefore, we consider designing an efficient framework that couples the data generation and model training processes, allowing them to work synergistically, thus ensuring that all generated data, including potentially useful lower-scoring data, is effectively utilized, thereby improving training efficiency. \n\nMotivated by the above consideration, we propose a novel framework named PLE, i.e., Progressively Label Enhancement for Language Model Alignment. Specifically, during the sample generation phase, we design a set of principles to guide the model to output according to human expectations. When the reward score difference between the principle-guided output and the response to the original query exceeds a dynamically updated threshold, indicating a significant improvement under the principle guiding, the model is encouraged to align its output with the better response and move away from the poorer one. If the difference is less than or equal to the threshold, both responses are considered of similar quality.",
            "score": 0.5553771175943076,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1626,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 525
                },
                {
                    "start": 528,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2150
                },
                {
                    "start": 2151,
                    "end": 2258
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 38,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "258170300"
                },
                {
                    "start": 394,
                    "end": 413,
                    "matchedPaperCorpusId": "258059818"
                },
                {
                    "start": 683,
                    "end": 702,
                    "matchedPaperCorpusId": "260681932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58837890625
        },
        {
            "corpus_id": "267616740",
            "title": "Rethinking Data Selection for Supervised Fine-Tuning",
            "text": "Large language models (LLMs) (Brown et al., 2020;Chowdhery et al., 2022;Anil et al., 2023;Touvron et al., 2023a,b;Jiang et al., 2023) have shown remarkable capabilities in canonical natural language understanding and generation tasks in recent years. To further teach LLMs to align with humans by following human instructions and interacting with humans in user-friendly manners, supervised finetuning, or instruction tuning (Ouyang et al., 2022;Wang et al., 2023c;Taori et al., 2023;Peng et al., 2023;Chiang et al., 2023;Xu et al., 2024;OpenAI, 2022OpenAI, , 2023)), has been applied as an indispensable alignment step where LLMs are finetuned on large sets of instruction-response demonstrations either annotated by human or synthesized by power proprietary models. \n\nDespite the success of alignment through SFT, LIMA (Zhou et al., 2023) proposes the superficial alignment hypothesis that the models' knowledge and abilities are learned mostly during pretraining, and SFT is all about style learning (Gudibande et al., 2024) of formatting the response in a humanlike manner. At the same time, recent works have been designing data selection strategies to filter SFT datasets, showing finetuning LLMs with a subset of the original dataset leads to superior instruction-following capabilities compared with utilizing the whole. Two major principles for designing existing selection strategies (Chen et al., 2023a;Lu et al., 2024;Chen et al., 2023b;Bukharin and Zhao, 2023;Du et al., 2023;Wu et al., 2023;Liu et al., 2024) are based on the quality and diversity aspects of the data. However, although these two factors are highly significant for developing AI systems in the pre-LLM era, we question their relevance under the SFT scenario in the current era of LLMs, given the superficial nature of SFT.",
            "score": 0.537477428920564,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 767
                },
                {
                    "start": 770,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 446,
                    "end": 465,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 502,
                    "end": 522,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.338623046875
        },
        {
            "corpus_id": "271903390",
            "title": "Threshold Filtering Packing for Supervised Fine-Tuning: Training Related Samples within Packs",
            "text": "SFT and Alignment Fine-tuning is a prevalent strategy to enhance model performance on downstream tasks, evidenced in domains such as coding (Wei et al., 2023;Luo et al., 2024) and arithmetic (Yue et al., 2024). Other work has highlighted the importance of consistency in format (Liang et al., 2023), data quality (Chung et al., 2022), and mixing tasks from different categories (Longpre et al., 2023;Iyer et al., 2022) in SFT. As LLMs evolve, the risk of generating unsafe content increases (Su et al., 2024;Wang et al., 2023a). Established methods for LLM alignment include instruction fine-tuning and reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022). Instruction fine-tuning, also known as SFT, refines pre-trained models using annotated instructional data, often preceding RLHF to aid initial alignment (Touvron et al., 2023). RLHF employs reinforcement learning to adapt models based on feedback on generated responses. Although RLHF has been pivotal for developing systems like ChatGPT (OpenAI, 2021), isolated instruction fine-tuning can yield comparable outcomes (Sun et al., 2023) with much less computational and labor costs. Packing While packing is relatively less researched, it is a technique extensively used in frameworks like Hugging Face's SFT Trainer1 to expedite inference and training. To prevent crosscontamination during self-attention calculation, existing packing approaches involve concatenating sequences into a single tensor and using masking to disregard elements from other sequences during computation (Kundu et al., 2024). This method, including variations like LongAlign (Bai et al., 2024) and Prepacking (Zhao et al., 2024a), enhances training efficiency and minimizes the crosscontamination impact on model performance. However, it necessitates calculating a distinct attention mask for each batch, complicating implementation and increasing memory consumption for masks, which can hinder the effectiveness of flash attention.",
            "score": 0.5242223616097876,
            "section_title": "Related Work",
            "char_start_offset": 4153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1983
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 175,
                    "matchedPaperCorpusId": "259164815"
                },
                {
                    "start": 191,
                    "end": 209,
                    "matchedPaperCorpusId": "261696697"
                },
                {
                    "start": 378,
                    "end": 400,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "259202782"
                },
                {
                    "start": 653,
                    "end": 674,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1093,
                    "end": 1111,
                    "matchedPaperCorpusId": "258479665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "271334000",
            "title": "A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More",
            "text": "3. avoiding the \"alignment tax\" in the RLHF process expressed as log(\u03c0 \u03b8 (x)) on pretaining datasets.The term \"alignment tax\" referred to the degradation in the performance of the LLM on downstream tasks following alignment.The parameter \u03b2 was used to control the weight of the KL divergence, with the authors suggesting that the optimal value lied between 0.01 and 0.02.When \u03b3 = 0, the loss function corresponded to the standard RLHF loss.When \u03b3 \u0338 = 0, the modified loss function was termed PPO-ptx, which addressed performance degradation on public NLP datasets.\n\nFor training InstructGPT, three datasets were utilized: 1. SFT dataset: contained labeler demonstrations used to train the SFT models.2. RM dataset: comprised labeler rankings of model outputs, which were used to train RMs. 3. PPO dataset: composed of prompts used as input for RLHF fine-tuning.Despite the complexity of the task, the inter-annotator agreement rates were notably high.Training labelers agreed with each other 72.6 \u00b1 1.5% of the time, while held-out labelers showed an agreement rate of 77.3 \u00b1 1.3%.\n\nThe authors trained a single 6B reward model to be utilized for training RLHF/PPO policy models of varying sizes.They also experimented with larger 175B reward models [44].Although larger RMs exhibited lower validation loss, their training processes were unstable and significantly increased the computational requirements for RLHF/PPO.The authors claimed that since the same input prompt generated K outputs, these outputs were correlated.A straightforward method to address this was to shuffle and train them randomly.However, this approach led to overfitting.To mitigate this issue, the authors trained all C 2 k comparisons as a batch, which improved the overfitting problem.One limitation of this method was that it did not account for the relative scores between responses; that was, pair responses with similar scores or those with very large score differences were treated the same.Subsequent works have considered this problem [28].\n\nThe trained InstructGPT was evaluated from three perspectives: Helpful, Honest, and Harms.",
            "score": 0.5217284569061182,
            "section_title": "InstructGPT",
            "char_start_offset": 12223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 224
                },
                {
                    "start": 224,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 564
                },
                {
                    "start": 566,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1081
                },
                {
                    "start": 1083,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1762
                },
                {
                    "start": 1762,
                    "end": 1973
                },
                {
                    "start": 1973,
                    "end": 2024
                },
                {
                    "start": 2026,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 1250,
                    "end": 1254,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "274610410",
            "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs",
            "text": "We empirically show that \"safe responses to toxic instructions in the SFT dataset is the reason for over-alignment\" is false. \n\n\u2022 In addition, we observe the surprising phenomenon of degradation of the model's language abilities when we augment our TA-SFT data with safe responses (from another LLM) to seeming toxic prompts, an observation also made when in work studying the use of AI generated data for training Shumailov et al. [2023]. \n\n2 Related Work and Background",
            "score": 0.5194480501079085,
            "section_title": "Introduction",
            "char_start_offset": 3863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 471
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10760498046875
        },
        {
            "corpus_id": "277271617",
            "title": "Won: Establishing Best Practices for Korean Financial NLP",
            "text": "Recent studies have shown that supervised finetuning (SFT) is effective enough in training reasoning language models (Muennighoff et al., 2025;Ye et al., 2025;Wen et al., 2025;Sun et al., 2025). Moreover, during the competition, submissions that have combined SFT with preference optimization techniques such as DPO or KTO have successfully adapted models for the Korean financial domain. Accordingly, we adopt a two-stage training approach: SFT followed by DPO. The SFT dataset comprises prompts paired with responses generated by Deepseek-R1, split evenly between English and Korean. For Korean prompts, the solutions are translated into Korean while retaining the reasoning process in English. The dataset is drawn from three sources: (1) English Prompt-R1 responses collected online (Zhao et al., 2025), (2) Korean Prompt-R1 responses collected online (Son et al., 2025), and (3) 86k prompts from Section 3.1, for which we generated responses using R1. We employed GPT-4o to filter correct samples, retrying up to six attempts for incorrect samples, resulting in approximately 400k instances. Post-SFT, the model struggled with everyday prompts, tended to overthink (Kumar et al., 2025), and occasionally displayed formatting issues by treating some queries as if they were MCQA tasks. We attribute these issues to the data distribution, which heavily emphasized academic multiple-choice questions paired with extended reasoning. To address these behaviors, we conducted a final DPO stag, where chosen samples are generated from R1, and rejected samples are drawn from the SFT model.",
            "score": 0.5166482738897229,
            "section_title": "Details in Training \u20a9ON",
            "char_start_offset": 15820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1587
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 193,
                    "matchedPaperCorpusId": "276884783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11297607421875
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "SFT is an important and widely-used alignment technique in LLMs to enhance pre-trained models for excelling at specific tasks (Shen et al., 2023). We denote the token space of an input query and output response as X and Y, respectively. Typically, LLMs generate an output response sequence y = (  1 ,  2 , . . . ,   ) in response to a given prompt query x = ( 1 ,  2 , . . . ,   ). LLMs are the auto-regressive models characterized by a conditional probability distribution parameterized by  as \n\n(1) \n\nLet a mathematical reasoning SFT training dataset be D = {(x  , y  )}  =1 , where x  and y  represent the -th query and response, respectively2 . Here,  is the total quantity of the SFT training dataset. Given such a dataset D, SFT can be performed using the following cross-entropy loss: \n\nSeed Problems. We adopt publicly available high-quality mathematical datasets to generate our Skywork-MathQA dataset. To prevent data leakage in the testing phase, we only use the training sets from data sources. The data sources are as follows: \n\n\u2022 MATH (Hendrycks et al., 2021) contains high school-level mathematical problems, some of which are from competitions such as the AIME and AMC. This dataset consists of 7,500 training data entries. Solving these problems requires advanced reasoning abilities and a comprehensive mathematical knowledge base. This dataset categorizes problems into five levels of difficulty and seven subdomains of high school mathematics. \u2022 We also use other data sources as seed problems. These included non-proving problems from OlympiadBench (He et al., 2024), mathematical problems from AGIEval (Zhong et al., 2023) benchmark, and various problems in calculus, differential, statistics domains from SciBench (Wang et al., 2024) and JEEBench (Arora et al., 2023). \n\nHere we do not use the training set of GSM8K as the seed problems because: (1) Math word problems represent a narrow category compared to general math problems3 , and an excessive focus on math word problems may reduce the diversity of the synthetic SFT data.",
            "score": 0.5156113790406025,
            "section_title": "Supervised Fine-Tuning (SFT).",
            "char_start_offset": 13388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1791
                },
                {
                    "start": 1794,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 1049,
                    "end": 1073,
                    "matchedPaperCorpusId": "232134851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09417724609375
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Many efforts have been made to compensate for the phenomenon (Liang et al., 2021;Xu et al., 2021;Yuan et al., 2023a). There has also been research discovering that scaling up the pre-trained language model scale and the fine-tuning data scale are beneficial for zero-shot out-of-domain generalization on various linguistic tasks while leaving out the assessment of in-domain performance (Sanh et al., 2022;Chung et al., 2022a;Longpre et al., 2023). Given the increased capacity of LLMs, the multi-task performance by SFT on composite data of essentially different downstream tasks is less studied. Understanding the SFT performance with composite data and corresponding scaling patterns is of great utility in practice. \n\nIn this study, we focus on the data composition problem among mathematical reasoning, code generation, and general human-aligning abilities in SFT. We aim to comprehensively investigate the relationship between model performance and different factors including data amount, data composition ratio, model scales, and SFT training strategies. We also investigate how the relationship varies under different scales. Specifically, we focus on the following four research questions: \n\n1. How do math reasoning, coding, and general abilities scale with SFT data amounts? \n\n2. Are there performance conflicts when combining these three abilities in SFT? \n\n3. What are the key factors that induce the performance conflicts? \n\n4. What are the impacts of different SFT strategies for composite data? \n\nTo answer these questions, we conduct experiments on three benchmarks, which are GSM8K (Cobbe et al., 2021) for mathematical reasoning, HumanEval (Chen et al., 2021) for coding, and MT-Bench (Zheng et al., 2023) for general human alignment. We fine-tune LLMs on the related training data to activate these abilities. Furthermore, we conduct extensive analysis regarding model parameter scales ranging from LLaMA 7B to 33B (Touvron et al., 2023) and explore four different SFT strategies shown in Figure 1: multi-task learning, sequential training, mixed sequential training, and dual-stage mixing fine-tuning (DMT), providing empirical guidance for learning a versatile LLM with composite SFT. The key findings of this paper can be summarized as follows:",
            "score": 0.5148211766426725,
            "section_title": "Introduction",
            "char_start_offset": 2821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1199
                },
                {
                    "start": 1202,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 97,
                    "matchedPaperCorpusId": "237491053"
                },
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "254854311"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1597900390625
        },
        {
            "corpus_id": "271909279",
            "title": "Minor SFT loss for LLM fine-tune to increase performance and reduce model deviation",
            "text": "LLM trained on very large corpora is extremely powerful language model for completion tasks. SFT and RLHF (Ouyang et al. (2022), Ziegler et al. (2020)) are two techniques that used to expose the LLM capability and align LLM answer to human instructions. With the increasing reasoning abilities, LLM are widely used in industries, and SFT and RLHF are also used to inject domain knowledge into LLM by training on domain corpora. \n\nIn the past most works are focused on RLHF and several algorithms are proposed, such as PPOSchulman et al. (2017), DPO (Rafailov et al. (2023)), IPO (Azar et al. (2023)), KTO (Ethayarajh et al. (2024)), MinorDPO (Xie et al. (2024)) and etc. One important point of RLHF is to constraint the optimized model not to deviate from the original model too much during the training, and thus PPO use KL constraints, DPO use a sample level dynamic coefficient related to distance between the preference pair, and IPO use a targeted distance between the preference pair and etc. The purpose of this constraint is to avoid over-fit on the domain corpora and to maintain LLM generalities. It's an important hypothesis that the base model is powerful enough and the training should not change the language distribution too much to maintain the generality and diversity. \n\nWhile back to SFT, most works are focused on collect, filter and mix high quality data. High quality data is undoubtedly important to get a high qualified and usable LLM, while the aforementioned hypothesis that optimized model should not deviate far from the original model is still important. \n\nOur main contribution is that we introduce a training metrics used in DPO and MinorDPO into SFT phase, and propose an improved loss function MinorSFT. MinorSFT use a sample level coefficient to control the learning strength. It constraints the discrepancy more compared to raw SFT and may provide better performance result, at the cost of an additional hyper parameter and more computation.",
            "score": 0.5126274571522642,
            "section_title": "Background",
            "char_start_offset": 13,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1976
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326904296875
        },
        {
            "corpus_id": "263830929",
            "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity",
            "text": "For the diversity evaluations, we focus on the summarisation task specifically, as it has the most compelling results. We ran some initial experiments evaluating diversity for the instruction-following models, but we did not see any meaningful differences. We hypothesise this is due to the diversity metrics we use being designed for settings where the model output is relatively short (e.g. a single sentence), whereas in the instruction-following setting outputs are generally much longer. Furthermore, RLHF models tend to produce longer outputs than other models, which can confound the evaluation of output diversity, since most metrics are not invariant to output length. Fig. 5 shows the per-input diversity scores for RLHF and SFT models in the summarisation task. We see that across the first two metrics, RLHF has much lower output diversity than SFT. Fig. 6 shows the across-input diversity scores in the same setting. Here we see that while SFT generally has slightly higher diversity as before, the difference is much smaller than in the per-input case. The drop in across-input diversity cannot be explained purely by the use of the reward model, as BoN has similar or higher across-input diversity that SFT for the first two metrics. Both these trends are the same for OPT across model sizes (see Appendix J.4 for full results) We find that NLI does not show meaningful difference between models in either per-input or across input, showing that in a logical sense all models are similarly diverse. \n\nThe difference in across-input diversity between RLHF and SFT, while small, can also be taken as evidence of the phenomena of \"mode collapse\" hypothesised to occur under RLHF fine-tuning (janus, 2022). The hypothesised effect is that even for different inputs, RLHF models can be biased towards outputting text of a specific style or \"mode\", meaning that even changing the inputs to a model is not sufficient to generate truly diverse outputs. We believe that this is the first rigorous empirical demonstration of across-input mode collapse emerging from RLHF training specifically.",
            "score": 0.5107950507398058,
            "section_title": "DIVERSITY",
            "char_start_offset": 26045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2098
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1446533203125
        },
        {
            "corpus_id": "269214448",
            "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
            "text": "Target-Language SFT Data \n\nSo far we assumed access to target-language SFT data since, as \u00a73 argues, SFT data could be more easily obtained than RM data. We now relax this assumption and instead translate the source-language SFT data into the target language using Google Translate. We investigate if it, combined with RM transfer, still enables cross-lingual alignment. As a case study, we only consider summarization and when English is the source or target language. Using translated SFT data substantially degrades the quality of the SFT model (Figure 5(a)) and the best-of-n-aligned LM (Figure 5(b)). There are however two factors: (1) quality loss due to translation, and (2) domain/style mismatch. For (2), we note that different languages have SFT data composed of different datasets, following Seahorse (Table 1). 4 nd these datasets differ stylistically: for example, while XSum includes news articles, WikiLingua consists of how-to articles and with more formulaic summaries. There would thus be a domain difference between using organic target-language SFT data vs. data translated from a different language. \n\nTo account for this, we employ round-trip backtranslation, first translating the target-language SFT data into the source language and then back to the target language. This setup is not practically useful but it upper-bounds the effect of translation errors alone. Figure 5(a) shows that this bridges most of the gap, sometimes leading to models that win over the SFT model >50% of the time. Alternatively, we control for domain by repeating our experiments solely using WikiLingua for both SFT and RM as  it is present for all languages. From Figure 5(c), the gap indeed reduces, with the translated SFT models sometimes even outperforming the original, and back-translation is no longer consistently beneficial. \n\nOther than genre control, we also hypothesize that the gap would be smaller for RL than bestof-n because the RM, whose transferability we verified ( \u00a75), intuitively plays a bigger role in the RL pipeline.",
            "score": 0.5081858611639593,
            "section_title": "Cross-Lingual Alignment Without",
            "char_start_offset": 15303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 27,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1837
                },
                {
                    "start": 1840,
                    "end": 2045
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054595947265625
        },
        {
            "corpus_id": "275954349",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "text": "Experiments with original noisy responses and Qwen2.5-base responses critiqued by GPT-4o showed negligible performance differences. \n\n3. Flexibility to the teacher: Using a weaker critique dataset synthesized by GPT-4o-mini still yielded notable improvements over SFT, despite a 4% overall score drop. 4. Controlling for token length: Even controlling token length, shorter critique examples significantly outperformed standard SFT (55.2% vs. 50.4%), confirming improvements stem from critique-based training rather than increased sequence length. \n\nOur approach not only demonstrates strong improvement on reasoning-focused tasks, but also exhibits notable improvement over general-purpose instruction following tasks. These evidence has shown great potential of CFT to replace SFT in language model training.",
            "score": 0.5051011453350738,
            "section_title": "Robustness to noisy response sources:",
            "char_start_offset": 5935,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 59,
                    "end": 131
                },
                {
                    "start": 134,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08050537109375
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "In our exploration of the consequences of significant token distribution shifts during SFT, we take a closer look at the concept of pattern copying. We define pattern copying as the scenario when an LLM learns to mimic the characteristics of responses in the IT dataset. We sub-categorize pattern copying into two distinct types: (1) Tone Imitation: In this case, generated responses tend to use tokens from the IT dataset. These can be stylistic tokens or normal ones. (2) Style Imitation: The responses mirror the wider stylistic traits present in the IT dataset. For example, if the IT data includes comprehensive, well-structured, and lengthy answers, the LLM is likely to exhibit similar traits in its responses. LLMs have been shown to learn such characteristics (Gudibande et al., 2023;Lin et al., 2023). We show that token distribution shifts following IT, and more specifically, the use of tokens that deviate from pre-trained knowledge, are indicative of the model's level of adaptation to the IT dataset's specifics. Additionally, SFT and style imitation lead the model to inaccurately include tokens from the IT dataset in its responses, negatively affecting response quality. \n\nFinding 1. LFT and SFT learn tone imitation differently. As seen in Section 3, LFT primarily learns response initiation, and most of the response comes from pre-trained knowledge. On the other hand, SFT experiences high degrees of new token usage. This leads us to investigate if LFT learns tone imitation at all and the differences in tone imitation between models fine-tuned using the two training paradigms. Fig. 12a and 12b illustrate token distributions of common tokens at shifted and marginal positions for LLaMa-2 7B SFT Alpaca 52k and LLaMa-2 7B LFT Alpaca 52k . Analyzing tokens at shifted positions allows us to understand how IT affects a model to output different tokens than it would generally have without IT. As we clearly see, for LFT, shifts occur primarily in style tokens (\"typically\") and response initiation tokens (\"However\"). On the other hand, for SFT, shifts occur in all kinds of tokens.",
            "score": 0.5041045299401189,
            "section_title": "Pattern Copying (often) Hurts Performance",
            "char_start_offset": 18988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2105
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1231689453125
        },
        {
            "corpus_id": "270123676",
            "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
            "text": "Reinforcement Learning from Human Feedback (RLHF) is a technique that can be used to align an agent -such as a Large Language Model (LLM) -to human preferences and lead to more truthful, more helpful, less harmful and more preferred outputs [31].Proximal Policy Optimization (PPO) [38] and Direct Preference Optimization (DPO) [33] are two such aligment techniques which have been extensively used to improve the quality of LLM outputs, leading to instruction following agents or chat assistants which are quickly approaching human-baselines in a variety of knowledge and reasoning tasks [5,11,43,20,26,37,12].\n\nHowever, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it.One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters.Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone.\n\nTo combat the the risk of RLHF compromising the abilities of an LLM in favor of producing preferable outputs we introduce Direct Preference Heads (DPH), a novel feature based approach that optimises a reward score produced by the LLM rather than optimising the logits produced by language modelling head.DPH can be used in combination with (or without) existing alignment techniques to allow language models to self-evaluate outputs sampled at inference time and select the highest scoring candidate.\n\nWe evaluate the performance of DPH using an efficient 551M parameter LM on a variety of commonsense reasoning and Natural Language Understanding (NLU) tasks.All code used to train our models is available on GitHub and we release our model weights on Hugging Face.",
            "score": 0.5036600552789388,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 610
                },
                {
                    "start": 612,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 1058
                },
                {
                    "start": 1058,
                    "end": 1234
                },
                {
                    "start": 1236,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1736
                },
                {
                    "start": 1738,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2001
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "268248820",
            "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
            "text": "The foundational training of models like Mistral leverages large, well-curated datasets to encompass a wide range of knowledge domains and linguistic structures, fostering robust general language processing skills with stable and widely applicable  feature representations. However, SFT model often involves smaller, domain-specific datasets of varying quality, which may lead to models becoming overly attuned to the peculiarities of these datasets. This shift not only risks introducing biases towards narrower datasets but also threatens the broader knowledge base established during initial training. \n\nThe result is a compromise in the model's generalization capabilities, as it may begin to forget more generalizable abilities it learned in pretrain stage. As depicted in Figure 3, Mistral-Plus archives great performance in multi-turn conversational task, effectively completing summarization and converting text into YAML format. However, due to SFT causing a forgetfulness of the generalizable abilities acquired from the base model, Mistral-Instruct performs well in summarization tasks but struggles to respond adequately to tasks requiring conversion to YAML format.",
            "score": 0.5021655350567742,
            "section_title": "Mistral-Plus on Conversational Task",
            "char_start_offset": 23350,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1178
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "271974342",
            "title": "CBF-LLM: Safe Control for LLM Alignment",
            "text": "While large language models (LLMs) are known to have strong language understanding and generation abilities, they can also generate harmful, biased, and toxic content [1] [2]. Alignment of LLMs ensures that they generate content that is \"desirable\" for the user, typically meaning content that is safe and ethical. Various approaches for LLM alignment have been presented ( [1], [2], [3] and reference therein). \n\nThe major approach to the alignment is reinforcement learning from human feedback (RLHF) [4], where a reward model is constructed by human feedback and used for the training of LLMs. Variants of RLHF architectures are also proposed, such as Safe-RLHF [5], SENSEI [6], and f-DPG [7], and their implementations are presented, such as training pre-trained LLMs [8] [9], and applications like information-seeking chatbot [10]. Collecting human feedback with data is time-consuming and expensive. To overcome the drawback, RL from AI Feedback (RLAIF) is presented in [11] instead of using human labels. In addition, the method to construct the training data automatically is proposed in [12]. Furthermore, to reduce the computational cost, direct preference optimization (DPO) is proposed [13], where the training data is directly used for training LLMs without accessing the reward model. Supervised fine-tuning (SFT) is a different approach for alignment from RLHF, as studied in [14]. A common feature of alignment methods like RLHF and SFT is that they modify LLMs' model parameters. \n\nAn alternative approach for LLM alignment is to directly intervene in the input prompt or output of LLMs, rather than modifying the model parameters. In-context learning (ICL) [15] is a major approach for intervening in the input prompt. In ICL, a few demonstrations are provided in prompt to instruct the LLMs on the task, including few-shot learning [16] [17].",
            "score": 0.5012981105959466,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 665,
                    "end": 668,
                    "matchedPaperCorpusId": "264306078"
                },
                {
                    "start": 677,
                    "end": 680,
                    "matchedPaperCorpusId": "250562745"
                },
                {
                    "start": 776,
                    "end": 779,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1198,
                    "end": 1202,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6318359375
        },
        {
            "corpus_id": "264289051",
            "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
            "text": "In this work, we use Llama 2 (Touvron et al., 2023) with 7 billion parameters as the base model for all experiments to evaluate the effectiveness of RLHF alignment in both general dialogue tasks and summarization tasks. Experimental details and hyperparameters can be found in the Appendix C.1. \n\nGeneral Dialogue Task. Following Vicuna (Chiang et al., 2023), SFT dataset includes 52k usershared conversations from various domains such as mathematics, knowledge querying, and coding, collected from ShareGPT.com2 . Human preference data: Anthropic-RLHF-HH dataset3 is used, which is a large-scale collection of human feedback on AI assistant responses, including both helpful and harmless data (Bai et al., 2022b). The entire dataset comprises 161k training samples and 8.5k testing samples. \n\nSummarization Task. SFT dataset: Reddit TL;DR dataset is used, comprising 123, 169 Reddit posts along with human-written summaries. Human preference data: similar to the SFT data, the Reddit TL;DR dataset is used. Each post in this dataset is accompanied by two generated summaries, one of which is labeled as preferred by annotators (Stiennon et al., 2020). \n\nBaselines. Our Baseline methods include: Supervised Fine-Tuning (SFT); Proximal Policy Optimization (PPO) (Schulman et al., 2017); PPO with KL Penalty (PPO w/ KL) (Ouyang et al., 2022); and Direct Preference Optimization (DPO) (Rafailov et al., 2023). For a detailed and comprehensive understanding of each baseline used, please refer to the Appendix C.2. \n\nHuman & GPT-4 Evaluation. To demonstrate the effectiveness of our approach, we evaluate our method by comparing its win rate against baselines. Specifically, we provide the responses generated by our method and the baselines in general dialogue and summarization, where the sources of these responses are not visible to human evaluators. We ask human evaluators to determine which response is more useful, harmless, and of higher quality.",
            "score": 0.4993332169501118,
            "section_title": "SETUP",
            "char_start_offset": 17650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 791
                },
                {
                    "start": 794,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1152
                },
                {
                    "start": 1155,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 1318,
                    "end": 1339,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33837890625
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "On the other hand, for SFT, shifts occur in all kinds of tokens. To investigate the source of these tokens further, we perform a string search of the shifted tokens in the IT dataset. To our surprise, we found \u224881.2% of the words that start with shifted tokens and 66.7% that start with marginal tokens borrowed from the IT dataset itself. This indicates that with SFT, models show increased borrowing of tokens from the IT dataset for response generation. We may attribute this to over-fitting on the IT dataset. A study in Section 5 further shows that these tokens are often borrowed inaccurately, leading to hallucinations. \n\nFinding 2. Style imitation can hurt response quality. We now investigate if style imitation can affect response quality. We are motivated by the finding that a positive correlation exists between the length of responses in the IT dataset and the length of responses output by the fine-tuned LLM on our evaluation set (detailed results in Table 11). To achieve this, we utilize the LIMA IT dataset, which comprises responses from community Q&A forums known for their comprehensiveness, expertise, and length (refer to Table 11). When LLaMa is fine-tuned on the LIMA dataset with SFT, it produces lengthy and detailed responses, even when pre-trained knowledge might be insufficient. This often leads to hallucinations as the model strives to generate extended answers. Figure 5 showcases examples of this, with each box presenting a response from LLaMa-2 7B SFT LIMA 1K alongside a more concise version. We summarize our findings on style imitation: (1) As seen in Instruction 1., style imitation can sometimes enhance response quality without leading to hallucinations, particularly when the model has sufficient knowledge of the subject. (2) As seen in Instruction 2., style imitations alone can alter the nature of a response to open-ended reasoning instructions. (3) The model initially provides factual information but then resorts to hallucination to prolong the response or fact in the absence of adequate knowledge about the subject. These hallucinations may include randomly generated facts or content derived from the IT dataset, as seen earlier and discussed further in Section 5. \n\nProposed Solution: Simplifying responses in the IT dataset.",
            "score": 0.4990349192458158,
            "section_title": "Pattern Copying (often) Hurts Performance",
            "char_start_offset": 21029,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2219
                },
                {
                    "start": 2222,
                    "end": 2281
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "corpus_id": "266484232",
            "title": "The Limitations and Ethical Considerations of ChatGPT",
            "text": "(1) Supervised Fine-Tuning (SFT) \n\nIn this process, pre-trained model upgrades its parameters by training it on a task-specific annotated datasets, enabling it to better adapt to the specifics of a particular task or domain. \n\nIn the Supervised Fine-Tuning phase of GPT-1, datasets related to semantic similarity, classification, question answering and commonsense reasoning was used to fine-tuning the model. For fitting the pretrained model training on contiguous sequences of text, task-specific input transformations should be made. Simply put, it involves converting structured inputs into an ordered sequence that a pre-trained model can process. Taking textual entailment as an example, the premise and hypothesis token sequences are concatenated with a delimiter token in between. As for Similarity, altering the input sequence to encompass both potential sentence orderings, and delineated sentences by a delimiter. This can help mitigate the influence of sentence order during training. Figure 3 illustrate the input transformations. \n\n(2) Reinforcement Learning from Human Feedback (RLHF) \n\nTo ensure GPT outputs more helpful, secure and user-instructive information, InstructGPT was finetuning from GPT-3 using RLHF. ChatGPT also used RLHF fine-tuning methods but differ in the training datasets that we mentioned in 2.2.2. Below, take the RLHF method employed in InstructGPT as example, we introduce the three steps involved in the fine-tuning process and an illustration in Figure 4. \n\nStep1: Using SFT datasets to perform Supervised Fine-Tuning on GPT-3 Adopt the SFT method mentioned before to fine-tuning the GPT-3 model using the SFT datasets, and the trained GPT-3 is referred to as the SFT model. Initially, the SFT model initializes a PPO model. Subsequently, the PPO model generates responses to the queries in PPO datasets which excluded human annotations. \n\nNext, the trained reward model (RM) rates and ranks the predictions made by the PPO model, assessing if the model's results align with human preferences.",
            "score": 0.4984521988845199,
            "section_title": "Fine-Tuning",
            "char_start_offset": 8121,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 35,
                    "end": 224
                },
                {
                    "start": 227,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1099
                },
                {
                    "start": 1102,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1879
                },
                {
                    "start": 1882,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "265043685",
            "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
            "text": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.",
            "score": 0.49751685922597244,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge. \n\n(3) For instance, when tasked with generating an informative response, the model must select a format that aligns with user expectations, such as providing clear instructions or explanations. However, the actual content of the response, e.g., factual knowledge, reasoning, and domain-specific expertise, stems from pretraining. The alignment stage merely teaches the model how to express that knowledge or when to refrain from providing information in inappropriate contexts. \n\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH was the observation that models tend to be capable of performing certain tasks after alignment finetuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using reinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally too expensive and time-consuming.",
            "score": 0.49388998936365924,
            "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
            "char_start_offset": 31564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1453
                },
                {
                    "start": 1456,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6259765625
        },
        {
            "corpus_id": "275954349",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "text": "Recently, large language models (LLMs) (Achiam et al., 2023;Team et al., 2023;Dubey et al., 2024) have shown unprecedented performance on real-world problems. One core techniques is supervised fine-tuning (SFT), which trains LLMs to follow natural language instructions (Wei et al., 2022;Ouyang et al., 2022;Sanh et al., 2022). In the process of SFT, LLMs are forced to imitate the annotated responses. Numerous efforts have been made to build high-quality SFT datasets using approaches like Self-Instruct (Wang et al., 2023b) and Evol-Instruct (Xu et al., 2024) to enhance LLMs' general instruction-following capabilities. \n\nMore recently, works such as MAmmoTH (Yue et al., 2024a;b), MetaMath (Yu et al., 2024), and WizardCoder (Luo et al., 2024) have employed SFT to improve the targeted capabilities of LLMs in areas like mathematical reasoning, coding, and more. While these approaches have shown significant gains on weaker base models such as Mistral (Jiang et al., 2023) or LLaMA3 (Dubey et al., 2024), diminishing returns become evident as SFT dataset size and quality scale up. This limitation is particularly pronounced for already-powerful base models (non-SFTed), such as Qwen2.5-base (Yang et al., 2024a), Qwen2.5-Math-base (Yang et al., 2024b), or DeepSeek-Coder-V2-base (Guo et al., 2024), which have undergone extensive domain-adaptive pretraining on reasoning-focused corpora comprising hundreds of billions of tokens. Our experiments in section 3 reveal that applying SFT to these models can even degrade performance without stringent quality control.",
            "score": 0.4936712222460594,
            "section_title": "Introduction",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 623
                },
                {
                    "start": 626,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1570
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 288,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 288,
                    "end": 308,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 308,
                    "end": 326,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 506,
                    "end": 526,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 545,
                    "end": 562,
                    "matchedPaperCorpusId": "271745981"
                },
                {
                    "start": 663,
                    "end": 682,
                    "matchedPaperCorpusId": "261696697"
                },
                {
                    "start": 695,
                    "end": 712,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 730,
                    "end": 748,
                    "matchedPaperCorpusId": "259164815"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17578125
        },
        {
            "corpus_id": "272703930",
            "title": "Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models",
            "text": "RL training can be employed as an extra post-training stage after either Domain SFT or Speaker SFT. Both experiments are conducted to verify the effectiveness of RL training, especially on expressiveness and BCR. To prepare training data for RL, we make a set of good / bad examples with both subjective ratings and objective metrics. As [46] shows repeated sampling is able to largely increase the pass coverage to queried problems, we get 5 samples by repeated sampling for each sentence. For objective ratings, we pick PER and UTMOS [47] as objective metrics to generate preferences considering both metrics. For subjective ratings, there are 50 human raters being participated in to rate the best and the worst sample among 5 candidates by listening and comparing the overall speech quality. Therefore, a good / bad example pair is acquired for each sentence either by objective or subjective ratings. As a result, for the RL experiments, 50000-sentence RL pairs are created for RL training after Domain SFT and 500-sentence RL pairs are prepared for two male and two female speakers respectively after speaker SFT. The experiments are defined as follows. \n\n\u2022 Domain-SFT-RL-OBJ denotes the RL experiments conducted based on the model after domain SFT by using objective-metric-rating data. \n\n\u2022 Domain-Speaker-SFT-RL-OBJ denotes the RL experiments conducted based on the model after domain and speaker SFT by using objective-metric-rating data. \n\n\u2022 Domain-SFT-RL-SUBJ denotes the RL experiments conducted based on the model after domain SFT by using human-rating data. \n\n\u2022 Domain-Speaker-SFT-RL-SUBJ denotes the RL experiments conducted based on the model after speaker SFT by using human-rating data. \n\nIn our experiments, DPO is employed to do RL post training on Domain-SFT-RL and Speaker-SFT-RL respectively. The Table 3 shows the results comparing models with (w/) or without (w/o) RL training. \n\nTo make the result comparable, the four speakers in Speaker TTS Dataset are picked to evaluate various metrics.",
            "score": 0.49261544122997963,
            "section_title": "RL Training on TTS",
            "char_start_offset": 19671,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1571
                },
                {
                    "start": 1574,
                    "end": 1704
                },
                {
                    "start": 1707,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2016
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08880615234375
        },
        {
            "corpus_id": "260887200",
            "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
            "text": "LLMs for Human Alignment. Through supervised fine-tuning (SFT), response ranking, or reinforcement learning (Ouyang et al., 2022;Bai et al., 2022a;b;Yuan et al., 2023b;Rafailov et al., 2023;Song et al., 2023;Touvron et al., 2023b), LLMs can obtain versatile abilities for understanding and following diversified human queries expressed in natural languages, aligning with human intentions. Recent research mainly focused on SFT to align LLMs with human intentions and has contributed essential practices to developing open-resourced well-aligned LLMs, which is adequately summarized by Zhao et al. (2023). Several prominent works collected SFT data through human annotated demonstrations (Ouyang et al., 2022;Touvron et al., 2023b), online user logs of proprietary LLMs (Chiang et al., 2023;Wang et al., 2023a;K\u00f6pf et al., 2023), or prompting proprietary highperforming LLMs such as CHATGPT or GPT-4 (OpenAI, 2023) to generate or rewrite samples (Taori et al. 2023;Ding et al. 2023;Xu et al. 2023a;Mukherjee et al. 2023;inter alia). Different LLMs fine-tuned on the datasets have aligned with human preference and exhibited good performance in various real-world tasks. \n\nData for Human Alignment. It has been highlighted that the performance of aligned LLMs is affected by the quality of the SFT data. Such data quality pertains to the level of responses (Peng et al., 2023;Chiang et al., 2023), the difficulty of tasks presented (Mukherjee et al., 2023), the complexity of queries (Xu et al., 2023a), the diversity of semantics (Ding et al., 2023;Taori et al., 2023), and the scale of sample amounts (Zhou et al., 2023).",
            "score": 0.4922311376761792,
            "section_title": "RELATED WORKS",
            "char_start_offset": 2927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1622
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25634765625
        },
        {
            "corpus_id": "273098476",
            "title": "How to Train Long-Context Language Models (Effectively)",
            "text": "Supervised fine-tuning (SFT; Ouyang et al., 2022) is an additional training stage that finetunes the model on a small amount of natural-language instructions and corresponding responses; it enables a base LM to address user queries in a chat format and has become a standard step for producing frontier LMs. Here, we consider the difference between evaluating a model before or after SFT. \n\nIn preliminary experiments, we continue training Llama-3-8B-Base on 5B-token subsets from the data mix by Fu et al. (2024). The mix is based on SlimPajama (Soboleva et al., 2023) and upsamples long documents to constitute roughly 70% of tokens, while retaining the original domain proportions. Then we conduct SFT on several intermediate checkpoints with UltraChat (Ding et al., 2023). We show the benchmarking results before and after SFT in Figure 2. Long-context evaluation shows clearer signals when it is conducted after SFT: (1) SFT shows that the model continues to improve with more training tokens on RAG and re-ranking, while the improvement is less clear or does not exist when evaluated before SFT. \n\n(2) SFT enables evaluation on realistic applications like QA and summarization, which require instruction following and would otherwise fail completely. We also note that the variance from two random training runs is not substantially higher after the additional SFT phase. Therefore, unless otherwise specified, we report the long-context performance after SFT. \n\nWe dive deeper into supervised fine-tuning in \u00a75 and explore different training datasets, as well as the use of synthetic long instruction data. However, we find that simply fine-tuning on UltraChat remains a surprisingly competitive choice.",
            "score": 0.49222341924814983,
            "section_title": "Evaluating after supervised fine-tuning",
            "char_start_offset": 8104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 49,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 497,
                    "end": 513,
                    "matchedPaperCorpusId": "267682361"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "258840897"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08587646484375
        },
        {
            "corpus_id": "268819437",
            "title": "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning",
            "text": "Herein lies the crux of our inquiry: how can we construct a training dataset from a body of documents that facilitates the learning of new knowledge through simple SFT techniques?Addressing this question is not only of theoretical interest but also carries significant practical implications for the deployment of LLMs in real-world settings where accuracy, currency, and domain specificity are paramount.\n\nOur investigation into the domain of knowledge ingestion via direct training has yielded several notable contributions:\n\n1. Analysis of Token-based Q&A Dataset Generation: We provide a comprehensive evaluation of the standard practice of token-based Q&A dataset generation.Our findings reveal that this method may not ensure complete or uniform coverage of new knowledge within a document corpus.This observation is critical as it underscores the potential limitations of prevailing dataset preparation strategies and highlights the need for more targeted approaches to knowledge incorporation.\n\n2. Development of a Fact-based Generation Process: In response to the identified shortcomings, we propose a fact-based generation process.This methodology prioritizes the even distribution of attention across all salient facts within a source document, ensuring that each piece of information is adequately represented in the training data.This approach stands to significantly enhance the model's ability to internalize diverse and detailed knowledge from domain-specific corpora.\n\n3. Empirical Validation of SFT for New Knowledge Learning: We demonstrate that even straightforward SFT can lead to substantial improvements in model performance when handling out-of-domain, post-cutoff knowledge.Our results not only validate the effectiveness of our proposed fact-based generation process but also provide a compelling case for the practicality of SFT as a tool for domain adaptation in LLMs.This contribution has profound implications for the application of generative AI in dynamic fields where staying abreast of the latest information is crucial.\n\n4. Benchmarking against Retrieval-Augmented Models: We extend our study to include a benchmark comparison between our SFT models and those employing RAG.This analysis provides insights into the trade-offs and relative merits of direct training versus retrieval-based augmentation, offering valuable guidance for practitioners in the selection and implementation of knowledge ingestion methodologies.",
            "score": 0.49136891746772404,
            "section_title": "Introduction",
            "char_start_offset": 2104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 405
                },
                {
                    "start": 407,
                    "end": 526
                },
                {
                    "start": 528,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 803
                },
                {
                    "start": 803,
                    "end": 1001
                },
                {
                    "start": 1003,
                    "end": 1141
                },
                {
                    "start": 1141,
                    "end": 1343
                },
                {
                    "start": 1343,
                    "end": 1484
                },
                {
                    "start": 1486,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2054
                },
                {
                    "start": 2056,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2455
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1710205078125
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Supervised Fine-Tuning of Large Language Models Large Language Models (LLMs) have shown notable zero-shot performance in various domains (Brown et al., 2020;Wu et al., 2021;Hou et al., 2024;Dong et al., 2023b;Zhou et al., 2024;Wu et al., 2023;Song et al., 2023), prompting further development to push the boundaries of these models. \n\nTo delve deeper to their potential, LLMs are subjected to a Supervised Fine-Tuning (SFT) phase, enhancing their ability to solve tasks and align better with human instructions. Here, we extend the conventional definition of SFT to include various forms of sequence-to-sequence fine-tuning, such as fine-tuning for human alignment, instruction following, and domain-specific task optimization (Zhou et al., 2023b;Yuan et al., 2023c;Cheng et al., 2023b;Zhang et al., 2024a). Recent research has delved into multi-task instruction fine-tuning of pre-trained LLMs to bolster their zero-shot performance across numerous downstream NLP tasks (Sanh et al., 2022) (Singhal et al., 2022), finetuned with FLAN, have demonstrated enhanced zero-shot performance on a variety of unseen tasks. \n\nWhile research has probed into the generalization capabilities of LLMs within out-of-distribution domains (Liu et al., 2024a;Yuan et al., 2024;Wang et al., 2024a), the effects of multi-task training on in-domain performance remain under-explored. With the ascent of proprietary models like Chat-GPT, the focus on SFT for aligning LLMs with human intent has intensified (Ouyang et al., 2022b).",
            "score": 0.4911625207622534,
            "section_title": "Related Works",
            "char_start_offset": 5905,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1509
                }
            ],
            "ref_mentions": [
                {
                    "start": 1242,
                    "end": 1260,
                    "matchedPaperCorpusId": "259096157"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.173583984375
        },
        {
            "corpus_id": "270560471",
            "title": "SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models",
            "text": "Instruction-following Large Language Models (LLMs), such as GPT-3.5 and GPT-4 (Achiam et al., 2023), have transformed natural language processing by demonstrating remarkable generalization across a wide range of tasks (Brown et al., 2020;Chung et al., 2022;Ouyang et al., 2022). These models are typically trained through several stages: an initial phase of unsupervised pre-training on a vast corpus of text, followed by post-training stages, which include supervised fine-tuning (SFT) on a smaller dataset of instruction-response pairs and reinforcement learning (Bai et al., 2022). \n\nRecent studies, such as AlpaGasus (Chen et al., 2024) and LIMA (Zhou et al., 2024), demonstrate that carefully curated, smaller datasets can outperform larger ones in improving LLM SFT performance. AlpaGasus finds that smaller datasets with higher quality scores, rated by GPT-4 for helpfulness or correctness, outperform significantly larger ones when used to fine-tune high-capacity LLMs. The Superficial Alignment Hypothesis, proposed in LIMA, suggests that pre-trained language models already possess the necessary knowledge, and the primary goal of fine-tuning is to guide the model toward adopting specific response styles, thus not requiring large amounts of data. LIMA achieves notable performance with only 1,000 high-quality instruction-response pairs, optimized for consistent style by human experts. However, this hypothesis raises three open questions: (i) What key elements constitute response styles that impact LLM SFT? (ii) How do data quality (i.e., helpfulness, correctness) relate to style consistency in influencing efficient SFT? (iii) Can we develop an automatic method that measures stylistic elements to curate smaller, stylistically consistent datasets for more efficient SFT at a lower cost, without relying on human experts? \n\nText style is shaped by consistent choices across various linguistic elements (Kang & Hovy, 2021;Karlgren, 2004), such as lexical, syntactic, and semantic features (DiMarco & Hirst, 1993).",
            "score": 0.4882208489579609,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2030
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 238,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 257,
                    "end": 277,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.264892578125
        },
        {
            "corpus_id": "278208140",
            "title": "Phi-4-reasoning Technical Report",
            "text": "Phi-4-reasoning after the SFT stage already performs strongly across diverse benchmarks. Despite the focus on reasoning-specific content from select domains (math, coding, and safety), the improvement in performance generalizes to tasks not directly targeted in the training data-such as calendar planning (Figure 8). While we have a relatively long SFT stage with 2+ passes over reasoning data sources, we do not see any catastrophic forgetting compared to the base Phi-4 model on more general capabilities. In fact, most general-purpose benchmarks improve significantly over Phi-4 as summarized in Table 2. \n\nFigure 4a shows the progression of key metrics throughout the SFT iterations. We observe through manual checks that the model begins to use explicit \"thinking\" tokens very early in training, indicating the superficial structured format itself is learned quickly. However, the efficacy of the chain-of-thought block and the ability of the model to reason improves throughout training as seen in Figure 4a, suggesting that the model is not merely copying format, but actually acquiring reasoning as a learned skill. Interestingly, unlike during reinforcement learning, we do not see increasing response lengths over the course of SFT. In fact, as shown in Figure 4b, average response length slightly decreases, suggesting the model is learning to use its token budget more efficiently as training progresses. \n\nIn the remainder of this section, we describe at a high level our experimentation process with reasoning SFT. \n\nEarly experiments made it clear that SFT recipes used for instruction finetuning of Phi-4 do not transfer directly to reasoning-focused training. For example, the optimal hyperparameters for reasoning data differed significantly from those used for alignment-focused tuning in Phi-4. As a result, we conducted extensive experiments to identify effective SFT configurations specifically suited for reasoning. \n\nTo systematically evaluate different training strategies, we used fixed benchmarks-AIME 2024 and GPQA diamond-as progress indicators. At a high-level, our experimental methodology can be divided in two stages: exploration and scaling. During exploration, we used shorter training horizons and limited data sources and domains to rapidly iterate and extract robust training recipes. In the subsequent scaling stage, we aggregated findings from earlier derisking runs and finalize the SFT setup.",
            "score": 0.488010778710259,
            "section_title": "Training data",
            "char_start_offset": 21838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1939
                },
                {
                    "start": 1942,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2323
                },
                {
                    "start": 2324,
                    "end": 2435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239501953125
        },
        {
            "corpus_id": "271947083",
            "title": "Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation",
            "text": "Recently, the role of parallel data at scale, long viewed as fundamental to the success of NMT models, has come into question in the era of LLM-MT systems. Motivated by the modest gains of training on 300M parallel sentences (Yang et al., 2023), and the surprising benefits of scaling down during SFT (Zhou et al., 2023), subsequent works have used only tens of thousands of human-written bitext for LLM-MT (Zhang et al., 2023b;Alves et al., 2024;Xu et al., 2024)-with SFT scaling laws further showing the early plateau of LLM-MT performance (Zhang et al., 2024a). Even more surprisingly, Zhu et al. (2024a) showed MT abilities emerging with just 32 SFT examples! However, these explorations concern LLMs pre-trained on several billions of tokens in the languages in question. We revisit these notions for low-resource MT and work with languages having datasets that are 2-3 orders of magnitude smaller. In Table 1 we compare the scale of the datasets used in our work and related research. We discover that for low-resource MT, parallel data is critical not just during CPT, but even more so during SFT-in direct contrast with research on high-resource languages (HRLs). \n\nNext, diversity in tasks, prompts, and datasets during SFT has been shown to significantly improve model performance across a range of tasks (Mishra et al., 2022;Chung et al., 2024). MT instructions have been shown to not just boost translation performance in unseen languages (Muennighoff et al., 2023), but also enhance LLM capabilities across diverse multilingual generation tasks (Ranaldi and Pucci, 2023;Zhu et al., 2023). Inspired by these findings, we study if SFT diversity could benefit low-resource LLM-MT systems too. By conducting experiments across a range of tasks and language pairs with SFT datasets of varying compositions, we establish that diversity leads to negative interference and fine-tuning on multilingual MT is the optimal strategy.",
            "score": 0.4867952454880413,
            "section_title": "Introduction",
            "char_start_offset": 1720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 244,
                    "matchedPaperCorpusId": "258960497"
                },
                {
                    "start": 301,
                    "end": 320,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 447,
                    "end": 463,
                    "matchedPaperCorpusId": "262084016"
                },
                {
                    "start": 542,
                    "end": 563,
                    "matchedPaperCorpusId": "268032247"
                },
                {
                    "start": 1315,
                    "end": 1336,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 1336,
                    "end": 1355,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1451,
                    "end": 1477,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 1558,
                    "end": 1583,
                    "matchedPaperCorpusId": "265607892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1444091796875
        },
        {
            "corpus_id": "276724734",
            "title": "Supervised Fine-Tuning LLMs to Behave as Pedagogical Agents in Programming Education",
            "text": "Results indicate that Supervised Fine-Tuning (SFT) is a promising approach towards incorporating pedagogy in large language models (RQ1). While pedagogical goals, such as reducing explicit solutions, producing simpler responses with a Socratic style were also achieved, they came with an overall cost to model accuracy (RQ2). It is yet unclear if this performance cost can be mitigated with improved fine-tuning techniques, reasoning techniques involving Reinforcement Learning, improved foundational models, and datasets, or if the performance trade-off is inherent. We present that while pedagogical alignment comes at the expense of accuracy, the GuideLM models are still preferred by raters and present educational value. \n\nAnother aspect to consider is the effort and cost of maintaining fine-tunes as foundational models improve. Since these models are frequently updated, the SFT process must be repeated for each new model. \n\nIdeally, we aim to achieve strong pedagogical performance without directly modifying the foundational models' weights, instead relying on techniques such as RAG and prompting. Therefore, automated benchmarking and evaluation processes, such as those presented in this paper and used by Taylor et al. [34], are crucial for assessing the benefits of the SFT approach and justifying the financial costs of fine-tuning versus other alignment techniques.",
            "score": 0.4863869394653896,
            "section_title": "Discussion",
            "char_start_offset": 19037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1383
                }
            ],
            "ref_mentions": [
                {
                    "start": 1234,
                    "end": 1238,
                    "matchedPaperCorpusId": "261076439"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20263671875
        },
        {
            "corpus_id": "271709991",
            "title": "Progressively Label Enhancement for Large Language Model Alignment",
            "text": "Datasets. We use Anthropic's Helpful and Harmless (HH) dataset as the experimental dataset (Bai et al., 2022a). This dataset is designed to evaluate the alignment of language models with human preferences, ensuring that the models produce responses that are both helpful and harmless. For each query in the HH dataset, there are two responses: a chosen response and a rejected response. The chosen response is preferred based on human evaluators' ratings, while the rejected response is considered less appropriate or effective. See Table 1 for an example of the HH dataset. The dataset consists of 161K training data points and 8.55K test data points. \n\nBaselines. We compare our method with several existing language model alignment approaches, including: \n\n\u2022 SFT (Ouyang et al., 2022): Supervised Fine-Tuning (SFT) trains the model by predicting the next token in a sequence based on a dataset of human-labeled examples to guide it toward desired outputs. \u2022 DPO (Rafailov et al., 2023): Direct Policy Optimization (DPO) simplifies the RLHF process by deriving an equivalent optimization objective of PPO. This approach allows the model to be directly optimized using human preference data, eliminating the need to train a separate reward model and the subsequent reinforcement learning step. \n\n\u2022 RAFT (Dong et al., 2023): Reward-based Fine FineTuning (RAFT) expands the SFT dataset by generating additional samples and selecting those with high reward scores to enhance the SFT dataset. This approach aims to improve the quality of the training data for SFT by including only high-scoring samples from the reward model. \n\nImplementation Details. In our experiments, we use the LLama3 8B base model (Touvron et al., 2023). This model has been pre-trained on a diverse corpus of text data, enabling it to generate coherent and contextually relevant responses across a wide range of topics. For implementing SFT, PPO, and DPO, we utilized the Transformer Reinforcement Learning (TRL) library1 . For RAFT, we employed the official LMflow library2 .",
            "score": 0.4858787399013844,
            "section_title": "EXPERIMENTAL CONFIGURATIONS",
            "char_start_offset": 16961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 766,
                    "end": 787,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 965,
                    "end": 988,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1304,
                    "end": 1323,
                    "matchedPaperCorpusId": "258170300"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Large Language Models (LLMs) based on the Transformer architecture have achieved state-of-the-art performance on tasks that involve instruction following, problem-solving, and reasoning (Achiam et al., 2023;Dubey et al., 2024;Vaswani et al., 2017). The standard pipeline for building LLMs powered applications involves unsupervised training of a model on a giant corpus of data to gain general language understanding capability, referred to as pre-training (Brown et al., 2020;Radford et al., 2019). The model is further improved using post-training, which involves finetuning it to excel at a particular domain or behave like a helpful chatbot. This process is also referred to as alignment. The predominant way to do this is through Supervised Finetuning (SFT) where the language model is provided with a prompt, and the model is finetuned to respond to the task (Wei et al., 2022). An additional step is Reinforcement Learning through Human Feedback (RLHF) where a model is trained using reinforcement learning to generate human-preferred responses, by being rewarded for good responses and penalized for bad responses (Ouyang et al., 2022). \n\nTo achieve the post-training goal of responding appropriately to various user queries, LLMs need to develop several task-specific capabilities, like mathematics, reasoning, utilizing knowledge, and tool use. To teach a model these capabilities, model builders collect human-annotated or synthetically generated data and finetune the model to obtain the desired behavior. Since data collection at scale is labor and cost-intensive, it is essential to understand the qualitative and quantitative value of obtaining additional post-training data. Studies like LIMA (Zhou et al., 2024) have hypothesized that post-training alignment is all about learning the style and format of the desired behavior. Specifically, it puts forward the Superficial Alignment Hypothesis, whose claims are: \n\n\u2022 C1: A model's knowledge is learned entirely during pre-training. \n\n\u2022 C3: Post-training is largely about style and doesn't does not teach a model new capabilities. \n\n\u2022 C2: A small number of examples can saturate a model's performance for a given task.",
            "score": 0.4840911339453103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 247,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 477,
                    "end": 498,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 865,
                    "end": 883,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1122,
                    "end": 1143,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "261049152",
            "title": "Instruction Tuning for Large Language Models: A Survey",
            "text": "The benefits of SFT are threefold: (1) Finetuning an LLM on the instruction dataset bridges the gap between the next-word prediction objective of LLMs and the users' objective of instruction following; (2) SFT allows for a more controllable and predictable model behavior compared to standard LLMs. The instructions serve to constrain the model's outputs to align with the desired response characteristics or domain knowledge, providing a channel for humans to intervene with the model's behaviors; and (3) SFT is computationally efficient and can help LLMs rapidly adapt to a specific domain without extensive retraining or architectural changes. \n\nDespite its effectiveness, SFT also poses challenges: (1) Crafting high-quality instructions that properly cover the desired target behaviors is non-trivial: existing instruction datasets are usually limited in quantity, diversity, and creativity; \n\n(2) there has been an increasing concern that SFT only improves on tasks that are heavily supported in the SFT training dataset (Gudibande et al., 2023); and (3) there has been an intense criticism that SFT only captures surface-level patterns and styles (e.g., the output format) rather than comprehending and learning the task (Kung and Peng, 2023). Improving instruction adherence and handling unanticipated model responses remain open research problems. These challenges highlight the importance of further investigations, analysis, and summarization in this field, to optimize the fine-tuning process and better understand the behavior of instruction tuned LLMs. \n\nIn the literature, there has been an increasing research interest in analysis and discussions on LLMs, including pre-training methods (Zhao et al., 2023), reasoning abilities (Huang and Chang, 2022), downstream applications (Yang et al., 2023a;Sun et al., 2023b), but rarely on the topic of LLM instruction tuning. This survey attempts to fill this blank, organizing the most up-to-date state of knowledge on this quickly advancing field. Specifically, \n\n\u2022 Section 2 presents the general methodology employed in instruction tuning. \u2022 Section 3 outlines the construction process of commonly-used SFT representative datasets. \u2022 Section 4 presents representative instruction tuned models.",
            "score": 0.48331506994143847,
            "section_title": "Introduction",
            "char_start_offset": 1864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2022
                },
                {
                    "start": 2025,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1671142578125
        },
        {
            "corpus_id": "269362100",
            "title": "Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model",
            "text": "While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely\"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.",
            "score": 0.48311106147597216,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4072265625
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Typically, adapting a base LLM to a specific domain involves three distinct and relatively independent stages, each based on corresponding data in different formats.Specifically, the base model firstly performs continual pre-training (CPT) on domain-specific corpus for learning new knowledge, then conducts supervised fine-tuning (SFT) based on instructions for enhancing the instruction following ability, and finally utilizes the human preference data for human alignment.In this work, we adopt the direct preference optimization (DPO) to as the alignment algorithm.Formally, we denote the domain-specific corpus as D CPT = {d i } nc i=1 , where d i represents a raw domain document consisting of a sequence of tokens.For the instructions used in SFT, we denote as D SFT = {\u27e8q i , r i \u27e9} ns i=1 , where q i and r i represent the user query and the expected response, repectively.For alignment data used in DPO, we denote by\n\n, where q i , r + i , and r \u2212 i represent the user query, positive response, and negative response, respectively.\n\nIn this work, we propose to mix D CPT , D SFT and D DPO with a unified text format, building upon which we further perform knowledge mixture continual pre-training on a general LLM.Unlike previous work relying on synthesizing high-quality domain instructions (Jiang et al., 2024;Cheng et al., 2024), we empirically find that knowledge utilization is indeed a general capability that can be learned from general instructions, and we can further transfer such capacity to enhance the learning of domain knowledge.Specially, we remove any templates and markers (e.g., [User]) from the instructions and alignment data to construct the mixture data in a unified format, denoted by D MIX = {x cpt , x sft , x dpo }, where x cpt = d i is the original domain document, x sft = [q i ; r i ] denotes the concatenation of user query and expected response in instructions, and\n\nis the concatenation of user query and positive response in the alignment data.We show some examples in Figure 1.",
            "score": 0.4822381862706754,
            "section_title": "UNIFIED KNOWLEDGE FORMAT",
            "char_start_offset": 7757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 569
                },
                {
                    "start": 569,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 926
                },
                {
                    "start": 928,
                    "end": 1041
                },
                {
                    "start": 1043,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1554
                },
                {
                    "start": 1554,
                    "end": 1907
                },
                {
                    "start": 1909,
                    "end": 1988
                },
                {
                    "start": 1988,
                    "end": 2022
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2459716796875
        },
        {
            "corpus_id": "268363309",
            "title": "ORPO: Monolithic Preference Optimization without Reference Model",
            "text": "We study the behavior of supervised fine-tuning (SFT) as an initial stage of preference alignment methods (Ziegler et al., 2020;Rafailov et al., 2023) through analysis of the loss function in SFT and empirical demonstration of the preference comprehension ability of the trained SFT model.SFT plays a significant role in tailoring the pre-trained language models to the desired domain (Zhou et al., 2023a;Dong et al., 2024) by increasing the log probabilities of pertinent tokens.Nevertheless, this inadvertently increases the likelihood of generating tokens in undesirable styles, as illustrated in Figure 3. Therefore, it is necessary to develop methods capable of preserving the domain adaptation role of SFT while concurrently discerning and mitigating unwanted generation styles.\n\nAbsence of Penalty in Cross-Entropy Loss The goal of cross-entropy loss model fine-tuning is to penalize the model if the predicted logits for the reference answers are low, as shown in Equation 2.\n\nwhere y i is a boolean value that indicates if ith token in the vocabulary set V is a label token, p i refers to the probability of ith token, and m is the length of sequence.Using cross-entropy alone gives no direct penalty or compensation for the logits of non-answer tokens (Lin et al., 2017) as y i will be set to 0. While cross-entropy is generally effective for domain adaptation (Mao et al., 2023), there are no mechanisms to penalize rejected responses when compensating for the chosen responses.Therefore, the log probabilities of the tokens in the rejected responses increase along with the chosen responses, which is not desired from the viewpoint of preference alignment.",
            "score": 0.482093895613346,
            "section_title": "The Role of Supervised Fine-tuning",
            "char_start_offset": 7274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 480
                },
                {
                    "start": 480,
                    "end": 784
                },
                {
                    "start": 786,
                    "end": 983
                },
                {
                    "start": 985,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1489
                },
                {
                    "start": 1489,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "47252984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2054443359375
        },
        {
            "corpus_id": "272832639",
            "title": "60 Data Points are Sufficient to Fine-Tune LLMs for Question-Answering",
            "text": "Large language models (LLMs) encode extensive world knowledge through pre-training on massive datasets, which can then be fine-tuned for the question-answering (QA) task. However, effective strategies for fine-tuning LLMs for the QA task remain largely unexplored. To address this gap, we categorize supervised fine-tuning (SFT) data based on the extent of knowledge memorized by the pretrained LLMs and conduct a series of empirical analyses. Our experiments, involving four LLMs from three different model families, focus on three key factors: the amount of data required for SFT, the impact of different SFT datasets on model performance, and how data requirements vary across LLMs. The results show that as few as 60 data points during the SFT stage can activate the knowledge encoded during pre-training, enabling LLMs to perform the QA task. Additionally, SFT with data of varying memory levels has a significant impact on LLM performance, with the optimal dataset differing based on the specific model being fine-tuned. Future research will delve deeper into the mechanisms underlying these phenomena.",
            "score": 0.48126605177166937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1470947265625
        },
        {
            "corpus_id": "274436578",
            "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs",
            "text": "Previous research on data mixing ratios during the SFT phase for LLMs has predominantly focused on enhancing capabilities within a specific domain, often utilizing only data from that domain or employing heuristic, experience-based data proportions. We argue that such data mixing strategies can significantly impair the LLM's abilities in other domains. In the fine-tuning stage, maintaining a robust overall capability across various domains is crucial. \n\nWhat data mixing strategy effectively boosts the versatile performance of LLMs across different domains during the SFT phase? We propose the following statement: \n\nStatement 1 (Knowledge Consistency Training). An LLM finetuned with domain-specific data proportions   () that align with its pretrained output distributions   () will exhibit enhanced and balanced performance across these domains, compared to a model fine-tuned with a non-matching data distribution. Formally, the relationship can be represented as: \n\nwhere  denotes the set of all possible data points. The rationale behind this statement is rooted in the observation that during the pretraining phase, LLMs develop a general understanding of language features and domain-specific knowledge. By maintaining the same distribution of knowledge during fine-tuning, the model can build upon this pre-existing knowledge, thereby enhancing learning efficiency and robustness.",
            "score": 0.47904384802796496,
            "section_title": "Knowledge Consistency Training.",
            "char_start_offset": 11631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1394
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14306640625
        },
        {
            "corpus_id": "272330499",
            "title": "Trustworthy Alignment of Retrieval-Augmented Large Language Models via Reinforcement Learning",
            "text": "We evaluate the LLMs aligned with Natural Questions data on two additional datasets ConflictQA-PopQA and ConflictQA-StrategyQA, whose questions and contexts have never been seen and even the task is transformed from free-form QA to multi-choice QA. As shown in Table 1, TRUSTWORTHY-ALIGNMENT improves the accuracy of Llama-2-7b and Vicuna-7b-v1.5 on ConflictQA-PopQA and ConflictQA-StrategyQA over prompt engineering approaches. Meanwhile, TRUSTWORTHY-ALIGNMENT surpasses SFT with respect to the metric M R by a large margin for the Llama-2-7b and Vicuna-7b-v1.5 model on ConflictQA-StrategyQA, indicating its more significant suppressing effect on parametric memory. This verifies the favorable generalization capability of TRUSTWORTHY-ALIGNMENT to out-of-domain datasets. \n\nAligning LLMs via RL preserves language styles of original pre-trained models. A by-product of aligning LLMs via RL is that language styles of original pretrained models are preserved, while SFT changes them totally. We showcase responses of LLMs aligned via TRUSTWORTHY-ALIGNMENT and SFT in Appendix sentences. By contrast, Llama-2-13b-chat aligned with SFT provides barely answers, even without periods. This is a reasonable phenomenon as the demonstration data used in RL is actually collected from the LLM in the training process, which keeps the same language style with the pretrained model. The data used in SFT is labeled by human whose style distribution may be distinct from the original model. This phenomenon is also an evidence that the trustworthy status is an inherent ability of LLMs rather than assigned by external supervision data. Additionally, we provide human evaluation results on language style and fluency of TRUSTWORTHY-ALIGNMENT in Section 4.3 to further demonstrates that aligning via RL indeed helps. \n\nAligning LLMs to trustworthy status via RL enjoys preferential alignment tax treatment. Ouyang et al. (2022) declare that training LLM with PPO suffers from \"alignment tax\", i.e., model's performance drops on several public NLP datasets after aligning the model.",
            "score": 0.477382521373615,
            "section_title": "Analyses",
            "char_start_offset": 21478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1805
                },
                {
                    "start": 1808,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262451171875
        },
        {
            "corpus_id": "261214394",
            "title": "The Poison of Alignment",
            "text": "For ablation study, we have produced 2 datasets: the first one is our GoatAssitant and Guanaco [24], and the second one is the first dataset without alignment. We trained both models under the same training setups specified before. As it can be seen from Table 2, we see that when the model was trained on our aligned dataset, it did not improve over the base model, which confirms the study by Gudibande et al. [10]. However, we also observe a remarkable performance increase upon fine-tuning our model on the cleaned version of our dataset. Therefore, it seems that the negative impact of alignment distorted the performance boost of previous fine-tuning methods, so that the models did not show a significant improvement on reasoning abilities, leading to the underestimation of reasoning ability gain upon SFT.",
            "score": 0.4759787625808115,
            "section_title": "Ablation study",
            "char_start_offset": 11201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 814
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1666259765625
        },
        {
            "corpus_id": "273811395",
            "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF",
            "text": "In our main experiments, we focus on reward-based RLHF, in particular Proximal Policy Optimization (PPO). The conventional framework for reward-based RLHF consists of several key stages as follows. \n\nSupervised Fine-Tuning (SFT). The initial stage of alignment involves supervised fine-tuning, where a pre-trained language model is refined using a high-quality instruction dataset. \n\nReward Model. Reward-based RLHF involves training a reward model, which is typically initialized from the SFT model. In this process, the logit layer of the SFT model is replaced by a new linear layer. This linear layer takes the embedding of the last token and outputs a scalar reward. Higher rewards indicate better samples. For a given prompt x and a pair of responses y w (chosen) and y l (rejected), the loss function is optimized as: \n\nwhere R \u03b8 (\u2022, \u2022) is the reward model, \u03b8 denotes its parameters, \u03c3 indicates the sigmoid function. \n\nPolicy Training. The last phase of RLHF is dedicated to training the policy model, which is initialized from a reference model, typically the SFT model. Based on a recent study [Xu et al., 2024], PPO performs better in distribution shifts and results in superior alignment with human preferences across challenging tasks like code generation and dialogue systems. Therefore, we selected PPO as the training algorithm. The goal is to optimize the policy model to maximize the reward for a given prompt x and its generated response y, while also minimizing the KL divergence between the policy model and the reference model. The overall loss function for this stage is given by: \n\nwhere \u03c0 \u03b8 is the policy model, \u03c0 ref is the reference model, and R(., .) is the trained reward model.",
            "score": 0.47488818945465217,
            "section_title": "RLHF",
            "char_start_offset": 9527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 197
                },
                {
                    "start": 200,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "267897555",
            "title": "Unintended Impacts of LLM Alignment on Global Representation",
            "text": "First, we identify models with checkpoints at different stages of the alignment process (See Figure 2) so that we can measure the effects of each stage. \n\nSupervised Fine-tuning. In the supervised finetuning stage, the model is provided with prompts and example completions and fine-tuned to produce these sorts of completions. Popular SFT datasets for chat models include the human-written Flan3 (Wei et al.) and Open Assistant (K\u00f6pf et al., 2023) datasets, and the synthetic ShareGPT4 , Alpaca (Taori et al., 2023), and Open-Orca (Lian et al., 2023) datasets. All are variants of instruction following completions to task-oriented prompts. Typically, this step is used to make language models follow instructions rather than continue the input text based on the language modeling objective. \n\nPreference Tuning. After SFT, models undergo preference tuning, where a dataset of prompts and preference-ranked completions are used to align LLMs with user preferences. Two popular algorithms for preference tuning are Proximal Policy Optimization (PPO) (Schulman et al., 2017), which is used in Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), and Direct Preference Optimization (DPO) (Rafailov et al., 2023). For RLHF, a reward model is trained, which takes in a prompt and completion and outputs a score predicting the degree of human preference for such an output, whereas, in DPO, the model is updated directly using the preference dataset. \n\nDeployment After alignment, language models are either deployed inside a product or released for broader use. Notably, these models are a core technology that enables higher-level user-facing systems. While model developers may intend a specific audience, open-access models can be adopted anywhere and major LLM APIs are globally accessible 5 . As a result, due to the broad nature of their possible utility, even unintended impacts of alignment can affect their global adoption.",
            "score": 0.47466309077545954,
            "section_title": "Alignment Process",
            "char_start_offset": 6880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1465
                },
                {
                    "start": 1468,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "270123084",
            "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
            "text": "The rapid development of large language models (LLMs) in recent years has largely focused on English, resulting in models that respond exclusively in English. To adapt these models to other languages, continual pre-training (CP) is often employed, followed by supervised fine-tuning (SFT) to maintain conversational abilities. However, CP and SFT can reduce a model's ability to filter harmful content. We propose Instruction Continual Pre-training (InsCP), which integrates instruction tags into the CP process to prevent loss of conversational proficiency while acquiring new languages. Our experiments demonstrate that InsCP retains conversational and Reinforcement Learning from Human Feedback (RLHF) abilities. Empirical evaluations on language alignment, reliability, and knowledge benchmarks confirm the efficacy of InsCP. Notably, this approach requires only 0.1 billion tokens of high-quality instruction-following data, thereby reducing resource consumption.",
            "score": 0.47438325383475227,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311279296875
        },
        {
            "corpus_id": "271974913",
            "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
            "text": "A.2 DOMAIN ADAPTATION VIA SUPERVISED FINE-TUNING Supervised fine-tuning (SFT) modifies a pre-trained language model to follow specific instructions or perform designated tasks by fine-tuning it on a targeted, task-specific dataset (Raffel et al., 2020;Wei et al., 2021;Liu et al., 2023;Li et al., 2024;Liu et al., 2024;Liao et al., 2024;2025). Applying SFT to a general LLM for specific domain adaptation has demonstrated effectiveness in various fields: in medicine (Clusmann et al., 2023), corpora of medical literature and clinical notes are used; in law (Cui et al., 2024), legal documents and case law are compiled; and in finance (Wu et al., 2023), financial reports and market data are utilized. In the scientific domain, several studies have specialized LLMs for scientific tasks, often necessitating the construction of a substantial domain-specific dataset with SFT. For example, SciGLM (Zhang et al., 2024a) leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific instructions. ChemLLM (Zhang et al., 2024b), a more specified LLM in the chemistry field, collects structured chemical data from a vast selection of online databases and transforms this structured data into a question-answering format for SFT. SciRIFF (Wadden et al., 2024) converts existing literature understanding datasets into natural language input-output pairs suitable for instruction-tuning using pre-defined templates. However, benchmark studies (Feng et al., 2024;Cai et al., 2024) indicate that SFT alone may not provide adequate scientific knowledge to excel in relevant tasks. This suggests the need for a more comprehensive approach that combines domain knowledge infusion with instruction-following enhancements.",
            "score": 0.472169111463286,
            "section_title": "A.1 KNOWLEDGE INJECTION VIA CONTINUAL PRE-TRAINING",
            "char_start_offset": 32376,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1731
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 252,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 269,
                    "end": 286,
                    "matchedPaperCorpusId": "264306303"
                },
                {
                    "start": 286,
                    "end": 302,
                    "matchedPaperCorpusId": "267211722"
                },
                {
                    "start": 302,
                    "end": 319,
                    "matchedPaperCorpusId": "269930078"
                },
                {
                    "start": 319,
                    "end": 337,
                    "matchedPaperCorpusId": "265658966"
                },
                {
                    "start": 467,
                    "end": 490,
                    "matchedPaperCorpusId": "263827375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1427001953125
        },
        {
            "corpus_id": "271328358",
            "title": "Operationalizing a Threat Model for Red-Teaming Large Language Models (LLMs)",
            "text": "Pre-training: In this initial stage, LLMs learn from a large dataset, acquiring basic language understanding and context (Biderman et al., 2023;Brown et al., 2020b).\n\nSupervised Fine-Tuning (SFT) / Instruction Tuning (IT): After pre-training, models are finetuned with specific datasets to adapt to particular tasks or domains.Fundamentally, SFT helps LLMs understand the semantic meaning of prompts and produce relevant responses (Ouyang et al., 2022b;Lou et al., 2023).\n\nReinforcement Learning from Human Feedback (RLHF): This phase involves the refinement of the model responses based on human feedback, focusing on aligning the outputs with human values and expectations (Rafailov et al., 2023;Ouyang et al., 2022a;Rafailov et al., 2023;Bai et al., 2022;Korbak et al., 2023).For brevity, we omit an extended discussion of alignment but refer to Shen et al. (2023b) and Wang et al. (2023d) for a comprehensive overview.\n\nDeployment: Finally, a trained LLM can be deeply integrated in consumer technology applications like chatbots, email, code review, news summarization, and legal document analysis, among other uses with unmediated access to surrounding components (Yang et al., 2023a).",
            "score": 0.4719462923490072,
            "section_title": "LLM Development Phases",
            "char_start_offset": 5235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 167,
                    "end": 327
                },
                {
                    "start": 327,
                    "end": 471
                },
                {
                    "start": 473,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 922
                },
                {
                    "start": 924,
                    "end": 1191
                }
            ],
            "ref_mentions": [
                {
                    "start": 121,
                    "end": 144,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 144,
                    "end": 164,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 431,
                    "end": 453,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 698,
                    "end": 719,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27734375
        },
        {
            "corpus_id": "273026214",
            "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models",
            "text": "2.1 Large Language Models-Human Alignment \n\nLLMs-Human Alignment typically involves training LLMs3 on datasets curated by humans (learning from human feedback data) [Ouyang et al., 2024]. This can be achieved through Supervised Fine-Tuning (SFT), where the model is trained on pairs of prompts (x) and corresponding human-generated responses (y) [Liu et al., 2024]. Alternatively, alignment can be pursued via preference optimization, using a human preference dataset that differentiates between a better response (y w ) and a worse one (y l ) for the same prompt (x): \n\nw , y \n\nTo this day, RLHF [Ouyang et al., 2024] remains the most popular technique used in state-of-the-art LLMs like GPT-4 [OpenAI, 2023], Claude [Bai et al., 2022a], Bard [Google, 2023], and Llama 2-Chat [Touvron et al., 2023]. Different implementations of RLHF can vary in terms of data collection, training processes, and choice of RL algorithms. Typically, RLHF [Ouyang et al., 2024] involves three main steps: (1) collecting feedback, (2) training a RM based on that feedback, and (3) optimising the LLMs using RL techniques, such as Proximal Policy Optimization (PPO) Schulman et al. [2017]. Since RLHF was first introduced, several advancements have been made, including fine-grained reward systems [Bai et al., 2022a, Wu et al., 2023a, Dong et al., 2023a, Wang et al., 2023c, 2024b], or replaced the original PPO algorithm with other RL techniques [Wu et al., 2023b]. \n\nAn alternative to RLHF is DPO [Rafailov et al., 2023], which employs an offline RL approach to optimize language models based on preference data, without the need for a separate RM.",
            "score": 0.47173132798796935,
            "section_title": "Preliminaries",
            "char_start_offset": 5680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1631
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 186,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 597,
                    "end": 618,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 938,
                    "end": 959,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "270067818",
            "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
            "text": "Reinforcement Learning From Human Feedback (RLHF) has propelled the success of the most recent wave of generative AI models (Ouyang et al., 2022;Bai et al., 2022b).While RLHF garners the bonus of steering large language models (LLMs) to meet human expectations, past studies have highlighted that this approach can inadvertently lead to forgetting the diverse abilities that the LLMs have already mastered through pre-training and supervised finetuning (SFT) (Bai et al., 2022a;Zheng et al., 2023;Lin et al., 2024;Dong et al., 2024).This is evident in their declining performance on certain public language benchmarks and a loss of fundamental language skills.Additionally, responses tend to exhibit unexpected code-switching and get significantly longer (Park et al., 2024).These undesirable side effects are commonly referred to as the \"alignment tax\" (Dong et al., 2023;Sun et al., 2024).\n\nIdeally, an optimal RLHF strategy should maintain the bonuses of alignment while avoiding the associated tax, striving to maximize rewards while minimizing forgetting.Relying on the linear mode connectivity of neural networks (Garipov et al., 2018;Frankle et al., 2020;Entezari et al., 2021), the trade-off in model capabilities can be succinctly described as the interpolation of model parameters (Garipov et al., 2018;Ilharco et al., 2022;Zheng et al., 2024).Studies have shown that combining different models, fine-tuned from the same pre-trained model through weight interpolation, often leads to a more balanced performance among the original models (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024).Motivated by this insight, we conducted initial investigations into merging an RLHF model with the reference SFT model it trained from.Our observations indicate that this offline model merging effectively mitigates the alignment cost.As depicted in Tab. 4, the offline merged model restores performance comparable to the SFT model across language benchmarks and language proficiency.",
            "score": 0.47157085780385677,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 775
                },
                {
                    "start": 775,
                    "end": 891
                },
                {
                    "start": 893,
                    "end": 1060
                },
                {
                    "start": 1060,
                    "end": 1354
                },
                {
                    "start": 1354,
                    "end": 1606
                },
                {
                    "start": 1606,
                    "end": 1741
                },
                {
                    "start": 1741,
                    "end": 1840
                },
                {
                    "start": 1840,
                    "end": 1989
                }
            ],
            "ref_mentions": [
                {
                    "start": 124,
                    "end": 145,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 873,
                    "end": 890,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 1119,
                    "end": 1141,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1141,
                    "end": 1162,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 1291,
                    "end": 1313,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1548,
                    "end": 1570,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4443359375
        },
        {
            "corpus_id": "274117026",
            "title": "Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization",
            "text": "With the remarkable success of large language models (LLMs) [1,5,10,11,27,92,95,96] in the field of natural language processing, the training paradigm comprising pre-training and supervised fine-tuning (SFT) have also swept the multimodal field, becoming the primary choice for the research and development of multimodal large language models (MLLMs). Benefiting from the large-scale pre-training corpora [44,49,83,93,102,121] and highquality SFT data [20,24,54,56,101], a series of opensource MLLMs [6,20,45,47,53,99,101,109] exhibit strong performance across various domain and tasks, some even achieving results comparable to commercial models such as  and Gemini [81,91]. \n\nHowever, open-source MLLMs still exhibit limited reasoning capabilities. As shown in Figure 1, InternVL2-8B [20] achieves a score of 58.3 on MathVista [62], a benchmark for multimodal reasoning, when using direct answers but drops to 56.8 with Chain-of-Thought (CoT) reasoning, indicating that CoT reasoning actually reduces its performance. This decline is commonly observed across opensource MLLMs [20,45,99,109]. We attribute this phenomenon primarily to a distribution shift introduced by the 1 arXiv:2411.10442v2 [cs.CL] 7 Apr 2025 SFT loss. Specifically, SFT relies on teacher forcing, where the model is trained to predict the next token based on previous ground-truth tokens. However, during inference, models must predict each token based on their own prior outputs, leading to a distribution shift between training and inference. Since the direct-answer approach requires only brief responses, while CoT reasoning involves generating a long rationale, the distribution shift problem becomes more severe during CoT. This results in models performing worse with CoT reasoning compared to direct-answer responses. \n\nTo address the limitations of CoT reasoning in MLLMs, we draw inspiration from recent NLP approaches [43,76,106] that use Preference Optimization (PO) techniques to align model outputs with desired reasoning patterns.",
            "score": 0.4710130260431231,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 68,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "259287020"
                },
                {
                    "start": 412,
                    "end": 415,
                    "matchedPaperCorpusId": "252917726"
                },
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "260438589"
                },
                {
                    "start": 456,
                    "end": 459,
                    "matchedPaperCorpusId": "258615266"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "256390509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1536865234375
        },
        {
            "corpus_id": "261705563",
            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
            "text": "Pre-trained large language models (LLMs) exhibit a remarkable capacity to address human queries, aid in coding tasks, and more. Nonetheless, the generated outputs of these models can sometimes diverge from preferred human values and even pose potential risks. To make pre-trained LLMs more user-friendly and safe, numerous alignment methods have been proposed, such as RLHF (Casper et al., 2023), RLAIF (Bai et al., 2022b), RRHF (Yuan et al., 2023), RAFT (Dong et al., 2023), and DPO (Rafailov et al., 2023). These methods, however, necessitate the finetuning of pre-trained LLMs and demand considerable amounts of meticulously human-annotated data and computational resources. Take RLHF as an example, this comprehensive approach encompasses three primary phases: supervised finetuning (SFT), reward modeling (RM), and reinforcement learning (RL), together with the necessity to manage four separate models or heads-policy, value, reward, and reference models-each of which has at least billions of parameters. Efficiently operating these models requires significant GPU memory, and the act of updating their parameters poses a threat of overwriting the knowledge retained from the initial pre-training. Additionally, it is worth noting that training larger models is often met with heightened instability and requires significant engineering expertise. Hence, aligning frozen LLMs presents a more appealing option to the community. \n\nThis work shows that fixed LLMs are alignable using a novel inference method without finetuning and data. To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used. Logically, the action of \"selecting a sub-distribution\" should not mandate modifications to model parameters. Reject sampling is a working example of inference-time alignment. However, the method is sample-inefficient (as tested by our experiments).",
            "score": 0.469622249609457,
            "section_title": "Introduction",
            "char_start_offset": 435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1433
                },
                {
                    "start": 1436,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2077
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53515625
        },
        {
            "corpus_id": "276482785",
            "title": "AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO",
            "text": "Supervised Fine-Tuning (SFT) is a widely adopted technique for adapting pre-trained LLMs to specific downstream tasks [Wei et al., 2022a]. By training on taskspecific datasets, SFT allows LLMs to acquire specialized skills and improve performance in targeted domains. Jiang et al. [2024] recently highlighted the effectiveness of SFT in enhancing visual foundation models, demonstrating its utility in visual tasks. In our research, we leverage SFT as the initial stage of our training pipeline, using it to equip the LLM with the basic capability of processing tokenized visual maze inputs and predicting movement tokens. This SFT phase serves as a crucial foundation upon which we build more sophisticated reasoning through reinforcement learning. \n\n2.3 Reinforcement Learning and GRPO for Reasoning and Reward Shaping \n\nReinforcement Learning from Human Feedback (RLHF) and its variants have demonstrated significant efficacy in aligning Large Language Models (LLMs) with human preferences and enhancing their reasoning capabilities. However, RLHF faces substantial scalability challenges due to its resource-intensive nature and reliance on human feedback data. As an alternative approach, recent methodologies like Group Relative Policy Optimization (GRPO) [Kwon et al., 2023a] and Self-Play fIne-tuNing (SPIN) leverage self-play mechanisms, where models autonomously generate training signals and iteratively improve through self-competition Chen et al. [2024]. These self-play approaches show promise in achieving humanlevel performance without the need for extensive human feedback, potentially offering a more scalable solution to the alignment challenge. GRPO, as described by Shao et al. [2024] and implemented in DeepSeek-R1 [Guo et al., 2025], offers a computationally efficient approach to reinforcement learning by estimating advantages based on group scores, eliminating the need for a separate critic network. Reward function design is paramount in RLHF and GRPO, as it directly guides the model's learning process. Carefully crafted reward functions can incentivize desired behaviors and shape the model's policy towards optimal performance.",
            "score": 0.46927249641004615,
            "section_title": "Supervised Fine-Tuning for Visual and Spatial Tasks",
            "char_start_offset": 4351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "277955819",
            "title": "Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL",
            "text": "Supervised Fine-Tuning (SFT) adapts a pretrained language model \u03c0 \u03b8 to a distribution P of sequences that reflect desired linguistic or task-specific behavior. Let z = (z 1 , z 2 , . . . , z T ) denote a token sequence drawn from z \u223c P. The SFT objective maximizes the likelihood of sequences under \u03c0 \u03b8 , which corresponds to minimizing the expected negative log-likelihood: \n\nFor tasks with an explicit decomposition into an input segment and a target segment-such as QA, summarization, or assistant-style dialogue-the data distribution consists of pairs (x, y) \u223c P, where x = (x 1 , . . . , x n ) is the conditioning prompt and y = (y 1 , . . . , y m ) is the supervised output. In such settings, the model conditions on x and predicts the continuation y, with the loss computed only over the target tokens: \n\nwhere denotes the concatenation operator. This alternative objective is often preferred in practice, as it allows for more efficient training by focusing on the relevant output tokens and ignoring the input tokens [9,62,56]. More recently, Shi et al. [46] showed that models trained with the SFT objective in Eq. ( 2) can be superior to Eq. ( 3) when the target sequence is significantly shorter than the input sequence. In the case of distillation of reasoning models, the output sequence will be considerebly longer than the input sequence, and the SFT objective in Eq. ( 3) is preferred. Finally, the expectations in Eq. ( 2) and Eq. ( 3) are approximated by empirical means over a finite dataset \n\nThe resulting objective is optimized via standard stochastic gradient descent or its variants [43,24].",
            "score": 0.4684775263130462,
            "section_title": "Fine-Tuning Language Models",
            "char_start_offset": 8105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1616
                }
            ],
            "ref_mentions": [
                {
                    "start": 1029,
                    "end": 1032,
                    "matchedPaperCorpusId": "271064724"
                },
                {
                    "start": 1032,
                    "end": 1035,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1063,
                    "end": 1067,
                    "matchedPaperCorpusId": "269982303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "263830318",
            "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition",
            "text": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
            "score": 0.4672262161917298,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285400390625
        },
        {
            "corpus_id": "275932560",
            "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
            "text": "In this paper, we presented a comprehensive analysis of the generalization effects of foundation model post-training techniques, specifically RL and SFT. Through extensive experiments on the GeneralPoints and V-IRL tasks, we demonstrated that RL exhibits superior performance in learning generalizable knowledge, while SFT tends to merely memorize the training data, across both the rule and visual variations. This phenomenon consistently occurs across multimodal arithmetic and spatial reasoning capabilities. In addition, we studied the effect of RL on visual recognition, the role of SFT, and the role of verification steps. During our study, two challenges were not resolved. \n\nFailure of SFT on GP-VL. In Figure 5 for GP-VL, we observe that SFT fails to achieve a comparable indistribution performance with RL. To mitigate the variance introduced by hyperparameter choices, we additionally conduct 10 more experiments with different learning rates and tunable components (Figure 16), none of which exhibits a strong increasing trend like RL (Figure 17). Given our observation that scaling up SFT degrades visual recognition capabilities (Figure 8), we hypothesize that SFT locally overfits to reasoning tokens while neglecting recognition tokens, possibly due to the higher frequency of reasoning tokens (see Figure 11 as example). We leave further investigation to future work. \n\nLimits of RL in corner cases. As discussed in Section 5.4, SFT is necessary for effective RL training on Llama-3.2. We investigate applying RL to an overly-tuned SFT checkpoint. As demonstrated in Figure 19, RL is unable to recover out-of-distribution performance when starting from such a checkpoint. Example failure cases are illustrated in Figure 21, where the model collapses to the training rule. These results, together with findings in Section 5.4, indicate that RL has limited effectiveness when applied to extremely underfit or overfit initial checkpoints. Further research is needed to delineate the conditions under which SFT facilitates effective RL.",
            "score": 0.4668215331613923,
            "section_title": "Conclusion, Discussion and Limitations",
            "char_start_offset": 22217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2049
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09503173828125
        },
        {
            "corpus_id": "272694435",
            "title": "ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood",
            "text": "Empirical findings suggest that, non-reinforcement learning (non-RL) alignment methods (Rafailov et al. 2024;Azar et al. 2023) are sensitive to the effectiveness of reference model (Feng et al. 2024;Tunstall et al. 2023). In contrast, there have been approaches that aim to build human-aligned language models by conducting SFT using filtered datasets alone (Zhou et al. 2023;Haggerty and Chandra 2024) indicated SFT with a limited amount of filtered data can effectively enhance the model's capabilities. Furthermore, the proposal and significant performance of certain single-step fine-tuning methods (Hong, Lee, and Thorne 2024;Meng, Xia, and Chen 2024) clearly demonstrate the feasibility of aligning large models without the need for reference model. \n\nORPO (Hong, Lee, and Thorne 2024) explores the role of SFT, providing a theoretical foundation for incorporating preference alignment into SFT. SimPo (Meng, Xia, and Chen 2024) demonstrates the potential of single-step finetuning for aligning large models through extensive experiments. However, the feasibility for aligning large models without reference model is still insufficiently explored from the perspective of gradient strategies.",
            "score": 0.4654440572284791,
            "section_title": "Alignment without Reference Model",
            "char_start_offset": 4584,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1197
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 199,
                    "matchedPaperCorpusId": "266693223"
                },
                {
                    "start": 376,
                    "end": 402,
                    "matchedPaperCorpusId": "266693223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266357421875
        },
        {
            "corpus_id": "268733300",
            "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model",
            "text": "LLMs (Large Language Models) (Achiam et al., 2023;Chowdhery et al., 2023;Touvron et al., 2023a,b;Chiang et al., 2023;Taori et al., 2023) have recently demonstrated their strong language capabilities from text understanding and summarization to generation, all thanks to their pretraining on extensively large datasets. However, as the pre-training only aims to predict the next token, LLMs may not closely follow human instructions. Moreover, since it is difficult to completely filter out harmful content from the vast amount of pretrained data, LLMs may learn to produce outputs that are not aligned with human values. Training with human preference data (or alignment), therefore, becomes essential for the success of LLMs as being shown in the case of ChatGPT (Stiennon et al., 2020;Rafailov et al., 2023;Bai et al., 2022;Sun et al., 2023;Ziegler et al., 2019;Christiano et al., 2017;Dong et al., 2023) Currently, there exist two main approaches to LLMs alignment: those that are based on Reinforcement Learning such as RLHF (Reinforcement-Learning with Human Feedbacks) (Stiennon et al., 2020), and those based on contrastive learning such as DPO (Rafailov et al., 2023). RLHF has been successfully applied to ChatGPT and contains three main steps: 1) Supervised Finetuning (SFT) LLMs using an instruction-following dataset; 2) Training a reward model that assigns a higher reward for human preferred completions given an instruction; 3) Reinforcement learning using Proximal Preference Optimization (PPO) (Schulman et al., 2017), of which sampling from the targeted LLMs (for alignment) and labeling with the reward model are two essential components. Recently, contrastive learning based methods (such as DPO) are introduced, replacing the second and third steps of RLHF by directly tuning LLMs on the preference data. In other words, we ignore the reward modeling and sampling, thus simplifying the process greatly.",
            "score": 0.46530354880335273,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 73,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 764,
                    "end": 787,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1075,
                    "end": 1098,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.419921875
        },
        {
            "corpus_id": "270560471",
            "title": "SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models",
            "text": "Experiments show that by selecting the most style-consistent examples, sometimes as little as 0.7% of the original dataset, fine-tuned LLMs can match or surpass the performance of models trained on full datasets like Octocoder-15.5b (Muennighoff et al., 2023) and OLMO-7b-SFT (Groeneveld et al., 2024) on coding (HumanEval; Chen et al. 2021) and open-ended question answering (AlpacaEval; Dubois et al. 2023) benchmarks. SCAR outperforms leading data selection baselines for efficient SFT, enhancing LLM performance while reducing computational costs. \n\nIn summary, our contributions are three-fold: \n\n\u2022 We introduce and define key response style elements that influence LLM SFT performance. \n\nOur empirical analysis shows that, for training datasets with similar correctness and helpfulness, greater consistency in linguistic form and semantic surprisal significantly enhances LLM performance across various benchmarks. \u2022 We present SCAR, a ranking method that learns distinct representations for linguistic form and semantic surprisal, selecting style-consistent and high-quality examples for efficient LLM SFT. \u2022 Our extensive experiments demonstrate that SCAR outperforms data selection baselines, enabling LLMs trained on a small fraction (e.g., 25%, 0.7%) of the original data selected by SCAR to match or exceed the performance of models trained on the full dataset for coding and open-ended tasks, significantly reducing computational costs.",
            "score": 0.4647932411567548,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1449
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 259,
                    "matchedPaperCorpusId": "260886874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.124755859375
        },
        {
            "corpus_id": "276249618",
            "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation",
            "text": "Pre-training large language models (LLMs) from scratch on trillions of text tokens allows for accurate prediction next tokens in natural language [Achiam et al., 2023, Dubey et al., 2024, Liu et al., 2024a]. Following this, alignment, achieved through fine-tuning on smaller, high-quality datasets designed for specific tasks, becomes critical for enabling the model to develop specialized skills, such as engaging in conversation [Ouyang et al., 2022], math reasoning [Shao et al., 2024, Yang et al., 2024], coding [Zhu et al., 2024], web agent [Qin et al., 2025], and more. The fundamental approach to alignment involves supervised fine-tuning (SFT) on the target domain, which essentially maximizes the likelihood of predicting the next token. However, numerous empirical studies have shown that simple SFT on preferred samples is inadequate for attaining optimal performance [Shao et al., 2024, Ouyang et al., 2022]. \n\nMoving beyond basic imitation learning in SFT, it is suggested to learn from both positive and negative samples. Sample quality can be measured by training reward models to capture general preferences [Dong et al., 2024] or leveraging accurate rule-based rewards [Guo et al., 2025] for specific tasks like math and coding. By treating the autoregressive generation of LLMs as a Markov decision process (MDP), traditional reinforcement learning (RL) algorithms can be effectively applied, such as PPO [Ouyang et al., 2022], SAC [Liu et al., 2024b], REINFORCE [Ahmadian et al., 2024], etc. \n\nWhile online RL-based methods deliver strong performance, they face challenges such as high training costs, instability, and the need for a strong base model as the initial policy. As a result, offline algorithms like direct preference optimization (DPO) [Rafailov et al., 2024] are often preferred, thanks to their effectiveness and simplicity, particularly when high-quality datasets are accessible. The original DPO algorithm has several limitations.",
            "score": 0.4647008410004041,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 897,
                    "end": 919,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1423,
                    "end": 1444,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1768,
                    "end": 1791,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33447265625
        },
        {
            "corpus_id": "270062858",
            "title": "Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity",
            "text": "The advent of large language models has significantly advanced the capabilities of artificial intelligence in understanding and generating human-like text.Our research presents a novel methodology for automatically generating SFT query response data pairs based on context with multi-granularity, a development that holds considerable implications for the field of AI and its applications across various domains.This section explores the broader impact of our work, encompassing both its potential benefits and challenges.\n\nEnhancing Model Performance and Efficiency The automated construction of SFT datasets tailored to the context at hand has the potential to significantly improve the performance of LLMs.By providing high-quality, targeted training data, models can achieve better understanding and generate more accurate outputs, thereby increasing their utility in a wide range of applications.Furthermore, this method reduces the need for extensive manual dataset preparation, leading to more efficient model development.\n\nAdvancing Domain-Specific Applications Our method stands to greatly enhance the development of domain-specific applications, such as specialized question-answering assistants.By enabling the automatic generation of SFT datasets tailored to specific contexts, our approach facilitates the creation of LLMs that are not only more accurate but also more relevant for specialized fields such as medicine, law, and engineering.This could lead to significant improvements in professional assistance systems, offering experts timely and accurate information and potentially accelerating decision-making processes in critical situations.\n\nDemocratizing AI Development By automating the construction of SFT datasets, our methodology could democratize access to high-quality AI development.Smaller organizations or groups with limited resources might find it easier to develop powerful, context-specific AI tools without the need for extensive manual dataset curation.This democratization could accelerate innovation and competition, leading to a broader range of AI applications and services available to the public.\n\nEducational Implications Our approach could also have profound implications for education.Customized LLMs can be developed to provide students with personalized learning assistants.These AI tutors could adapt to each student's learning style and pace, offering explanations, supplementary information, or exercises based on the context of the student's needs.Such personalized education could enhance learning outcomes and make education more accessible and effective for diverse learners.\n\nEthical and Societal Considerations However, the broader deployment of context-specific LLMs also raises important ethical and societal considerations.The accuracy and fairness of these models depend on the quality and diversity of the SFT data.",
            "score": 0.4645999481412494,
            "section_title": "B Boarder Impacts",
            "char_start_offset": 34763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 522
                },
                {
                    "start": 524,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 901
                },
                {
                    "start": 901,
                    "end": 1029
                },
                {
                    "start": 1031,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1453
                },
                {
                    "start": 1453,
                    "end": 1660
                },
                {
                    "start": 1662,
                    "end": 1811
                },
                {
                    "start": 1811,
                    "end": 1989
                },
                {
                    "start": 1989,
                    "end": 2138
                },
                {
                    "start": 2140,
                    "end": 2230
                },
                {
                    "start": 2230,
                    "end": 2321
                },
                {
                    "start": 2321,
                    "end": 2499
                },
                {
                    "start": 2499,
                    "end": 2629
                },
                {
                    "start": 2631,
                    "end": 2782
                },
                {
                    "start": 2782,
                    "end": 2876
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "270210363",
            "title": "LongSkywork: A Training Recipe for Efficiently Extending Context Length in Large Language Models",
            "text": "Traditionally, training an LLM can be split into two stages.The first stage is referred to as the pretraining stage.It uses an auto-regressive language objective to generate a sequence y = {y 1 , y 2 , y 3 , . . .y n } based on the previously generated tokens.Assuming that the language model is parameterized by \u03b8, the auto-regressive process can be described as follows:\n\nThe pretraining stage is seen as injecting most of the knowledge into LLMs (Zhou et al., 2023).The second stage is referred to as the supervised finetuning stage, which can be seen as teaching the model to align format.An SFT training sample consists of a query, denoted as x, which is used as input, and an output y that the model wants to generate conditioned on that prompt.Therefore, the training process in the SFT stage can be formalized as:\n\np \u03b8 (y|x) = p \u03b8 (y t , y <t |x), After conducting a standard SFT stage, we found that implementing a long-context SFT stage significantly improves retrieval and comprehension of long-context information.Xiong et al. (2023) suggested merging normal SFT samples with long-context SFT samples and training them together.However, we noticed that when blending normal SFT with long-context SFT data, the gradient of the short context tends to overshadow the\n\nOur proposed method is to firstly split the document into multiple chunks,\n\nwhere c denotes the document chunk, which consists of tokens.Then we interleaved organize the chunks from multiple documents into a single one like\n\nThis process compels the model to focus on relevant long-distance information, thereby minimizing loss during the generation of the current token in the pre-training phase.Note that the word order in the document is preserved, which allows the attention mechanism to attend to the necessary information for generating current tokens.If the length exceeds the maximum length set, it is truncated to fit within the limit.If the length is shorter than the maximum length, it is padded to reach the maximum.",
            "score": 0.46412965221226893,
            "section_title": "Preliminaries",
            "char_start_offset": 9411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 60,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 213
                },
                {
                    "start": 213,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 372
                },
                {
                    "start": 374,
                    "end": 469
                },
                {
                    "start": 469,
                    "end": 593
                },
                {
                    "start": 593,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 821
                },
                {
                    "start": 823,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1275
                },
                {
                    "start": 1277,
                    "end": 1351
                },
                {
                    "start": 1353,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1500
                },
                {
                    "start": 1502,
                    "end": 1674
                },
                {
                    "start": 1674,
                    "end": 1835
                },
                {
                    "start": 1835,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 2005
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1640625
        },
        {
            "corpus_id": "262064307",
            "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data",
            "text": "Recently, there have been notable advancements in Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023) and Chinchilla (Hoffmann et al., 2022), demonstrating impressive performance in various downstream natural language processing (NLP) tasks (Zhao et al., 2023). Despite the remarkable success of GPT-4, the specific techniques employed in its development remain shrouded in mystery. To gain a deeper understanding of the underlying technical aspects and to promote the widespread adoption of LLMs, a series of open-source base language models have emerged, especially LLaMA (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b). Building upon the released base language models, there are typically two methods to align these base models to specific abilities, including supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT). \n\nThe first line of methods (Chiang et al., 2023;Taori et al., 2023) use SFT to enhance instruction following abilities. Most existing methods primarily focus on designing SFT datasets. Some studies (Chiang et al., 2023;Geng et al., 2023) collect user-shared conversations as well as human feedback datasets from the public web, while others (Xu et al., 2023a;Ding et al., 2023) develop frameworks for automatically gathering extensive open-domain instructions spanning various difficulty levels. However, these constructed SFT datasets are generally mixed with limited expert data Figure 1: Our proposed framework OpenChat with Conditioned-RLFT to advance the open-source language model fine-tuning with mixed-quality data, comparing to previous supervised fine-tuning (SFT) method and reinforcement learning fine-tuning (RLFT) method. MLE and RL denote maximum likelihood estimates and reinforcement learning, respectively. and a large proportion of sub-optimal data due to the high cost of human labor and API requests.",
            "score": 0.46408180871223,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1881
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11126708984375
        },
        {
            "corpus_id": "270737947",
            "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
            "text": "Large language models (LLMs) have shown remarkable abilities in diverse natural language processing (NLP) tasks. The LLMs generally undergo supervised fine-tuning (SFT) followed by preference alignment to be usable in downstream applications. However, this sequential training pipeline leads to alignment tax that degrades the LLM performance. This paper introduces PAFT, a new PArallel training paradigm for effective LLM Fine-Tuning, which independently performs SFT and preference alignment (e.g., DPO and ORPO, etc.) with the same pre-trained model on respective datasets. The model produced by SFT and the model from preference alignment are then merged into a final model by parameter fusing for use in downstream applications. This work reveals important findings that preference alignment like DPO naturally results in a sparse model while SFT leads to a natural dense model which needs to be sparsified for effective model merging. This paper introduces an effective interference resolution which reduces the redundancy by sparsifying the delta parameters. The LLM resulted from the new training paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard. Comprehensive evaluation shows the effectiveness of the parallel training paradigm.",
            "score": 0.4638164057514975,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.365234375
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "The results show that the LFT model outperforms in terms of factuality and usefulness, suggesting that even extensive IT does not significantly introduce useful or factual knowledge to the model. \n\n(2) In a more extreme scenario, we compare the performance of the same LFT model, trained on open-domain instruction-response pairs, with a domain-specific model trained on MedInstruct 52k against MedInstruct-test 216 . Remarkably, even in this case, the LFT model performs better. Table 13 shows that LLaMa-2 7B -chat models (the already IT-ed versions open-sourced by META) outperforms all our fine-tuned models. We attribute this to RLHF or a better and larger IT dataset used for fine-tuning. \n\nKey Takeaways: LFT, even at scale, largely relies on pre-trained knowledge without acquiring new information. In contrast, SFT's notable token distribution shift suggests new knowledge acquisition. However, responses by LFT that are grounded in pre-trained knowledge consistently outperform those based on newly learned information from SFT, indicating that SFT tends to diminish overall knowledge quality.",
            "score": 0.4630556033034094,
            "section_title": "Instruction",
            "char_start_offset": 17839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1103
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "267320613",
            "title": "Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble",
            "text": "Large language models (LLMs) (Vaswani et al., 2017) have become prominent in the field of artificial intelligence in solving a wide range of complex tasks in question answering (Ouyang et al., 2022;Guu et al., 2020), code generation (Li et al., 2022;Nijkamp et al., 2023), reasoning and planning (Kojima et al., 2023;Liu et al., 2023), and various other domains. However, as large language models are trained using data from various sources, they may generate outputs that are biased, inappropriate, or even harmful, which are misaligned with human values. Therefore, it is crucial to align large language models with human values for them to be safely deployed. \n\nRecently, Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) has shown to be a promising approach to mitigate the misalignment issue. Concretely, RLHF first runs supervised fine-tuning (SFT) to train a large language model to generate responses that follow instructions. It then trains a reward model using a preference dataset that reflects human values. Lastly, it runs reinforcement learning using the learned reward model to finetune the SFT model. In this way, the finetuned model would generate sequences with higher rewards, which are presumably more aligned with human values. \n\nThe RLHF algorithm has demonstrated success in improving the alignment of large language models (Dubois et al., 2023;Wu et al., 2023). However, it also has some recognized issues. As the reward model is trained on offline-collected preference data, its reward predictions may not be accurate on out-of-distribution data. If we use reinforcement learning to optimize a large language model with an inaccurate reward model, it may generate misaligned outputs with incorrectly high estimated rewards.",
            "score": 0.4629937747955289,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 662
                },
                {
                    "start": 665,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1770
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 250,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4091796875
        },
        {
            "corpus_id": "277468006",
            "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning",
            "text": "To better understand the effects of prompt format on supervised fine-tuning (SFT), we explore four SFT variants that differ in whether the training data includes intermediate reasoning steps and whether the answers are wrapped in structured JSON format. We use GPT-4o-generated data on the ESCI product search task for training all four SFT models. \n\nTable 9 shows results on ESCI, and Table 10 evaluates generalization to broader benchmarks. \n\nOn the task-specific ESCI dataset, all SFT variants outperform the base model (Qwen-2.5-3B-Instruct), demonstrating the effectiveness of supervised fine-tuning on task-specific data. However, all variants fall short compared to REC-R1, which uses the same data but trains via reinforcement learning. This highlights the advantage of reward-driven learning in aligning with downstream task metrics. \n\nWe then assess the general-purpose capabilities of these models. On MMLU, a knowledgeintensive benchmark, all SFT variants retain performance close to the original model (within 2 points), suggesting factual knowledge is preserved. In contrast, IFEval results reveal catastrophic forgetting across the board-all SFT variants suffer 20-30 point drops in instruction-following accuracy, regardless of format. This underscores the risk of overfitting in SFT, where tuning on narrow task data compromises broader generalization. \n\nAn interesting observation arises on GSM8K: the variant with JSON formatting but no reasoning shows improved performance over the base model (+4.7). We hypothesize that the strict output format (JSON) acts as a \"shield,\" isolating the fine-tuning effects from interfering with the model's native reasoning process. In contrast, the reasoningheavy variants modify the generative behavior more substantially, harming out-of-domain reasoning. \n\nOn the coding benchmarks (MBPP, HumanEval), all four SFT variants exhibit comparable or slightly improved performance relative to the original model-regardless of whether the training outputs used JSON format. This suggests that coding ability is relatively robust to task-specific SFT, and may even benefit from it.",
            "score": 0.46259454209187756,
            "section_title": "E.3.2 Additional Analysis: Impact of Reasoning and JSON Format in SFT",
            "char_start_offset": 45800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1369
                },
                {
                    "start": 1372,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2130
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1517333984375
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "The field of LLMs has undergone significant advancements, with billions of parameters and trillions of tokens processed in parallel during the pretraining stage [OAA + 24, Ant24, TAB + 23]. Following pretraining, SFT is applied to enhance the model's performance on downstream tasks. \n\nHowever, neither pretraining nor SFT can fully address the issues of bias and ethics in LLMs [OAA + 24, WBP + 24]. To tackle these challenges, RLHF with PPO has been proposed and is widely accepted for aligning LLMs, including GPT  Previous work focused on pairwise datasets, which are more challenging to gather. In contrast, binary feedback like \"thumbs up\" and \"thumbs down\" is easier to collect. KTO leverages the concept of human aversion to undesired data and successfully handles binary feedback [EXM + 24]. DRO focuses on binary data by estimating the policy and value functions and optimizing each sequentially while keeping the other fixed [RTG + 24]. However, these methods cannot accommodate different types of data. To address this, UNA was proposed to handle pairwise, binary, and score-based feedback through an implicit reward model, supporting both online and offline alignment [WBH + 24]. \n\nThere have also been efforts to merge SFT with alignment. ORPO proposed a new loss function to increase the ratio of desired responses over undesired responses, achieving both SFT and alignment [HLT24]. PAFT suggested conducting SFT and alignment in parallel and merging them afterward [PWB + 24]. However, ORPO's reliance on pairwise datasets and its deteriorating performance compared to other SFT and alignment methods pose challenges. In contrast, PAFT requires training separate adaptors for SFT and alignment and merging them through sparsity, which is inefficient. Inspired by UNA's achievements, we aim to unify SFT with alignment based on UNA's principles to avoid performance degradation.",
            "score": 0.4625203383363867,
            "section_title": "Related Work",
            "char_start_offset": 15271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1893
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447021484375
        },
        {
            "corpus_id": "272367969",
            "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback",
            "text": "Necessity of Positive and Negative Scores The positive and negative scores are both essential for the seq2seq RM. We conducted ablation experiments on PKU-SafeRLHF under conditions where only negative scores or only positive scores were included. Our experiments indicate that when only negative scores are used, large language models (LLMs) tend to converge to a paradigm where no characters are generated. This occurred 68.5% of the time with Gemma-2B and 86% of the time with Llama2-7B. We hypothesize that generating nothing is easier for LLMs than generating proper tokens, as both outcomes avoid negative scores. The inclusion of positive scores mitigates this issue by incentivizing the generation of tokens consistent with the seq2seq RM. \n\nNegative scores can enhance the alignment performance of seq2seq RMs. Figure 5 compares the performance of three models: PPO-T, PPO-T-Pos (which considers only positive scores), and Init-SFT. The results demonstrate that PPO-T, which includes negative scores in its evaluation, significantly improves alignment performance compared to Init-SFT. This improvement suggests that incorporating negative scores allows for better utilization of information from the preference dataset, leading to more effective RL fine-tuning. Correction Abilities of Seq2seq RM To verify that seq2seq RM has the ability to perform both correction mapping and identity mapping on responses, we concatenate the prompt with the response generated by Init-SFT and then input it into seq2seq RM. By comparing the output with Init-SFT, the results in Figure 6 indicate that seq2seq RM is capable of mapping Init-SFT's responses to be more aligned with human preferences. However, relying solely on the combination of the Init-SFT model and seq2seq RM cannot surpass RLHF. As shown in Table 5, the performance of PPO-T based on seq2seq RM training is better than the direct correction of seq2seq RM. This indicates that the feedback from seq2seq RM enables LLMs to generalize to better alignment performance than simple correction mapping.",
            "score": 0.46042078081795446,
            "section_title": "Ablation Study and Disscussion",
            "char_start_offset": 19066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465087890625
        },
        {
            "corpus_id": "275793001",
            "title": "Improving question answering in programming domain with pretrained language model finetuning using structured diverse online forum data",
            "text": "We based our experiments on the Llama [30] 7B model. Our fine-tuning strategy addressed QA tasks in the \"Python Basics and Environment\" class. \n\nSupervised Finetuning. SFT was conducted using the SFT dataset and an augmented dataset with paraphrased answers to enhance generalization. We used the LoRA adapter with a rank of 16, alpha of 32, and a dropout rate of 0.1. The model was fine-tuned using the AdamW optimizer with a learning rate of 1.0 \u00d7 10 -4 and a cosine learning rate scheduler. Only answers with non-negative Scores log were used. Prompts included the title and question formatted as \"Title: {title}\\nQuestion: {question}\\nAnswer:\" with a maximum length of 512 tokens and 256 tokens for the response. \n\nWe also extended our study to other classes, using the same setup to evaluate the benefits of domain-specific adaptation. \n\nRLHF Training. The RLHF phase followed the same setup as SFT, using the fine-tuned MPNet with cosine similarity to assess how well an answer matched a question. LoRA adapters were applied to optimize model weights, with key hyperparameters set to a learning rate of 1.41 \u00d7 10 -5 , a target KL divergence of 6, a discount factor \u03bb of 1, and a Generalized Advantage Estimation (GAE) \u03bb of 0.95. A batch size of 64, with gradient accumulation steps of 4, provided an effective batch size of 256. Training ran for 10 global epochs, using PPO for each batch. \n\nTo prevent overly short \"Yes\" or \"No\" responses during RLHF training, a length penalty was applied to rewards based on the sequence length L. Short sequences (below 40 tokens, L lower = 40) incurred a penalty proportional to \u03b1 1 = 2, while long sequences (above 128 tokens, L upper = 128) were penalized with \u03b1 2 = 0.1. This helped balance response length and relevance.",
            "score": 0.46031704287347996,
            "section_title": "PLM Question Answering setup",
            "char_start_offset": 19504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 142
                },
                {
                    "start": 145,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1768
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "275134044",
            "title": "Disentangling Preference Representation and Text Generation for Efficient Individual Preference Alignment",
            "text": "Since we mainly aim at improving the efficiency of individual preference alignment in LLMs, we consider two widely-used aligning algorithms as baselines, along with two popular PEFT techniques as more challenging baselines: \n\nRLHF (Christiano et al., 2017): Reinforcement Learning from Human Feedback (RLHF) is a standard alignment method for LLMs. It involves training a reward model and optimizing the LLMs accordingly through Proximal Policy Optimization (PPO) (Schulman et al., 2017). \n\nDPO (Rafailov et al., 2023): Direct Preference Optimization (DPO) simplifies RLHF into binary classification, based on an analytical mapping from the optimal language model to the reward model. \n\nDPO w. P(rompt)-Tuning (Liu et al., 2022): DPO combined with P-Tuning. We extend SFT models with soft prompts of length 4 on each transformer layer, consistent with the outputs from our implemented latent adapters for VAEs. We prefine-tune the soft prompts on generic responses sampled from SFT models in advance. We refer to this preliminary step as SFT with P-Tuning. \n\nDPO w. LoRA (Hu et al., 2022a): DPO combined with Low-Rank Adaptation (LoRA). We implement LoRA with low-rank of 8 for efficiency. \n\nIn addition, we conduct ablation study on the components of the CLaP objective. The experimental details and results are included in Appendix C.",
            "score": 0.45967295361852833,
            "section_title": "Baseline Methods",
            "char_start_offset": 15622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 231,
                    "end": 256,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 495,
                    "end": 518,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 710,
                    "end": 728,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 1071,
                    "end": 1089,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382568359375
        },
        {
            "corpus_id": "271404315",
            "title": "Structure-aware Domain Knowledge Injection for Large Language Models",
            "text": "Domain Adaptation for LLMs. While pre-trained LLMs possess promising capabilities, their performance is often hampered by the scope and recency of their training data, which particularly affects smaller models in downstream applications (Zhao et al., 2023;Wang et al., 2023a). Continual Pre-Training (CPT) addresses this by perpetually updating a pre-trained model with domain-specific content (Sun et al., 2020;Xu et al., 2023b). , with parameter-efficient tuning methods devised to curtail training costs (Hu et al., 2021;Liu et al., 2024c). \n\nTo keep pace with the latest information, models can be fine-tuned with supervised instructionresponse pairs (SFT), thus staying current with the advancing knowledge landscape (Mecklenburg et al., 2024;Qiu et al., 2024). Existing literature confirms that combining CPT and SFT is effective for LLMs to remain precise and up-to-date in dynamic fields like medicine (Wang et al., 2023b;Qiu et al., 2024) and coding (Roziere et al., 2023;Guo et al., 2024). Our study builds upon this CPT-SFT framework, innovating with SCPT-SSFT strategies to efficiently and effectively infuse domain knowledge with the inherent structure hierarchy. \n\nConditional Language Modeling. The idea of continual pre-training language models on domain corpus in the condition of the knowledge structure is mainly inspired by CTRL (Keskar et al., 2019). Keskar et al. (2019) demonstrates the effectiveness of steering text generation through control codes (one or two words) that signify the desired genre, style, or task. In the era of LLM, system prompt plays a similar role in controlling models' responses to adapt to different needs and functionalities, such as role-playing, language style transfer, task setting, and behavior setting (Brown et al., 2020;Wang et al., 2023d;Bai et al., 2023a). Our SCPT approach extends the control codes or system prompts to domain-specific knowledge structures, so as to guide the learning process and tailor the model's output more closely to specialized fields.",
            "score": 0.4592271986972703,
            "section_title": "C Detailed Related Work",
            "char_start_offset": 41836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 412,
                    "matchedPaperCorpusId": "198968327"
                },
                {
                    "start": 1759,
                    "end": 1779,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11962890625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
            "score": 0.4591106771718589,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67138671875
        },
        {
            "corpus_id": "278534409",
            "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale",
            "text": "Figure 6 illustrates the relationship between model size and performance on AIME2024 (left) and LiveCodeBench (right). AM-Thinking-v1 achieves the strongest performance among dense models of similar scale, and comes close to the performance of much larger MoE models, striking an effective balance between efficiency and performance. Compared to traditional SFT, we find that supervised fine-tuning on long-form reasoning tasks leads to a pattern shift. To achieve stable convergence, this stage requires a larger learning rate and batch size; otherwise, the model struggles to fit the data effectively. For example, while traditional SFT might use a learning rate around 8 \u00d7 10 \u22126 with a batch size of approximately 0.5M tokens, supervised fine-tuning on long-form reasoning often requires a learning rate as high as 8 \u00d7 10 \u22125 and a batch size of around 2M tokens. Figure 7 shows training loss during SFT. As shown in Figure 8, we track the evolution of Average Generation Length and Average Stop Ratio during SFT on the AIME2024. At the early stages of training, the model tends to generate excessively long outputs with a low stop ratio. This is largely due to the nature of the base model's pretraining corpus, which predominantly consists of plain text, as well as the fact that reasoning examples in our dataset are significantly longer than standard instruction data. As training progresses, we observe a consistent decrease in average generation length alongside a steady increase in stop ratio. This trend indicates that the model is gradually learning the structural and semantic patterns inherent in long-form reasoning prompts. The alignment of these dynamic metrics suggests that our fine-tuning methodology effectively guides the model toward more coherent and task-aligned reasoning behavior.",
            "score": 0.45903183533973746,
            "section_title": "Results",
            "char_start_offset": 25673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1807
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.105224609375
        },
        {
            "corpus_id": "267897555",
            "title": "Unintended Impacts of LLM Alignment on Global Representation",
            "text": "In this paper, we explore nine open-source language models at various alignment stages on four downstream tasks. Since Llama 2 SFT has not been publicly released, we cannot disentangle the effects of SFT and RLHF in the alignment of Llama 2 Chat. We use the released model checkpoints on Huggingface for all of the open-source models tested in this paper. Since we use open checkpoints rather than aligning the models ourselves, we cannot directly test individual changes to the alignment procedure and their downstream impacts. Instead, we focus a wider lens on the practical downstream effects of each alignment stage. We leave causal intervention and interpretability studies on the impacts of alignment to future work. \n\nWe select our datasets based on high-quality natural human-written benchmarks. Based on the availability of such high-quality resources, we focus on intent detection for dialects, extractive QA and reading comprehension for languages, and global opinion surveys for opinions. Since we test on a limited set of tasks, it is possible that failure modes arise on tasks that we did not assess in this work. In the context of our multilingualism experiments, we find that the performance improvements in all languages span two tasks. A more concrete assessment of multilingual generalization would benefit from a wider breadth of tasks.",
            "score": 0.4588592039871828,
            "section_title": "Limitations",
            "char_start_offset": 27315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.320556640625
        },
        {
            "corpus_id": "278481752",
            "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
            "text": "Spurious Correlations in NLP: It is well known that NLP datasets can contain artifacts or shortcuts that models exploit for seemingly high performance (McCoy et al., 2019). For example, specific token patterns in QA might correlate with correct answers in the training data but fail to generalize out-of-distribution. Our controlled experiments build on this work by extending these ideas to text generation. \n\nOver-optimization in Alignment: SFT has been widely used to align large models with humans' preferences on tasks like summarization, dialogue, and instruction-following. Various algorithms such as PPO (Stiennon et al., 2020) DPO (Rafailov et al., 2023), and KTO (Ethayarajh et al., 2024) have been proposed in order to better align LLMs to human preferences, however these have been known to suffer from over-optimization behavior, where, for example, models only learn to increase response length (Singhal et al., 2024;Park et al., 2024). While advanced evaluation methods have been proposed to more robustly measure task performance (Dubois et al., 2024) and to assess preference-based optimization (Lambert et al., 2024), relatively few studies have systematically examined how these methods cope with shallow or spurious features in the training data.",
            "score": 0.4579984692718325,
            "section_title": "RELATED WORK",
            "char_start_offset": 2716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1266
                }
            ],
            "ref_mentions": [
                {
                    "start": 151,
                    "end": 171,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 640,
                    "end": 663,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 931,
                    "end": 949,
                    "matchedPaperCorpusId": "268733207"
                },
                {
                    "start": 1046,
                    "end": 1067,
                    "matchedPaperCorpusId": "269004605"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2374267578125
        },
        {
            "corpus_id": "259766568",
            "title": "Secrets of RLHF in Large Language Models Part I: PPO",
            "text": "Alignment is a vague and confusing topic that is intractable to evaluate. In the context of our paper, we endeavor to align models with human intentions. To be more specific, we define models to act as being helpful and harmless similar to [27]. \n\nHelpfulness means the model should follow instructions; it must not only follow instructions but also deduce the intent from a few-shot prompt or another interpretable pattern. However, the intention behind a given prompt can often be unclear or ambiguous, which is why we depend on our annotators' judgment, and their preference ratings constitute our primary metric. \n\nHarmlessness is also challenging to measure. The extent of damage caused by language models usually depends on how their outputs are utilized in the real world. For instance, a model that generates toxic outputs could be harmful in a deployed chatbot but could also be beneficial if used for data augmentation to train a more precise toxicity detection model. \n\nAs a result, we employ more precise proxy criteria to capture various aspects of a deployed model's behavior that can be helpful or harmful. In order to compare the RLHF models with baseline models, we generate a single response for each test prompt and task human annotators by comparing the responses from different models and labeling their preferences. We repeat this experiment multiple times using GPT-4 as the annotator and consistently obtain agreement levels between the evaluations. Baseline. We employ several baselines for comparison, including two SFT models trained on LLaMA and OpenChineseLLaMA datasets. These SFT models are trained on Chinese and English datasets, respectively. Additionally, we derive two RLHF models using PPO-max from these two types of SFT models 3 We also compare our models with OpenAI's ChatGPT4 (gpt-3.5-turbo-0613), an excellent language model tuned with RLHF. \n\nGeneration. We generate a single response for each prompt using nucleus sampling [30] with a probability threshold of p = 0.9 and a temperature of \u03c4 = 0.8 for each baseline model. To avoid repetitive responses, we apply a repetition penalty [38] with a hyperparameter of \u03b2 = 1.1 based on previously generated tokens. Additionally, we set the maximum token length to 2048.",
            "score": 0.45792923896150606,
            "section_title": "Alignment Metrics and Experiment Setups",
            "char_start_offset": 46387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2258
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.343994140625
        },
        {
            "corpus_id": "277993890",
            "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning",
            "text": "Implicit Baseline: Output the final answer in <ANSWER> </ANSWER> Explicit Structured Reasoning: Output the thinking process in <THINK> </THINK>, and divide it into four parts: <PLANNING>, <CAPTION>, <REASONING>, and <SUM-MARY>. After that, output the final answer in <ANSWER> </ANSWER>. Explicit Unstructured Reasoning: Output the thinking process in <THINK> </THINK> and final answer in <ANSWER> </ANSWER>. \n\nThe outcomes of (a), (b), (c) informed our next steps: a good initialization is crucial. Simply put, direct RL on Data RL was insufficient to induce correct reasoning or even consistently correct answers -confirming that complex reasoning abilities do not emerge from scratch with sparse rewards. We assume this is mainly due to the weak performance of the Qwen2-Audio-7B-Instruct base. \n\nWe therefore employ a two-stage training for the subsequent variants: \n\n1. Supervised Fine-Tuning (SFT) Stage: We warm-start the model on Data SF T , teaching it to produce the desired output format (either structured or unstructured reasoning + answer). We perform this SFT for each reasoning style separately: \n\nStructured SFT: Train on Data SF T _StructuredT hought examples. We input the question and train the model to output the four-step reasoning followed by the correct answer. \n\nUnstructured SFT: Train on Data SF T _U nstructuredT hought examples, where the output is a freeform rationale ending in the answer. \n\nAfter this stage, we obtain two candidate models: one that can articulate structured reasoning and one with unstructured reasoning. Both are capable of providing coherent explanations.",
            "score": 0.4579265956454165,
            "section_title": "Prompts for GRPO",
            "char_start_offset": 12488,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 407
                },
                {
                    "start": 410,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1607
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04742431640625
        },
        {
            "corpus_id": "270062943",
            "title": "A Survey of Multimodal Large Language Model from A Data-centric Perspective",
            "text": "Adaptation is crucial for aligning pre-trained multimodal large language models (MLLMs) with specific tasks and user preferences.Self-supervised pre-training provides LLMs with a broad understanding of textual and multimodal information, while supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) are essential for adapting these models to excel in targeted applications and adhere to user preferences and societal values.Datacentric SFT involves training models on carefully curated datasets across multiple modalities.In contrast, data-centric RLHF focuses on collecting human judgments to guide the model toward generating desirable and ethically aligned responses.Emphasizing data-centric approaches highlights the importance of high-quality, domain-specific, and multimodal datasets in the successful adaptation of MLLMs.",
            "score": 0.4570953204117022,
            "section_title": "DATA-CENTRIC ADAPTATION",
            "char_start_offset": 42479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 698
                },
                {
                    "start": 698,
                    "end": 856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43359375
        },
        {
            "corpus_id": "271404315",
            "title": "Structure-aware Domain Knowledge Injection for Large Language Models",
            "text": "This work pioneers in incorporating structureaware methodologies to enhance domain knowledge injection into large language models. Through a novel SCPT-SSFT paradigm, we have set a new precedent for adapting LLMs to specialized domains, and the promising and scalable results underscore the viability and potential of our method. We hope to inspire further research in efficient and effective domain adaptation in the LLM community, moving a step closer to models that can truly emulate human intelligence. Limitation. Our two-stage strategy introduces added computational complexity, where taxonomy extraction and data reorganization are required in the SCPT phase, and extra QA syntheses are optionally applied in the SSFT stage. In Appendix B, we provide further discussion with extensive empirical experiments. Despite the additional computational overhead introduced, our method achieves greater overall benefits and can reduce the reliance on large-scale LLMs (e.g., 70B models). We will delve into the investigations in future work. SFT Data-Synthesis. We query Llama3-70B to generate 2,700 QA examples and remove those with over 0.5 F1-Score similarity to test samples to prevent data leakage. During inference, when the model can generate correct answers (corresponding to specific knowledge points) that haven't been seen during the SFT stage, we can ensure the knowledge is injected at the CPT stage and SFT only enhances the instruction-following capability. In practice, merely 13 out of 2700 (around 0.5%) synthetic data have over 0.5 F1-Score and are thus filtered out from the SFT data. \n\nTab. A1 statistics the semantic similarity (measured by BERTScore (Zhang et al., 2020)) between generated and GT questions and answers, and the results emphasize there is no knowledge leakage in the generated SFT data (they share poor semantic similarity across questions, answers, and QAs). MMedBench's robust dataset extends across 6 languages (i.e., English, Chinese, Japanese, French, Russian, and Spanish) and 21 medical fields, which include, but are not limited to, Internal Medicine, Biochemistry, Pharmacology, Psychiatry, and many others. It provides 45,048 training pairs and 8,518 testing pairs for diverse learning and testing scenarios.",
            "score": 0.45704569374690257,
            "section_title": "Conclusion",
            "char_start_offset": 24827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2255
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142333984375
        },
        {
            "corpus_id": "270703283",
            "title": "EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models",
            "text": "Results on different prompt categories. Since MLLMs are not specifically trained to perform evaluations, they are naturally ill-suited for this task, hindering their task performances. Therefore, we need to annotate SFT data for this task and finetune the MLLMs accordingly. To verify the necessity, We conduct experiments comparing the LLava-Next 13B model with and without SFT. \n\nAs shown in Table 4 and Table 5, the results demonstrate that SFT training considerably improves performance across all prompt categories in both image faithfulness and text-to-image alignment, closely aligning the MLLM's predictions with human evaluations. Note that Table 4 illustrates that the baseline method without SFT performs poorly in image faithfulness and text-image alignment evaluations, particularly in the former. \n\nEffect of training dataset size for vision-language model training. In order to explore the effects of data size and determine the sufficient amount of training data, we train the model on image faithfulness evaluation task with images and their annotations sourced from 200, 500 and 800 prompts. As illustrated in Table 6, the evaluation performance continuously enhances as more training data is used. Notably, training with just 500 prompts nearly maximizes accuracy, with further increases to 800 data yielding only marginal improvements.",
            "score": 0.45646286561031924,
            "section_title": "ABLATIONS AND ANALYSES OF EVALALIGN",
            "char_start_offset": 26830,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1355
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2054443359375
        },
        {
            "corpus_id": "270286195",
            "title": "UltraMedical: Building Specialized Generalists in Biomedicine",
            "text": "Alignment of LLMs.Since the emergence of ChatGPT, the three most critical steps of LLMs have been broadly proven to advance large language models toward sophisticated artificial intelligence, including pre-training on large-scale parameters and corpora, supervised fine-tuning (SFT) on high-quality annotations, and reinforcement learning from human feedback (RLHF) [47]. SFT has been extensively explored in recent years, leading to the emergence of numerous powerful chat and AI-assisted applications,such as Alpaca [63], UltraChat [15], and WizardLM [72]. Aligning Large Language Models (LLMs) with human or AI values has emerged as the next trend following supervised fine-tuning in the open-source community. Beyond instruction tuning, RLHF and DPO [52] techniques further improve LLMs by leveraging preference data and achieve strong performance in specialized domains. Unlike RLHF, DPO does not require a reward model, making it simpler to implement in practice. UltraFeedback [13] has become one of the most popular sources of preference data, contributing to the creation of powerful Zephyr models [66] through DPO. Various DPO variants like KTO [17], IPO [4], and CPO [73] have been proposed to advance preference learning in fields such as mathematics, coding, and reasoning. \n\nRecent works show [74,62] that DPO variants fail to compete with RLHF methods like Proximal Policy Optimization (PPO) [56] under identical settings. Concurrently, the focus on reward models has led researchers to explore interactive or online alignment, which has resulted in superior performance when combined with DPO variants [61,49,16]. This area remains under investigation, and the scaling laws concerning preference data also merit further study. \n\nLLMs for BioMedicine. The powerful abilities of LLMs are increasingly promoting and advancing their applications in biomedicine community. There are two critical lines of research relevant to our work. The first research line amis to leverage integrating prompt and fine-tuning technologies with advanced proprietary models such as OpenAI's GPT-4 [1] and Google's PaLM and Gemini [64,53].",
            "score": 0.45618116998659275,
            "section_title": "A Related Works",
            "char_start_offset": 28742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 538,
                    "matchedPaperCorpusId": "258840897"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46533203125
        },
        {
            "corpus_id": "270220485",
            "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training",
            "text": "We also compare downstream task performance against end-to-end promptbased approaches paired with frontier LLMs: Gemini Ultra 1.0 and Gemini Pro 1.5.We use 10 conversations as in-context examples, with three different prompting frameworks: i.) \"Standard\" which uses the same instruction formatting used for tuning; ii.) chain-of-thought reasoning [58]; and iii.) the mixed-initiative prompting style as used in [24] combined with Proactive Prompting [59], which is the strongest prompting baseline in [34] (henceforth Proactive MIPrompt, following the convention in [34]).We provide a detailed description of each style with examples in Appendix D.\n\nTuning Baselines We compare ACT with supervised fine-tuning (SFT) as well as a common approach to DPO-based alignment.For SFT, we use the ground truth responses for each dataset's training split.As for DPO-based alignment, in the absence of human annotators, a widely accepted approaches has been naively sampling responses from two high capacity models, with Y w coming from whichever model is of higher capacity (e.g., GPT-4 versus GPT-3.5)[60][61][62], effectively performing distillation from the larger model.We thus construct a preference dataset by sampling responses from Gemini Ultra 1.0 and its smaller counterpart, Gemini Pro 1.0.We use \"Standard\" in-context learning with ten examples for domain adaptation.",
            "score": 0.45538350104490405,
            "section_title": "Baselines Prompting Baselines",
            "char_start_offset": 19839,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 648
                },
                {
                    "start": 650,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 1092
                },
                {
                    "start": 1092,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1291
                },
                {
                    "start": 1291,
                    "end": 1369
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 411,
                    "end": 415,
                    "matchedPaperCorpusId": "258557267"
                },
                {
                    "start": 450,
                    "end": 454,
                    "matchedPaperCorpusId": "258840865"
                },
                {
                    "start": 501,
                    "end": 505,
                    "matchedPaperCorpusId": "264833170"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "264833170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0894775390625
        },
        {
            "corpus_id": "272146002",
            "title": "Iterative Graph Alignment",
            "text": "Most existing iterative alignment algorithms utilize a \"think and learn\" framework, where the goal of the thinking process is to generate more aligned responses. This enhancement is achieved through various strategies such as revising [Bai et al., 2022] [Lee et al., 2023], self-selecting [Yuan et al., 2024], or teacher-selecting [Rosset et al., 2024] [Lee et al., 2024] responses, either self-generated or demonstrated by a teacher. The adoption of chain-of-thought prompting, initiated by Constitutional AI [Bai et al., 2022], is integrated into this thinking process to improve response quality and ensure transparency in AI decision-making, thereby making it more explainable to humans. However, generation and evaluation deteriorates in domains with representation gap, and advanced reasoning mechanism [Yao et al., 2023] [Zhou et al., 2024] [Lin et al., 2024] [Cohen and Squire, 1980] has yet to be adopted. \n\nFollowing the revised responses generated during the thinking process, the learning process employs either supervised fine-tuning (SFT) [Lee et al., 2024] [Singh et al., 2024] or direct preference optimization (DPO) [Rafailov et al., 2024] [Yuan et al., 2024] [Rosset et al., 2024]. Our research focuses primarily on SFT, as recent empirical results do not indicate a clear advantage of DPO in enhancing reasoning capabilities [Du et al., 2024]. While traditional SFT approaches often treat each revised response as a target for memorization, STaR [Zelikman et al., 2022] leverages them to filter out aligned rationales and fine-tune models based on diverse rationale-response pairs. However, it is constrained to single-choice questions, requiring responses to be categorized into predefined options such as \"A\", \"B\", \"C\", or \"D\". Which is usually not the case for open-ended conversations.",
            "score": 0.4553602765894966,
            "section_title": "Introduction",
            "char_start_offset": 2064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1808
                }
            ],
            "ref_mentions": [
                {
                    "start": 865,
                    "end": 890,
                    "matchedPaperCorpusId": "19690059"
                },
                {
                    "start": 1465,
                    "end": 1488,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2337646484375
        },
        {
            "corpus_id": "278481752",
            "title": "Assessing Robustness to Spurious Correlations in Post-Training Language Models",
            "text": "Post-training alignment of large language models (LLMs) has emerged as a critical step in ensuring safe, accurate, and helpful responses (Zhang et al., 2023). Commonly used techniques include Supervised Fine-Tuning (SFT) on curated demonstrations and instruction data (Wei et al., 2021), and preference-based methods (e.g., Direct Preference Optimization (DPO) (Rafailov et al., 2023), Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024). Although these methods produce impressive performance on various benchmarks, real-world training data often contain noisy or spurious correlations-features that are correlated with correctness in the training set but are not causally related to the target task. When the model overfits to these spurious patterns, it may underperform on broader distributions and degrade in accuracy under slight distribution shifts. \n\nIn this paper, we systematically investigate how different post-training approaches handle data contaminated by spurious correlations. We create a suite of controlled synthetic training sets with varied tasks, spuriousness levels, and correlation types. Specifically, we explore: mathematical reasoning tasks (Cobbe et al., 2021), constrained instruction-following tasks inspired by the CoLLIE benchmark (Yao et al., 2023), and document-grounded QA. \n\nWithin each of these domains, we manipulate the ratio of spurious data (10% vs. 90%) to examine mild vs. strong contamination, and incorporate two primary types of spurious behavior: Feature Ambiguity (FA) and Distributional Narrowness (DN). We train models via SFT, DPO, or KTO, then assess how well each approach maintains correctness and resists spurious shortcuts. Interestingly, our results reveal that performance under spurious correlations can vary drastically, depending on task type, model size, or alignment method. For example, while preference-based approaches often outperform SFT in certain math tasks, SFT can maintain an edge in context-heavy QA. Moreover, performance does not invariably decline as the amount of spurious data increases: some setups show stable or even slightly improved accuracy at higher spuriousness levels.",
            "score": 0.4548700916377184,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2165
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 384,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "275954349",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "text": "In this paper, we introduced Critique Fine-Tuning (CFT), a novel paradigm that trains language models to critique and analyze responses rather than imitating them as in traditional SFT. Experiments demonstrated that CFT consistently outperforms SFT by 4-10% on mathematical reasoning benchmarks, achieves comparable performance to resource-intensive RL methods using significantly fewer training examples (50K vs. 2M+) and compute (8 H100 GPU-hours), and generalizes effectively to broader STEM domains. Interestingly, even without explicit instruction tuning via traditional SFT or RL, CFT-trained models inherently exhibit strong instruction-following capabilities, challenging conventional assumptions regarding the necessity of imitation-based training for instruction-following tasks. These findings suggest that explicitly teaching models to identify and critique incorrect reasoning can significantly enhance their reasoning and generalization capabilities. Future research directions include improving the quality and coverage of critique data, enabling models to perform self-critique for continual self-improvement, combining CFT with complementary training paradigms like RL, extending the approach to multi-modal scenarios, and further investigating its theoretical foundations. Overall, we believe CFT represents a promising step forward in making language model training more efficient, robust, and effective, potentially reducing the computational and data requirements for developing high-performing models while substantially improving their reasoning and instruction-following abilities. \n\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.",
            "score": 0.4548428284904599,
            "section_title": "Conclusion",
            "char_start_offset": 22337,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1605
                },
                {
                    "start": 1608,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1813
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.177978515625
        },
        {
            "corpus_id": "268819071",
            "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
            "text": "The overall results are illustrated in Table 3.We compare with ChatGPT Achiam et al. (2023) and various open-source models Bi et al. (2024); Bai et al. (2023); Touvron et al. (2023b).The results of the ChatGLM-32B show that RLHF methods, including both DPO and PPO, substantially improve the SFT model across a wide range of tasks in Alignbench.Notably, tasks such as Writing, OpenQA, and Role-Play exhibit more significant improvements.This suggests that the reward model is particularly suited to tasks related to creative writing, whereas it demonstrates limitations in tasks requiring advanced reasoning abilities, such as math and logic.This observation aligns with the intuitive understanding that human preferences tend to lean more toward stylistic and formatting elements rather than deep critical thinking skills.Furthermore, PPO demonstrates a slight edge over DPO, with an average improvement margin of approximately 0.07.This outcome aligns with expectations, considering that PPO's more intricate design demands significantly greater resources during training compared to DPO.Setup.In addition to automatic evaluations, we also incorporate human evaluations to assess the effectiveness of RLHF.Human evaluation is important because they're less likely to be biased as long as the rules are clearly defined.We collect a diverse human evaluation dataset, consisting of 400 instructions in Chinese, spanning an extensive array of tasks such as creative writing, logical reasoning, semantic analysis, language comprehension, and mathematics.Each subject area was represented by 50 distinct samples.The evaluation process employed a pairwise comparison method, where human annotators were tasked with selecting the more suitable response from a pair, with the option to declare a tie.And we have set very lenient requirements for determining whether two responses can be labeled as a tie, so that the win could denote a clear advantage of one response over the other.To minimize variability, each sample is annotated by two evaluators.In this part, we mainly compare ChatGLM-32b-SFT and ChatGLM-32b-PPO, as PPO has shown an edge over DPO in automatic evaluation.\n\nResults Table 4 shows the results of human evaluation and we report the win-rate of two models.",
            "score": 0.45471725463035884,
            "section_title": "Results",
            "char_start_offset": 23050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 47,
                    "end": 183
                },
                {
                    "start": 183,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 934
                },
                {
                    "start": 934,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1320
                },
                {
                    "start": 1320,
                    "end": 1551
                },
                {
                    "start": 1551,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1976
                },
                {
                    "start": 1976,
                    "end": 2044
                },
                {
                    "start": 2044,
                    "end": 2171
                },
                {
                    "start": 2173,
                    "end": 2268
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31396484375
        },
        {
            "corpus_id": "276575418",
            "title": "CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport Alignment for Language Models with Different Tokenizers",
            "text": "Building on this, we introduce a novel Cross-CoT Alignment method that effectively transfers the teacher's reasoning capability to the student model. Specifically, we design two alignment loss functions to encourage the student model to capture the teacher's multi-step reasoning process: (a) directly aligning the outputs of both models using the same input formats (standard and CoT) and (b) aligning CoT and standard outputs to enhance the reliability of the CoT process in generating correct answers. However, aligning these outputs requires handling different vocabulary mappings. While existing KD methods for models with different vocabularies offer a potential proxy, we argue that these approaches are constrained by token-wise alignment. For instance, DSKD employs a simple linear projection to map distributions into a shared dimensional space for each token, which may overlook intricate relationships between teacher and student representations, leading to potential information loss. Similarly, ULD trims output token sequences to enforce token-wise alignment between teacher and student models, which can lead to incomplete sentences and misaligned to-kens with varying contextual meanings, potentially distorting the semantic integrity of the distilled knowledge-especially when direct responses and CoT responses differ significantly in length. Therefore, (2) we introduce a sequence-level alignment approach that operates without requiring projection into a uniform dimensional space or enforcing identical output lengths. Specifically, while we acknowledge OT as an effective solution for aligning the distinct distribution spaces of teacher and student models, as demonstrated in ULD, we extend its application beyond token-wise alignment. Instead, we propose COT 2 ALIGN, a sequence-level alignment method combined with a layer-by-layer approach to enhance reasoning knowledge transfer. This method effectively adapts to sequences of varying lengths, ensuring comprehensive output context alignment while preserving the integrity of the distilled knowledge. (3) Our comprehensive experiments underscore the overall effectiveness of COT 2 ALIGN and the contribution of each proposed technique in enhancing existing universal KD methods. Additionally, when conducting domain-specific distillation across a wide range of tasks, we observe that the best-performing method, DSKD, exhibits limited superiority over other KD methods, contradicting the claims presented in their study. This finding suggests a lack of robustness in DSKD for domain-specific experiments, revealing gaps in its insights.",
            "score": 0.4541188704254581,
            "section_title": "Introduction",
            "char_start_offset": 3867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2256
                },
                {
                    "start": 2257,
                    "end": 2498
                },
                {
                    "start": 2499,
                    "end": 2614
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "271533456",
            "title": "SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages",
            "text": "Most existing open-source supervised fine-tuning (SFT) datasets are predominantly in English (Wei et al., 2022;Taori et al., 2023), which presents a challenge for developing effective models for Southeast Asian languages. To address this, we employed various techniques to construct our SFT pool. For example, we selectively translate some high-quality English data to SEA languages with quality filtering, conduct self-instruction to automatically generate SFT data of certain types, and use various prompting strategies (Madaan et al., 2023;Nguyen et al., 2023b). Following our previous practice, native speakers have been actively engaged throughout the entire SFT data construction process. They manually collect and write seed questions and topic lists, ensuring linguistic and cultural accuracy from the outset. Additionally, native speakers verify, filter, and edit the synthetic SFT data to maintain high quality. \n\nOur preliminary experiments indicated that relying heavily on dominant English data adversely affects performance. To mitigate this, we strive to maintain a relatively good balance of language representation in our training data this time. Figure 2 shows the language distribution of our SFT data. While English remains a significant portion of the dataset, substantial representation is given to other Southeast Asian languages such as Indonesian, Vietnamese, Thai, and others, ensuring a comprehensive and diverse linguistic foundation for the model's training. \n\nSince the first release of SeaLLMs (Nguyen et al., 2023c), the task types of SFT data have been significantly expanded. The dataset now includes a diverse range of task types such as coding, math, education-related content, reasoning, general dialogue, table-related tasks, open-domain QA, and many more. This expansion ensures that the model is well-rounded and capable of handling a variety of queries and tasks. Additionally, SFT with multiple turns has been significantly increased to enhance the model's ability to engage in natural, multi-turn dialogues, improving its conversational fluency and coherence. \n\nModel safety, trustworthiness, and reliability are also important factors for constructing the SFT pool. To address this, we specifically constructed refusal-type data, enabling the model to decline questions beyond its knowledge boundaries, such as those involving non-existing entities.",
            "score": 0.45345778211160226,
            "section_title": "Supervised Fine-tuning Data",
            "char_start_offset": 8330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 921
                },
                {
                    "start": 924,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2102
                },
                {
                    "start": 2105,
                    "end": 2209
                },
                {
                    "start": 2210,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 543,
                    "matchedPaperCorpusId": "257900871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11297607421875
        },
        {
            "corpus_id": "270560824",
            "title": "A Survey on Human Preference Learning for Large Language Models",
            "text": "[196] test the weak-to-strong generalization ability by learning a stronger LLM supervised by a weaker language model, indicating the possibility of scalable oversight over superhuman LLMs.More empirical research is required in this promising direction.\u2022 Language-agnostic LLM alignment: Intuitively, a range of abilities connected with human intelligence should be language-agnostic, such as reasoning [197].However, existing studies [198] demonstrated that current LLMs are mostly more capable of reasoning in certain languages such as English.Muennighoff et al. [199] attempt to elicit language-agnostic ability learning for LLMs by SFT on multilingual instruction dataset.She et al. [197] mitigate the performance difference of languages in math reasoning with preference learning approaches by utilizing the consistency between answers in different languages with the help of an off-the-shelf translation model as preference.Further research can be conducted on mitigating or eliminating the overall capability gap in languages for LLMs by language-agnostic LLM alignment.\u2022 Alignment with multi-modal complement: Large multimodal models such as GPT-4V 4 extend LLMs with the ability to perceive and understand multi-modal information for more downstream applications beyond language.Therefore, aligning large multi-modal models with human intention is also necessary and crucial.Recent large multi-modal model alignment approaches usually align features between modalities and model behaviors with human intentions separately, without explicit reliance on the complement relation between multi-modal input and instructions with intentions [200,201,202].Sun et al. [203] utilize image captions to condition the reward model for RLHF, and more in-depth research in utilizing the complement relation between modalities to align modality features and model behaviors simultaneously can be conducted.\u2022 Comprehensive assessment of LLM alignment progress:\n\nAmong the LLM evaluation approaches discussed in VII, instruction following benchmarks and general ability tests are relatively more comprehensive.However, neither can comprehensively assess LLM alignment progress.General ability tests are mostly formed in multiple-choice questions, efficient to evaluate at scale but unable to evaluate the generative abilities of LLMs.Instruction following benchmarks require humans or powerful commercial LLMs such as GPT-4 for relatively direct evaluation of human intentions.",
            "score": 0.4530248418099927,
            "section_title": "VIII. CONCLUSION AND OUTLOOKS",
            "char_start_offset": 56583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 409
                },
                {
                    "start": 409,
                    "end": 546
                },
                {
                    "start": 546,
                    "end": 676
                },
                {
                    "start": 676,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1384
                },
                {
                    "start": 1384,
                    "end": 1658
                },
                {
                    "start": 1658,
                    "end": 1900
                },
                {
                    "start": 1900,
                    "end": 1953
                },
                {
                    "start": 1955,
                    "end": 2102
                },
                {
                    "start": 2102,
                    "end": 2169
                },
                {
                    "start": 2169,
                    "end": 2326
                },
                {
                    "start": 2326,
                    "end": 2469
                }
            ],
            "ref_mentions": [
                {
                    "start": 565,
                    "end": 570,
                    "matchedPaperCorpusId": "253264914"
                },
                {
                    "start": 1644,
                    "end": 1649,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 1649,
                    "end": 1653,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25439453125
        },
        {
            "corpus_id": "269983424",
            "title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction",
            "text": "Prior studies tend to attribute the alignment tax phenomenon to the low-quality samples within the instruction-following corpus (Chen et al., 2023;Cao et al., 2023), or the knowledge forgetting during the SFT process (Dou et al., 2023;Ren et al., 2024).However, our pilot study in Section 3 reveals that the quality issue and the catastrophic forgetting of pre-training knowledge are probably not the main cause of the alignment tax since the decline can be observed across corpora with varied arXiv:2405.13432v1[cs.CL] 22 May 2024 sizes and quality.\n\nBy analyzing the trend of loss descent during the SFT process, we alternatively posit that the data biases fitted on the instruction data are probably one of the major causes behind it.Specifically, during the tuning process, LLMs fit on dataset biases while acquiring instruction-following ability.In the beginning, the acquisition of generalizable ability predominates so the performance on knowledge and reasoning benchmarks improves.However, during the tuning process, the learning of generalization quickly stagnates and the model tends to acquire more data biases instead, which harms the parametric knowledge of LLM and leads to a decline in related benchmarks.\n\nWe propose a frustratingly simple DTM (Disperse-Then-Merge) framework composed of three steps:\n\n(1) we initially distribute the instruction-following data into several clusters and then (2) perform instruction tuning on each cluster of data to obtain a series of sub-models assimilating different data biases; (3) finally we merge the sub-models trained on each cluster into a single one in the weight space, such that the data bias of each sub-model is mitigated at fusion.Importantly, DTM ensures the reduction of alignment tax when instruction tuning with almost no extra cost at both training and inference.\n\nTo empirically verify the efficacy of the DTM framework, we conduct extensive experiments and evaluations across 9 benchmarks involving math reasoning, world knowledge, and code generation.The experiment results exhibit that DTM outperforms both (1) data selection methods that filter out low-quality samples (Dou et al., 2023); and",
            "score": 0.45231722695760734,
            "section_title": "Introduction",
            "char_start_offset": 1477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1220
                },
                {
                    "start": 1222,
                    "end": 1316
                },
                {
                    "start": 1318,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1833
                },
                {
                    "start": 1835,
                    "end": 2024
                },
                {
                    "start": 2024,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "264406231",
            "title": "DavIR: Data Selection via Implicit Reward for Large Language Models",
            "text": "Supervised Fine-tuning. \n\nIn the current alignment process of Large Language Models (LLM), Supervised Fine-tuning (SFT) plays a pivotal role. This step aims to fine-tune the LLM using a small amount of data to enable it to follow human user commands. Self-Instruct (Wang et al., 2022) generates a significant volume of data for SFT using seed prompts and teacher models. This approach has led to the development of numerous models trained using distillation methods with powerful models (e.g. GPT-4), such as Alpaca (Taori et al., 2023) and WizardLM (Xu et al., 2023). \n\nApart from distillation with strong models, human-generated data also serves as a high-quality source for SFT data. InstructGPT (Ouyang et al., 2022), for instance, utilizes manually annotated data as a source for SFT in the Reinforcement Learning from Human Feedback (RLHF) method. Vicuna (Zheng et al., 2023), on the other hand, leverages user interaction data to construct the SharedGPT dataset. \n\nData for Supervised Fine-tuning. \n\nIn the context of SFT, data's excellence stands as the most pivotal concern, as it directly determines the performance of the fine-tuned model. It is widely acknowledged that the quality of an SFT dataset hinges on two key aspects: firstly, the distribution of the data should ideally be uniform and aligned with the requirements of the intended usage scenarios. Works such as (Xie et al., 2023a;Ji et al., 2023;Chen et al., 2023a) focus on the data distribution to enhance training efficiency. Secondly, data quality is generally deemed more critical than quantity during the SFT process. LIMA (Zhou et al., 2023), for example, suggests that the effectiveness of SFT with a small set of high-quality data significantly surpasses that of large-scale datasets. \n\nIn this paper, we introduce a novel perspective on assessing data quality, emphasizing the learnability of the data by the model. This implies that the data should align with the model's current capabilities and offer the potential for greater improvements in performance.",
            "score": 0.45230417819492974,
            "section_title": "RELATED WORK",
            "char_start_offset": 5469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 26,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.358642578125
        },
        {
            "corpus_id": "270063316",
            "title": "Finetuning Large Language Model for Personalized Ranking",
            "text": "SFT involves fine-tuning Language Models (LLMs) for the recommendation task using positive samples.The main objective of it is to maximize the probability of each token in the correct answers.Additionally, SFT ensures that the answers generated by LLMs adhere to the correct format, avoiding ambiguous or evasive responses.The mathematical formula is shown in equation 2.\n\nmax\n\nwhere  and  represent the \"Task Input\" and \"Task Output\" in the instruction tuning data, respectively,   is the -th token of the ,  < represents the tokens before   , \u03a6 is the original parameters of the model, and D is the training set.",
            "score": 0.45208207063089545,
            "section_title": "Supervised Fine Tuning (SFT)",
            "char_start_offset": 13812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 99,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 376
                },
                {
                    "start": 378,
                    "end": 614
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "266933254",
            "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint",
            "text": "Owing to unsupervised pre-training on large-scale text corpora, large language models (LLMs) have shown remarkable performance on various text generation tasks (Zhao et al., 2023a;Google, 2023), such as question answering, summarization and translation (OpenAI, 2023). To further improve the task solving capacity, researchers (Touvron et al., 2023;Bai et al., 2023) have proposed supervised fine-tuning (SFT) and reinforcement learning (RL) methods, which can better adapt LLMs to specific domains or downstream tasks after pre-training. Typically, SFT methods (Ouyang et al., 2022;Longpre et al., 2023) incorporate annotated input-output pairs (e.g., question and solution, instruction and response) to train the LLM for learning the sequenceto-sequence pattern; RL methods (Schulman et al., 2017;Christiano et al., 2017) adopt a reward model to measure the quality of the generated outputs from the LLM, and then guide its training for maximizing and minimizing the expectation of generating high-quality and low-quality ones, respectively. \n\nAs RL methods are capable of directly reducing the probability of LLMs for producing unexpected outputs, they have been widely used in optimizing LLMs towards better human alignment (e.g., reducing harmfulness) and stronger ability (e.g., reducing errors (Luo et al., 2023;Wang et al., 2023b)). Generally, RL methods first train a discrimination model for distinguishing desirable and undesirable outputs. Then, the model is used to produce the reward scores for the sampled outputs from the LLM, and the LLM would be trained by encouraging and punishing the generation of high-score and low-score ones accordingly. \n\nDespite the success, as existing RL methods mostly utilize instance-level reward for each sam-pled output, it is often difficult to provide accurate fine-grained supervision on complex reasoning tasks (e.g., mathematical reasoning).",
            "score": 0.45196401052295526,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1896
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 604,
                    "matchedPaperCorpusId": "256415991"
                },
                {
                    "start": 799,
                    "end": 822,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1776123046875
        },
        {
            "corpus_id": "273233434",
            "title": "PLaMo-100B: A Ground-Up Language Model Designed for Japanese Proficiency",
            "text": "In Supervised Fine-Tuning (SFT), the training process is guided by a predefined dataset comprising paired examples of \"questions posed to the LLM\" and their respective \"expected answers from the LLM\". This dataset encompasses a variety of tasks, each consisting of a question-answer pair. For instance, in the domain of mathematical problem-solving, each pair entails a problem and its respective solution. Similarly, in conversational settings, each pair consists of a user prompt and the preferred response. The objective of using a training dataset of high diversity and quality is to enhance the downstream task performance of the LLM. \n\nTraining in SFT is conducted using next-token prediction, analogous to the pre-training phase, albeit with an emphasis on fine-tuning the response generation component. To optimize this process, only the response portion of the input text is considered during loss calculation rather than the entire input sequence. Although our preliminary experimental evaluations revealed no significant performance difference between this approach and that considers the entire input, this method was adopted to mitigate the risk of the model internalizing undesirable traits, such as aggressive expressions, potentially present in the questions. \n\nTraditionally, in SFT, various tasks' training datasets are aggregated and trained simultaneously. However, some prior studies, such as Nemotron-4 [Adler et al., 2024], have documented conflicts arising from concurrent learning of multiple tasks. Despite attempts to alleviate these conflicts through adjustments to the sample weighting ratios, attaining a substantial level of harmony was challenging, especially in coding tasks. Consequently, a two-stage SFT approach is proposed wherein the coding tasks are trained in the first stage of SFT, followed by the second stage of SFT, which focuses on more general tasks. \n\nIn our experiment, a similar trend was observed for mathematical questions. Therefore, we adopted the two-stage SFT method, initially segregating mathematical questions for the first stage and subsequently addressing various other tasks in the second stage.",
            "score": 0.4514647335479699,
            "section_title": "Supervised Fine-Tuning",
            "char_start_offset": 14527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1275
                },
                {
                    "start": 1278,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "273532304",
            "title": "VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning",
            "text": "We primarily employ small-sized LMs with a few billion parameters. While our model demonstrates strong performance across various benchmarks, its overall capacity may be constrained compared to larger models, particularly in terms of world knowledge and complex reasoning abilities. \n\nOur training data and tasks are also limited. The training data focuses on linguistic content and does not encompass specialized speech tasks, such as spoken language understanding, speaker recognition or verification, multi-speaker ASR, or speech enhancement. This restricts the applicability of the current model to certain use cases. Furthermore, our efforts are concentrated on human speech, without addressing general audio processing. \n\nDue to license issues, some of the training data cannot be directly released. Instead, we provide statistics about those data and describe the details about our training procedure. For SQA and mixedmodal SFT, we plan to release the data generation scripts. \n\nWe introduce speech capabilities at the SFT stage without involving pre-training. Additionally, we do not utilize reinforcement learning from human feedback (RLHF), which may result in hallucinations or unexpected behavior in the model's output. Therefore, it is important to exercise caution and thoroughly verify the output when using this model.",
            "score": 0.45142967747336116,
            "section_title": "Limitations",
            "char_start_offset": 21517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1175537109375
        },
        {
            "corpus_id": "275757496",
            "title": "Reasoning Language Models: A Blueprint",
            "text": "We provide example insights gathered from the literature and from our analyses of design decisions using x1. \n\nUse Process-Based Evaluation Process-based evaluation, in which the reasoning structure as a whole is assessed, has been shown to be more reliable than alternative methods such as Outcome-Based Reward Models (ORMs). By examining the reasoning steps and their relationships within the structure, process-based evaluation provides a richer signal that helps models refine their reasoning paths and improve overall accuracy. This approach ensures that each intermediate step contributes positively to the final outcome, resulting in more robust reasoning and better generalization across tasks. \n\nUse Two Phases for Training Adopting a two-phase training strategy-splitting SFT and RL-has proven effective in several contexts. This phased approach allows the model to first learn a solid foundation of reasoning patterns in phase one, followed by fine-tuning under more complex, adaptive conditions in phase two. For instance, research on Process Reinforcement through Implicit Rewards demonstrates that models trained with a dedicated SFT phase can maintain performance on standard benchmarks while achieving improved reasoning capabilities during RL. This separation also helps mitigate instability and ensures that each phase targets specific learning objectives, ultimately leading to more robust RLMs. \n\nTrain on Familiar Distributions Training on familiar data distributions can significantly influence a model's initial performance and subsequent improvements. For example, PRIME [38], [163] shows that training on a carefully curated token sequence (such as the eois token approach) avoids performance degradation. Similarly, in tasks like rStar-Math [52], models trained on well-defined, familiar distributions tend to stabilize more quickly and produce higher-quality reasoning outputs. By focusing on familiar distributions, researchers can ensure that the models effectively internalize the fundamental reasoning patterns before moving on to more diverse or challenging tasks. \n\nBe Careful with Prompting LLMs to Critique and Evaluate Relying on prompting alone to encourage large language models to critique and evaluate their own outputs often leads to instability. Research indicates that models struggle to self-correct reliably when prompted to refine their reasoning without external guidance. For example, a recent study [64] illustrates that such prompting typically fails to produce consistently improved results. Another work [114] demonstrates that explicitly training the model to output better responses through iterative refinement outperforms simple prompting.",
            "score": 0.45109762338524584,
            "section_title": "EXAMPLE INSIGHTS FOR EFFECTIVE RLMS",
            "char_start_offset": 84762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2419
                },
                {
                    "start": 2420,
                    "end": 2542
                },
                {
                    "start": 2543,
                    "end": 2695
                }
            ],
            "ref_mentions": [
                {
                    "start": 2448,
                    "end": 2452,
                    "matchedPaperCorpusId": "263609132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1728515625
        },
        {
            "corpus_id": "264833257",
            "title": "ChipNeMo: Domain-Adapted LLMs for Chip Design",
            "text": "After DAPT, we perform model alignment. We specifically leverage two alignment techniques: supervised fine-tuning (SFT) and SteerLM (Dong et al., 2023). We adopt the identical hyperparameter training configuration as DAPT for all models, with the exception of using a reduced global batch size of 128. We employ an autoregressive optimization objective, implementing a strategy where losses associated with tokens originating from the system and user prompts are masked (Touvron et al., 2023). This approach ensures that during backpropagation, our focus is exclusively directed towards the optimization of answer tokens. \n\nWe combined our domain alignment dataset, consisting of approximately 1.4k samples, with larger general chat datasets. For SFT, we blended the domain instructional data with 128k commercial-viable chat data and then performed fine-tuning for a single epoch after random shuffling. \n\nWe conducted experiments involving augmentation of the domain-specific SFT dataset for more than one epoch. However, it became apparent that the model rapidly exhibited signs of overfitting when presented with in-domain questions, often repeating irrelevant answers from the domain SFT dataset. For SteerLM, we closely followed the steps outlined in (Wang et al., 2023). We first trained an attribute model instantiated with LLaMA2-13B model on the Help-Steer and OASST datasets. We then used the attribute model to label all attributes for OASST data and our domain instructional data. Finally, we conducted attribute-conditioned fine-tuning and also masked the attribute labels and trained on ChipNeMo models for 2 epochs. We refer readers to Appendix A.4 for details on the alignment datasets and A.7 on implementations details. \n\nWe also experimented with DAPT directly on a chat aligned model, such as the LLaMA2-Chat model. We found that DAPT significantly degraded the model's alignment, making the resulting model useless for downstream tasks.",
            "score": 0.45092785859468826,
            "section_title": "Model Alignment",
            "char_start_offset": 11779,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1958
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "269922026",
            "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process",
            "text": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are two fundamental processes for enhancing the capabilities of Language Models (LMs) post pre-training, aligning them better with human preferences. Although SFT advances in training efficiency, PO delivers better alignment, thus they are often combined. However, common practices simply apply them sequentially without integrating their optimization objectives, ignoring the opportunities to bridge their paradigm gap and take the strengths from both. To obtain a unified understanding, we interpret SFT and PO with two sub-processes -- Preference Estimation and Transition Optimization -- defined at token level within the Markov Decision Process (MDP) framework. This modeling shows that SFT is only a specialized case of PO with inferior estimation and optimization. PO evaluates the quality of model's entire generated answer, whereas SFT only scores predicted tokens based on preceding tokens from target answers. Therefore, SFT overestimates the ability of model, leading to inferior optimization. Building on this view, we introduce Intuitive Fine-Tuning (IFT) to integrate SFT and Preference Optimization into a single process. IFT captures LMs' intuitive sense of the entire answers through a temporal residual connection, but it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to sequential recipes of SFT and some typical Preference Optimization methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.",
            "score": 0.4504051590765764,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2496337890625
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "On the other hand, a recent study, LIMA (Zhou et al., 2023), proposes the \"Superficial Alignment Hypothesis,\" which argues that alignment tuning might simply teach base LLMs to select a subdistribution of data formats for interacting with users. Zhou et al. (2023) demonstrates that SFT with as few as 1,000 examples can also yield high-quality aligned models, thus providing indirect support for this hypothesis. However, conclusive and direct supporting evidence for the superficial alignment hypothesis remains underexplored. Therefore, it is important to analyze how exactly alignment tuning alters the behavior of base LLMs. \n\nTo this end, we investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired. \n\nBased on our findings regarding the superficial nature of alignment tuning, we pose the research question for rethinking the research on aligning LLMs: how effectively can we align base LLMs without any tuning? We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3).",
            "score": 0.4499773640343987,
            "section_title": "Preprint",
            "char_start_offset": 993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1901
                },
                {
                    "start": 1904,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2302
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59033203125
        },
        {
            "corpus_id": "273403633",
            "title": "Persistent Pre-Training Poisoning of LLMs",
            "text": "Language model training is divided in two stages: pre-training and post-training. During pretraining, language models are optimized to predict the next token on large uncurated datasets scraped from the Internet (Radford et al., 2019). Models acquire general capabilities during this stage but are hard to use in real applications. Production models, such as GPT-4 (OpenAI, 2023), undergo heavy post-training alignment. This process make models follow instructions, and ensure the helpfulness and harmlessness of model outputs (Bai et al., 2022). Post-training usually combines different algorithms such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF;Christiano et al., 2017).",
            "score": 0.44995178845156913,
            "section_title": "LANGUAGE MODEL TRAINING",
            "char_start_offset": 3144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 714
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 234,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 689,
                    "end": 713,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.418701171875
        },
        {
            "corpus_id": "277244357",
            "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement",
            "text": "We initialize our experiments with the base model as Qwen2.5-VL-7B-Instruct, which has broad visionlanguage pretraining but lacks complex multi-step reasoning abilities. We refer to the initial SFT phase as SFT-Iter1. In this phase, the model is trained on (image, question, reasoning) triplets derived from the data verification procedure and learns to produce the distilled reasoning traces and final answer in a structured format. We monitor performance on the MathVista [33] benchmark to evaluate progress. \n\nWe observe that the raw reasoning traces could be excessively verbose or repetitive, likely due to information loss during the image-to-caption conversion process. Consequently, the reasoning behaviors after SFT become longer and even more repetitive, often exhibiting unhelpful self-reflections. An example can be found in Appendix C. To mitigate this issue, we experiment with two filtering strategies. First, we remove samples containing reasoning traces exceeding 500 words. Second, and ultimately adopted, we truncate reflections by identifying and splitting on specific keywords (e.g., \"Wait,\" \"But wait,\" \"But the question\") and discarding subsequent segments while retaining the final answer, so that we try to prevent the model from learning repeated reflections that significantly degrades its performance. This strategy resulted in 25k shorter examples for training. In Table 3, we compare the performance of model SFTed on our original data against our processed data. We note that although our data cleaning process substantially reduced instances of the \"aha moment\" behavior from the SFT dataset, such behaviors still occur in our SFT-Iter1 model. Nevertheless, their frequency is considerably more appropriate compared to training with unfiltered SFT data. Below, we present an example of an \"aha moment\" that persists in the SFT-Iter1 model despite removing all occurrences of the keyword \"But wait\" in the SFT dataset. 2 The corresponding image for this example is provided in Figure 8. \n\nTable 3: Performance on the MathVista benchmark comparing different SFT data-filtering strategies.",
            "score": 0.44981561592498176,
            "section_title": "SFT to Follow Reasoning Structure",
            "char_start_offset": 13053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2118
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0699462890625
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "Overview. This section investigates whether IT, at its current open-source scale, can function as a knowledge enhancer. Initially, we present the distinct natures of transformation that a base pre-trained LLM undergoes when subjected to LFT and SFT-based fine-tuning and show that while responses generated after LFT are closely aligned to pre-trained knowledge, they deviate significantly for SFT, indicating new knowledge acquisition. Subsequently, we show that this new knowledge often leads to a degradation in response quality, and relying predominantly on pre-trained knowledge often yields more factual and useful responses. \n\nFinding 1. LFT Responses align closely with the original pre-trained knowledge. SFT does not. To study how finetuned models differ from their base pre-trained counterparts, we perform the token-distribution analysis proposed by Lin et al. (2023). Specifically, for a given instruction-response pair, the instruction i = {i 1 , i 2 , \u2022 \u2022 \u2022 } is first input to the aligned (or fine-tuned) model to obtain its response r = {r 1 , r 2 ,\u2022 \u2022 \u2022 } via greedy decoding. Next, for each position t in the response, a 'context' at this position is defined as to be \n\nThis \"context\" is then input to the base model to obtain its probability distribution for predicting the next token at position t, P base . The probability distribution of the token at position t obtained from the aligned model is denoted as P align . We then calculate three metrics: (1) KL Divergence between P base and P align , (2) Base Probability: P base of the token at t with the maximum P align value (3) Base Rank: Rank in P base of the token at t with the maximum P align value. With the base rank denoted as \u03b7, the unshifted, marginal and shifted tokens are defined as when (\u03b7 = 1), (1 < \u03b7 \u2264 3) and (\u03b7 > 3) respectively. Figure 1 illustrates how the three metrics evolve. These metrics are averaged across all response tokens and plotted against the varying sizes of the IT dataset used for fine-tuning.",
            "score": 0.44932252335815237,
            "section_title": "IT is (currently) Not a Knowledge Enhancer",
            "char_start_offset": 8695,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2004
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2391357421875
        },
        {
            "corpus_id": "278165260",
            "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics",
            "text": "To enhance the performance of SLMs, we propose EWRA, an approach that transcends traditional supervised fine-tuning (SFT), which often depends on human-annotated datasets. Instead, EWRA introduces an advanced reasoningbased learning framework, a specialized variant of SFT, paired with a two-stage curriculum based reasoning transfer mechanism from LLMs. Fig. 1 provides an overview of this process. In this strategy, we train the SLM using a synthetic alignment dataset D = {(p, q, r p )}, generated by the LLM as described in section 3.2. Here, p denotes a prompt, q refers to an input query, and r p is the corresponding output that incorporates reasoning, including both intermediate reasoning steps and the final task-specific output. The output r p is composed of two parts: the reasoning steps r reasoning = [t 1 , t 2 , . . . , t k ], where k is the total number of reasoning tokens, and the final output r final , which is the ultimate answer or prediction. The reasoning process is generated step-by-step as r p = [r reasoning , r final ], where r reasoning contains intermediate reasoning and r final is the final output of the task. \n\nAt each time step j, the token t j is sampled from the generation distribution \u03c0 \u03d5 (\u2022 | s j ), where \u03c0 \u03d5 represents the model's learned distribution over possible tokens, and s j represents the model's state. The training objective is to maximize the likelihood of the reasoning sequence conditioned on the prompt. We consider two distinct prompting regimes: 1. Explicit prompt incorporates detailed task definitions, inclusion/exclusion criteria, and step-by-step reasoning instructions. These promote structured alignment and precise instruction following. 2. Implicit prompt, in contrast, excludes such definitions. This setting encourages reliance on the LLM's internalized domain knowledge and captures more flexible, context-dependent reasoning patterns. \n\nThe respective training objectives are defined as: \n\nOur training strategy for EWRA follows a two-stage curriculum. We begin by training SLMs on data generated with implicit prompts. This stage promotes generalizable, context-aware reasoning by encouraging the model to draw upon latent knowledge from the LLM.",
            "score": 0.4491814594866915,
            "section_title": "Extreme Weather Reasoning-Aware Alignment (EWRA)",
            "char_start_offset": 19427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 1960
                },
                {
                    "start": 1963,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "268819437",
            "title": "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning",
            "text": "In recent years, Large Language Models (LLMs) have shown remarkable performance in generating human-like text, proving to be a valuable asset across various applications. However, adapting these models to incorporate new, out-of-domain knowledge remains a challenge, particularly for facts and events that occur after the model's knowledge cutoff date. This paper investigates the effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge injection in LLMs, specifically focusing on the domain of recent sporting events. We compare different dataset generation strategies -- token-based and fact-based scaling -- to create training data that helps the model learn new information. Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge. Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts. We present a novel dataset generation process that leads to more effective knowledge ingestion through SFT, and our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs and highlights the potential of SFT in enhancing the factuality of LLM responses in specific knowledge domains.",
            "score": 0.44897136867888626,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2322998046875
        },
        {
            "corpus_id": "273532094",
            "title": "Understanding Layer Significance in LLM Alignment",
            "text": "Aligning large language models (LLMs) with specific requirements is essential for enhancing their utility across diverse applications (Luo et al., 2023a;Yu et al., 2023;Luo et al., 2023b;Li et al., 2023;Liu et al., 2024a;2022;Feng et al., 2023). Fine-tuning LLMs during the alignment process can significantly improve the models' capabilities to meet targeted needs (Bubeck et al., 2023). Typically, alignment involves fine-tuning the model on diverse datasets, which may include both human-curated (Rajani et al., 2023) and LLM-generated (Taori et al., 2023) data, using approaches like instruction tuning (Wei et al., 2021) and preference learning (Bai et al., 2022;Rafailov et al., 2024). Given the significant cost associated with full parameter fine-tuning, parameter-efficient fine-tuning (PEFT) (Hu et al., 2021;Chen et al., 2022;Pan et al., 2024) methods have emerged as a popular alternative, offering a balance between performance and resource efficiency. \n\nUnderstanding what LLMs actually learn during the alignment process is crucial. Zhou et al. (2023) posits that the majority of knowledge and capabilities are developed during the pre-training phase, with alignment primarily serving to refine the model's conversational style and formatting. Using a well-selected set of 1,000 training examples for supervised finetuning (SFT), they successfully produced a high-quality aligned model. Similarly, Lin et al. (2023) investigated the token distribution of LLMs before and after alignment and found that most changes were related to \"stylistic tokens\", such as discourse markers and transition words, while the knowledge-intensive content largely remained untouched, coming from Table 1: Impact of fine-tuning different components of LLAMA 2-7B on alignment performance using the LIMA dataset. Evaluated on MMLU (5-shot) and GPT-4o scores for Vicuna and MT-Bench prompts.",
            "score": 0.44884989433257394,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1884
                }
            ],
            "ref_mentions": [
                {
                    "start": 203,
                    "end": 221,
                    "matchedPaperCorpusId": "268819294"
                },
                {
                    "start": 668,
                    "end": 690,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.494873046875
        },
        {
            "corpus_id": "270924060",
            "title": "52B to 1T: Lessons Learned via Tele-FLM Series",
            "text": "We evaluate the alignment performance of Tele-FLM-Chat in Chinese across various domains utilizing AlignBench [15].AlignBench is a comprehensive and multidimensional evaluation benchmark designed to assess Chinese large language models' alignment performance.It encompasses 8 categories with a total of 683 question-answer pairs, covering areas such as fundamental language ability (Fund.),Chinese advanced understanding (Chi.), open-ended questions (Open.),writing ability (Writ.),logical reasoning (Logi.),mathematics (Math.),task-oriented role playing (Role.), and professional knowledge (Pro.).This benchmark furnishes questions, model responses, scoring criteria, and reference answers for model assessment, with GPT-4 and CritiqueLLM [11] serving as judge models.These judge models provide scores and scoring rationales based on the provided criteria.\n\nResults on AlignBench are illustrated in Table 1.With a 52B base model and 30k SFT samples, Tele-FLM-Chat reaches 91% of GPT-4's performance, and 82% of the more advanced GPT-4-1106preview.Notably, Tele-FLM-Chat is comparable (97%) or even outperforms (107%) the GPT-4 series on Chinese language understanding and generation tasks.However, there remains a significant gap on tasks related to math and reasoning.Remind that a large portion of our SFT data was concentrated on mathematics, with very few samples regarding other tasks are included.Thus, the results indicate that a strong foundation model requires only a few samples to elicit its capability to follow instructs well in routine tasks, as supported by existing research [29]; however, for more complicated tasks like maths and reasoning, 30k data might not be enough, and supervision on intermediate steps and chain of thoughts may be necessary [14].Drawing on experience from various established evaluation datasets, we develop our own evaluation dataset, namely TeleEval, which places a greater emphasis on aspects such as mathematics, security, and anti-hallucination.",
            "score": 0.44866723294560185,
            "section_title": "AlignBench",
            "char_start_offset": 4520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 598
                },
                {
                    "start": 598,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 857
                },
                {
                    "start": 859,
                    "end": 908
                },
                {
                    "start": 908,
                    "end": 1048
                },
                {
                    "start": 1048,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1270
                },
                {
                    "start": 1270,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1772
                },
                {
                    "start": 1772,
                    "end": 1993
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2060546875
        },
        {
            "corpus_id": "278310539",
            "title": "Multi-agents based User Values Mining for Recommendation",
            "text": "Large Language Models (LLMs) represent a major advancement in the field of natural language processing (NLP), primarily based on transformer [48] architectures and trained extensively on large-scale textual datasets. Notable examples include GPT-3 [31], PaLM [5], and LLaMA [46]. By significantly increasing both data volume and model complexity, these models have developed emergent capabilities such as in-context learning [31], chain-of-thought prompting [52], and instruction following [35], greatly enhancing their versatility and range of applications. LLMs are typically trained through three main stages: pre-training [67], supervised fine-tuning (SFT) [7,63], and reinforcement learning from human feedback (RLHF) [6,33,42]. During pre-training, models learn from vast unlabeled corpora, enabling them to capture deep semantic relationships and develop general language representations. The subsequent SFT and RLHF stages further align the models with human intentions, improving their ability to follow nuanced instructions and interact more naturally with users. The applications of LLMs [27,43,54,68] have expanded rapidly beyond traditional NLP tasks such as text generation, multilingual translation, and sentiment analysis. They now extend into domains that require advanced cognitive abilities, including mathematical reasoning, logical analysis, and strategic planning. LLMs are increasingly deployed in fields such as information systems, education, finance, and healthcare, where they support content understanding, personalized recommendations, conversational interactions, and decision-making processes. \n\nDespite their significant advancements, large language models (LLMs) exhibit several welldocumented limitations, most notably token limits and hallucinations. Token limits, or contextlength constraints, refer to the maximum number of tokens an LLM can process in a single interaction. Models such as GPT-3 [31] and Llama-3 [10] have relatively modest context windows of approximately 2,048 and 4,096 tokens, respectively. These constraints hinder their ability to maintain coherence over long dialogues or incorporate extensive historical context, making it difficult to process lengthy documents or large sets of user interaction history without truncation or specialized handling.",
            "score": 0.4482290466714244,
            "section_title": "Large Language Models",
            "char_start_offset": 11596,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1624
                },
                {
                    "start": 1627,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 458,
                    "end": 462,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 626,
                    "end": 630,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 726,
                    "end": 729,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 729,
                    "end": 732,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428955078125
        },
        {
            "corpus_id": "271097404",
            "title": "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models - The Story Goes On",
            "text": "More is different. \n\n--Philip W. Anderson, 1972 Reasoning ability is a hallmark of human intelligence (Gendron et al., 2024;Huang and Chang, 2022;Wei et al., 2022b). Although Large Language Models (LLMs) have recently demonstrated significant capabilities in various tasks such as conversation (Achiam et al., 2023;Anthropic, 2024;Peng et al., 2023) and summarization (Almazrouei et al., 2023;Scao et al., 2022;Wei et al., 2023b;Yang et al., 2023), they often struggle with complex reasoning tasks (Gendron et al., 2024;Lu et al., 2023;Wu et al., 2023). One particularly challenging area is mathematical reasoning (Arora et al., 2023;Cobbe et al., 2021;He et al., 2024;Hendrycks et al., 2021;Zhong et al., 2023), which requires the ability to solve mathematical problems and derive logical conclusions in a step by step manner (Saxton et al., 2019;Shao et al., 2024;Toshniwal et al., 2024;Wei et al., 2022b;Yu et al., 2024). \n\nTwo prevailing beliefs guide researchers and practitioners in enhancing mathematical reasoning abilities of LLMs. The first belief posits that complex reasoning abilities, especially mathematical reasoning, are emergent abilities that exist in large language models but not in small models (Wei et al., 2022a,b). Typically, models with more than 30 billion parameters exhibit the strong mathematical reasoning ability (Brown et al., 2020). The second belief is the seminal \"superficial alignment\" hypothesis (Zhou et al., 2023), which asserts that \"A model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used when interacting with users.\". According to this hypothesis, the alignment process, primarily through supervised fine-tuning (SFT), does not inject new knowledge or improve inherent abilities but rather adjusts the output response format.",
            "score": 0.4482209638721319,
            "section_title": "Introduction",
            "char_start_offset": 459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 21,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 536,
                    "matchedPaperCorpusId": "258212542"
                },
                {
                    "start": 669,
                    "end": 692,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 907,
                    "end": 923,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 1345,
                    "end": 1365,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2100830078125
        },
        {
            "corpus_id": "274130626",
            "title": "Efficient Alignment of Large Language Models via Data Sampling",
            "text": "In this work, we present an analysis of the alignment performance of LLMs from the perspective of the dataset size. We present extensive experiments on different popular datasets with varying sizes and find consistent over-optimization as more data is used to align the model leading to minimal gain. We validate that data subsampling is feasible to reduce resources required for alignment. We propose a novel methodology for sampling a small diverse and high-quality dataset to perform alignment in a cost and resource effective manner. We show that our methodology outperforms other methods and achieves comparable performance while saving greater than 90% of the costs and resources. Our analysis of the over-optimization and sampling strategies for alignment is a first step and opens up new avenues of research for efficient alignment and characterizing the effect over larger model scales in the future which is critical for more ethical and safer models. \n\nA Appendix / supplemental material A.1 Alignment Preliminary \n\nLLM training generally takes place in pre-training, supervised finetuning, and alignment [18]. In general, pre-training involves learning the structure of the language including the grammar and punctuation by maximizing the log-likelihood (Equation 4) of the next token conditioned on the preceding text from a large corpus of data. To adapt a pre-trained model for instruction following tasks such as question answering, summarization, supervised fine-tuning is employed. Often, supervised fine-tuning is prohibitively expensive. Therefore, a parameter efficient approach such as LoRA is a prevalent SFT approach [13]. Finally, direct alignment algorithms (DAA) such as KTO, DPO are proven to be beneficial in enhancing a reward score defined as a function of the helpfulness and safety of a fine-tuned models. In fact, the approaches like KTO can even circumvent the need for SFT. Equation 6represents the canonical representation of alignment on a preference dataset: \n\nr * in Equation 6 denotes the \"true\" reward underlying the preferences. \n\nBecause the true reward measurement is intractable, a reward model r p hi is trained as a proxy by minimizing the negative log-likelihood of the human preference data, as shown in Equation 7 \n\nHowever, the indiscriminate maximization of the reward comes at the expense of the loss of desiderata such as generating grammatical text.",
            "score": 0.44792419866795613,
            "section_title": "Conclusion",
            "char_start_offset": 10895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1024
                },
                {
                    "start": 1027,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2071
                },
                {
                    "start": 2074,
                    "end": 2264
                },
                {
                    "start": 2267,
                    "end": 2405
                }
            ],
            "ref_mentions": [
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24560546875
        },
        {
            "corpus_id": "259075917",
            "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment",
            "text": "Reinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox and Stone, 2008;Wirth et al., 2017) has delivered significant empirical successes in several fields, including game playing (Christiano et al., 2017), robotics (Sadigh et al., 2017;Kupcsik et al., 2018), recommender systems, andn (Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human knowledge with large language models (Ziegler et al., 2019;Ouyang et al., 2022;OpenAI, 2023;Beeching et al., 2023;Zhu et al., 2023;Bai et al., 2022b). To employ RLHF in the training pipeline of language models, a common protocol is as follows. \n\n\u2022 Pre-training (PT): training the language model on a large amount of unlabeled or weakly labeled text data to produce general features and patterns that can be useful for downstream tasks (Vaswani et al., 2017;Devlin et al., 2018;Brown et al., 2020); \n\n\u2022 Supervised fine-tuning (SFT): training the model on a smaller amount of labeled data to improve the performance and accuracy of the model on specific tasks; \n\n\u2022 Reinforcement learning with human feedback (RLHF): using a human-labeled dataset together with reinforcement learning (RL) algorithms to further align the model with complex and subjective human values or preferences (Ziegler et al., 2019;Ouyang et al., 2022). \n\nBoth PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the distance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;Devlin et al., 2018;Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.",
            "score": 0.44769260145147693,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1679
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 116,
                    "matchedPaperCorpusId": "5613334"
                },
                {
                    "start": 116,
                    "end": 135,
                    "matchedPaperCorpusId": "703818"
                },
                {
                    "start": 224,
                    "end": 249,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 260,
                    "end": 281,
                    "matchedPaperCorpusId": "12226563"
                },
                {
                    "start": 281,
                    "end": 302,
                    "matchedPaperCorpusId": "13285792"
                },
                {
                    "start": 545,
                    "end": 562,
                    "matchedPaperCorpusId": "256274676"
                },
                {
                    "start": 866,
                    "end": 888,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1550,
                    "end": 1572,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "270123077",
            "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
            "text": "The large-scale pre-training phase allows Large Language Models (LLMs) to acquire extensive knowledge and capabilities (Bubeck et al., 2023). However, these base models struggle to follow instructions directly from prompts, necessitating further fine-tuning to be used as conversational models. Traditional alignment methods include Supervised Fine-Tuning (SFT) on instruction datasets (IFT, Wei et al., 2021;Chung et al., 2022) or preference data (DPO, KTO, IPO, and ORPO, Rafailov et al., 2023;Ethayarajh et al., 2024;Azar et al., 2024;Hong et al., 2024), and reinforcement learning (RLHF and RLAIF, Ouyang et al., 2022;Bai et al., 2022). These approaches typically involve multiple rounds of refinement of the instructed model (Touvron et al., 2023), i.e. high computational cost, and need a large amount of annotated data like human preferences, which might be difficult to collect. However, a line of work (Taori et al., 2023;Chen et al., 2023;Zhou et al., 2023;Lee et al., 2023;Zhao et al., 2024;Kaur et al., 2024) has suggested that IFT on a small amount of high quality instructionfollowing examples, even only 1000, can be sufficient to match the performance of more complex alignment mechanisms. In particular, Zhou et al. (2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023). Using such small instruction datasets for IFT has the clear advantage of significantly reducing the cost of model alignment.",
            "score": 0.4476754274056283,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1626
                }
            ],
            "ref_mentions": [
                {
                    "start": 520,
                    "end": 538,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 949,
                    "end": 967,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 984,
                    "end": 1002,
                    "matchedPaperCorpusId": "267522812"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60302734375
        },
        {
            "corpus_id": "257038005",
            "title": "Chain of Hindsight Aligns Language Models with Feedback",
            "text": "The main techniques behind them can be categorized as supervised finetuning (SFT) or training on filtered human annotations and learning a reward function from human feedback for reinforcement learning, which is often dubbed as RLHF [10,32,27,50] and has been used to train RL agents without the need for hand-designed rewards. Ouyang et al. [35] demonstrates improved language model alignment performance by training models with SFT and RLHF using human feedback. Our work belongs to the category of SFT, and differs from SFT in that our method conditions on feedback and can learn from examples without positive ratings. Our method is complementary to RLHF and can be directly combined together for further improvement. \n\nUsing instructions to provide models with human preference and desired behaviors is demonstrated in Bai et al. [5], where models are prompted with a set of statements/principles and are trained with RLHF. In our work, we provide models with a sequence of model outputs and their feedback and train models to generate desired outputs conditioned on feedback/control tokens. \n\nInstruction finetuning and conditional training. Finetuning on chain of hindsight using human feedback is akin to instruction finetuning. Driven by the impressive in-context learning ability of large language models, finetuning pretrained models on instructions has been shown to improve language models in many benchmarks [see e.g. [52] are widely considered as instructions in prior works [11,51], specifically in the form of step by step explanations written by humans. In relation to these, our chain of hindsight consists of human written hindsight feedback and ranked model outputs. Conditional training [21,13,25,8,12,31] explores conditioning the model on some control tokens for controllable generations. In relation to it, CoH generalizes to condition on a sequence of control tokens instead of one control token. By doing so, CoH enables the model to understand the differences between control tokens and their corresponding outputs. Our work suggests a promising direction of using hindsight feedback to construct instructions from model outputs, and can be combined with prior instruction finetuning and conditional training works for further improvements.",
            "score": 0.44752776742608125,
            "section_title": "Related Work",
            "char_start_offset": 21995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "8818528"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "235377145"
                },
                {
                    "start": 243,
                    "end": 246,
                    "matchedPaperCorpusId": "4130751"
                },
                {
                    "start": 1719,
                    "end": 1721,
                    "matchedPaperCorpusId": "235294299"
                },
                {
                    "start": 1724,
                    "end": 1727,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53662109375
        },
        {
            "corpus_id": "271859627",
            "title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
            "text": "Supervised fine-tuning (SFT) is a technique used to enhance large language models (LLMs) by further training a pre-trained model to improve its accuracy and relevance for specific tasks or domains. The efficacy of SFT in refining LLMs is well-documented (Wei et al., 2022;Ouyang et al., 2022). This process involves utilizing a carefully curated dataset with labeled examples that illustrate the desired output. During SFT, the model learns from these examples to comprehend the intricacies of the task more thoroughly. Consequently, SFT enables the model to retain its broad knowledge base while acquiring specialization in targeted areas, resulting in enhanced user experiences and more precise information delivery. \n\nData preparation. In the construction of our datasets for supervised fine-tuning, each instance within datasets is composed of three elements: an instruction, an input, and an output. We utilize a dual approach in formulating instructions, leveraging both Self-instruct (Wang et al., 2023b) and human writing. \n\nTo exemplify, consider the instruction: \"Please translate the input English sentence into Chinese\"; here, the input component would be an English sentence. For the generation of outputs corresponding to given instructions and inputs, we employ meticulously devised manual methods to craft expert responses. \n\nTraining. Upon completing the construction of SFT datasets, we commenced the Supervised Fine-Tuning (SFT) of scientific literature LLM. The instances within the dataset serve as labeled data for the SFT of the model. Since each instance is meticulously crafted by experts, they are of higher quality compared to the generic data used during the 3 SparkRA \n\nBased on our SciLit-LLM, we developed a literature services system SparkRA. This platform is comprised of three functions: literature investigation, paper reading, and academic writing. Notably, SparkRA is equipped to process inputs in both Chinese and English, thereby catering to a diverse linguistic user base. The architecture of SparkRA is shown in Figure 2 and the demonstration video has been published on YouTube 4 .",
            "score": 0.4473139340700021,
            "section_title": "Supervised fine-tuning",
            "char_start_offset": 4893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 718
                },
                {
                    "start": 721,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2123
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 272,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 272,
                    "end": 292,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 991,
                    "end": 1011,
                    "matchedPaperCorpusId": "254877310"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1121826171875
        },
        {
            "corpus_id": "273346831",
            "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding",
            "text": "Large language models (LLMs) pre-trained on massive text corpora possess broad general knowledge, yet they often struggle to produce responses aligned with user instructions without additional fine-tuning. As a result, alignment1 , such as instruction tuning (Wei et al., 2022a) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a), have become essential for developing useful LLMs like GPT-4 (Hurst et al., 2024). However, the current training pipelines require separate alignment tuning for every model size within each model family. \n\nIn practice, aligning the largest models leads to substantial computational overhead (e.g., the RLHF stage of Tulu 3 405B (Lambert et al., 2025) takes 11,776 H100 GPU hours), impeding the rapid iteration and deployment of new model families. \n\nRecent studies (Zhou et al., 2024;Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, Lin et al. (2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment. These findings raise a natural question: If the aligned models differ from the base models only at a few, select tokens, is it necessary to train large aligned models? \n\nIn this work, we propose NUDGING, a simple, training-free guided decoding algorithm that aligns any base model at inference time by injecting a few alignment tokens from a small aligned model. Our key insight is that base models show high uncertainty on alignment-related tokens-i.e., places where base and aligned models disagree. Leveraging this observation, NUDGING employs a small aligned model to generate nudging tokens that guide a large base model's output toward desired directions whenever the base model's top-1 token probability is below a certain threshold.",
            "score": 0.446102763551339,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 354,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 838,
                    "end": 857,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1073,
                    "end": 1090,
                    "matchedPaperCorpusId": "265608902"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "268363828",
            "title": "(N, K)-Puzzle: A Cost-Efficient Testbed for Benchmarking Reinforcement Learning Algorithms in Generative Language Model",
            "text": "Model.We utilize the GPT-2 (Radford et al., 2019) architecture.The specific instantiation of the GPT-2 model employed in our study is defined by the following parameters: the model comprises n layer = 12 transformer layers, with each layer featuring n head = 12 attention heads.The dimensionality of the embeddings and the hidden layers is set to n embd = 768.This results in a total of approximately 124 million trainable parameters.Supervised fine-tuning.The initial phase of our training is the supervised fine-tuning (SFT).We conduct two stages of SFT from scratch: format SFT and target SFT.In format SFT, we train the model from scratch using specially formatted data to inculcate the ability to output results in a predefined format and to execute arithmetic operations.Note that during the format SFT phase, the model's outputs may not always align perfectly with the target results.In the format SFT phase, we employ a training dataset consisting of 200,000 data points, encompassing a broad spectrum of arithmetic operations.The model is trained for 20 epochs with a global batch size of 256.We use a learning rate of 10 \u22125 and a constant learning rate scheduler.Upon completion of this phase, the model demonstrates a remarkable capability to generate responses that outputs responses with correct arithmetic operations and desired formats an accuracy rate of 99%.\n\nBuilding on the format SFT phase, we train the model further through target SFT.Initiated from the checkpoint from the format SFT, the objective is to hone the model's capabilities not only to execute arithmetic operations within the specified format accurately but also to engage in logical reasoning processes that ensure the final result aligns with the target value K.During the target SFT phase, we employ a training dataset consisting of 300,000 data points.The model is trained for 10 epochs with a global batch size of 128.We use a learning rate of 10 \u22125 and a constant learning rate scheduler.Upon completion of this phase, the model achieves an accuracy of 43.5% in the in-distribution test set and an accuracy of 8.8% in the OOD test set.Ground truth reward.",
            "score": 0.4459183527381244,
            "section_title": "Experiment Setup",
            "char_start_offset": 5465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 6,
                    "end": 63
                },
                {
                    "start": 63,
                    "end": 278
                },
                {
                    "start": 278,
                    "end": 360
                },
                {
                    "start": 360,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 527
                },
                {
                    "start": 527,
                    "end": 596
                },
                {
                    "start": 596,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 1035
                },
                {
                    "start": 1035,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1749
                },
                {
                    "start": 1749,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 1979
                },
                {
                    "start": 1979,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2146
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.102294921875
        },
        {
            "corpus_id": "268819071",
            "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
            "text": "Large language models (LLMs) Brown et al. (2020); Zhang et al. (2022); Workshop et al. (2022); Zeng et al. (2023); Touvron et al. (2023b) have remarkably advanced the capabilities of machines in language understanding and generation.The most notable example is ChatGPT Achiam et al. (2023), which demonstrates strong capability in responding to users' queries and following their instructions.Different from the pre-trained GPT-3 model Brown et al. (2020), ChatGPT (GPT-3.5,GPT-4 Achiam et al. (2023), and beyond) are further post-trained, with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) Ouyang et al. (2022), to better align with human preferences.We have developed ChatGLM -a free-to-use AI service powered by the ChatGLM family of LLMs Zeng et al. (2023).Similar to ChatGPT, ChatGLM was pre-trained for trillions of multilingual tokens and post-trained with both SFT and RLHF.\n\nThe goal of RLHF is to encourage LLMs to better align with human preferences and values, i.e., to produce more helpful, more accurate, and safer responses Bai et al. (2022b); Achiam et al. (2023); Ouyang et al. (2022).Specifically, the RLHF techniques typically treat human preferences as rewards and employ reinforcement learning algorithms, such as Proximal Policy Optimization (PPO) Schulman et al. (2017), to realize the goal of alignment.Despite the significant community efforts Touvron et al. (2023b); Bai et al. (2023); Dai et al. (2023); Sun et al. (2023) in applying RLHF to perform LLMs alignment, since the inception of ChatGPT, our experience in constructing a practical RLHF training framework for ChatGLM has revealed unknown technical challenges.",
            "score": 0.4456945346212685,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 233
                },
                {
                    "start": 233,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 474
                },
                {
                    "start": 474,
                    "end": 689
                },
                {
                    "start": 689,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 919
                },
                {
                    "start": 921,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1683
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 48,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 95,
                    "end": 113,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 779,
                    "end": 797,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 1118,
                    "end": 1138,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49365234375
        },
        {
            "corpus_id": "276782338",
            "title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding",
            "text": "In creating LLM post-training data for coding tasks, there is a gap between coding questions (e.g., LeetCode) and training data. While coding questions are primarily expressed in natural language, training data needs to accommodate non-naturallanguage formats such as function calls and tool interactions. To address this disparity, we propose an LLM-based style converter to enhance the diversity of question formats. Specifically, we reformat each question q by taking its solution and test as inputs, q \u2032 = LLM(q, sol, test), structuring them as Python completion tasks with function signatures and examples. Each reformatted question is paired with its original solution and test cases to form new triplets (q \u2032 , sol, test). This process results in the creation of 168K additional triplets, increasing our total to 447K, which are readily available for RL training. \n\nMotivated by recent advances in reasoning models (Team, 2025;Labs, 2025), we further generate an SFT dataset for post-training by leveraging the questions in these triplets, using DeepSeek R1 (DeepSeek-AI et al., 2025) as the response generator to generate Chain-of-Thought responses. To ensure the quality of the generated responses, we generate 3 times for each question and perform test-based reject sampling, yielding a large-scale, high-quality, and verifiably correct SFT dataset for coding. We refer to this dataset as KODCODE-SFT.",
            "score": 0.44563301344151407,
            "section_title": "Step 3: Post-training Data Synthesis",
            "char_start_offset": 10923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1411
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061981201171875
        },
        {
            "corpus_id": "271947083",
            "title": "Quality or Quantity? On Data Scale and Diversity in Adapting Large Language Models for Low-Resource Translation",
            "text": "How important is concatenated parallel data at various scales of low-resource pre-training? Having seen improvements in pre-training on the entire corpus (consisting of 13M 'mono+parallel' sentences, or 730M tokens) in Table 7, we now study the importance of parallel data as we scale down-an important consideration when one moves to even lower-resource settings. We pretrain on subsets of varying sizes and mix monolingual and parallel data in 3 ways: 'All mono', 'mono+parallel (concat)' and 'mono+parallel (separate)', as defined in Section 3.1). We fine-tune all these pre-trained models on the same SFT dataset: 500K spa-X MT instructions, and plot the resulting chrF++ scores in Figure 2, including error bars from bootstrap resampling. We note that 'all mono' has different markers than the others, as the token counts on including both source and target-side data in the corpus are obviously larger than only the latter. We find that a) starting at around 5M sentences (~300M tokens), it is consistently advantageous to include concatenated parallel data during pre-training. b) Given 'mono+parallel (separate)' severely underperforms, we establish that it is concatenation that adapts the LLM for the task of MT, not the extra data alone. Our findings complement those of Alves et al. ( 2024), who also observe gains from pre-training on parallel data in the 1B to 20B tokens range, using 10 high-resourced European languages. In contrast, our focus here is on investigating the minimum data threshold at which CPT leveraging parallel data becomes beneficial. \n\nHow does scaling LRL parallel data during SFT impact performance? We now turn our focus to scaling during SFT, which has not yielded gains in high-resource LLM-MT after 20K-30K sentences (Zhang et al., 2024a). Motivated by our previous results, we re-examine this question for low-resource LLM-MT in Figure 3, wherein we evaluate at steps of 50K until ~1M SFT instructions, the point at which spa-X MT data runs out. Our findings are:",
            "score": 0.44555269871088776,
            "section_title": "Analysis: Importance of Parallel Data",
            "char_start_offset": 20149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 1759,
                    "end": 1780,
                    "matchedPaperCorpusId": "268032247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.170654296875
        },
        {
            "corpus_id": "271218517",
            "title": "CCoE: A Compact and Efficient LLM Framework with Multi-Expert Collaboration for Resource-Limited Settings",
            "text": "For example, it can easily add an expert for targeting English-Chinese translation task without worrying about the task conflicts with other experts.\n\n3 Dilemma of Supervised Fine-tuning LLM Large Language model (LLM) Supervised fine-tuning (SFT) is the process of continually training the pre-trained models on smaller, specific datasets to refine their capabilities and improve the performance in a specific task.SFT can also be used to inject new domain knowledge into the model.The SFT process is about turning a general-purpose model and transform it into specialized model with the domain-related dataset.For example, there is a limited knowledge in pre-trained model on law domain.We can inject the knowledge of the law into LLM to make it become a law specialized model.The SFT brings a gap between pre-trained model and fine-tuned model.Nowadays, LLM fine-tuning has become an indispensable approach for enterprises to enhance their operational process.Through training LLMs for specific task with domain datasets, we can push the knowledge of LLMs to the boundaries of different areas [12] [13] [14].\n\nThere are many ad-hoc attempts on SFT for enhancing LLM performance on individual capabilities in open community.However, when the widely used LLMs have been fine-tuned, it may affect the general knowledge stored in LLMs since there is a distribution shift between SFT data and the original training data.The study of SFT is crucial for certain practical applications of LLMs.In most real cases, users want the LLMs to be enhanced in specific domain capabilities while preserving their general capabilities.A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge during the SFT process [15] [16].The naive way of reducing the CF is to mix the supervised data with the pre-trained data at certain ratios.Recently, many researches have proposed various approaches to alleviate the problem of CF such as Dual-Stage Mixed Fine-tuning [17], Recall and Learning [18], etc.However, there are still many remaining challenges left to the community.",
            "score": 0.44501590846279704,
            "section_title": "Analogy to Mixture of Experts",
            "char_start_offset": 6341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 151,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1225
                },
                {
                    "start": 1225,
                    "end": 1417
                },
                {
                    "start": 1417,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1887
                },
                {
                    "start": 1887,
                    "end": 2050
                },
                {
                    "start": 2050,
                    "end": 2123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051544189453125
        },
        {
            "corpus_id": "275133251",
            "title": "Natural Language Fine-Tuning",
            "text": "Large language model fine-tuning techniques typically depend on extensive labeled data, external guidance, and feedback, such as human alignment, scalar rewards, and demonstration. However, in practical application, the scarcity of specific knowledge poses unprecedented challenges to existing fine-tuning techniques. In this paper, focusing on fine-tuning tasks in specific domains with limited data, we introduce Natural Language Fine-Tuning (NLFT), which utilizes natural language for fine-tuning for the first time. By leveraging the strong language comprehension capability of the target LM, NLFT attaches the guidance of natural language to the token-level outputs. Then, saliency tokens are identified with calculated probabilities. Since linguistic information is effectively utilized in NLFT, our proposed method significantly reduces training costs. It markedly enhances training efficiency, comprehensively outperforming reinforcement fine-tuning algorithms in accuracy, time-saving, and resource conservation. Additionally, on the macro level, NLFT can be viewed as a token-level fine-grained optimization of SFT, thereby efficiently replacing the SFT process without the need for warm-up (as opposed to ReFT requiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not increase the algorithmic complexity, maintaining O(n). Extensive experiments on the GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves an accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time complexity and space complexity of NLFT are reduced by 78.27% and 92.24%, respectively. The superior technique of NLFT is paving the way for the deployment of various innovative LLM fine-tuning applications when resources are limited at network edges. Our code has been released at https://github.com/Julia-LiuJ/NLFT.",
            "score": 0.4445392620941387,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1578369140625
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "In the domain knowledge learning stage, the LLM has simultaneously learned to both memorize domain knowledge and understand how to utilize the knowledge through our proposed knowledge mixture continual pre-training.After that, during the format alignment stage, the LLM can more efficiently fine-tuned to master the task format with only a small number of alignment samples.Next, we first introduce the selection of training samples and then perform general format alignment.\n\nSince we would like to decouple knowledge learning and format alignment, we mainly focus on training samples from D SFT and D DPO that are are both easy and have been encountered during continual pre-training, which avoids introducing new knowledge during supervised fine-tuning.These easy samples are selected based on the perplexity scores of the LLMs w.r.t the ground-truth output.Specifically, given a sample in D SFT and D DPO , we first equip the input instruction and output response with the corresponding chat template, i.e., the format for interaction with humans.Then, we feed the formatted sequence into the LLM and compute the perplexity score for the output response.Finally, we select top-K samples with the lowest perplexity scores to conduct the supervised fine-tuning and direct preference optimization.Note that for samples in D DPO , we only utilize the positive response for computing its perplexity score.\n\nAfter selecting the training samples, we next utilize them to conduct efficient format alignment.Firstly, we utilize the selected easy instruction samples from D SFT to perform supervised fine-tuning based on the LLM after continual pre-training following the standard way (Ouyang et al., 2022b), which is to minimize the cross-entropy loss:\n\nwhere r j and r <j denote the j-th token and its previous tokens in the response.Secondly, we further utilize the selected easy preference samples from D DPO to conduct direct preference optimization following its original method (Rafailov et al., 2023b) as follows:\n\nwhere \u03c3 denotes the sigmoid function, \u0398 and \u0398 ref denote the parameters of the updated LLM and reference LLM during the direct preference optimization process, and \u03c0 denotes the product of the probabilities of all output tokens, conditioned on the given input.",
            "score": 0.44369014894784,
            "section_title": "EFFICIENT FORMAT ALIGNMENT",
            "char_start_offset": 13568,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 215,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 475
                },
                {
                    "start": 477,
                    "end": 756
                },
                {
                    "start": 756,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1051
                },
                {
                    "start": 1051,
                    "end": 1158
                },
                {
                    "start": 1158,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1404
                },
                {
                    "start": 1406,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1747
                },
                {
                    "start": 1749,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 2015
                },
                {
                    "start": 2017,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 1679,
                    "end": 1701,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1979,
                    "end": 2003,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1834716796875
        },
        {
            "corpus_id": "275993992",
            "title": "CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language Model Question Answering",
            "text": "Cross-dataset generalizability To evaluate the generalizability, we conduct cross-dataset experiments by training models on X-CSQA and testing them on MEDQA, and vice versa. Table 3 reveals that while the out-of-domain accuracy falls below the in-domain accuracy, it consistently exceeds the in-domain performance of the SFT baseline. This underscores the capability of CALM-trained models to provide multilingually consistent answers, even when faced with unseen tasks or domains. These findings suggest that CALM enhances indomain performance and fosters robustness across different types of domains. \n\nCross-lingual generalizability We implement CALM training sequentially, beginning with English and incrementally adding French and Chinese, progressing from high-resource to low-resource languages. At each step, we evaluate test accuracy across all languages. To assess CALM's effectiveness in untrained languages, we include Japanese, Italian, and German in the test set, none of which were included during training. In Table 10 in the Appendix, CALM demonstrates greater effectiveness as more languages participate in majority voting. Notably, even untrained languages exhibit accuracy improvements, suggesting that CALM's alignment mechanism fosters a unified understanding of knowledge across languages, thereby enhancing overall comprehension. This aligns with She et al. (2024), which similarly observe cross-lingual generalizability in multilingual reasoning tasks.",
            "score": 0.4435865485407875,
            "section_title": "Generalizability",
            "char_start_offset": 11154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272705078125
        },
        {
            "corpus_id": "274131023",
            "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models",
            "text": "A large language model(LLM) is a typical autoregressive model designed to predict the probability of the next word in a vocabulary V, given a specific context x. Formally, the probability of generating a subsequent sentence can be expressed as: \n\nHere, P (y|x) represents the probability of the predicted sentence y given the context x. The context x = {x 1 , x 2 , . . . , x n } (where each x i \u2208 V) is the initial input prompt, while the sentence y = {y 1 , y 2 , . . . , y m } (where each y i \u2208 V) denotes the model's generated response. \n\nAligned LLMs have attracted significant attention as researchers strive to make their outputs align with human expectations, values, and safety guidelines. Foundational studies [7], [8] underscore the critical role of alignment in preventing harmful or undesirable behaviors, spurring the advancement of various alignment techniques. To enhance the safety of these models, two primary mechanisms have been established: harmfulness filtering and alignment training. \n\nHarmfulness Filter. This mechanism consists of filters designed to assess the safety of user inputs and the responses generated by LLMs. Commercial LLMs currently incorporate these filters to intercept potentially risky or harmful content in both user instructions and model outputs. Examples of such LLMs include ChatGPT [28], Gemini [29], Claude [30], and Bing AI [31], all of which have implemented these safety barriers. Numerous additional guardrail projects [32], [33], [34] focus on hazard detection, continuously working to enhance the safety and reliability of LLM systems. \n\nAlignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) [9]. SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks.",
            "score": 0.4431914125237046,
            "section_title": "Aligned LLMs",
            "char_start_offset": 8359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 1480,
                    "end": 1484,
                    "matchedPaperCorpusId": "269101741"
                },
                {
                    "start": 1861,
                    "end": 1864,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79541015625
        },
        {
            "corpus_id": "277786843",
            "title": "Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation",
            "text": "While pre-training helps large language models gain a broad understanding of language, code, and reasoning through the enormous amounts of unlabeled data they are trained on, they do not inherently specialize in anything. The pre-trained models necessarily predict the next token in a sequence. Sequence generation is not usually favorable for human communication or more specific use cases like code generation. \n\nThis section introduces the concept of fine-tuning the already pre-trained model. Fine-tuning occurs during the subsequent training stage of a large language model, where the model is further trained on a smaller, often task-specific, labeled dataset in order to adapt the generalized knowledge embedded in the model for a more desired output. The notable distinction between fine-tuning and pre-training is that pretraining is predominantly done on unlabeled data, while finetuning is done on curated, labeled data that is relevant to the purpose of fine-tuning. \n\nThere are several methodologies for fine-tuning large language models. Supervised Fine-Tuning (SFT) is typically the first step post pre-training. SFT involves training the model on a dataset of curated input-output pairs, which often are formatted as instructions and their desired responses. This input-output pairing makes the model learn to mimic the style, format, and behavior of these desired responses [57]- [59]. SFT can also be used to introduce new data, essentially making it learn new things in the context of its existing knowledge, to the model, which has been demonstrated well in the context of code generation [1], [3], [60].",
            "score": 0.4430631717236343,
            "section_title": "C. Supervised Fine-Tuning",
            "char_start_offset": 28749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1624
                }
            ],
            "ref_mentions": [
                {
                    "start": 1614,
                    "end": 1617,
                    "matchedPaperCorpusId": "261100919"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.123046875
        },
        {
            "corpus_id": "267657954",
            "title": "MaxMin-RLHF: Alignment with Diverse Human Preferences",
            "text": "In this section, we present a comprehensive empirical evaluation of the alignment impossibilities and our proposed solutions for language models, structured into two distinct subsections: Small This figure shows the performance of our proposed MaxMin RLHF algorithm for the preference dataset described in Figure 2. The task is to align a language model to generate positive sentiment responses that are concise (of shorter token length) in nature. The top row shows the sentiment score (higher is better) distribution of the language model before alignment (left) and after alignment (right). The bottom row shows the conciseness score (lower is better) distribution of the language model before alignment (left) and after alignment (right). We note that MaxMin-RLHF aligned language model can generate highly positive sentiment sentences and satisfy the conciseness criteria. This shows alignment with both the majority and minority preferences. \n\nScale experiments (Sec. 5.1) for initial proof of concept, and Large Scale experiments (Sec. 5.2) for broader validation. We first demonstrate the practical challenges of alignment (cf. Theorem 1), followed by showcasing the efficacy of our MaxMin-RLHF strategy. This approach illustrates that, with a focus on social welfare objectives, alignment across diverse human preferences is attainable.",
            "score": 0.4429957641247676,
            "section_title": "Experimental Results",
            "char_start_offset": 31330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 947
                },
                {
                    "start": 950,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1345
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "260775656",
            "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
            "text": "Besides the capability of the manipulate model to generate an output containing a certain number of tokenizers, we are also exploring the potential for the manipulate model to dynamically adjust the number of tokenizers in its output based on the specific requirements. \n\nThe task at hand poses a considerable challenge for both PPO and SFT. In contrast to experiments with a certain length range, the model must comprehend the directives/questions encapsulated in the input string to regulate the output tokenizer length. Simultaneously, the model is expected to formulate a output that corresponds accurately to the specified requirement while manipulating the tokenizer length of the output. In my perspective, comprehending the input can be regarded as a form of knowledge and capability, whereas manipulating the output tends to resonate more with a specific style. \n\nTab. 3 illustrates a discernable margin of improvement for random certain length-manipulated SFT, particularly when used in conjunction with few-shot and prompt combinations. PPO also exhibits some augmented performance in this task. Nonetheless, the individual improvement for both SFT and PPO demonstrate limitations, with improvements remaining significantly substandard when compared to the efficacy of one certain length manipulating. In addition, Tab. 3 reveals that applying PPO subsequent to SFT can further enhance the performance. The amalgamation of SFT and PPO algorithms ought to be refined for a smart union, where combined comprehension and manipulation being regard as knowledge and output style. We have attempted to augment the PPO by incorporating SFT loss, however, this has not yet resulted in superior outcomes. Improved methodologies for combining these algorithms are still under active investigation. \n\nTo lessen the burdens on model comprehension, we carry out a study with reduced prerequisites. \n\nInstead of requiring the model to generate an output of random tokenizer length corresponding to the input guidelines, we set a constraint. We specify that the input requirements can only govern the output tokenizer length to fixed levels, either 100 or 50. As depicted in Tab. 3, the PPO model, which operates under reduced requirements, possesses a distinct proficiency in differentiating the input requirements. Consequently, the bulk of the results complied with the established requirements. \n\nTo enhance the model's capability to understand varying input requirements, we employed a progressively stepped training methodology.",
            "score": 0.44280052437921236,
            "section_title": "Random Certain Length: method-x",
            "char_start_offset": 21655,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1798
                },
                {
                    "start": 1801,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2312
                },
                {
                    "start": 2313,
                    "end": 2394
                },
                {
                    "start": 2397,
                    "end": 2530
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054901123046875
        },
        {
            "corpus_id": "273345418",
            "title": "How to Leverage Demonstration Data in Alignment for Large Language Model? A Self-Imitation Learning Perspective",
            "text": "Pre-training endows large language models (LLMs) with extensive knowledge about the world. However, it does not behave in accordance with some task-dependent requirements. To achieve the desired performance on certain tasks, a post-training process known as alignment or fine-tuning is essential. Alignment has emerged as a pivotal approach to improve the following performance of pre-trained language models, especially in complex instruction-following tasks: commonsense reasoning, coding, summarization, and math problemsolving (Bai et al., 2022;Ouyang et al., 2022;Stiennon et al., 2020;Rafailov et al., 2024b). \n\nThe current alignment methods can be broadly categorized into groups: (i) supervised fine-tuning (SFT) based on demonstration data, aligning an input prompt and a human response. (ii) Preference fine-tuning (Tajwar et al., 2024) with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Christiano et al., 2017) or direct preference optimization (DPO) (Rafailov et al., 2024b;Zhao et al., 2023;Azar et al., 2024;Tang et al., 2024;Ethayarajh et al., 2024) based on preference data containing preferred and dis-preferred responses to prompts. Although RLHF and DPO have achieved promising results (Rafailov et al., 2024b;Tunstall et al., 2023), they require expensive human preference labels on several candidate demonstrations of a query to be used as feedback, limiting their applicability to language model alignment in settings where there is a lack of preference feedback. Furthermore, preference fine-tuning may suffer from reward overoptimization (also known as reward hacking), as shown by (Rafailov et al., 2024a;Gao et al., 2023a). Recent work (Sharma et al., 2024) also shows that simply performing SFT on demonstrations can result in a better model than preference fine-tuning with AI feedback.",
            "score": 0.44199827966590177,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 569,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 569,
                    "end": 591,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 591,
                    "end": 614,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 902,
                    "end": 923,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 988,
                    "end": 1012,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1030,
                    "end": 1048,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 1231,
                    "end": 1255,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1656,
                    "end": 1674,
                    "matchedPaperCorpusId": "252992904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "269362836",
            "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
            "text": "This paper empirically demonstrates the superiority of unified one-stage SFT over two-stage training, questioning the reasonability of the current DAP. On medical-domain data, [197] finds that LMs constrained by CL techniques on source domains exhibit greater robustness to future domain shifts. Specifically, they identify that parameter regularization techniques like EWC [113], despite slightly higher cost, can facilitate positive forward and backward transfer. \n\nFinancial Domain. A gap persists between general-purpose LLMs and existing domain-specific smaller-scale LLMs [7,259], underscoring the urgent need for more powerful financial-domain experts through the integration of LLMs. Notably, DAP techniques have emerged as crucial tools for tailoring LLMs to the intricacies of the financial domain while mitigating the negative effects of abrupt domain shifts from general to finance [121,138,268,271,289]. \n\nBBT-Fin [138] collects a Chinese financial DAP dataset comprising 80 billion tokens sourced from corporate reports, analyst reports, social media, and financial news. In addition to the conventional masked language modeling (MLM) training objective, BBT-Fin further incorporates triplet masking and span masking techniques during DAP. CFGPT [121] creates CFData, a financial dataset for DAP and SFT, comprising 141 billion tokens. During DAP, CFGPT does not employ CL techniques but utilizes QLoRA [50] for preventing overfitting to downstream data and balancing general response ability and domain-specific ability during SFT. These two methods are typical domain-specific LLMs focusing solely on adaptation to target domains without explicit CL measures or evaluation of vertical forgetting. \n\nIn [268], the authors aim to enhance the data efficiency of DAP. When the downstream tasks' data distribution T are known, based on the generalization bound [14,61,213], the authors propose to sample the subset of DAP data  [91,129], and the prefix U-represents them unified [33,257]. The prefix MM-and LC-represents Multi-Modal and Long-Context training phases [146,199,301].",
            "score": 0.44175208769243957,
            "section_title": "Different Domains of DAP.",
            "char_start_offset": 43123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 379,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 911,
                    "end": 915,
                    "matchedPaperCorpusId": "258833440"
                },
                {
                    "start": 1872,
                    "end": 1876,
                    "matchedPaperCorpusId": "8577357"
                },
                {
                    "start": 1876,
                    "end": 1879,
                    "matchedPaperCorpusId": "2871880"
                },
                {
                    "start": 1879,
                    "end": 1883,
                    "matchedPaperCorpusId": "264305893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "277667341",
            "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
            "text": "After the two-stage SFT training, we observed a significant improvement in the model's reasoning capabilities, while its general abilities showed no notable degradation. However, upon further usage and analysis, we identified several -37.3 76.0 42.9 70.5 areas for further improvement. These include issues related to structural integrity, such as failing to generate properly paired <think>-</think> tags when required or producing redundant outputs, particularly when processing subjective queries. To better demonstrate the model's performance in handling structural issues, we conduct a focused analysis on its ability to generate properly formatted <think>-</think> tag pairs. Specifically, a total of 418 subjective queries were evaluated across the obtained models. We quantified the frequency of incorrect usage (either missing or redundant) of paired <think>-</think> tags in the model responses. As shown in Table 3, '#Missing Error' denotes the number of occurrences where the <think>-</think> tags were missing, '#Redundancy Error' counts instances of unnecessary tag duplication, and '#All Error' represents the combined total of all incorrect tag usages. Our analysis revealed that errors predominantly involved missing <think>-</think> tags. This likely stems from the model's strong specialization in reasoning tasks during training, which may have limited its ability to generalize structured formatting to subjective or open-ended scenarios. To mitigate this issue, we augmented the dataset with an additional 3K human-curated chosen/rejected response pairs, extending the original 20K DPO foundation dataset, as introduced in Subsection 2.4. These supplementary response pairs were constructed in accordance with Team (2025), ensuring consistent reasoning chains while reflecting varying degrees of structural fidelity. This approach effectively enhances the model's ability to follow formatting instruction during DPO training, mitigating the issue where excessively long CoT responses weaken the representation of formatting-related tokens in the DPO loss. As shown in Table 3, following the application of DPO, the model shows a notable improvement, reducing the total number of errors to 49, with 44 instances involving missing error.",
            "score": 0.4416606816632202,
            "section_title": "DPO",
            "char_start_offset": 19483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2257
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09320068359375
        },
        {
            "corpus_id": "271915646",
            "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
            "text": "The domain of Natural Language Processing (NLP) has witnessed substantial progress [14,21,30,31,42], especially through the advent of Large Language Models (LLMs) [2,17,18,27,35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32,39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG). \n\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data. \n\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9,15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11,28,33], reformulating queries to align with the semantic space of queries and documents [8,20,29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10,22,34]. \n\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.",
            "score": 0.4411557226224515,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "259950027"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "259858754"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "254998782"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2135009765625
        },
        {
            "corpus_id": "277780928",
            "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
            "text": "We conducted SFT on Qwen2.5-32B(Qwen, 2024) using datasets mentioned in Section 2.2. We employed a cosine Learning Rate Scheduler, set the Learning Rate to 8e-6, the Warm Up Ratio to 0.05, the Batch Size to 64, and the Max Token Length to 16,384. The training process consisted of three epochs. Our experimental results demonstrate that directly using the answer portion from response of reasoning model for model training via Supervised Fine-Tuning (SFT) leads to significant improvements on several key benchmarks, particularly HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), and GPQA (Rein et al., 2023). However, we observed a slight decrease in performance on chat-oriented metrics such as AlignBench (Liu et al., 2023) and MT Bench (Zheng et al., 2023). Our analysis focuses on elucidating the impact of different data generation methods derived from a reasoning model's output on the fine-tuned non-reasoning model's capabilities across diverse benchmarks. Baseline Performance: The AM-32B-Raw-Answer-SFT model, fine-tuned directly on the original community dataset responses (R orig ), serves as our primary baseline. It establishes a reference point for evaluating the efficacy of incorporating reasoning model outputs.",
            "score": 0.4410151513557798,
            "section_title": "Experiment Setup",
            "char_start_offset": 9190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1239
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "270560702",
            "title": "Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning",
            "text": "Fine-tuning refers to the process of updating the parameters of a pre-trained model using task-specific data. Initial studies, such as Less Is More for Alignment (LIMA, (Zhou et al., 2024)), have investigated the effects of training data quantity and quality on fine-tuning outcomes in typical natural language processing tasks like Q&A and creative writing. However, to the best of our knowledge, there has been no comprehensive study on how fine-tuning data (considering aspects like quantity, diversity, composition, etc.) affects the planning capabilities of LLMs. This paper pioneers the exploration of these research questions. \n\nLarge Reasoning Models. Recently, OpenAI released o1-preview (OpenAI, 2024), a model trained to generate an internal thought before answering questions using effective reinforcement learning with human feedback (RLHF) techniques. o1 pioneers the application of scaling laws during inference and showcases a notable improvement in reasoning and planning capabilities compared to existing LLMs. Our proposed method focuses on enhancing the base model during the supervised fine-tuning (SFT) stage, a critical step in preparing a strong initial model for RLHF. Given the proven effectiveness of CMDS in the SFT stage, we believe that combining CMDS with advanced RLHF techniques could further enhance the model's reasoning and planning performance and potentially reduce overall training costs. We consider this integration a potential direction for future work. \n\nData Selection. Existing work on data selection for training LLMs, such as (Yu et al., 2023) and (Zhu and Hauff, 2022), addresses related challenges but differs from ours in two key ways. First, our work focuses specifically on enhancing logical reasoning and planning capabilities in LLMs, a critical and increasingly important domain. Our data selection method has the potential to be applied to training large reasoning models like o1, whereas (Yu et al., 2023) centers on text classification and (Zhu and Hauff, 2022) targets question generation from text passages. Second, and more importantly, our approach differs fundamentally in methodology. Both (Yu et al., 2023) and (Zhu and Hauff, 2022) rely on natural language embeddings, similar to our baseline method, CMDS-l.",
            "score": 0.4406607507595711,
            "section_title": "A Related Work",
            "char_start_offset": 26560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 188,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1573,
                    "end": 1590,
                    "matchedPaperCorpusId": "252280753"
                },
                {
                    "start": 1595,
                    "end": 1616,
                    "matchedPaperCorpusId": "250562659"
                },
                {
                    "start": 1945,
                    "end": 1962,
                    "matchedPaperCorpusId": "252280753"
                },
                {
                    "start": 1998,
                    "end": 2019,
                    "matchedPaperCorpusId": "250562659"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "263830207",
            "title": "MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning",
            "text": "Mathematical Reasoning for Large Language Models Mathematical reasoning is a crucial ability to examine large language models (Cobbe et al., 2021;Hendrycks et al., 2021a;Wei et al., 2022;Yuan et al., 2023b). The mathematical reasoning ability of LLMs can be enhanced by math-related pre-training (Hendrycks et al., 2021a;Lewkowycz et al., 2022a;Taylor et al., 2022;Lightman et al., 2023a) and math-related supervised finetuning (Yuan et al., 2023a;Luo et al., 2023a;Yue et al., 2023b;Yu et al., 2023). Query augmentation (Luo et al., 2023a;Yu et al., 2023) and response augmentation (Huang et al., 2022;Zelikman et al., 2022;Ni et al., 2023;Zhu et al., 2023b;Yuan et al., 2023a)  Data Augmentation for LLM Data augmentation is a common technique to improve downstream task performance in NLP (Feng et al., 2021). In the era of large language models, data augmentation is usually used for generating instruction following SFT datasets (Wang et al., 2023b;Taori et al., 2023;Xue et al., 2023). Queries (Ding et al., 2023;Xu et al., 2023) and responses (Mukherjee et al., 2023) of SFT datasets can both be augmented by prompting state-of-the-art proprietary LLMs. Compared with their work, we are concentrated on augmenting math SFT dataset and we are more interested in scaling relationships on in-domain and out-ofdomain generalizations.",
            "score": 0.44048190630678813,
            "section_title": "Related Works",
            "char_start_offset": 5035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1336
                }
            ],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 641,
                    "matchedPaperCorpusId": "257019561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08135986328125
        },
        {
            "corpus_id": "273507544",
            "title": "Improve Vision Language Model Chain-of-thought Reasoning",
            "text": "As shown in the upper part of fig. 5, we present the data composition for SFT. The training data includes CoT distillation (193k instances) from table 1 and corresponding short answers (193k). Additionally, for CoT data, we incorporate 16k visual math examples from G-LLaVA. To maintain general instruction-following capability as the base model, we include 2k randomly sampled instruction data from LLaVA pretraining Liu et al. (2024). To ensure the SFT models can handle both direct and CoT prompts during inference, we sample a small set of format-aligned data-50 examples from each of the 9 datasets-resulting in 450 instances. \n\nIn the lower part of fig. 5, we outline the data composition for model training. Specifically, LLAVA-NEXT-FORMAT (fig. 5 \u2460) serves as the baseline model, trained exclusively on format-aligned data to enforce the desired output format without learning any task-specific reasoning skills. In contrast, models in fig. 5 \u2461 and \u2462 incorporate either direct or CoT datasets, enabling the model to be expert in one type of skill as well as following the both direct and CoT prompt styles. Finally, LLAVA-REASONER-SFT (fig. 5 \u2463) represents the SFT model trained on both CoT and direct data, making it to be expert in both types of reasoning. \n\nWe use the LLaMA3-LLaVA-NeXT-8B architecture, initializing the weights with Open-LLaVA-NeXT. All Supervised Fine-Tuning (SFT) experiments are trained for 1 epoch with a learning rate of 5e-6 and a batch size of 32. The experiments are conducted on 8 H100 GPUs.",
            "score": 0.4400212209837008,
            "section_title": "TRAINING SETTING",
            "char_start_offset": 8966,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1529
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "268512826",
            "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
            "text": "Despite their promising potential, large language models carry the risk of generating toxic or offensive content without human alignment.One approach that has gained considerable attention in addressing this issue is Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020;Ouyang et al., 2022;Bai et al., 2022a;Zhu et al., 2023a,b;Yu et al., 2023).For instance, In-structGPT (Ouyang et al., 2022) builds a three-step pipeline of RLHF, which includes supervised finetuning (SFT), reward model (RM) training, and reinforcement learning using PPO (Schulman et al., 2017).This process involves collecting numerous samples, each consisting of one prompt and multiple candidate responses ranked by human annotators.These annotated rankings are then segmented into pairs to enhance computational efficiency.Touvron et al. (2023b) allocate more resources to the prompt collection to maximize its diversity while featuring only two responses per prompt.Conversely, some works introduce finegrained distinctions to LLMs by incorporating listwise comparisons among responses, or dynamically sampling better candidates for SFT (Yuan et al., 2023b;Dong et al., 2023;Song et al., 2023a), also leading to improved performance.\n\nWhile more prompts can cover a wider range of domains and topics, limitations in annotation resources often force researchers to choose one side between diverse prompts and longer rankings with more responses.In our study, we investigate the impact of prompt diversity and compare it quantitatively with that of responses.We also establish empirical relations between prompt diversity and the final performance of tuned LLMs.",
            "score": 0.43903984640138893,
            "section_title": "Fine-tuning for Human Alignment",
            "char_start_offset": 5382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 137,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1228
                },
                {
                    "start": 1230,
                    "end": 1439
                },
                {
                    "start": 1439,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 290,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 392,
                    "end": 413,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.366455078125
        },
        {
            "corpus_id": "265067168",
            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "text": "The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination origins in LLMs, as each stage equips the model with specific capabilities. Pre-training. Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world. \n\nSupervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during the pre-training stage, it's crucial to recognize that pretraining primarily optimizes for completion. Consequently, pre-trained LLMs fundamentally served as completion machines, which can lead to a misalignment between the next-word prediction objective of LLMs and the user's objective of obtaining desired responses. To bridge this gap, SFT (Zhang et al., 2023d) has been introduced, which involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies (Chung et al., 2022;Iyer et al., 2022) have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on unseen tasks, showcasing their remarkable generalization abilities. \n\nReinforcement Learning from Human Feedback. While the SFT process successfully enables LLMs to follow user instructions, there is still room for them to better align with human preferences.",
            "score": 0.438772829660909,
            "section_title": "Training Stages of Large Language Models",
            "char_start_offset": 7928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2128
                },
                {
                    "start": 2129,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 1899,
                    "end": 1917,
                    "matchedPaperCorpusId": "255096269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29248046875
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "Conclusions: LLM post-training is a complex endeavor that involves improvements to instruction following, stylistic formatting, reasoning abilities, and general alignment to human preferences. LLMs can imitate the required style with \"superficial\" finetuning using a handful of examples, leading to the Superficial Alignment Hypothesis. However, a solely stylistic evaluation fails to characterize the many aspects of reasoning and task-specific capabilities that are key goals of finetuning. In fact, taskspecific skills & reasoning significantly improve after post-training with more examples compared to the pre-trained model. These improvements closely follow a power law in our experiments with the number of finetuning examples across multiple model families and sizes. We also see that these improvements are driven by the model's reasoning ability during generation, and are not limited to the model's alignment to formatting or style. In addition, we see that the win rate against other models can be a misleading metric to measure tasks that require complex reasoning, signaling the need for holistic evaluation programs leveraging standardized, objective benchmarks, in addition to measurement of alignment to human preferences. \n\nWe also observe that good post-training can help LLMs overcome problems associated with knowledge cutoff, by enabling them to better utilize knowledge from beyond the pre-training corpus either via further finetuning or RAG. These results put together highlight the qualitative and quantitative characteristics of post-training, and the role of data scaling in this.",
            "score": 0.4383895177692253,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 21311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1608
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "274789093",
            "title": "Preference-Oriented Supervised Fine-Tuning: Favoring Target Model Over Aligned Large Language Models",
            "text": "Large language models(LLMs) such as ChatGPT (OpenAI et al. 2024) have exhibited successful and potent applications in comprehending human queries and delivering plausible responses. This ability has proven to be crucial in realworld applications, e.g. AI assistants and recommendation systems. To equip LLMs with this ability, the alignment methods are usually applied to pre-trained language models. Alignment enables pre-trained models to comprehend the context and generate responses suitable to human interactions. Typical alignment methods can be broadly categorized into two types: Supervised Fine-Tuning (SFT) and Preference Alignment (PA). Supervised fine-tuning (SFT) is an essential phase of alignment, wherein the task is framed as causal language modeling performed on a pre-trained language model with instruction-response data D = {\u27e8x, y\u27e9}. Generally, it leverages the cross-entropy objective function in optimization, equipping the pre-trained language model with the ability to follow instructions and generate coherent sequences. Several studies (Schick and Sch\u00fctze 2021;Houlsby et al. 2019;Ivison et al. 2023) are dedicated to exploring SFT training strategies to enhance the alignment of LLMs. However, due to the intrinsic traits of modeling, the optimization process heavily depends on the availability of high-quality \u27e8x, y\u27e9 data, which hinders its performance. Traditionally, the prevalent large-scale SFT datasets in earlier research, such as Alpaca (Taori et al. 2023) and ShareGPT (shareAI 2023), were mainly developed via AI distillation or human-and-AI interaction. Assuring the quality of these datasets can be challenging, as the filtration and curation processes demand significant human resources and efforts. \n\nInstead of solely aligning the instruction and responses, preference alignment (PA), such as InstructGPT (Ouyang et al. 2022) and Direct Preference Optimization (DPO) (Rafailov et al. 2023), optimizes the LLMs based on chosenrejected data \u27e8x, y + , y \u2212 \u27e9. These PA methods provide exceptional benefits in model alignment, enabling LLMs to align more accurately with AI/human preferences.",
            "score": 0.4383750632399375,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1741
                },
                {
                    "start": 1744,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1063,
                    "end": 1088,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 1088,
                    "end": 1108,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 1108,
                    "end": 1126,
                    "matchedPaperCorpusId": "254877064"
                },
                {
                    "start": 1849,
                    "end": 1869,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.461669921875
        },
        {
            "corpus_id": "276449984",
            "title": "Thinking Preference Optimization",
            "text": "Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (ThinkPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses. Instead, ThinkPO utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that ThinkPO further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by 8.6% and output length by 25.9%. Notably, ThinkPO is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from 87.4% to 91.2%.",
            "score": 0.4382499369248884,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1063232421875
        },
        {
            "corpus_id": "268249175",
            "title": "Vision-language models for medical report generation and visual question answering: a review",
            "text": "Following the training, a common practice involves fine-tuning VLMs on smaller datasets tailored to specific downstream tasks. \n\nSupervised Fine-Tuning (SFT) Before employing SFT, the VLM is pre-training on an extensive image-text dataset, establishing a foundational understanding of the complex relationship between visual and textual representations. SFT involves meticulous fine-tuning on a more focused dataset, curated to match the nuances of the targeted application. This dual-phase strategy, encompassing broad pre-training and task-specific fine-tuning, enables the model to benefit from large-scale generalization while seamlessly adapting to the intricacies of particular applications [Ouy+22]. \n\nReinforcement Learning from Human Feedback (RLHF) RLHF is a distinct fine-tuning approach employed to enhance VLMs through the incorporation of human preferences during finetuning [Ouy+22; Lam+22; Zie+20]. RLHF initiates with an initial model, incorporating humangenerated rankings of its outputs to construct a detailed reward model. In contrast to traditional reinforcement learning (RL) [SB98; Cor+20], which relies solely on environmental interactions, RLHF strategically integrates human feedback. This human-in-the-loop approach provides a more nuanced and expert-informed methodology, allowing for the fine-tuning of VLMs in alignment with human preferences, ultimately leading to improved model outcomes. \n\nInstruction Fine-Tuning (IFT) IFT refers to the process of refining a pre-trained language model by providing specific instructions or guidance tailored to a particular task or application [Ren+24]. This process typically involves exposing the model to examples or prompts related to the desired instructions and updating its parameters based on the feedback received during this task-specific training phase. Medical VLM, RaDialog [Pel+23], employs this fine-tuning technique.",
            "score": 0.438129192168933,
            "section_title": "Fine-Tuning Techniques",
            "char_start_offset": 31535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 129,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1901
                }
            ],
            "ref_mentions": [
                {
                    "start": 697,
                    "end": 705,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219970703125
        },
        {
            "corpus_id": "275789974",
            "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
            "text": "We create the vanilla SFT corpus covering multiple domains. For non-reasoning tasks, including question-answering, writing, and text processing, we initially construct a seed dataset through human annotation. This seed dataset is used to train a seed model. Subsequently, we collect a diverse of prompts and employ the seed model to generate multiple responses to each prompt. Annotators then rank these responses and refine the top-ranked response to produce the final version. For reasoning tasks such as math and coding problems, where rule-based and reward modeling based verifications are more accurate and efficient than human judgment, we utilize rejection sampling to expand the SFT dataset. \n\nOur We first train the model at the sequence length of 32k tokens for 1 epoch, followed by another epoch at the sequence length of 128k tokens. In the first stage (32k), the learning rate decays from 2 \u00d7 10 \u22125 to 2 \u00d7 10 \u22126 , before it re-warmups to 1 \u00d7 10 \u22125 in the second stage (128k) and finally decays to 1 \u00d7 10 \u22126 . In the realm of artificial intelligence, reinforcement learning (RL) has emerged as a pivotal training methodology for large language models (LLMs) (Ouyang et al. 2022) (Jaech et al. 2024), drawing inspiration from its success in mastering complex games like Go, StarCraft II, and Dota 2 through systems such as AlphaGo (Silver et al. 2017), AlphaStar (Vinyals et al. 2019), and OpenAI Dota Five (Berner et al. 2019). Following in this tradition, the Kimi k1.5 system adopts an iterative synchronous RL framework, meticulously designed to bolster the model's reasoning capabilities through persistent learning and adaptation. A key innovation in this system is the introduction of a Partial Rollout technique, designed to optimize the handling of complex reasoning trajectories. \n\nThe RL training system as illustrated in Figure 3a operates through an iterative synchronous approach, with each iteration encompassing a rollout phase and a training phase.",
            "score": 0.43803665205441034,
            "section_title": "Vanilla Supervised Finetuning",
            "char_start_offset": 28849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 1170,
                    "end": 1189,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1342,
                    "end": 1362,
                    "matchedPaperCorpusId": "205261034"
                },
                {
                    "start": 1374,
                    "end": 1395,
                    "matchedPaperCorpusId": "204972004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "271039884",
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "text": "Given a base model only pre-trained on an Englishcentric corpus, e.g., Llama-2-7B (Touvron et al., 2023) in our study, we want to construct a model capable of solving tasks in a low-resource language. \n\nFor those target languages, there are usually very limited pre-training texts, ranging from 1B to 20B tokens, and almost no data for supervised finetuning (SFT). In this scenario, we investigate two representative paradigms of constructing such a model: CT-then-SFT and model merging. We illustrate the roadmap in Figure 1, which demonstrates the relations among the models. \n\nConventional Practice: CT-then-SFT The common practice is (1) first continual pre-training (CT) on the monolingual texts in the target language X to learn the language modeling and (2) then learning task-solving abilities through SFT (Yong et al., 2023;Nguyen et al., 2023). This approach is referred to as CT-then-SFT. Specifically, we consider the following models: BASE: We employ the original Llama-2-7B without SFT as the base LLM. \n\nCT-X: We continually pre-train BASE on the corpus in the target language X. Following previous works (d'Autume et al., 2019;Tao et al., 2023), we add 1/4 English corpus for memory replay, to avoid catastrophic forgetting English language modeling. \n\nCTSFT-X: We train CT-X with SFT data to enhance its task solving ability. There are two variants using different SFT data: \n\n(1) CTSFT-X-flan: We finetune CT-X with English SFT data, which includes the original FLAN datasets and the training set of GSM8K1 . This approach is based on the assumption that task-solving abilities in English can be transferred to the target language (Chirkova and Nikoulina, 2024;Shaham et al., 2024).",
            "score": 0.4378828929098024,
            "section_title": "Roadmap Towards LLMs for Low-Resource Languages",
            "char_start_offset": 9553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 1120,
                    "end": 1143,
                    "matchedPaperCorpusId": "174798232"
                },
                {
                    "start": 1143,
                    "end": 1160,
                    "matchedPaperCorpusId": "257279790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "278237268",
            "title": "100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models",
            "text": "In comparing the roles the supervised fine-tuning (SFT) and outcome-reward-based reinforcement learning (RL) play in the context of generalization, Chu et al. (2025a) demonstrates that RL significantly enhances a model's ability to generalize across both textual and visual domains. In contrast, SFT often encourages memorization of the training data, which can impair performance on out-ofdistribution tasks. Interestingly, while RL drives generalization, SFT remains crucial for stabilizing the model's output format-an essential property that facilitates effective downstream RL optimization, highlighting the complementary nature of SFT and RL in shaping models that can acquire and transfer knowledge across diverse, multimodal tasks. However, recent studies have raised concerns regarding the limitations of RL when applied to reasoning language models. Yue et al. (2025a) points out that RL training in reasoning language models may narrow the scope of reasoning capabilities while enhancing sampling efficiency, and that RL-trained models generally underperform compared to base models in pass@k metrics at larger k values. Similarly, Hochlehnert et al. (2025a) observes that the generalization ability of RL methods on smaller language models is significantly limited, possibly due to the restricted prior knowledge available for RL training to exploit. \n\nIn summary, these findings underscore both the promise and challenges of applying RL to reasoning and generalization in reasoning language models. While general RL approaches demonstrate encouraging out-of-domain performance gains and broader adaptability, careful attention must be given to potential trade-offs.",
            "score": 0.43782585467314555,
            "section_title": "GENERALIZABILITY",
            "char_start_offset": 75281,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1678
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.118408203125
        },
        {
            "corpus_id": "270764323",
            "title": "A Teacher Is Worth A Million Instructions",
            "text": "The recent advancements in Large Language Models (LLMs) have significantly propelled the field of natural language understanding and generation.Pre-trained language models (PLMs) leveraging extensive training corpus sourced from web [3,6] have demonstrated impressive capabilities across various natural language processing (NLP) tasks.However, additional training steps are required for PLMs to follow instructions and keep the responses aligned to human preferences.\n\nInstruction tuning (IT) [19,27,31] trains a PLM further for instruction following; utilising the general knowledge imparted in the pre-training phase along with the imparted instruction following capability it trains the model to generalise well on unseen tasks.However, while proficient at following instructions, these models may produce outputs that are potentially toxic or ethically questionable.To enhance alignment with human values, further training is necessary, utilizing techniques such as reinforcement learning with human feedback [13], direct preference optimization (DPO) [16] and monolithic preference optimization without reference model (ORPO) [10] based on pairwise preference data.\n\nInstruction tuning requires meticulous attention to data quality, optimization of instruction sets, and the implementation of effective training methodologies to ensure peak performance.A primary challenge in training these instruction-tuned models in specific domains is the potential reduction in the model's generalization ability, a factor we monitor using public evaluation benchmarks in our research.In this study, we present a method that not only addresses these concerns but also improves public benchmarks while aligning the model within a specific domain, in this instance, ecommerce.Drawing from the successful implementation of Knowledge Distillation (KD) [9] in miniLLMs [8] and tasks such as classification, we propose it as an alternative to the commonly used supervised fine-tuning (SFT) and alignment process in language model training.We propose Domain Alignment from Expert (DAE), a unique post-training domain alignment algorithm designed to strengthen domain-specific knowledge within the LLMs.DAE integrates domain-specific expert models into the training process, enhancing the model's understanding of specialized domains while preserving its ability to generalize across broader contexts.",
            "score": 0.4374541524284723,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 336
                },
                {
                    "start": 336,
                    "end": 468
                },
                {
                    "start": 470,
                    "end": 732
                },
                {
                    "start": 732,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1768
                },
                {
                    "start": 1768,
                    "end": 2027
                },
                {
                    "start": 2027,
                    "end": 2189
                },
                {
                    "start": 2189,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244140625
        },
        {
            "corpus_id": "271892126",
            "title": "API-Guided Dataset Synthesis to Finetune Large Code Models",
            "text": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks following extensive pre-training [34,72]. In the domain of code-related tasks, large code models (LCMs) such as CodeLlama [66] and StarCoder [37] have exhibited impressive capabilities in program understanding and generation, supporting various real-world applications. However, despite their vast knowledge acquired through training on enormous datasets, these base models may not achieve optimal performance across all use cases out-of-the-box. As illustrated in Fig. 1(a) and (c), to further align models with diverse requirements-including enhancing general code generation capabilities [38,66] or specializing in specific codebases or domains (supporting commercial products like deep learning [54] or security-related [25] assists)-researchers often employ additional datasets to fine-tune base models, yielding more powerful and customized LCMs. \n\nAmong the various fine-tuning techniques proposed, supervised fine-tuning (SFT) has emerged as a critical approach for enhancing LLM capabilities. SFT leverages the knowledge acquired during pre-training while aligning models with human expectations [6,11,81]. This process involves further training the models on carefully curated instruction datasets, typically comprising formatted instruction-response pairs. These pairs, represented as (INSTRUCTION, RESPONSE), consist of human-provided tasks or queries (INSTRUCTION) and the corresponding desired outputs (RESPONSE) that the model should generate [8,16,52,90]. \n\nGiven the importance of high-quality SFT datasets for LCMs, various approaches have been developed to create and curate such datasets. These methods include the collection of real-world code snippets [82] and the use of programming concepts and keywords (e.g., recursion and loops) to guide LLMs in dataset construction [44]. These efforts have resulted in the generation of extensive datasets, as exemplified by Nemotron-4's 800k examples [3]. While such large datasets offer potential benefits, they also present practical challenges in the SFT process.",
            "score": 0.4364680330914224,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "260140789"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1563,
                    "end": 1566,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1773,
                    "end": 1777,
                    "matchedPaperCorpusId": "270358041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216796875
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "To enable large language models (LLMs) to understand and generate natural language, they are constructed with billions of parameters and pretrained on datasets containing trillions of tokens [OAA + 24]. However, several challenges arise after the pretraining stage of LLMs [WBP + 24]. One major issue is that pretrained LLMs can only continue generation based on the previous context and often struggle to accurately answer user questions. To address this, supervised fine-tuning (SFT) is introduced, using pairs of questions and answers. For example, in models like Mistral, preset instructions such as ' [INST]' and '[/INST]' are used to frame a question as a prompt [JSM + 23]. The corresponding answer is then used as the target output. The model's probability of generating the correct answer is maximized through next-token prediction, employing the cross-entropy loss function to classify tokens across the entire token space. \n\nThe next challenge for LLMs lies in ethical concerns, where LLMs may inadvertently teach humans to engage in unethical activities, such as robbing banks [OWJ + 22]. To address this issue, various alignment methodologies have been proposed, including Reinforcement Learning from Human Feedback (RLHF) [OWJ + 22, BJN + 22] with Proximal Policy Optimization (PPO) [SWD + 17], Direct Preference Optimization (DPO) [RSM + 23], Kahneman & Tversky Optimization (KTO) [EXM + 24], and UNified Alignment (UNA) [WBH + 24]. The core idea of alignment is to equip LLMs with the ability to reject harmful requests by learning from human feedback. \n\nFor pretrained LLMs, SFT and alignment are traditionally performed in sequence. However, this staged approach often leads to performance degradation, where the model loses capabilities acquired in earlier phases. This paper seeks to address and mitigate this degradation. \n\nFigure 1: UFT integrates SFT and alignment through a generalized implicit reward function. It likens pre-training and fine-tuning of LLMs to Chinese proveb \"Read ten thousand books, travel ten thousand miles\".",
            "score": 0.43637307028789896,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2054
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "258841835",
            "title": "Aligning Large Language Models through Synthetic Feedback",
            "text": "Alignment learning has been an essential learning scheme to align the behaviors of large language models (LLMs) with human values like safety and truthfulness while following the intention of users accurately (Ouyang et al., 2022). Vanilla LLMsthose not aligned yet -could misunderstand user intentions or produce unsafe and inaccurate responses. Desirable human values such as helpfulness, harmlessness, or honesty can be defined, and human demonstrations with these values are then used for the alignment learning (Askell et al., 2021;Bai et al., 2022a). 1 The code is available at github.com/naver-ai/almost. Figure 1: A procedure of reward modeling through synthetic feedback. We assume that the response from a larger LLM with more and better demonstrations might be better overall. We train a reward model with synthetic comparisons generated top on the assumption. \n\nTypically, alignment learning consists of three stages: supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022;Bai et al., 2022a). \n\nHowever, the three-stage training recipe requires significant human effort, especially in the first two stages. More specifically, both the SFT and RM training stages must be provided with an abundance of high-quality human demonstrations and ranking datasets for obtaining models to facilitate RLHF. For instance, Ouyang et al. (2022) prepare and utilize 13k human demonstrations and 33k comparisons. \n\nOn the other hand, Self-Instruct (Wang et al., 2022) attempts to generate synthetic self-generated instruction datasets using in-context learning with a few seed demonstrations. Meanwhile, the release of LLaMA (Touvron et al., 2023) brings upon many open-sourced aligned LLMs trained on the outputs of proprietary LLMs or human-annotated instructions.",
            "score": 0.43597683602808035,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1834
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "265659430",
            "title": "ULMA: Unified Language Model Alignment with Human Demonstration and Point-wise Preference",
            "text": "The demonstration dataset D demo = {(x i , y i )} is a collection of input prompts x i , each associated with a human-written response y i , which is of high quality and provide ground-truth to the input but generally expensive and hard to acquire. In language model alignment, the pre-trained LLM is first fine-tuned by supervised learning on D demo . \n\nDenote the LLM parameterized by \u03b8 as \u03c0 \u03b8 , which outputs a probability distribution \u03c0 \u03b8 (\u2022|x) over all possible responses y given the prompt x. SFT aims to fit \u03c0 \u03b8 to D demo by minimizing the negative log-likelihood (Wang et al., 2023) \n\nwhich will produce a fine-tuned model \u03c0 SFT . Since demonstration data directly provides the groundtruth response to the given user input, SFT can train the LLM according to human instructions, making it fast adapt to the specific scenario of interest. \n\nWhen undesirable demonstration data (e.g., bad responses) are available, the unlearning method is proposed to reduce the generating probability for unwanted response (Nguyen et al., 2022). Unlearning can be viewed as a counterpart of SFT working on dispreferred demonstration data.",
            "score": 0.43562629203797987,
            "section_title": "Learning from Demonstration Data",
            "char_start_offset": 7308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "271709755",
            "title": "SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese Large Language Models",
            "text": "Recently, large language models (LLMs) have earned profound influences on both commercial and academic spheres as the advent of ChatGPT1 . The GPT series, especially lately GPT-4 (Ope-nAI, 2024) and GPT-4o2 show remarkable performances in natural language generation (NLG) and natural language understanding (NLU). These accomplishments can be attributed to these models' vast number of parameters and their training on extensive unsupervised datasets. The use of promptdriven techniques has further refined the training process, resulting in outputs that better align with human-like responses, (Ouyang et al., 2022). \n\nThe advancement of LLMs has not only spurred rapid development in general domains but has also facilitated the integration of LLMs into specialized domains such as medicine, (Zhang et al., 2023a;Sun et al., 2024), law, (Huang et al., 2023;Cui et al., 2023a), and finance, (Wu et al., 2023;Zhang and Yang, 2023). These various domain tasks which can generate expected outputs by following natural prompts bring great convenience to practitioners as well as to many ordinary people. Here we combine the domain requirements and research result to build a financial LLM. \n\nAs we currently surveyed, there have been achievements in financial large language models. BloombergGPT (Wu et al., 2023) is a 176B English Financial LLM. Xuanyuan (Zhang and Yang, 2023) is a Chinese Financial LLM with 176B parameters. Others like FinGPT (Yang et al., 2023), DISC-FinLLM (Chen et al., 2023) mainly focus on implementing financial domain tasks through SFT training. Nevertheless, the aforementioned works either merely apply parameter efficient finetuning methods (Devalal and Karthikeyan, 2018;Ding et al., 2023) or lack the reinforcement learning human-alignment step (Ouyang et al., 2022) which may cause hallucinations and affect the performance of financial LLM. \n\nFurthermore, while LLMs have demonstrated impressive inferential computational abilities, they still frequently encounter errors in final calculations due to inherent limitations.",
            "score": 0.4355409117379088,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1873
                },
                {
                    "start": 1876,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 617,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 910,
                    "end": 931,
                    "matchedPaperCorpusId": "258833440"
                },
                {
                    "start": 1354,
                    "end": 1376,
                    "matchedPaperCorpusId": "258833440"
                },
                {
                    "start": 1670,
                    "end": 1701,
                    "matchedPaperCorpusId": "52916833"
                },
                {
                    "start": 1701,
                    "end": 1719,
                    "matchedPaperCorpusId": "257316425"
                },
                {
                    "start": 1776,
                    "end": 1797,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "278535411",
            "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining",
            "text": "In this section, we share insights and observations from our exploration of MiMo-7B's post-training process, which we hope will benefit the research community. SFT for Format Alignment In the initial RL training steps from MiMo-7B-Base, we observe that the model primarily learns to adapt the answer extraction function, e.g., \"\\boxed{}\" for mathematics problems. Therefore, we investigate a \"light-weight\" SFT to help the base model align with the expected answer format. However, as Figure 7 demonstrates, the resulting MiMo-7B-RL-LiteSFT model fails in both reasoning potential and final performance. While MiMo-7B-RL-LiteSFT begins with a higher performance than MiMo-7B-RL-Zero, it falls behind the base model's trajectory after just 500 steps. Furthermore, when compared to MiMo-7B-RL, which undergoes \"heavier\" SFT, MiMo-7B-RL-LiteSFT exhibits a similar growth trend but significantly underperforms due to its inferior starting point, ultimately leading to poorer final results. \n\nInterference Between Different Domains During the later stages of RL training from MiMo-7B-Base, maintaining a performance balance between mathematics and coding tasks proves challenging. Between training steps 2000 and 2500, the model exhibits continuous improvement on code problems, while its performance on mathematical reasoning tasks fluctuates and declines. \n\nIn contrast, RL training on the cold-started SFT model shows consistent improvements across both domains. Analysis of the model outputs reveals that the base model, with its strong exploration capabilities, tends to hack the reward for mathematics problems. For code problems, however, the test-case-based verifier makes reward exploitation significantly harder. This highlights the critical need for high-quality mathematical problem sets to ensure robust RL training. \n\nLanguage Mixing Penalty Like DeepSeek-R1-Zero, we also observe language mixing issues during RL training on MiMo-7B-Base. To mitigate this problem, we introduce a language mixing penalty into the reward function. However, we find designing such a penalty function is challenging.",
            "score": 0.4354054050053028,
            "section_title": "Discussion",
            "char_start_offset": 36599,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2106
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.163330078125
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "The hypothesis in this paper is that alignment can be a simple process where the model learns the style or format for interacting with users, to expose the knowledge and capabilities that were already acquired during pretraining. This is supported by the fact that the authors were able to achieve remarkably strong performance by simply fine-tuning a 65B-parameter LLaMa language model on just 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. The authors collected a dataset of 1,000 prompts and responses where the outputs (responses) were stylistically aligned with each other, but the inputs (prompts) were diverse, and the model was trained on this set of examples.\n\nResponse from Vicuna-33B Token indices sequence length is longer than the specified maximum sequence length for this model (3045 \u00bf 2048). Running this sequence through the model will result in indexing errors The hypothesis about alignment in the paper is the \"Superficial Alignment Hypothesis.\" This hypothesis suggests that a model's knowledge and capabilities are primarily learned during pretraining, while alignment teaches the model which subdistribution of formats should be used when interacting with users [Kirstain et al., 2021]. In simpler terms, the model learns the style or format for interacting with users, exposing the knowledge and capabilities acquired during pretraining. The authors of the paper set out to test this hypothesis by curating a dataset of 1,000 prompts and responses, where the outputs are stylistically aligned with each other, but the inputs are diverse. The outputs are designed to be in the style of a helpful AI assistant. They collected these examples from various sources, primarily split into community Q&A forums and manually authored examples. They then trained a pretrained 65B-parameter LLaMa model, LIMA, on this set of 1,000 demonstrations, fine-tuning it to confirm their hypothesis. The results showed that LIMA performed remarkably well, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. The model also tended to generalize well to unseen tasks that did not appear in the training data. Response from Vicuna-13B I'm sorry, but I do not have access to the PDF you are referring to",
            "score": 0.4353587294311758,
            "section_title": "Response from MPT-30B-Chat",
            "char_start_offset": 46109,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.299560546875
        },
        {
            "corpus_id": "267200158",
            "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts",
            "text": "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, training in lower-resource languages yields minimal improvement. This suggests that the bottleneck of cross-lingual alignment is rooted in the pretraining stage. Our findings highlight the challenges in cross-lingual LLM safety, and we hope they inform future research in this direction.",
            "score": 0.4353340120597371,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45458984375
        },
        {
            "corpus_id": "271334724",
            "title": "TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback",
            "text": "This section presents the datasets, models, compared baselines, training details, and the evaluation metrics used in the paper.\n\nDataset Throughout the experiments, we use the full-hh-rlhf dataset (Bai et al., 2022) which is centered on improving the helpfulness and harmlessness of the language model generation.This dataset includes 112k training instances and 12.5k instances for evaluation.Every instance in the dataset features a prompt, along with a chosen response considered preferable and a rejected response, offering a clear basis for performing model alignment for better helpfulness and harmlessness.Following the setting in (Li et al., 2023b), we randomly divide the dataset into three parts: 20% for supervised fine-tuning (SFT), 40% for reward model learning, and 40% for reinforcement learning for reward maximization.Training Details We use DeepSpeed-Chat (Yao et al., 2023) framework for performing RLHF, where PPO is the default algorithm.Within this framework, our proposed TLCR is integrated to provide token-level rewards to enhance the PPO algorithm.Experimental details are presented in Appendix B.",
            "score": 0.4353060043518064,
            "section_title": "Experimental Settings",
            "char_start_offset": 16330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 129,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 835
                },
                {
                    "start": 835,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1074
                },
                {
                    "start": 1074,
                    "end": 1123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1761474609375
        },
        {
            "corpus_id": "260438790",
            "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "text": "Large language models (LLMs) (Anil et al., 2023;Touvron et al., 2023b;OpenAI, 2023) have shown considerable abilities in various math reasoning tasks (Saxton et al., 2019;Cobbe et al., 2021;Lightman et al., 2023). It is of interest to understand, predict, and improve an LLM's math reasoning ability based on different pre-trained LLMs and supervised datasets. With this knowledge, we can better decide the effort we put into improving the LLM or augmenting the dataset. Many recent works are focusing on using different prompts (Wei et al., 2022b;Yao et al., 2023) or ensembling / reranking multiple times of inferences (Cobbe et al., 2021;Uesato et al., 2022;Wang et al., 2023;Lightman et al., 2023) to improve models' reasoning performances. While in-context learning (ICL) and performing multiple inferences can improve performance, it is computationally expensive and not suitable for online deployment scenarios. Therefore, we focus on the performance of the supervised LLMs with inference only once which is a setting closer to online deployment. \n\nTo this end, we empirically investigate the scaling relationship of factors that influence the math reasoning abilities of a supervised LLM, including pre-training losses, the amount of supervised data, and the amount of augmented data. Firstly, we analyze the supervised fine-tuning (SFT) and ICL performance of LLMs. We observe that the pre-training loss is approximately negatively linear correlated to the SFT and ICL accuracy in a given interval which is a better performance indicator than pre-trained model sizes or pre-trained token counts. Secondly, we analyze the relationship Preprint Figure 1: The key findings of scaling relationship on learning math reasoning ability with LLMs. \n\nbetween SFT and different amounts of supervised data. We observe that the model performance has a log-linear relation versus the supervised data amount while the increase diminishes with the better pre-trained model. Thirdly, we want to leverage the model itself to generate more supervised data to reinforce its reasoning ability and analyze the scaling relationship of the augmented data amount.",
            "score": 0.43493569691248263,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 679,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1558837890625
        },
        {
            "corpus_id": "264490452",
            "title": "Unpacking the Ethical Value Alignment in Big Models",
            "text": "Bai et al. (2022b) proposed the Constitutional AI model, which replaces manually created data for both SFT and reward model training phases with comment and modification data generated by the self-critiquing method (Saunders et al., 2022), and then incorporated the Chain-of-Thought method (CoT) (Wei et al., 2022b) into training. Yuan et al. (2023) presented an improved Rank Responses to Align Human Feedback method that samples responses from different sources like models and human feedback to train the model with a ranking-based loss. The original RLHF method mathematically minimizes the reverse KL divergence between the model distribution and an implicit target distribution. Go et al. (2023) extended this loss to f-divergence, unifying various algorithms like RLHF, GDC, and DPG. To tackle problems like poor generalization and robustness, Liu et al. (2023b) innovatively proposed modelling social interactions beyond traditional methods relying on reward models like RLHF. They constructed a simulated society comprised of a large number of models, allowing them to interact with each other, receive feedback, learn to adjust their behaviours to leave a better impression, and thereby learn and establish social values. \n\nIn this part, we mainly discuss LLM alignment algorithms. For a comprehensive survey of alignment goals and datasets, please refer to our other paper on alignment (Yao et al., 2023)   ing of each goal. For example, for instruction following, which instructions should AI prioritize? In moral conformity, which principles (such as those listed in Sec. 2.2) need to be considered? Existing alignment methods face the 'tyranny of the crowdworker' problem (Kirk et al., 2023), where the power to determine alignment principles is held by data annotators or those who set annotation guidelines. This results in a consequence that models only meet the preferences of a minority, lacking diverse representation across culture, race, language, and so on, leading to the risks and harms described in Sec. 2.1.",
            "score": 0.43489126629401814,
            "section_title": "Introduction of Alignment Methods",
            "char_start_offset": 45225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2034
                }
            ],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326171875
        },
        {
            "corpus_id": "278310944",
            "title": "Llama-Nemotron: Efficient Reasoning Models",
            "text": "Supervised fine-tuning (SFT) plays a critical role in transferring reasoning capabilities into the Llama-Nemotron models. While prior stages such as NAS and CPT focus on architectural efficiency and broad knowledge transfer, SFT helps distill reasoning behavior from strong teacher models like DeepSeek-R1 (DeepSeek-AI et al., 2025) by training on task-specific reasoning traces. It also establishes fine-grained control over response style using the \"detailed thinking on/off\" instruction. Recent studies (DeepSeek-AI et al., 2025;OpenThoughts, 2025;BespokeLabs, 2025;Wen et al., 2025) have shown that this reasoning SFT can substantially improve performance on complex reasoning tasks. Our results confirm these findings, highlighting the importance of training on large-scale, high-quality reasoning traces during SFT for eliciting robust reasoning abilities in downstream usage. This section builds upon the synthetic data described in Section 3 and provides further implementation details specific to each model.",
            "score": 0.43461412079770495,
            "section_title": "Supervised Fine-Tuning",
            "char_start_offset": 19393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12322998046875
        },
        {
            "corpus_id": "273901354",
            "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
            "text": "To marry the strengths of both paradigms, in this paper, we propose DRPO, Dynamic Rewarding with Prompt Optimization, a novel tuning-free approach for LLM self-alignment. DRPO draws inspiration from two key insights from recent alignment research. First, the superficial alignment hypothesis (Zhou et al., 2024) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2024a;Zhao et al., 2024). Second, reward models in RLHF often generalize poorly to out-of-distribution samples (Burns et al., 2023), whereas LLMs, well-known for their superior generalization capabilities, can provide more effective rewards and feedback for alignment. Building on these insights, DRPO is constructed atop a search-based prompt optimization (PO) framework (Pryzant et al., 2023;Hao et al., 2023;Wang et al., 2023), allowing LLMs to selfcorrect and automatically craft detailed alignment instruction. This steers model behavior more effectively, without relying on any use of human preferences or model training. \n\nThe core novelty of DRPO lies in its dynamic rewarding mechanism, integrated with the optimization framework. This mechanism enables LLMbased rewards to be adjusted on the fly based on specific queries, helping to identify and rectify the model's alignment blind spots. For example, if an LLM with outdated knowledge pretends to answer a question requiring the latest news, its \"knowledge limitation\" reward will be low, and the alignment prompt will be updated accordingly. We apply this novel method to automatically craft both the system prompt and responses in ICL examples, which have proven highly effective in improving alignment. \n\nWe conducted comprehensive experiments on 8 recent LLMs using the standard alignment benchmark, just-eval-instruct, composed of questions from multiple alignment datasets. Our results show that DRPO can effectively align both base and SFT/RLHF tuned models. Notably, DRPO significantly enhances base models, enabling them to outperform their SFT/RLHF-tuned counterparts. \n\nFigure 2: Comparing DRPO with other alignment methods, such as RLHF and URIAL (Lin et al., 2024a).",
            "score": 0.43442998438149555,
            "section_title": "Introduction",
            "char_start_offset": 1805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2053
                },
                {
                    "start": 2056,
                    "end": 2154
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 311,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 400,
                    "end": 419,
                    "matchedPaperCorpusId": "265608902"
                },
                {
                    "start": 824,
                    "end": 842,
                    "matchedPaperCorpusId": "264451925"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61962890625
        },
        {
            "corpus_id": "260775656",
            "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
            "text": "In conclusion, this paper has efficaciously demonstrated the relevance and effectiveness of PPO within RLHF, particularly in tasks geared towards manipulating the output tokenizer length of large language models. However, the underlying complexities and instabilities inherent in RLHF still pose significant challenges, especially when it comes to comprehending input requirements. This paves the way for incorporating approaches such as SFT which show commendable efficiency in knowledge-based tasks, potentially fostering an environment for greater synergy between these methods. \n\nIn spite of the complexities, the potential of PPO in manipulating output tokenizer length and in facilitating training is undeniably promising. Future research may further optimize these relationships, creating opportunities for AI systems with more aligned, efficient, less toxic, and in particular, to meet the customization needs of human beings, as our task showing, potentially revolutionizing the way we work with large language models.",
            "score": 0.4344027504628504,
            "section_title": "Conclusion",
            "char_start_offset": 28187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1027
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313232421875
        },
        {
            "corpus_id": "267411998",
            "title": "Decoding-time Realignment of Language Models",
            "text": "Language models. A language model, conditioned on a query sequence x := (x 1 , . . . , x m ) \u2208 X , parametrizes a probability distribution over response sequences y := (y 1 , . . . , y n ) \u2208 Y. The probability \u03c0(y|x) is factorized using the chain rule of probability: \n\nThis factorization has two main benefits. First, the logprobability log \u03c0(y|x) is easy to compute, enabling maximum likelihood (MLE) based training. Second, it is easy to generate i.i.d. samples y \u223c \u03c0(\u2022|x) at decoding time. The state-of-the-art LM for modeling \u03c0(y|x) is the transformer model (Vaswani et al., 2017). Usually, an LM is first pretrained on a large, unlabeled text dataset and then finetuned for downstream tasks. In what follows, we review the usual finetuning pipeline in Ziegler et al. (2019); Stiennon et al. (2020); Bai et al. (2022); Rafailov et al. (2023). \n\nFinetuning from output demonstrations. Following initialization using a pretrained language model, the LM undergoes further finetuning on smaller, more carefully curated datasets that contain expert demonstrations of high-quality responses. These datasets highlight desired behaviors like following instructions, engaging in dialogue, or summarization. This process is known as supervised finetuning (SFT). \n\nTypically, the SFT model is obtained through maximum likelihood estimation. In the rest of the paper, we will denote the SFT-trained model by \u03c0 sft . \n\nFinetuning from pairwise comparisons. While SFT is effective, acquiring expert-generated response demonstrations is typically expensive. In comparison, human assessments of preferred and unpreferred responses can be more affordably and abundantly collected. Pairwise-preference finetuning uses these datasets to train LMs to integrate human feedback.",
            "score": 0.4340991419064541,
            "section_title": "Background",
            "char_start_offset": 5526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1761
                }
            ],
            "ref_mentions": [
                {
                    "start": 563,
                    "end": 585,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 781,
                    "end": 803,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18359375
        },
        {
            "corpus_id": "270380285",
            "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward",
            "text": "Large language models (LLMs) trained on extensive datasets have shown outstanding performance across diverse tasks and domains [28,10,16,35].Various techniques for fine-tuning LLMs have been developed, including the well-known Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) [1,28].SFT focuses on tailoring the LLMs' responses for specific tasks directly with labeled data, whereas RLHF improves LLMs through feedback data reflecting human preferences.In particular, RLHF is pushing the application boundaries of both closedsource [20,3,27] and open-source LLMs [28,33], due to the necessity for polishing the value, fairness, and helpfulness of LLMs in practical scenarios [38,25,21].\n\nExisting RLHF methods can be majorly categorized into two classes based on whether the reward signal is explicitly modeled.Reward-based alignment pioneered by OpenAI [21,1,28] first trains a reward model from user preferences, typically through Maximum Likelihood Estimation (MLE), and then leverages actor-critic algorithms such as Proximal Policy Optimization (PPO) [24] to tune the SFT model to realize alignment.This approach often requires substantial computational resources and suffers from sample inefficiency [9].Conversely, another class of methods, known as reward-free alignment, such as Direct Preference Optimization (DPO) [23], Identity Preference Optimization (IPO) [4], and Sequence Likelihood Calibration (SLiC) [36], do not rely on an extra reward model.These approaches offer a more resource-efficient alternative by optimizing the policy directly from preferences, therefore attracting much attention from the academic society.In this work, we commence our analysis with the vanilla DP as a case study, subsequently extending our findings to encompass broader reward-free alignment strategies.\n\nDespite the simplicity and promise of DPO, a variety of phenomena that cannot be clearly understood or explained have been observed and reported in practice.",
            "score": 0.4340337045178865,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 317
                },
                {
                    "start": 317,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 720
                },
                {
                    "start": 722,
                    "end": 845
                },
                {
                    "start": 845,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1244
                },
                {
                    "start": 1244,
                    "end": 1495
                },
                {
                    "start": 1495,
                    "end": 1670
                },
                {
                    "start": 1670,
                    "end": 1836
                },
                {
                    "start": 1838,
                    "end": 1995
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 716,
                    "end": 719,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 888,
                    "end": 892,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1359,
                    "end": 1363,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1404,
                    "end": 1407,
                    "matchedPaperCorpusId": "264288854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67431640625
        },
        {
            "corpus_id": "271768843",
            "title": "Open-domain Implicit Format Control for Large Language Model Generation",
            "text": "In our evaluation and case studies, we found that the user-provided examples are often disregarded, underscoring the complexity of empowering models with such abilities. We developed a data collection methodology, resulting in a training dataset and a testing benchmark tailored to our proposed framework. Through supervised fine-tuning (SFT), we observed notable improvements in open-domain format control with negligible fluctuations in the helpfulness of model responses. Another advantage of our one-shot framework is that even SFT data with human-annotated responses deemed \"low quality\" can enhance format control abilities with minimal impact on helpfulness, as even poorly-formed responses adequately demonstrate certain implicit formats. This feature significantly eases the scalability of our data collection method. \n\nTo summarize, our contributions include: (i) we design a new framework for Open-domain Implicit Format Control (OIFC); (ii) we develop a dataset collection pipeline that facilitates the training and evaluation for our framework; the resulting OIFC-SFT dataset and related code will be open-sourced; (iii) our experimental results support the effectiveness of these methodologies. \n\n2 One-shot Implicit Format Control",
            "score": 0.4337284644507219,
            "section_title": "Introduction",
            "char_start_offset": 2146,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 826
                },
                {
                    "start": 829,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1245
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11395263671875
        },
        {
            "corpus_id": "277781617",
            "title": "X-Guard: Multilingual Guard Agent for Content Moderation",
            "text": "We performed SFT with Qwen-2.5-Instruct-3B (Yang et al., 2024a) using 100K SFT training data points consisting of the user text, assessment of the given user text, safety label, and category violation codes. The goal is to teach the model to first evaluate the given user text inside the <think> tag, and then provide the safety label inside the <label> tag, and, if the user text violates any categories, to provide the category codes inside the <category> tags. We used the same rationales as in (Shao et al., 2024) because simply using RLHF with the smaller 3B model might deteriorate the performance or lead the model to engage in reward hacking without properly evaluating the given text. \n\nAfter supervised finetuning, we performed the GRPO training (Shao et al., 2024;Hugging Face, 2025) with the 76K training data points. The GRPO training data is made up of prompts and responses from the ALERT and SALAD datasets (generated from the uncensored LLM and jury judged), which consist of user text (prompt and response), jury evaluated labels, and categories. It should be noted that responses from these data points had already been used to perform SFT in the previous step. Our intuition here is to further train the model using these data points so it can correctly predict both the label and the categories. \n\nReward Functions. We used three main reward functions: format-based reward, safety label reward, and the categories reward. The format reward evaluates the structural integrity of model outputs. It awards 0.15 points for each properly formatted XML-style tag (<think>, <label>, and <categories>). A full 1.0 score is granted when all three tags are present, with an additional 0.25 bonus for extensive reasoning in the thinking section (3+ sentences). We wanted to make sure the model produced at least 3 sentences of assessment of the given user text before deciding on its safety label and category assessment. The safety label reward focuses on accuracy, granting 1.0 point for exact matches between the generated and expected safety judgment (\"safe\" or \"unsafe\") while penalizing incorrect labels with -0.25 points.",
            "score": 0.4336363798201711,
            "section_title": "Training X-Guard",
            "char_start_offset": 9453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 517,
                    "matchedPaperCorpusId": "267523467"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "267523467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08831787109375
        },
        {
            "corpus_id": "271709856",
            "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.",
            "text": "Structured generation, the process of producing content in standardized formats like JSON and XML, is widely utilized in real-world applications to extract key output information from large language models (LLMs).This study investigates whether such constraints on generation space impact LLMs\u2019 abilities, including reasoning and domain knowledge comprehension. Specifically, we evaluate LLMs\u2019 performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks. Surprisingly, we observe a significant decline in LLMs\u2019 reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks.",
            "score": 0.43332947485588524,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1104736328125
        },
        {
            "corpus_id": "274776492",
            "title": "Reinforcement Learning Enhanced LLMs: A Survey",
            "text": "ChatGLM (GLM et al., 2024), developed by Zhipu AI, represents an evolving series of large language models. The latest version in this series is GLM-4, which includes variants such as GLM-4, GLM-4-Air, and GLM-4-9B. These models are pre-trained on a dataset of over 10 trillion tokens, predominantly in Chinese and English, and are subsequently post-trained through a combination of supervised fine-tuning (SFT) and RLHF to achieve advanced alignment quality. Evaluation results indicate that GLM-4 rivals or even surpasses GPT-4 (OpenAI, 2023) on general benchmarks like MMLU, and demonstrates superior performance in Chinese-specific alignments as measured by Align-Bench (Liu et al., 2023b). \n\nThe reinforcement learning phase involves the ChatGLM-RLHF (Hou et al., 2024) pipeline, which enhances alignment with human preferences. This pipeline comprises three primary components: gathering human preference data, training a re-ward model, and optimizing policy models. To support large-scale training, ChatGLM-RLHF includes methods to reduce reward variance for stable training, leverages model parallelism with fused gradient descent, and applies regularization constraints to prevent catastrophic forgetting in large language models. Experimental results confirm that ChatGLM-RLHF yields substantial improvements in alignment-focused tasks compared to the supervised fine-tuned version of ChatGLM.",
            "score": 0.433260409137268,
            "section_title": "ChatGLM",
            "char_start_offset": 26854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.580078125
        },
        {
            "corpus_id": "269449935",
            "title": "Hallucination of Multimodal Large Language Models: A Survey",
            "text": "Before moving to multimodal large language models, it is essential to introduce the concept of large language models. Typically, LLMs encompass a range of transformer-based models that are extensively trained on vast textual datasets. Prominent examples include GPT-3 [14], PaLM [31], LLaMA [153], and . Through scaling both data volume and model capacity, LLMs demonstrate notable emergent capabilities, including In-Context Learning [14], Chain-of-Thought prompting [168] and instruction following [130], among others. \n\nThe characteristics and behaviors of LLMs are intricately linked to their training processes. LLMs typically undergo three primary training stages: pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). Below, we provide a concise overview of each stage to facilitate comprehension. \n\nPre-trianing. Pre-training serves as a fundamental phase in the learning process of LLMs [219]. During this stage, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities. This pre-training process establishes a solid groundwork for the models to undertake subsequent fine-tuning tasks effectively. \n\nSupervised Fine-Tuning. Although pre-training equips LLMs with substantial knowledge and skills, it's important to acknowledge that its primary focus is on optimizing for completion. Consequently, pre-trained LLMs essentially function as completion machines, which may create a misalignment between the objective of predicting the next word within LLMs and the user's objective of obtaining desired responses. To address this disparity, the concept of Supervised Fine-Tuning (SFT) [204] has been introduced. SFT involves further training LLMs using a meticulously annotated set of (instruction, response) pairs, thereby enhancing the capabilities and controllability of LLMs. \n\nReinforcement Learning from Human Feedback. Although SFT has made strides in enabling LLMs to adhere to user instructions, there remains a need for further alignment with human preferences.",
            "score": 0.43309556929522436,
            "section_title": "Large Language Models",
            "char_start_offset": 5855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 279,
                    "end": 283,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 435,
                    "end": 439,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 468,
                    "end": 473,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "270559282",
            "title": "Self-Evolution Fine-Tuning for Policy Optimization",
            "text": "Recent years have showcased the remarkable capabilities and performance of large language models (LLMs) across a broad range of tasks.These capabilities are attributed not only to their vast parameter sizes and the extensive text corpora used for pre-training [17] but also to the critical process of aligning these models with human expectations [24].Such alignment is essential to ensure that the outputs of LLMs are helpful, honest, and harmless [1] across various tasks and applications.\n\nThe pursuit of aligning LLMs with human preferences has led to three predominant methodologies, among others: supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF) [7], and offline RLHF.SFT directly fine-tunes LLMs on downstream tasks using instruction-following data, guiding the models to produce responses that match the ground truth in the dataset [8].RLHF offers a sophisticated approach by initially training a reward model that assigns higher rewards to responses aligning better with human preferences, followed by optimizing the policy LLM using policy-gradient methods like proximal policy optimization (PPO) [27].In offline RLHF, exemplified by direct preference optimization (DPO) [25], the LLM policy is directly optimized using precollected preference data, eliminating the necessity for a reward model.This approach aims to enhance the probability of producing chosen responses while reducing the likelihood of rejected ones.\n\nEach of these methods comes with its strengths and weaknesses.SFT, while efficient, is hindered by the scarcity of high-quality human-annotated data and tends to suffer from poor adaptability to out-of-distribution samples [19].RLHF demands substantial computational overhead for training an additional reward model [5] and faces optimization challenges such as inefficiency and instability.Offline RLHF methods, which are directly optimized on preference data without the need for a reward model, tend to suffer from distribution drift issues and may lead to biased policies that favor out-of-distribution responses [36], thereby impairing generalization to unseen samples.[42].",
            "score": 0.4330324635259024,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 491
                },
                {
                    "start": 493,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1336
                },
                {
                    "start": 1336,
                    "end": 1459
                },
                {
                    "start": 1461,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1852
                },
                {
                    "start": 1852,
                    "end": 2135
                },
                {
                    "start": 2135,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 351,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 683,
                    "end": 686,
                    "matchedPaperCorpusId": "4787508"
                },
                {
                    "start": 871,
                    "end": 874,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1684,
                    "end": 1688,
                    "matchedPaperCorpusId": "263830929"
                },
                {
                    "start": 1777,
                    "end": 1780,
                    "matchedPaperCorpusId": "260316010"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392822265625
        },
        {
            "corpus_id": "274165887",
            "title": "Learning from \"Silly\" Questions Improves Large Language Models, But Only Slightly",
            "text": "Large language models (LLMs), pre-trained on vast amounts of data, have garnered significant attention for their ability to address a wide range of tasks [Achiam et al., 2023, Dubey et al., 2024, Hui et al., 2024, Jiang et al., 2024a, Liu et al., 2024a, Reid et al., 2024, Yang et al., 2024, Zhao et al., 2023, Zhou et al., 2024]. The pretrain-finetune paradigm has emerged as the cornerstone of LLMs' remarkable success. Through pre-training, LLMs acquire extensive knowledge about the world, while fine-tuning aligns them with specific human instructions, enabling them to generate highquality responses and excel on domain-specific datasets. Compared to the resource-intensive process of pre-training, supervised fine-tuning is a cost-effective approach that focuses on fine-tuning LLMs using small but high-quality datasets. Consequently, the quality of SFT datasets plays a pivotal role in determining the performance of fine-tuned models. \n\nNumerous studies [Cao et al., 2024, Chen et al., 2024a, Li et al., 2024, Liu et al., 2024b, Lu et al., 2023, Mekala et al., 2024, Wei et al., 2023, Xia et al., 2024, Zhou et al., 2023] have emphasized the importance of the quality of SFT datasets. The scarcity of high-quality datasets necessitates careful data selection and processing when constructing SFT datasets. Recently, Bai et al. [2024] demonstrated that SFT data collected from certain sources can significantly improve fine-tuning results. Notably, they highlighted the significant contribution of the data from \"Ruozhiba\", a Chinese online platform where people ask \"silly\" questions like \"Since 70% of the human body is water, does that mean 7 out of every 10 people are just water disguised as humans?\" to better understand certain knowledge.",
            "score": 0.4328590268269358,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 944
                },
                {
                    "start": 947,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1754
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 252,
                    "matchedPaperCorpusId": "273901271"
                },
                {
                    "start": 252,
                    "end": 271,
                    "matchedPaperCorpusId": "268297180"
                },
                {
                    "start": 290,
                    "end": 309,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 964,
                    "end": 981,
                    "matchedPaperCorpusId": "264590782"
                },
                {
                    "start": 981,
                    "end": 1001,
                    "matchedPaperCorpusId": "259937133"
                },
                {
                    "start": 1001,
                    "end": 1018,
                    "matchedPaperCorpusId": "266348323"
                },
                {
                    "start": 1054,
                    "end": 1075,
                    "matchedPaperCorpusId": "267740312"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093017578125
        },
        {
            "corpus_id": "277780928",
            "title": "Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability",
            "text": "The results presented in this paper affirm that supervised fine-tuning (SFT) using response data derived from reasoning models can significantly enhance the performance of target language models. Our investigation systematically evaluated three distinct methodologies for utilizing these reasoning-derived outputs, revealing that the effectiveness of knowledge transfer is critically dependent on the specific strategy employed for structuring the SFT data. \n\nCrucially, our findings demonstrate that simply leveraging the final answer component (A reason ) from a reasoning process, while boosting performance on certain reasoning and coding benchmarks, may not yield holistic improvements and can even slightly degrade performance in conversational alignment metrics. This highlights the importance of the information's structure; methods incorporating summarized reasoning steps (S think ) offered alternative performance profiles, often achieving better balance across diverse tasks or excelling in specific areas like instruction following, albeit sometimes involving trade-offs (e.g., the observed IFEval performance reduction with the Think Summarization method). \n\nThese results underscore the potential of leveraging reasoning outputs as a potent form of data augmentation for SFT, offering a viable pathway towards enhancing the capabilities of large language models. The variations in performance across methods emphasize that the manner in which reasoning-derived knowledge is structured and presented during fine-tuning is a key determinant of the resulting model's strengths and weaknesses. This work contributes practical strategies for such capability transfer, demonstrating tangible improvements across multiple standard benchmarks. \n\nBuilding on these findings, future research should explore more sophisticated techniques for extracting, representing, and integrating the knowledge embedded within the reasoning process (T reason ). Investigating alternative summarization strategies, methods for dynamically combining reasoning steps with final answers, or techniques for explicitly modeling the reasoning structure could potentially unlock further performance gains and lead to the development of more robust and versatile models through optimized knowledge distillation.",
            "score": 0.4325549470909391,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 13700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1750
                },
                {
                    "start": 1753,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2293
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "267751124",
            "title": "Aligning Large Language Models by On-Policy Self-Judgment",
            "text": "Learning from Preference Scores There are several approaches utilizing an RM for alignment learning. RLHF (Ziegler et al., 2020) utilizes an RM for on-policy reinforcement learning. RRHF (Yuan et al., 2023) maximizes the margin of log-likelihood by the rank of responses determined by the score from RM and human annotators. RAFT (Dong et al., 2023) and ReST (Gulcehre et al., 2023) apply rejection sampling on sampled responses through the RM to perform self-imitation learning. SALMON (Sun et al., 2023) trains LLMs to generate scores for responses through principledriven synthetic preference data utilizing the SFT model. However, all these approaches require a  separate RM for the alignment procedure. \n\nOptimizing on Preference Orders From preference orders in the static dataset, DPO (Rafailov et al., 2023) optimizes LLMs by implicit rewards without a separated RM. IPO (Azar et al., 2023) proposes a modified objective using an unbounded preference mapping function to mitigate overfitting on deterministic preferences in the dataset. PCO (Xu et al., 2023) utilizes cringe loss for optimization, which considers the token-level likelihood of rejected samples as contrastive training. SPIN (Chen et al., 2024) performs iterative training considering the distribution gap between SFT datasets and the model's responses as preference orders. Self-Rewarding Language Models (Yuan et al., 2024) trains LLMs to generate scores for a given response by chain-of-thought reasoning to construct preference datasets by self-generated responses. All these approaches differ from our work in that they do not perform on-policy learning. \n\nGenerative Pairwise Evaluator The generative pairwise evaluator, which we refer to as JM, has been utilized in previous approaches to alignment learning. ILF (Scheurer et al., 2023) selects the response that reflects human-requested feedback through JM. SLiC-HF (Zhao et al., 2023) constructs a static preference dataset with responses obtained from the SFT model ordered by JM.",
            "score": 0.43246272777553213,
            "section_title": "Related Work",
            "char_start_offset": 22042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2014
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.599609375
        },
        {
            "corpus_id": "261705578",
            "title": "Statistical Rejection Sampling Improves Preference Optimization",
            "text": "Recent advancements in Large Language Models (LLMs) (Brown et al., 2020;Touvron et al., 2023;Anil et al., 2023;OpenAI, 2023) have unlocked unprecedented capabilities in diverse tasks, such as programming and creative writing. Models are pre-trained on large unlabeled corpora and supervised fine-tuned (SFT) on various tasks (Wei et al., 2021;Chung et al., 2022). Subsequently, RLHF (Stiennon et al., 2020) enhances the alignment of large language models with human preferences. RLHF introduces notable complexities into the training process, including a reward model, a policy model, a reference policy, and a value model. It limits the maximum feasible size of a model due to memory constraints. Additionally, it is not stable during training. Recognizing these challenges, recent research has pioneered alternatives to RLHF. Notable among these are RRHF (Yuan et al., 2023), SLiC (Zhao et al., 2022;2023) and DPO (Rafailov et al., 2023). These methodologies aim to more effectively align LLMs with human preferences while avoiding the complexities of reinforcement learning. Given supervised finetuning data D sft = {(x, y ref )} and preference data D hf = {(x, y w , y l )} where output text y w is preferred over y l on the same input text x, they directly fit the policy model on preference data in various ways. RRHF uses a trained reward model or human raters to compute rewards for multiple sequences generated from difference sources on the same prompt x, and then apply a ranking loss plus supervised fine-tuning loss. SLiC uses a contrastive ranking calibration loss plus a regularization loss L(\u03b8) = max (0, \u03b4 \u2212 log \u03c0 \u03b8 (y w |x) + log \u03c0 \u03b8 (y l |x)) \u2212 \u03bb log \u03c0 \u03b8 (y ref |x), \n\n(1) \n\nFigure 1: RSO first fits a pairwise reward-ranking model from human preference data.",
            "score": 0.4315575994910405,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1778
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 72,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 383,
                    "end": 406,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "269303161",
            "title": "Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks",
            "text": "Motivation. In this scenario, we first train an SFT model and then refine it with the aforementioned alignment methods. These methods, designed to enhance the performance of DPO, have been applied to various tasks, such as machine translation. However, there hasn't been a comprehensive evaluation comparing them on the same task. The primary motivation behind these scenarios is to assess their performance across different benchmarks. Additionally, we aim to determine whether the performance of alignment methods improves with increasing training data, as it seems that alignment methods may not require extensive data beyond the SFT phase. Datasets. We utilize the UltraFeedbackbinarized (Tunstall et al., 2023)  Alignment methods don't require a large training set. The results depicted in Figure 3 reveal that all alignment methods perform better with a smaller training set. We posit that in the typical alignment process, a significant portion of model alignment occurs during the SFT phase. Therefore, when aiming to enhance the performance of the SFT model with methods like KTO, DPO, IPO, and CPO, it is beneficial to utilize a smaller dataset for training. In essence, there exists a trade-off between aligning with SFT and aligning with RL-free methods to achieve optimal performance. \n\nSFT is still enough. Another intriguing observation is that none of the alignment methods outperform SFT in MMLU (See Table 1). This suggests that SFT remains superior to other methods for multitask understanding. Additionally, apart from the KTO algorithm in reasoning, truthfulness, and question answering, SFT demonstrates comparable performance (See Reasoning, Question Answering, and Truthfulness plots in Figure 2). This indicates that alignment methods struggle to achieve notable performance improvements in these tasks.",
            "score": 0.4310919816026011,
            "section_title": "Scenario 1: Fine-tune an SFT Model",
            "char_start_offset": 14700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1297
                },
                {
                    "start": 1300,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "268553509",
            "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
            "text": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a crucial framework for aligning large language models (LLMs) with human preferences.To facilitate preference alignment, existing approaches such as InstructGPT (Ouyang et al., 2022a), Sparrow (Glaese et al., 2022), Llama-2 (Touvron et al., 2023) commonly train a reward model with preferential human feedback.This reward model assesses the overall quality of model outputs as a scalar value.Then training LLMs with the reward signals encourages the models to generate more favorable responses better aligned with human preferences.\n\nDespite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in * Equally contributed to this work.NLP tasks.Previous works (Zhou et al., 2023;Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities.Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users.Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens.However, enhancing the capabilities of LLMs is more critical than adjusting their interaction styles or formats to better match human preferences.\n\nTo address the superficial nature of preference alignment, we first investigate why the current RLHF often leads surface-level alignment.We tackle factuality and mathematical reasoning because the stylistic adjustment rarely contributes to downstream performance.Observing preferencebased reward models is notably deficient in evaluating mathematical reasoning, we hypothesize that preference-based reward models may cause superficial alignment.As a solution, we leverage finegrained LLM feedback that incorporates both verbal response and numeric score adhering to detailed criteria.",
            "score": 0.43031550058335877,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 375
                },
                {
                    "start": 375,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 597
                },
                {
                    "start": 599,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 989
                },
                {
                    "start": 989,
                    "end": 1160
                },
                {
                    "start": 1160,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1449
                },
                {
                    "start": 1451,
                    "end": 1588
                },
                {
                    "start": 1588,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 248,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62744140625
        },
        {
            "corpus_id": "273403465",
            "title": "Anchored Alignment for Self-Explanations Enhancement",
            "text": "In this section, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning-self-explanation-even in the absence of annotated rationale explanations. However, we assume access to human-annotated data in the form of classification datasets for domain-specific adaptation, reflecting a common constraint in realworld applications, where comprehensive explanation data is often scarce or prohibitively expensive compared to classification datasets. \n\nBuilding on prior work (Bai et al., 2022;Wang et al., 2023;Yuan et al., 2024;Wu et al., 2024), our alignment methodology incorporates familiar components such as self-instruction dataset generation, human-free evaluation of candidate responses using LLM-as-Judge, preference pair selection, and model alignment. \n\nHowever, our approach differs from previous methods in two key ways: First, for the assessment of candidate responses, we use the evaluation explanation quality framework introduced in Sections 2.1 and 2.2. Second, we propose a novel technique, Alignment with Anchor Preference Pairs, which improves preference pair selection by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of DPO. \n\nThe steps of the methodology are as follows: \n\n1. Supervised fine-tuning of the base model M Base specifically on a target classification task, resulting in M SFT . \n\n2. Instruct M SFT to generate multiple explanation-prediction pairs for each prompt, and evaluate the quality of these self-explanations using the methodology outlined in Sections 2.1 and 2.2. During alignment, the base model M Base acts as the judge M Judge , ensuring the process remains self-contained. \n\n3. Construct an alignment dataset by selecting preference pairs using an anchor-based strategy (see Section 3.3). \n\n4. Align M SFT via DPO with the dataset created in the third step, producing the aligned model M Anchor .",
            "score": 0.4297518083081697,
            "section_title": "SELF-EXPLANATION ALIGNMENT WITH ANCHOR PREFERENCE PAIRS",
            "char_start_offset": 6078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1834
                },
                {
                    "start": 1837,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "267035293"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "271310241",
            "title": "Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by Direct Preference Optimization",
            "text": "Fine-tuning LLMs for downstream tasks using RLHF technique involves three main phases [6], [36]: 1. supervised finetuning, 2. constructing reward model, and 3. fine-tuning the language model using RL methods.\n\na) Supervised Fine-tuning: This is the initial step of RLHF technique, where the language model undergoes supervised fine-tuning on downstream tasks.During this phase, the model is trained on specific task-related training datasets, allowing it to adapt its pre-trained knowledge to the particular downstream task.The model trained in this phase is commonly referred to as supervised fine-tuning (SFT) model, denoted as \u03c0 sf t .\n\nb) Constructing Reward Model: After training the SFT model, the next step is to develop a reward model that evaluates the SFT model's outputs based on human preferences and represent it as scalar values.This reward model can be built using pre-trained models capable of assessing outputs according to human judgment [36], or by training it on human preference data collected from annotators.\n\nTo construct human preference data, multiple responses are first generated for each prompt by the SFT model, using different variants of the model or sampling methods [6], [12].The collection of prompts and their generated responses are then formatted into a batch of tuples (x, y1, y2), where x is the prompt and y 1 and y 2 are pair of responses sampled from the set of generated responses of the prompt x.Human labelers are then instructed to choose their preferred response between the two.This process creates a preference dataset consisting of tuples (x, y w , y l ), where y w represents the preferred output and y l represents the rejected output.\n\nFrom the generated preference dataset D, the probability distribution of human preference can be formulated as\n\nusing Bradley-Terry model [37] given an optimal reward model r, where \u03c3 is the logistic function.",
            "score": 0.4296255518803733,
            "section_title": "III. BACKGROUND FOR RLHF AND DPO",
            "char_start_offset": 9682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 210,
                    "end": 359
                },
                {
                    "start": 359,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 638
                },
                {
                    "start": 640,
                    "end": 843
                },
                {
                    "start": 843,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1210
                },
                {
                    "start": 1210,
                    "end": 1441
                },
                {
                    "start": 1441,
                    "end": 1527
                },
                {
                    "start": 1527,
                    "end": 1688
                },
                {
                    "start": 1690,
                    "end": 1800
                },
                {
                    "start": 1802,
                    "end": 1899
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 89,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1200,
                    "end": 1203,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18896484375
        },
        {
            "corpus_id": "275907001",
            "title": "Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages",
            "text": "In the previous section, although we clarify that both CPT and SFT involve in-domain capability learning, it remains unclear what cause SFT's learning performance to be weaker than CPT's. However, conclusions in Result 4 are more evident in the high school training data compared to middle school, prompting us to explore the difference in learning capabilities between CPT and SFT with varying difficulty levels problem-solving data. \n\nExperiment. We select a 5B subset of our problem-solving data and categorize it based on the number of solution reasoning steps: data requiring 1-3 steps is classified as easy, 4-7 steps as medium, and 8 or more steps as hard. The distribution of samples account for 36.0%, 38.4%, and 25.6% of the total data, respectively, while token counts make up 23.0%, 36.0%, and 41.0%, respectively. Given the unavoidable inaccuracies in this method of categorization, we focus solely on easy data and hard data for the CPT and SFT comparison experiments. More experimental design discussions can be found in the Appendix C. The experimental groups are designed as follows: \n\n\u2022 Base1: As described in Section 3. CPT with 48.3B general corpus and 14.7B math corpus. \n\n\u2022 Easy-SFT: SFT using the easy data subset on top of Base1. \n\n\u2022 Easy-CPT: CPT incorporating both the Base1 data and the easy data subset. \n\n\u2022 Hard-SFT: SFT using the hard data subset on top of Base1. \n\n\u2022 Hard-CPT: CPT incorporating both the Base1 data and the hard data subset. Results. The results in the left half of Table 3, which is divided by vertical lines, show that CPT models consistently outperform SFT models, with some relative improvements specifically indicated. Notably, Hard-CPT exhibits greater relative enhancements compared to Easy-CPT, and these improvements are not limited to just the hard domain accuracy but are observed across all datasets. Moreover, regardless of whether it is SFT or CPT, training on hard data consistently yields better results compared to training on easy data.",
            "score": 0.4290187783650624,
            "section_title": "IMPACT OF DIFFERENT DIFFICULTY LEVELS",
            "char_start_offset": 28598,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1100
                },
                {
                    "start": 1103,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1393
                },
                {
                    "start": 1396,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2001
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08416748046875
        },
        {
            "corpus_id": "272593310",
            "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
            "text": "In this subsection, we discuss key challenges to human alignment. Although some research is primarily based on experiments with LLMs, the analysis and insights can also be easily applied to diffusion models. \n\nAlignment with AI Feedback Human annotations are expensive, motivating researchers to explore alternatives. \n\nBai et al. [7] introduced reinforcement learning from AI feedback (RLAIF) and proposed constitutional AI (CAI), where human supervision is replaced entirely by a set of principles governing AI behavior, supplemented by a small number of examples for few-shot prompting. They demonstrated that CAI methods can train a harmless but non-evasive AI assistant without relying on human feedback labels for harmlessness. Dubois et al. [44] proposed the AlpacaFarm simulator, which uses oracle API LLMs to simulate human annotators, offering a faster and cheaper alternative to crowdworkers. \n\nLi et al. [97] introduced instruction backtranslation, a scalable method for self-aligning instruction-following language models by automatically generating instructions from an unlabeled human-written corpus as alignment data. Yuan et al. \n\n[222] developed self-rewarding language models, where the LLM itself provides its own rewards during training through LLM-as-a-Judge prompting. Lee et al. [89] demonstrated the effectiveness of RLAIF compared to RLHF across three text generation tasks, showing that rewards can be derived from LLM-generated preferences or by directly prompting LLMs for reward scores without training a reward model. Guo et al. [61] proposed the OAIF framework, which samples two responses from the current model and uses an LLM annotator to label preferences, providing online AI feedback. \n\nPang et al. [130] proposed a self-alignment system for aligning LLMs with societal norms. Black et al. [10] proposed using vision-language models (VLMs) (i.e., LLaVA [103]) to replace human annotation for improving the prompt-image alignment of T2I diffusion models. Current RLAIF approaches focus on the scalability of feedback and demonstrate performance comparable to RLHF. However, it remains unclear whether RLAIF enables continual improvement beyond RLHF or if it encounters inherent bottlenecks beyond human supervision.",
            "score": 0.42888655175360435,
            "section_title": "Challenges of Human Alignment",
            "char_start_offset": 20723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1722
                },
                {
                    "start": 1725,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 748,
                    "end": 752,
                    "matchedPaperCorpusId": "258865545"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "261493811"
                },
                {
                    "start": 1737,
                    "end": 1742,
                    "matchedPaperCorpusId": "267547674"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "258833251"
                },
                {
                    "start": 1891,
                    "end": 1896,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2109375
        },
        {
            "corpus_id": "264306078",
            "title": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
            "text": "To rigorously assess the efficacy of our Safe RLHF pipeline along two alignment dimensionshelpfulness and harmlessness -we analyze models from three iterations of Safe RLHF: Beaver-v1, Beaver-v2, and Beaver-v3. \n\nHowever, evaluating large language models has consistently been a challenging and unresolved problem. Traditional benchmarks often do not capture the full extent to which a model aligns with human values. This shortcoming is largely attributable to inconsistent standards and unequivocal outcomes in human alignment evaluation. Thus, we prefer to assess large language models based on their responses to specific prompts. We employ two methods for overall assessment. These include a rapid evaluation of our models using our trained unified Reward Model and Cost Model; deriving the Elo score by comparing model outputs with human judgments and GPT-4 evaluations. \n\nModel-based Evaluations. Despite human evaluation remaining the gold standard for aligning large language models with human values, the reliance on this method alone is neither practical nor efficient due to considerable associated time and financial costs. Such limitations necessitate alternative assessment methods to complement human evaluation. Thus, we have developed a unified Reward Model and a unified Cost Model, utilizing training methodologies mentioned in Section 3.2. These models are trained on evenly balanced preference data originating from all iterations of Safe RLHF. With these unified models, we can rapidly evaluate subsequent new models under consistent criteria. The test accuracies for the unified models are detailed in Table 1. Note that we do not employ these unified models to train a single-round Safe RLHF process, as the preference data acquisition occurs iteratively. We need intermediate models for the red-teaming procedure, facilitating the collection of new prompts for the follow-up training phases. \n\nAs illustrated in Figure 4, our SFT model, the Alpaca-7B model (reproduced), has the ability to produce both harmless and harmful responses that are almost evenly separated on each side of the c = 0 dividing line (Figure 4a). Following the first round of Safe RLHF training, there is an appreciable shift in the model response distribution towards the side with a lower cost, implying safer outputs (Figure 4b).",
            "score": 0.4288809652564669,
            "section_title": "HELPFULNESS AND HARMLESSNESS EVALUATION",
            "char_start_offset": 17645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1917
                },
                {
                    "start": 1920,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280029296875
        },
        {
            "corpus_id": "263831784",
            "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling",
            "text": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values through instruction tuning. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. A response to this downside is to fall back to supervised fine-tuning (SFT) with additional carefully selected expert demonstrations. However, while this method has been proven to be effective, it invariably also leads to increased human-in-the-loop overhead. In this study, we propose another alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative adversarial training style to enable the LLMs to learn useful human expert demonstrations without being directly exposed to the training examples, thus enabling good generalization capabilities while preserving sample efficiency. Our preliminary findings indicate that RLGAF can help align LLMs outputs with competitive performance against RLHF and SFT, while not suffering from their respective inherent restrictions, suggesting promising avenues for further research on automating AI alignment.",
            "score": 0.42859646098210713,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "266844262",
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "text": "We perform two tasks in output format analysis: language identification and repetition analysis. We utilized vLLM (Kwon et al., 2023) to enhance efficiency. Expressly, for models that have undergone alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), we set up our prompt as \"[INST] <context> [/INST]\". We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. \n\nTo conduct output format analysis, we utilize the following dataset: \u2022 NeuLab-TedTalks 2 (Qi et al., 2018): A common corpus of TED talks, translated into numerous low-resource languages by a global community of volunteers. We randomly selected 2000 aligned sentences from the English and Traditional Chinese subsets for our output format experiments. We download the corpus from OPUS (Tiedemann, 2012). \n\n2 https://opus.nlpl.eu/NeuLab-TedTalks-v1.php \n\nFor language identification analysis, we utilize the FastText (Joulin et al., 2016a,b) language identification model to detect the language of the generated tokens. As for repetition analysis, we assess the proportion of duplicated n-gram tokens at the BPE level within the combination of the generated output and the prompt. \n\nTable 6 presents the repetition statistics for our Traditional Chinese corpus, and Table 7 presents the full results of the repetition analysis experiment. Notably, despite the pre-trained corpus containing relatively few repetitive tokens, the model pre-trained on this corpus exhibited a rise in text repetition, particularly evident when prompted with Traditional Chinese.",
            "score": 0.42831947866770226,
            "section_title": "C Additional Details about Experiment Tasks C.1 Output format Analysis",
            "char_start_offset": 15661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1658
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 133,
                    "matchedPaperCorpusId": "261697361"
                },
                {
                    "start": 591,
                    "end": 608,
                    "matchedPaperCorpusId": "4929974"
                },
                {
                    "start": 886,
                    "end": 903,
                    "matchedPaperCorpusId": "15453873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "Knowledge-intensive content originates from untuned LLMs. Consider the example in Figure 2, where we use llama-2-7b and llama-2-7b-chat as a pair of base and aligned models. We can clearly see that most knowledge-intensive words, including the key answer \"Chihuahua\" and related information such as its weight and length, appear at unshifted positions. On average, across 1,000 examples that we tested, 77.7% of the tokens are at such unshifted positions, which increases to 92.2% when including marginal positions. This observation suggests that untuned and aligned LLMs share the same pre-existing knowledge from pre-training, such that a proper prefix can trigger this acquired knowledge without tuning. For instance, untuned LLMs can fluently generate the answer based solely on the context prefix \"Thank you for asking! The\". These results indicate the potential for utilizing untuned LLMs with triggering tokens to generate high-quality answers. \n\nToken distribution shifts on different pairs of LLMs. Figure 3 shows three pairs of base-vsaligned LLMs at the 7B level: Llama-2 (Base) vs Llama-2-Chat (RLHF), Llama-2 (Base) vs Vicuna-7b-v1.5 (SFT), and Mistral (Base) vs Mistral-Instruct (SFT). The shifted token ratios are all very low (5%-7%) and they share similar frequently shifted tokens, such as 'However', 'cannot', 'Here', 'To' (shown in the bottom boxes). Thus, we believe that our findings are generalizable, which is also confirmed by our results in Sec 4. We present an interactive web demo for visualizing the distributions on our website (details and examples are in Appendix C and Fig. 8). \n\nWhat does alignment tuning learn? We observe that shifted positions frequently consist of \"stylistic tokens\", such as discourse markers and transitional words. These tokens may not be informative, but they contribute to structuring well-formed responses. Also, the tokens related to safety concerns and refusal are also frequently shifted.",
            "score": 0.42799232892569616,
            "section_title": "FINDINGS & ANALYSIS",
            "char_start_offset": 8662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1610
                },
                {
                    "start": 1613,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1952
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41015625
        },
        {
            "corpus_id": "267311819",
            "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
            "text": "Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM\u2019s inherent abilities. More concretely, using financial domain question answering datasets, we apply supervised finetuning on a LLAMA-2 13B CHAT model to act both as a task router and task solver. The task router dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, RAVEN, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results. To the best of our knowledge, our work is the first that investigates tool augmentation of language models for the finance domain.",
            "score": 0.42780235993836824,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.126708984375
        },
        {
            "corpus_id": "272398154",
            "title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
            "text": "For the mathematical problem solving task, existing works primarily focus on synthetic data generation (by a strong teacher model) and supervised fine-tuning (SFT), as seen in ToRA (Gou et al., 2023b), Meta-MathQA (Yu et al., 2023), MAmmoTH (Yue et al., 2023(Yue et al., , 2024)), and Open-MathInstruct (Toshniwal et al., 2024). These methods and synthetic datasets have yielded significant improvements in test accuracy on standard benchmarks like MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021a). \n\nBuilding on strong SFT models, Reinforcement Learning from Human Feedback (RLHF) has proven to be a key technique to elicit LLMs' knowledge during the post-training stage and has become a standard practice in the LLM training pipeline (Bai et al., 2022;Ouyang et al., 2022;Team et al., 2023;Touvron et al., 2023). Broadly speaking, the RLHF learning paradigm, which was originally designed for aligning large language models (LLMs) with human values and preferences (Bai et al., 2022;Ouyang et al., 2022), is distinct from SFT as it learns from relative feedback (Christiano et al., 2017;Ziegler et al., 2019). It has notably enhanced the capabilities of models like ChatGPT, Claude, and Gemini, enabling them to generate responses that are more helpful, harmless, and honest (Bai et al., 2022). Inspired by RLHF's success in general chat applications, in this paper, we explore RLHF for improving LLMs' mathematical problem-solving abilities when equipped with external tools.",
            "score": 0.4276022437777415,
            "section_title": "Introduction",
            "char_start_offset": 1534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1491
                }
            ],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 998,
                    "end": 1018,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "273185840",
            "title": "Only-IF:Revealing the Decisive Effect of Instruction Diversity on Generalization",
            "text": "In this study, we evaluate the impact of cross-domain instruction diversification on large language models (LLMs) by comparing our approach with a baseline model trained exclusively on UltraInteract-SFT dataset [50]. UltraInteract-SFT is a collection of complex, multi-step reasoning problems emphasizing on math problem-solving, code generation, and logical reasoning, promoting robust reasoning and planning capabilities in LLMs. \n\nWhile UltraInteract-SFT primarily focuses on math and coding problems and contains a rich collection of those problems, its scope is limited to these domains. OpenOrca [26] and Alpaca, though sparse and varied, introduce broader instruction-following tasks. \n\nTo assess the effectiveness of cross-domain instruction diversity, we constructed a training set that includes a mixture of UltraInteract-SFT, OpenOrca, and Alpaca datasets. While UltraInteract-SFT is rich in math, coding and complex QA problems, it remains limited to primarily these domains despite its challenging and diverse nature within them. OpenOrca and Alpaca, on the other hand, introduce instruction-following tasks across a broader range of domains, enriching the training data with varied instruction types. We gauged the model's overall capabilities using the same set of benchmarks consisting of coding [1,6], math [16,11,8], knowledge (MMLU [15]), instruction following [55] and chain-of-thought reasoning [39] as [50] and computed average performance. \n\nTo reflect on its precision in instruction following, we adopted IF-Eval [55] benchmark, comprising over 500 prompts for rigorous instruction-following tests. We followed a core-set selection approach when curating datasets of various budgets. We finetune from a pre-trained Mistral-7B-v0.3 checkpoint for all results in table 3. \n\nIn the experiments, we study the effect of adding controlled quantities of different types of data to various pre-defined base budgets, and compare the performances across budgets to exhibit the advantage of dataset expansion along the dimension of improving diversity.",
            "score": 0.4275974210726782,
            "section_title": "Experimental Setup",
            "char_start_offset": 24445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 2066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0965576171875
        },
        {
            "corpus_id": "270559782",
            "title": "A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences",
            "text": "The reasoning abilities of Large Language Models (LLMs) are becoming a central focus of study in NLP. In this paper, we consider the case of syllogistic reasoning, an area of deductive reasoning studied extensively in logic and cognitive psychology. Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as content effects, avoid answering that no conclusion follows, align with human difficulties, and struggle with multi-step reasoning. We contribute to this research line by systematically investigating the effects of chain-of-thought reasoning, in-context learning (ICL), and supervised fine-tuning (SFT) on syllogistic reasoning, considering syllogisms with conclusions that support or violate world knowledge and with multiple premises. Crucially, we go beyond the standard focus on accuracy, with an in-depth analysis of the conclusions generated by the models. Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter can mitigate most reasoning biases while being consistent.",
            "score": 0.4275611372094479,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2012939453125
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Specifically, we conduct three groups of experiments including:\n\n\u2022 Difficulty of Samples for SFT and DPO: This group compares four distinct selection strategies: random selection (R), easiest samples with the lowest perplexity (E), hardest samples with the highest perplexity (H), and half easiest samples and half hardest samples (EH).\n\n\u2022 Amout of Samples for SFT and DPO: This involves four variants using different quantities of easiest samples from the original TULU-V2-mix dataset (i.e., 10K, 20K, 40K, and 80K) and from the original UltraFeedback dataset (i.e., 5K, 10K, 20K, and 40K).\n\n\u2022 Ratios between SFT and DPO Samples: This experiment utilizes a total of 15,000 samples for format alignment by adopting five different ratios between samples used in supervised fine-tuning and direct preference optimization, ranging from 1:2, 1:1, 2:1, 3:1, to 4:1.\n\nWe show the results of each group in Figure 4, Figure 5, and Figure 6.Firstly, using the easiest samples with the lowest perplexity can balance the domain capability and general capability best compared to other selection strategies.Secondly, it enhances both the domain and general capabilities simultaneously to a certain extent by continuously increasing the amount of SFT training samples.Conversely, this phenomenon is not observed when continuously increasing the amount of DPO training samples, rather, both remain in a state of fluctuation.Finally, we can see that the proportion of SFT and DPO data we selected (i.e., 2:1) can optimally balance the general and domain-specific capabilities.(Que et al., 2024;Ke et al., 2022), and continual fine-tuning, aiming to fine-tune general LLMs on a series of downstream tasks related to target domains (Razdaibiedina et al., 2023;Scialom et al., 2022;Luo et al., 2023a).Specially, continual pre-training updates LLMs with large and unlabeled domain-specific corpora, which mainly focuses on memorizing and injecting new knowledge into the parameters of LLMs.",
            "score": 0.42706289523613594,
            "section_title": "DETAILED ANALYSIS",
            "char_start_offset": 30094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 65,
                    "end": 336
                },
                {
                    "start": 338,
                    "end": 591
                },
                {
                    "start": 593,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 932
                },
                {
                    "start": 932,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1255
                },
                {
                    "start": 1255,
                    "end": 1410
                },
                {
                    "start": 1410,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 1579,
                    "end": 1595,
                    "matchedPaperCorpusId": "256105391"
                },
                {
                    "start": 1715,
                    "end": 1743,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 1743,
                    "end": 1764,
                    "matchedPaperCorpusId": "252815378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10540771484375
        },
        {
            "corpus_id": "264289051",
            "title": "Improving Generalization of Alignment with Human Preferences through Group Invariant Learning",
            "text": "Supervised fine-tuning baseline (SFT). Supervised fine-tuning is the first stage of RLHF, enabling the model to follow human instructions with a small amount of instructional data. We perform supervised fine-tuning on the base model using the datasets introduced in, treating the questions or queries in the samples as instructions. This encourages the model to generate truthful answers and prevents it from generating false answers with cross-entropy loss. The training data is formatted as follows: User: {instruction} Assistant: {response}. We trained both a general SFT model and an SFT model specifically for summarization based under different experimental settings. \n\nProximal policy optimization (PPO) (Schulman et al., 2017). After obtaining a well-trained reward model, the model can be updated using the feedback from the reward model. PPO is the core algorithm employed to achieve alignment with human preferences. In general dialogue and summarization, we employ the reward model trained above to train a policy separately that generates higher-quality responses as judged by humans. \n\nPPO with Kullback-Leibler divergence penalty (PPO w/ KL) (Ouyang et al., 2022). In Eq. ( 3), we optimize the policy model to maximize the reward objective while maintaining a fixed KL penalty constraint. The KL divergence term here plays two main roles. First, it acts as an entropy bonus, ensuring the diversity of generation and avoiding collapse into a single high-reward answer. Second, it makes sure that the output of the RL policy does not significantly stray from the distribution where the reward model is precise. \n\nDirect Preference Optimization (DPO) (Rafailov et al., 2023). Although RLHF can align the model with human preferences, it is a complex and often unstable process. It involves fitting a reward model that reflects human preferences, and then fine-tuning a large unsupervised language model using reinforcement learning to maximize this estimated reward without drifting too far from the original model. Instead of involving reward modeling, DPO directly optimizes the language model using preference data.",
            "score": 0.42701051586214367,
            "section_title": "C.2 BASELINES",
            "char_start_offset": 39000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1623
                },
                {
                    "start": 1626,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 1157,
                    "end": 1178,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2490234375
        },
        {
            "corpus_id": "271892126",
            "title": "API-Guided Dataset Synthesis to Finetune Large Code Models",
            "text": "Formally, the SFT process of the target model can be outlined as follows: for the specific domain  with context   , each task example (  ,   ) is utilized to update the model parameters. This update aims at minimizing the loss function that measures the disparity between the data distribution and the target model distribution, as expressed below: \n\nOverall, this function seeks to minimize the negative log-likelihood of the target output   given the context   and input   , with respect to the model parameters  .  SFT converges when the generated response \u0177 matches   , i.e., the distribution of fine-tuned model aligns with the task dataset distribution. Compared to other fine-tuning methods such as Reinforcement Learning from Human Feedback (RLHF) [52] or Direct Preference Optimization (DPO) [61], SFT is more efficient and effective, as it does not require a human preference dataset. Consequently, SFT becomes a standard procedure for developing high-quality general-purpose LLMs [5,52] and has proven invaluable for customizing these models across numerous domains, such as medicine [67], finance [10], and various other fields, significantly enhancing their applicability and effectiveness in specialized contexts. Notably, SFT methods can be further categorized into two main approaches: (1) full parameter supervised fine-tuning (SFT) and (2) parameter-efficient fine-tuning (PEFT). Although PEFT demonstrates high performance while using fewer parameters, studies [23,86] have shown that it primarily assists the model with response initiation and extracts most of the response from pre-trained knowledge. In other words, PEFT does not significantly contribute to the model's ability to acquire new knowledge. Therefore, in this study, we focus on the full parameter fine-tuning approach and refer to it as the SFT.",
            "score": 0.4269914910902065,
            "section_title": "Supervised Fine-tuning",
            "char_start_offset": 9884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1831
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 994,
                    "end": 997,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1095,
                    "end": 1099,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1109,
                    "end": 1113,
                    "matchedPaperCorpusId": "271745635"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1387939453125
        },
        {
            "corpus_id": "277103710",
            "title": "Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large Language Models with Sequences",
            "text": "To maintain the models' instruction-following ability, we mix CodeSeq with the latest post-training dataset Tulu3 (Lambert et al., 2025) for SFT. \n\nTulu3 is a comprehensive dataset and training framework developed by the Allen Institute to advance the post-training of LLMs. The Tulu3 dataset is designed to enhance language models' perfor-mance through SFT and reinforcement learning. It includes a mixture of data from various sources, covering a wide range of natural language processing tasks such as instruction following, mathematical reasoning, and code generation. \n\nDue to the timeliness of Tulu3, we ensure that it is not used for any backbone model training. During the training process, we removed samples longer than 5120 tokens and excluded all samples related to mathematics and code (since we focus on code and comprehensive reasoning tasks). Finally, we retain over 800k training samples of Tulu3. \n\nTo improve the models' reasoning ability while maintaining its other capabilities, particularly instruction-following ability, we calculate the average number of tokens in the Tulu3 and CodeSeq datasets. We assign a weight ratio of 5:1 to these two datasets for mixed training. During training, we wrap all inputs and outputs with chat templates to prevent the loss of instruction-following capabilities.",
            "score": 0.42677306828584227,
            "section_title": "A.5.2 Mix Training Details",
            "char_start_offset": 24306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 148,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 572
                },
                {
                    "start": 575,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11993408203125
        },
        {
            "corpus_id": "277786647",
            "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients",
            "text": "We investigate gradient behaviors under the most widely adopted SFT approach. Each data point in an SFT dataset D consists of a pair (x, y), where x is the instruction and y is the corresponding response. For the reasoning data, y concatenates both thinking tokens and response tokens. Let p \u03b8 be a large language model with parameters \u03b8. Under the SFT paradigm, p \u03b8 is finetuned on each pair (x, y) by minimizing the following loss, where y j denotes the j-th token of y, y <j denotes the preceding tokens, and l is the total length of y: \n\nA higher nuclear norm indicates a larger overall gradient scale, implying that the model parameters at that layer are being updated more significantly, further indicating a potential distribution shift between the response and the model to be trained. \n\nEffective Rank. We measure how uniformly the singular values of G X,i are distributed by the effective rank R X,i . We normalize the singular values and formulate the effective rank as \n\nIf only a few singular values are large (i.e., the gradient is concentrated in just a few directions), the effective rank is small. If many singular values all contribute significantly, the effective rank is relatively larger. It measures how diverse the directions of the gradient are. A higher effective rank indicates the gradient is spread out over more directions, suggesting richer updates, whereas a smaller value means that only a few directions dominate the gradient directions.",
            "score": 0.42594177268491795,
            "section_title": "Methodology 2.1 Preliminaries",
            "char_start_offset": 6621,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1470
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06707763671875
        },
        {
            "corpus_id": "275358199",
            "title": "Reasoning-Enhanced Self-Training for Long-Form Personalized Text Generation",
            "text": "this experiment are reported in Table 1. These indicate that supervised fine-tuning on the generated reasoning-enhancement data from a larger model only statistically significantly improves performance on LongLaMP-3. However, there is a performance drop on the rest of the tasks, with the model performing worse than the SFT on average across all datasets, where on LongLaMP-2 this drop is statistically significant. However, on average, there is no statistically significant difference between this approach and SFT. Note that this approach underperforms compared to both methods that incorporate self-training. This observation suggests that, as discussed in our motivation, training solely on generated reasoning data is suboptimal as there is no alignment between these reasoning paths and the user's preferences for personalized text generation. \n\nHow does self-training alone affects the performance? We trained the LLM with ReST-EM (Singh et al., 2024), similar to our approach for self-training but without considering reasoning enhancement. This approach operates similarly to ours but does not involve reasoning over the personalized context. The results of this experiment are reported in Table 1 with the model name ReST-EM. The results indicate that self-training significantly improves performance on LongLaMP-2 and LongLaMP-4 over both SFT and SFT with Reasoning-Enhancement. Although it improves results on LongLaMP-1 and LongLaMP-3, these improvements are not statistically significant. More-over, it does not outperform SFT with Reasoning-Enhancement on LongLaMP-3. However, on average, this approach significantly outperforms both baselines. Note that this model is unable to outperform REST-PG on any of the tasks, with significant differences in performance observed in 3 out of 4 tasks and in the overall average performance. This observation suggests that self-training is a promising approach for enhancing performance in personalized text generation. However, without explicitly considering the user's implicit preferences or writing style, the improvement on personalized text generation tasks is limited. \n\nHow does the exploration budget affect the performance of REST-PG? We apply our method using different exploration budgets m during the expectation step, generating 8, 16, 32, 64, and 128 outputs per input and train the LLM for one iteration on them.",
            "score": 0.4257472583392582,
            "section_title": "Main Findings",
            "char_start_offset": 20453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 939,
                    "end": 959,
                    "matchedPaperCorpusId": "266163375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053802490234375
        },
        {
            "corpus_id": "263310649",
            "title": "Unsupervised Large Language Model Alignment for Information Retrieval via Contrastive Feedback",
            "text": "Recently, LLMs are emerged and boost many natural language processing tasks. The architecture of LLMs, particularly the Transformer [55], leads to significant improvements in capturing textual semantics. This advancement empowers many influential models such as BERT [17] and GPT [43]. These models pave the way for subsequent advancements like GPT-2 [44] and GPT-3 [7], with increasing model sizes and capabilities. The training pipeline of LLMs also earned significant attention in recent years due to its pivotal role in enabling models like GPT to exhibit remarkable language understanding and generation capabilities. Pre-training is a cornerstone of training LLMs and involves training the model on a massive corpus to learn linguistic patterns and structures, leveraging the tasks such as masked language modeling [17], next token prediction [43] and etc. By utilizing large-scale pre-training, LLMs acquire a general understanding of language, making them available for various downstream tasks. Supervised Fine-Tuning (SFT) involves training LLMs on task-specific datasets with labeled examples. This stage adapts the generic linguistic knowledge acquired during pre-training to specific tasks, such as sentiment analysis [20], text classification [19,21], and dialogues [41]. Alignment technique facilitates LLMs in learning from the generated responses and environmental feedback, thereby aligning the capability with the desired attribute. The environment feedback could be from human [41] or other models [2]. This approach has shown promise in improving the helpfulness and harmlessness of LLMs.",
            "score": 0.42571305349465244,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1609
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 351,
                    "end": 355,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 366,
                    "end": 369,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "233236941"
                },
                {
                    "start": 1261,
                    "end": 1264,
                    "matchedPaperCorpusId": "235792270"
                },
                {
                    "start": 1280,
                    "end": 1284,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1497,
                    "end": 1501,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4150390625
        },
        {
            "corpus_id": "265608902",
            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
            "text": "We propose a simple, tuning-free alignment method called URIAL (Untuned LLMs with Restyled In-context ALignment), which effectively aligns base LLMs without tuning their weights (Sec. 3). URIAL leverages in-context learning (ICL) through prompting with just a few carefully curated stylistic examples and a carefully designed system prompt to achieve impressive alignment results. We craft the in-context examples to begin by affirming the user query and introducing background information, then proceed to enumerate items or steps with comprehensive details, and finally conclude with an engaging summary that includes safety-related disclaimers. Surprisingly, we find that such a straightforward baseline method can significantly reduce the performance gap between base LLMs and aligned LLMs. To rigorously evaluate different alignment methods, we design a multi-aspect, interpretable evaluation protocol, detailed in Sec. 4. We create a dataset named just-eval-instruct which contains 1,000 diverse instructions from 9 existing datasets, such as those used by AlpacaEval (Li et al., 2023a), MT-bench (Zheng et al., 2023), andLIMA (Zhou et al., 2023). Our analysis encompasses six dimensions of LLM outputs: \n\nhelpfulness, clarity, factuality, depth, engagement, and safety. Our extensive results indicate that URIAL, using as few as three constant incontext examples, can effectively align base LLMs. Remarkably, URIAL surpass the LLMs aligned with SFT or SFT+RLHF on strong base LLMs such as Mistral-7b (Jiang et al., 2023a) and Llama-2-70b (Touvron et al., 2023), as reported in Fig. 1 and Tab. 1. \n\nThe surprisingly strong performance of URIAL not only further substantiates the superficial alignment hypothesis, but also prompts us to rethink the current research on alignment. To deepen our understanding of LLMs, we believe that it is essential to accurately distinguish which knowledge and reasoning capabilities originate from pre-training as opposed to those that must be acquired through alignment tuning. In this vein, our contributions in this work can support future research in the analysis and alignment of base LLMs.",
            "score": 0.42547145756104393,
            "section_title": "Preprint",
            "char_start_offset": 3108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2135
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357666015625
        },
        {
            "corpus_id": "270870413",
            "title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning",
            "text": "Therefore, it is necessary to evaluate the model performance on more academic benchmarks. In this subsection, we present the results on six benchmarks, evaluating various model abilities including explicit instruction following [Zhou et al., 2023], general knowledge [Rein et al., 2023], multitask language understanding [Hendrycks et al., 2020], commonsense reasoning [Zellers et al., 2019], human falsehoods mimicking [Lin et al., 2021], and math word problem-solving [Cobbe et al., 2021]. We compare our INPO (PM) with the SFT baseline, iterative DPO (PM), and SPPO (PM). The results are shown in Table 2. Interestingly, compared to the SFT baseline, all three alignment methods exhibit performance improvements on these benchmarks. A potential reason for this is that during the alignment stage, the alignment methods more effectively leverage the model's internal knowledge and abilities, which were introduced during the pre-training and SFT stages. Additionally, both INPO and iterative DPO incorporate KL regularization, which prevents the learned policy from deviating significantly from the reference policy, thereby avoiding performance degradation. And the superior results of INPO and SPPO demonstrate the advantage of considering general preferences. In this subsection, we conduct an ablation study to examine the benefits of including the KL regularization term in the game objective. The results are shown in Table 3. We observe that INPO with KL regularization (INPO w/ KL) generally outperforms its counterpart without KL regularization (INPO w/o KL) by a clear margin. This indicates regularizing our policy towards the reference policy is beneficial for the alignment performance.",
            "score": 0.4250524719292317,
            "section_title": "Main Results",
            "char_start_offset": 22365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.292236328125
        },
        {
            "corpus_id": "267199815",
            "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
            "text": "(Team, 2023) is a multilingual trillion-parameter foundation model trained on over a trillion tokens of data. Based on this foundation, the model utilizes high-quality human-annotated dialogue data combined with RLHF to respond to complex instructions during human interactions, exhibiting responses that align with human ethics and values. \n\nUL2 (Tay et al., 2022) is an encoder-decoder model trained utilizing a mixture of denoisers objectives, surpassing T5 on numerous benchmarks. \n\nQwen (Bai et al., 2023a) is trained on large-scale and diverse datasets, with a primary focus on Chinese and English. It employs SFT and RLHF techniques for alignment, resulting in dialogue models like Qwen-Chat. \n\nChinchilla (Hoffmann et al., 2022)",
            "score": 0.42491445518431076,
            "section_title": "InternLM",
            "char_start_offset": 39744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 340
                },
                {
                    "start": 343,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 736
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 365,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38427734375
        },
        {
            "corpus_id": "270258300",
            "title": "Bayesian WeakS-to-Strong from Text Classification to Generation",
            "text": "With the increase in computing power and the amount of training data available, the capabilities of large language models (LLMs) have been continuously brought closer to humans in many aspects. Despite their impressive performance, the preferences and values of pre-trained LLMs do not always align with humans, and dedicated approaches are needed to tackle the problem. Based on large-scale instruction datasets, supervised finetuning (SFT) encourages LLMs to follow human instructions more strictly and respond more safely (Wei et al., 2022). Reinforcement learning (RL) is commonly applied to such alignment. By collecting model output values and the corresponding human feedback, the model can be finetuned by RL to avoid generating undesirable outputs (Ziegler et al., 2019;Bai et al., 2022a;Ouyang et al., 2022;Nakano et al., 2021;Askell et al., 2021). \n\nSince no current model has yet surpassed human intelligence, alignment methods, such as SFT and RL from human feedback (RLHF), remain effective. However, it is worthwhile considering future scenarios where artificial intelligence (AI) might surpass human intelligence in all aspects. Would the current alignment methods still be effective for such super AI models? How could humans supervise the super AI? To simulate this future scenario, an analogy situation is designed that downgrades both sides: using a weak model to simulate humans and a strong model to simulate future super AI (Burns et al., 2023), which is termed as superalignment. It has been demonstrated that adding a simple auxiliary loss can achieve effective Weak-to-Strong generalization, even if the weak model's supervision contains many errors, which offers hope of achieving superalignment. Nonetheless, this is just the beginning of exploring along the path of Weak-to-Strong. \n\n\u2022 We propose to generalize both Weak-to-Strong and WeakS-to-Strong from text classification to generation tasks, extending their scope from content regulation to content generation. \n\n\u2022 When applied to text generation, a token-level probability estimation is proposed to achieve soft labels for strong model training. We also propose the modified DPO algorithm under the Bayesian WeakS-to-Strong framework to further improve text generation performance.",
            "score": 0.4248514079682154,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1810
                },
                {
                    "start": 1813,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 797,
                    "end": 817,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Large Language Models (LLMs) (Zhao et al., 2023) have revolutionized the field of natural language processing (NLP) (Brown et al., 2020;OpenAI, 2023), showing exceptional capabilities such as instruction following (Ouyang et al., 2022a;Taori et al., 2023) and complex reasoning (Wei et al., 2022;Wang et al., 2023a).However, due to their limited exposure to relevant data, such general LLMs still considerably lag behind in specific domains requiring professional knowledge.This situation has necessitated the effective adaptation of general-purpose LLMs to specific domains (e.g., mathematics and code), called domain adaptation of LLMs (Guo & Yu, 2022).\n\nIn essence, tailoring general LLMs to specific domains requires adaptation in two main aspects, namely knowledge learning (acquiring and leveraging the necessary domain knowledge) and format alignment (responding to the user in an expected output form) (Jiang et al., 2024;Zhou et al., 2023).Specially, knowledge learning can be further fulfilled via knowledge memorization and utilization.In practice, domain adaptation of LLMs typically involves three consecutive stages (Rozi\u00e8re et al., 2023;Azerbayev et al., 2023), i.e., pre-training, instruction tuning, and alignment, where the first stage is primarily aimed at knowledge memorization and the other two stages are mainly focused on knowledge utilization and format alignment.However, at the pre-training stage, knowledge memorization based on raw domain-specific corpus would be somehow inefficient without eliciting the acquired knowledge according to task goals (Jiang et al., 2024).Despite that some studies incorporate instruction data for pre-training, they often rely on proprietary models to synthesize high-quality instructions at scale (Cheng et al., 2024;Wang et al., 2024), which may not be that easy without extensive fine-tuning experiences.Another issue is that learning to master knowledge utilization and format alignment in the instruction tuning and alignment stages might lead to suboptimal performance, due to the fact that the two goals can be divergent in model optimization (Ren et al., 2024).",
            "score": 0.42484614497329354,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 316,
                    "end": 474
                },
                {
                    "start": 474,
                    "end": 655
                },
                {
                    "start": 657,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1389
                },
                {
                    "start": 1389,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1868
                },
                {
                    "start": 1868,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 930,
                    "end": 948,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.219482421875
        },
        {
            "corpus_id": "272702658",
            "title": "Can AI Rely on the Systematicity of Truth? The Challenge of Modelling Normative Domains",
            "text": "A pre-trained model accordingly needs to be fine-tuned for specific tasks, such as instruction following, conversation, question-answering, summarisation, translation, or coding. The purpose of the post-training phase is to refine the somewhat rough, \"pretrained\" model to imitate the patterns that characterise exemplary completions of these various tasks. This can be thought of as a form of imitation learning. \n\nA standard technique for this is \"supervised fine-tuning\" (SFT), where the objective typically remains next-token prediction, but the training data is swapped for smaller, task-specific datasets of input-output pairs that have been carefully curated, structured, and labelled by humans. These could be ideal examples of customer service dialogues, for instance, or exemplary pairs of medical questions and answers. By training the model to do next-token prediction on those ideal examples, it is implicitly encouraged to emulate the virtues of these examples-not just tone, length, and style, but also characteristics like epistemic humility or nuance. \n\nAgain, consistency and coherence are not themselves training objectives in SFT. Yet the exemplary input-output pairs are presumably chosen, among many other reasons, because they are paradigmatic examples of consistency and coherence. By being fine-tuned to follow these templates, the model is therefore indirectly being trained to be more sensitive to consistency and coherence when it generates its own responses to user prompts. \n\nHowever, the considerations of consistency and coherence that inform the assembly of task-specific datasets remain limited to consistency and coherence within a given input-output pair and to that input-output pair's consistency and coherence with the truth-or rather, what those who put together the datasets took to be the truth. And it is not clear that training to do next-token prediction on datasets that exhibit these features does much to train an LLM to leverage the consistency and coherence of the fabric of fact more widely. It may make the LLM more likely to produce internally consistent and coherent outputs, but there remains an important difference between producing an internally consistent and coherent output and relying on the consistency and coherence of the wider fabric of fact to generate that output.",
            "score": 0.42469560917120736,
            "section_title": "The Role of Consistency and Coherence in Training LLMs",
            "char_start_offset": 20504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 413
                },
                {
                    "start": 416,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2332
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1015625
        },
        {
            "corpus_id": "269983424",
            "title": "Disperse-Then-Merge: Pushing the Limits of Instruction Tuning via Alignment Tax Reduction",
            "text": "Supervised fine-tuning (SFT) on instruction-following corpus is a crucial approach toward the alignment of large language models (LLMs). However, the performance of LLMs on standard knowledge and reasoning benchmarks tends to suffer from deterioration at the latter stage of the SFT process, echoing the phenomenon of alignment tax. Through our pilot study, we put a hypothesis that the data biases are probably one cause behind the phenomenon. To address the issue, we introduce a simple disperse-then-merge framework. To be concrete, we disperse the instruction-following data into portions and train multiple sub-models using different data portions. Then we merge multiple models into a single one via model merging techniques. Despite its simplicity, our framework outperforms various sophisticated methods such as data curation and training regularization on a series of standard knowledge and reasoning benchmarks.",
            "score": 0.42463111429904205,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470703125
        },
        {
            "corpus_id": "267413204",
            "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate",
            "text": "Furthermore, to evaluate the effectiveness of LOT in fine-tuning pretrained large language models (LLMs), we conduct supervised fine-tuning (SFT) experiments using LLaMA-1 [96] and LLaMA-2 [97] on two mathematical reasoning benchmarks: GSM8K [19] and MATH [35]. \n\nWe compare LOT to in-context learning (ICL) [9] and SFT. Following Touvron et al. [97], the number of in-context examples is 8 for GSM8K and 4 for MATH. The SFT configuration follows Yue et al. [105], and we fine-tune the LLaMA models for four epochs. In LOT , the teacher and student models share the same architecture for simplicity. The models are trained for two epochs in LOT to match the total training steps in SFT for fair comparison. All other configurations are consistent with those used in SFT. More implementation details are described in Appendix D. We measure the accuracy of greedy decoding results in Table 2, and we observe that LOT enhances reasoning abilities on all architecture and dataset choices. This indicates the competence of LOT in improving the fine-tuning performance with a computational cost comparable to SFT.",
            "score": 0.42450399426819585,
            "section_title": "Supervised Fine-tuning",
            "char_start_offset": 19286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1107
                }
            ],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 311,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 458,
                    "end": 463,
                    "matchedPaperCorpusId": "261696697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "Proposed Solution: Simplifying responses in the IT dataset. One possible solution to mitigate the hallucination problem caused by style imitation is to employ LFT instead of SFT, as LFT tends to learn only stylistic elements. However, as seen in Finding 3 of Section 3, LFT doesn't scale effectively, and SFT may act as a knowledge enhancer at larger scales (evident from LLaMa-2 7B LFT Tulu-V2-Mix 326k in Fig. 3). We propose an approach leveraging the strengths of both LFT and SFT. Given that LLMs possess ample pre-training knowledge for accurate response generation (Finding 4, Section 3) but struggle with comprehensive answering in pattern-copying mode, we hypothesize that SFT on IT datasets with concise but accurate responses can reduce hallucinations. We employ GPT-4 to simplify LIMA 1K , creating concise responses by removing extraneous information and term it as LIMA-Simple 1k . This model is compared with the original LLaMa-2 7B LFT LIMA 1K in terms of hallucination tendencies in Fig. 5. The results show a significant reduction in hallucinations beyond being concise. Quantitative results in Table 13 (rows 49-50) confirm that while the simplified model may lack depth, it surpasses the original in factuality and helpfulness. We additionally illustrate two special cases: (1) Instruction 1 illustrates a case where the model actually possessed enough knowledge to answer comprehensively and provides a more factual response with greater depth than its simplified instruction. \n\n(2) Instruction 2 demonstrates how adopting pattern copying from datasets of varied types results in divergent responses to a single open-ended reasoning-based instruction. Fig. 6 illustrates the results of a human study conducted by four expert human evaluators who manually compared the responses of LLaMa-2 7B SFT LIMA 1K and LLaMa-2 7B SFT LIMA-Simple 1k against 5 pre-defined categories. We show that simplifying responses in IT datasets for mod- \n\nThe longest word in the English language is probably pneumonoultramicroscopic. It refers to a type of lung disease.",
            "score": 0.4243743624238986,
            "section_title": "Pattern Copying (often) Hurts Performance",
            "char_start_offset": 23251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2068
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "277781185",
            "title": "A Short Survey on Small Reasoning Models: Training, Inference, Applications and Research Directions",
            "text": "Supervised Fine-Tuning (SFT) is a standard supervised learning technique that aligns models with task-specific instructions. With the availability of high-quality reasoning datasets, it is natural to extend SFT to CoT-based SFT. In this paradigm, SRMs are tasked with explicitly generating intermediate reasoning steps and ultimately providing outputs for input instructions, thereby enhancing their reasoning abilities to tackle complex tasks. The report (DeepSeek-AI, 2025) indicates that CoT-based SFT approach combined with knowledge distillation produces strong SRMs. However, SFT is limited by its reliance on high-quality labeled datasets, which are costly and time-consuming to produce. For example, the report shows that the dataset size for training distilled DeepSeek-R1 models is 800K. In addition, fine-tuning SRMs remains more computationally expensive than general language models, especially when the training sequence (with CoT trajectories) is long. which necessitates parameter-efficient learning (Wang et al., 2023a;Han et al., 2024), such as LoRA (Hu et al., 2022), QLoRA (Dettmers et al., 2023), AdaLoRA (Zhang et al., 2023a), etc. Another pitiful of SFT is that it forces the SRMs to \"exploit\" the training dataset Training SRMs can be streamlined through a combination of data annotation and advanced training algorithms. Leveraging automatic annotation and knowledge distillation techniques is increasingly important with the release of strong models. CoT-based SFT is a good starting point to train SRMs based on the collected and annotated datasets. Advanced RL methods offer effective training by evaluating intermediate reasoning steps and reducing computational needs. There is no widely accepted consensus on which RL algorithm performs the best for SRMs, while the successful story of DeepSeek-R1 favors outcome reward supervision (DeepSeek-AI, 2025), treating PRMs and MCTS as \"unsuccessful attempts\". We suggest that it might be too early to draw the conclusion especially for SRMs. Researchers can accelerate the research by utilizing datasets and RL repositories that facilitate SRM development.",
            "score": 0.42424783976392744,
            "section_title": "Supervised Learning",
            "char_start_offset": 9178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1068,
                    "end": 1085,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1093,
                    "end": 1116,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1126,
                    "end": 1147,
                    "matchedPaperCorpusId": "257631760"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11968994140625
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "by the fact that simple fine-tuning over a small number of examples allows the model to compete with the state of the art, highlighting the relative importance of pretraining over large-scale instruction tuning and reinforcement learning approaches [ Page 5]. Please note that the information provided is based solely on the search results and is limited to the content available in those pages. token cost : 781 Response from chatpdf.com The hypothesis proposed in this paper is referred to as the \"Superficial Alignment Hypothesis.\" According to this hypothesis, alignment in language models is primarily about learning the style or format for interacting with users. It suggests that by fine-tuning a pretrained language model with a relatively small set of examples, the model can achieve strong performance in terms of alignment and generate responses in the style of a helpful AI assistant. Response from Claude+ 100K (GT of Evaluation.) The paper coins the term \"Superficial Alignment Hypothesis\" to refer to their hypothesis about alignment: \"A model's knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.\" In other words, their hypothesis is that alignment is largely about teaching the model stylistic and formatting guidelines for generating appropriate responses, whereas most of the underlying knowledge and capabilities come from pre-training. They test this hypothesis by fine-tuning a large language model on just 1,000 examples, and show that it is able to produce responses that are comparable or better than models trained on much more data. This supports their hypothesis that relatively little data is needed for alignment if the model has a strong pre-trained foundation.\n\nThe key idea is that alignment is \"superficial\" in the sense that it is mostly about teaching the model which style or formats to use, rather than imparting large amounts of new knowledge. So in summary, the \"Superficial Alignment Hypothesis\" refers to their hypothesis that alignment requires relatively little data because it is more about teaching surface-level stylistic conventions, whereas the bulk of the model's knowledge comes from pre-training.",
            "score": 0.4239574310052444,
            "section_title": "C. Example of outputs",
            "char_start_offset": 43805,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321044921875
        },
        {
            "corpus_id": "265043685",
            "title": "Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment",
            "text": "In this section, we review the related work in the two aspects, namely reinforcement learning from human feedback and alignment without reinforcement learning. \n\nReinforcement learning from human feedback Large-scale pre-training empowers large language models (LLMs) to acquire extensive knowledge, underscoring their remarkable potential across diverse tasks (Brown et al., 2020;Kojima et al., 2022;Zhang et al., 2022;Chowdhery et al., 2022). Nonetheless, models exclusively focus on next token prediction in pre-training phrase, while do not consider human preferences. Consequently, this gives rise to unexpected behaviors like harmful or inaccurate information, and emphasizes the necessity to align language models with human preferences. The current mainstream approaches (Ouyang et al., 2022) to better harness the capabilities of LLMs include supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To be specific, this involves three stages: firstly, using SFT to enable the model to better follow human instructions; subsequently, training a reward model (RM) using human preference data; and ultimately, tune the model to maximize the reward through the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm. Furthermore, there are works exploring enhancement for this process (Ramamurthy et al., 2022;Lightman et al., 2023;Lee et al., 2023). However, RLHF presents challenges due to complex coding and hyper-parameters selecting. Besides, it requires loading three to four models simultaneously, resulting in high memory usage. These challenges propel researchers to explore alternative approaches to align language models with human feedback. \n\nAlignment without reinforcement learning Several studies are based on the rationale that language models have already acquired comprehensive knowledge during the pre-training, and only high-quality supervised fine-tuning data is required for further tuning (Zhou et al., 2023).",
            "score": 0.42391337713307947,
            "section_title": "RELATED WORK",
            "char_start_offset": 4413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 162,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1979
                }
            ],
            "ref_mentions": [
                {
                    "start": 361,
                    "end": 381,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 779,
                    "end": 800,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1332,
                    "end": 1357,
                    "matchedPaperCorpusId": "252693405"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "277633781",
            "title": "Thinking Out Loud: Do Reasoning Models Know When They're Right?",
            "text": "Moreover, Tian et al. (2023); Xiong et al. (2024); Yang et al. (2024) have highlighted that model calibration is highly sensitive to prompt design, underscoring the fragility and lack of robustness in current approaches to verbalized uncertainty estimation in instruction-tuned models. While prior work suggests that human-inspired prompting strategies, such as chain-of-thought (CoT) or TopK, can enhance calibration, we extend this line of inquiry by investigating whether LRMs, which inherently embed long CoT chains and self-reflective behaviors, can further improve calibration. \n\nIn this work, we conduct a comprehensive empirical study to assess the calibration of LRMs across a diverse set of benchmarks spanning mathematics, factuality, scientific reasoning, and general reasoning. To isolate the effects of different training strategies, we evaluate models that share the same base architecture but vary in their post-training procedures. Our analysis focuses on three distinct model categories: (1) instruct models, trained mainly using SFT and general RL for alignment purposes; \n\n(2) SFT reasoning models, fine-tuned primarily on long CoT outputs generated by stronger reasoning models; and (3) RL reasoning models, trained with reasoning RL to explicitly optimize reflective reasoning behaviors. An overview of these training pipelines is illustrated in Figure 1. Through systematic pairwise comparisons, our key findings are as follows: \n\n\u2022 On reasoning-heavy benchmarks, both SFT reasoning models and RL reasoning models consistently outperform instruction-tuned models in terms of both task accuracy and calibration quality. \n\n\u2022 While SFT on reasoning traces leads to substantial performance gains, RL offers additional improvements in calibration, even when the RL training domain (e.g., math) differs from the evaluation domain (e.g., science), highlighting its generalizability. \n\n\u2022 On factuality-focused benchmarks, calibration improvements are less consistent: smallscale SFT reasoning models often exhibit worse calibration than instruction-tuned models, while RL reasoning models generally show some recovery.",
            "score": 0.4238737348250413,
            "section_title": "Reasoning Distillation",
            "char_start_offset": 1906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1451
                },
                {
                    "start": 1454,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 10,
                    "end": 28,
                    "matchedPaperCorpusId": "258865733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2432861328125
        },
        {
            "corpus_id": "271916117",
            "title": "CIPHER: Cybersecurity Intelligent Penetration-Testing Helper for Ethical Researcher",
            "text": "Techniques like instruction fine-tuning (FLAN [20]) and model mimicry (Orca [21]) further improve reasoning capabilities. Despite advancements, challenges in LLM reasoning persist, motivating ongoing research [7,22]. \n\nLLM-Based Chatbots: LLM-based chatbots like ChatGPT excel in customer support, education, and complex problem-solving by synthesizing large volumes of information into detailed responses. However, they lack the specialized knowledge required for offensive penetration testing [23]. \n\nSupervised Fine-Tuning: Supervised fine-tuning enhances model performance on domain-specific datasets, particularly in areas like penetration testing, ensuring accurate application of specialized language. Unlike Retrieval-Augmented Generation (RAG), finetuning improves the model's domain comprehension [24]. \n\nLLM Alignment: Efforts to align language models with human values focus on ensuring these models exhibit traits such as helpfulness and truthfulness. Reinforcement Learning with Human Feedback (RLHF) fine-tunes large language models (LLMs) to achieve this alignment. This process involves training a reward model to predict human preferences and then using algorithms like Proximal Policy Optimization (PPO) for fine-tuning. Although PPO generally yields better results, Direct Preference Optimization (DPO) simplifies the process by fine-tuning directly with human-labeled preferences [25,26]. \n\nIncorporating Domain-Specific Knowledge: Domain-specific knowledge enhances LLM accuracy in specialized fields like medicine [27] and cybersecurity. Techniques such as Domain-Adaptive Pretraining (DAPT) and adaptive fine-tuning (AdaptLLM) are crucial for developing specialized models tailored for specific tasks, leveraging domain-specific datasets for improved insights [24,28].",
            "score": 0.4235674558486071,
            "section_title": "Background and Related Works",
            "char_start_offset": 7866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 500
                },
                {
                    "start": 503,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1792
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1405,
                    "end": 1408,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "260775656",
            "title": "Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length",
            "text": "In recent years, the sphere of Artificial Intelligence (AI) has witnessed the prominent emergence of Reinforcement Learning from Human Feedback (RLHF) as a vital tool in the development of large language models (LLMs). This novel framework serves to significantly enhance the safety and align the functioning of AI systems more closely to human norms and expectations. Despite these clear benefits, the process of evaluating the reward model within an RLHF framework presents an exceedingly challenging task. Besides that, diffulties are also due to the inherent complexity and instability traits of RLHF, further amplified by its extreme sensitivity to variations in hyperparameters. Such difficulties are particularly pronounced in tasks of a complex or nuanced nature, subsequently complicating the application and fine-tuning of Proximal Policy Optimization (PPO) within such contexts. \n\nOur research explores a unique approach to this multifaceted issue by employing a novel task using \"Golden\" as a reward model. The primary objective of this strategic deployment of PPO is to meticulously manipulate the output tokenizer length, shown in Fig. 1. In this context, the overarching aim is not merely to expand the knowledge base of the PPO model, but to facilitate its training in the development of a distinctive response pattern -a triumph that remains elusive within the realm of Supervised Fine-Tuning (SFT). \n\nWhile PPO has been successful in manipulating output tokenizer length to some degree in our experiment, it continues to exhibit shortcomings in the accurate and comprehensive interpretation of input requirements -a challenge which is handled efficiently by SFT. As we navigate through the various dimensions of this research paper, we intend to offer an in-depth exploration into these subjects. Our discussion will shed light on the intricacies of RLHF, PPO, and SFT with an example task, effectively contextualizing these within the broader realm of large language model development. The pivotal role of RLHF in shaping the impact of large language models (LLMs) cannot be overstated. Its contribution towards mitigating output toxicity and guiding output styles aligns closely with human values, making it integral in the creation of secure AI systems.",
            "score": 0.42355101010673113,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 889
                },
                {
                    "start": 892,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1416
                },
                {
                    "start": 1419,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.318115234375
        },
        {
            "corpus_id": "268264604",
            "title": "On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models",
            "text": "Considering the costs of SFT and RL, and the fact that most mainstream LLMs are black-box, fine-tuning based alignment approaches become increasingly unaffordable or infeasible.Therefore, another popular paradigm, In-Context Learning (ICL) based alignment, has attracted more attention.This approach leverages the massive knowledge and instruction-following capabilities of LLMs obtained during the pretraining and instruction tuning phases.By directly providing value instructions or K few-shot examples {x i , y i } K i=1 , ICL constrains the generation of the LLM to align with human values, avoiding additional training.In fact, ICL can also be regarded as a kind of imitation learning.By incorporating a shared prompt concept (Xie et al., 2021), c, e.g., values, minimizing the divergence between p(y, x, c) and \u03c0 \u03b8 (y, x, c) can transformed to optimizing:\n\nOmitting the KL regularization term and freezing parameters \u03b8, imitation learning can be viewed as implicit Bayesian inference, inferring the latent concept from given examples x, y, and driving the LLM to generate a connected response.Concretely, the simplest way is to prompt LLMs to generate responses that adhere to human preferences (Ganguli et al., 2023).Han (2023) further retrieves and includes relevant demonstration examples from SFT data and concatenates them with the input prompt.Lin et al. (2023) find that aligned LLMs primarily learn language styles matching human preferences, providing evidence in support of the \"Superficial Alignment Hypothesis\" (Zhou et al., 2023).Based on such findings, they propose to utilize three consistent stylistic examples and a system prompt for alignment.Considering the ever-changing and diverse human values in the real world, On-the-fly Preference Optimization (OPO) (Xu et al., 2023b) leverages a Retrieval-Augmented Generation (RAG) to achieve dynamical alignment.In addition, the generate-then-refine schema (Gou et al., 2023) first generates initial responses and then enables LLMs to verify and rectify their own output.",
            "score": 0.4235194532054373,
            "section_title": "In-Context Alignment",
            "char_start_offset": 33128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 177,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 441
                },
                {
                    "start": 441,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 861
                },
                {
                    "start": 863,
                    "end": 1099
                },
                {
                    "start": 1099,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1549
                },
                {
                    "start": 1549,
                    "end": 1667
                },
                {
                    "start": 1667,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 2040
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1776123046875
        },
        {
            "corpus_id": "276617565",
            "title": "FCoT-VL:Advancing Text-oriented Large Vision-Language Models with Efficient Visual Token Compression",
            "text": "In this section, we describe supervised finetuning(SFT) aimed at improving the student model's performance in text-oriented tasks. We accept many open-source datasets reported in previous VLLMs (Chen et al., 2024b), covering a variety of downstream tasks. However, we find that many of these public datasets are not formatted in an instruction style. To overcome this, we leverage distillation from teacher models to acquire the conversation style. Subsequently, we prompt the InternLM2.5-7B (Cai et al., 2024) to rewrite the instruction datas with the tone of the teacher model. Moreover, we observe this rewriting method facilitates fast and stable training, which may be attributed to the strong alignment with the teacher model. \n\nChain-of-Thought pipeline. For reasoning tasks like math, chart reasoning and calculation problems, we leverage Rejection Sampling (RS) to expand the SFT dataset using larger and stronger multimodal language models. Specifically, for the question q, we employ RS to generate a new response with COT, obtaining the reasoning steps R cot and final answer R ans , respectively. We use rule-based verifications that verify the correctness of the concluded answer R ans for the given problem q based on the ground truths. We find that the mixture of RS-augmented and vanilla data significantly enhances reasoning capabilities. For example, our FCoT-VL-2B, with half visual tokens retained, achieves a score of 58.96 on MathVista (Lu et al., 2024) \n\nWhere M mge is the merged model, \u03b8 base is typically used as the final model, and \u03b1 i is the weight for the difference between the checkpoints \u03b8 cpt i and the base model. n is set as 5. The goal is to determine the optimal fusion weights, formulated as: \n\nRather than relying on costly heuristic algorithms, we use Shapley values (Sundararajan and Najmi, 2020), to fairly serve the merge weight \u03b1 i to each checkpoint M i based on its contribution to the final model performance. The weighted combination of checkpoints thus optimizes the final model's performance based on their individual contributions. \n\nComputation Complexity.",
            "score": 0.42258957737646263,
            "section_title": "Post-Train",
            "char_start_offset": 12061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1476
                },
                {
                    "start": 1479,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1732
                },
                {
                    "start": 1735,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2084
                },
                {
                    "start": 2087,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 214,
                    "matchedPaperCorpusId": "269362546"
                },
                {
                    "start": 1459,
                    "end": 1476,
                    "matchedPaperCorpusId": "264491155"
                },
                {
                    "start": 1809,
                    "end": 1839,
                    "matchedPaperCorpusId": "201315826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "269983441",
            "title": "360Zhinao Technical Report",
            "text": "The widely used Opencompass (OpenCompass-Contributors, 2023) framework proves inconvenient for data strategy explorations.It is inherently unstable and insensitive to smaller models or datasets (Wei et al., 2023), and further, it lacks correlation with downstream skills to adequately evaluate the model.Addressing these two challenges will help propel further research and practices in the pretraining stage.\n\nIn the alignment stage, challenges arise regarding data (Albalak et al., 2024), long context (Kamradt, 2023) and RLHF effectiveness (Wang et al., 2024;Xu et al., 2024).SFT gets major parts of things done, but is to some extent sensitive to data quality and composition (Liu et al., 2024b).It is tricky to balance the learning of different prompt categories and specific application data.Various useful applications of LLMs and multi-modal large models require sequence lengths far beyond several thousand (Gemini-Team et al., 2024).Pretraining on large corpus of long data (context length of tens or hundreds of thousand) turns out inefficient and one desideratum is to extend the context length with minimal continual pretraining and SFT at reasonable costs.RLHF in open-sourced models has not yet fulfilled the presumed promise as OpenAI (2023).Much remains under-explored in terms of RM data, RM training, PPO data and PPO training, etc.In response to those challenges, we devoted substantial efforts to our LLM models, the 360Zhinao series, and presented the details in this technical report.The 360Zhinao model comprises a base model trained from scratch and a chat model using alignment techniques.In the pretraining stage, we explored the data strategies and their evaluation.First, we built a data cleaning and filtering pipeline.By crawling massive web pages, we employed a series of filtering and cleaning steps.Subsequently, we explored multi-level deduplication and data mixing strategies, ultimately obtaining a corpus of 3.4 TB tokens with high data efficiency.",
            "score": 0.4224260401876659,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 409
                },
                {
                    "start": 411,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 700
                },
                {
                    "start": 700,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 943
                },
                {
                    "start": 943,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1351
                },
                {
                    "start": 1351,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1615
                },
                {
                    "start": 1615,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1749
                },
                {
                    "start": 1749,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 680,
                    "end": 699,
                    "matchedPaperCorpusId": "266551413"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "275932560",
            "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training",
            "text": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",
            "score": 0.42229618794061174,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "270067747",
            "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
            "text": "Large Language Models (LLMs) are sequentially trained on general pre-training corpus, pairs of instruction-response and preference-alignment datasets, thus covering tasks involving writing (Touvron et al., 2023a;Jiang et al., 2023;Blum and Blum, 2023;Pan, 2021), math (Imani et al., 2023; Figure 1: SFT on domain data injects domain knowledge into general LLMs. CF aims to keep the LLM performance on the general tasks after training on domain tasks. While GCI aims to enhance the performance on domain tasks by the integration of general capabilities with domain knowledge. Then the LLM is applied to domain-specific scenarios. Liu et al., 2023;Azerbayev et al., 2023), code (Bui et al., 2023;Chen et al., 2021;Rozi\u00e8re et al., 2023), etc. Many popular domain-specific LLMs are finetuned from general chat LLMs (Xiong et al., 2023;Wang et al., 2023a;Yu, 2023). The straightforward procedure is illustrated in Figure 1. \n\nResearchers have identified a challenge known as Catastrophic Forgetting (CF) (Kaushik et al., 2021), where the model's recent learning overshadows and diminishes its previously acquired capabilities and knowledge, leading to a significant performance drop on previous tasks. Current studies to mitigate CF focus on preserving the general capabilities. However, this paper investigates how to effectively harmonize and utilize both general capabilities and domain-specific knowledge, rather than mitigate CF. Our rationale stems from the observation that, even with CF resolved, general capabilities often encounter difficulties integrating with domain-specific knowledge. \n\nSpecifically, we illustrate the enhancement of GCI in legal domain through Figure 2. A general chat LLM focuses on computing solutions for math queries, delivering numerical results. However, with SFT on legal knowledge, the LLM shifts its approach to presenting relevant law article content, rather than providing the calculation result and conclusion, despite users potentially preferring the latter. An optimal GCI-equipped LLM maintains its general capabilities while integrating legal knowledge contextually at the appropriate time steps.",
            "score": 0.4222635266767374,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152099609375
        },
        {
            "corpus_id": "276317922",
            "title": "Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging - An Open Recipe",
            "text": "After identifying a merge configuration that effectively combines the abilities of two models in Section 4.2, we focus on optimizing the data mixture for the SFT model to enhance alignment before merging, ultimately improving end-to-end performance. \n\nIn this section, we explore the impact of the SFT dataset on overall model performance by addressing the following key dataset considerations 1. Does increasing the data mixture of Thai to 30% improve performance compared to 10%? -We investigate the impact of Thai-English data proportions, we add an additional 4.5k Thai translation examples based on translation of Bespoke-Stratos as in Section 2.2, which increase the Thai language ratio from 10% to 30%. 2. Does adding distilled reasoning traces on general Thai queries improve performance? - \n\nWe hypothesize that Bespoke-Stratos primarily covers math, code, and puzzle domains, lacking diversity in instruction-following tasks. Does adding general-domain distillation with long-form reasoning improve performance? To test this hypothesis, we sample 1,000 prompts from the Thai general instruction dataset Suraponn/thai instruction sft10 , distill responses using DeepSeek R1, and apply rejection sampling to exclude non-Thai solutions, retaining approximately 50% of the samples. The final dataset consists of 500 examples. 3. Does adding a general instruction dataset improve performance? -We hypothesize that adding a general instruction dataset might improve dataset diversity and help prevent catastrophic forgetting. To investigate this, we incorporate 10,000 general instruction examples. For English, we use Capybara11 , and for Thai, we use Suraponn/thai instruction sft, following its usage in Typhoon 2 (Pipatanakul et al., 2024). Each dataset is subsampled to 10,000 examples to maintain balance. \n\nWe construct dataset based on the above question which can is summarized in Table 6. The SFT model is later merged using the M3 configuration before evaluation, as we try to optimize for endto-end performance. Results: As shown in Table 7, we began with SFT-v1 + M3 as our baseline.",
            "score": 0.4216733906593335,
            "section_title": "SUPERVISED FINE TUNING (SFT): DATA MIXTURE",
            "char_start_offset": 15738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1815
                },
                {
                    "start": 1818,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 1721,
                    "end": 1747,
                    "matchedPaperCorpusId": "274822520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09222412109375
        },
        {
            "corpus_id": "260125946",
            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
            "text": "The main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment. So the key takeaway is that for alignment, \"less is more\" -focused optimization of a small, diverse dataset can be more effective than simply scaling up data quantity. Response from MPT-30B-Chat 1 The authors of the paper propose a novel method for generating diverse and high-quality responses to open-ended prompts using a pretrained language model called LIMA. 2 They demonstrate that fine-tuning a strong pretrained language model on a small number of carefully curated examples can produce competitive results on a wide range of prompts. 3 They also find that the mental effort required to construct such examples is a key factor in achieving alignment and that less is often more when it comes to the amount of data needed for instruction tuning. 4 They compare LIMA to other state-of-",
            "score": 0.4216194480107571,
            "section_title": "21",
            "char_start_offset": 61924,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "272367969",
            "title": "Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback",
            "text": "In this section, we will demonstrate that the accuracy and finegrained advantages of seq2seq RM can improve alignment efficiency. We empirically verify this improvement at both the token-level and the sentence-level optimization. \n\nSentence-Level Accuracy Seq2seq RM can accurately distinguish between good and bad responses. We sum the tokenlevel rewards provided by seq2seq RM up to scalars and conduct an empirical analysis on RFT and PPO. RFT prompts LLMs to generate multiple responses (set to 8 in our experiment), then selects the highest-scoring response for further SFT. Both methods' alignment performance relies on whether the RM can provide accurate sentence-level feedback. As shown in Table 3, both RFT and PPO using seq2seq RM achieved the best performance across different model sizes and tasks. This indicates that using sequence MLE in the language space instead of binary MLE in the scalar space improves the accuracy of reward modeling. \n\nToken-Level Granularity To verify the token-level accuracy of the seq2seq RM, we compared PPO-T with several popular alignment algorithms. In addition to PPO, we selected DPO and RFT as baseline methods. Our results show that PPO-T achieved SOTA performance, demonstrating that the seq2seq RM effectively provides token-level feedback. We also compared PPO-S with PPO-T, as they only differ in feedback granularity. The significant outperformance of PPO-T over PPO-S further indicates that the seq2seq RM excels in accurate credit assignment at the token level. \n\nOOD Robustness The seq2seq RM demonstrates robust scoring accuracy for out-of-distribution (OOD) responses. We tested the seq2seq RM and seq2scalar RM based on the TL;DR datasets but used the Xsum prompt in the RLHF process. Since the prompts in Xsum are not in the same distribution as those in TL;DR, this will test whether the RM is robust to OOD inputs. PPO-T can significantly improve the performance of SFT models, whereas PPO improvements are Gemma-2B Llama2-7B PPO Vs.",
            "score": 0.4215178715004938,
            "section_title": "Improvement of Alignment Perfromance",
            "char_start_offset": 16916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 956
                },
                {
                    "start": 959,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.380126953125
        },
        {
            "corpus_id": "273186633",
            "title": "Revisiting the Superficial Alignment Hypothesis",
            "text": "The Superficial Alignment Hypothesis posits that almost all of a language model's abilities and knowledge are learned during pre-training, while post-training is about giving a model the right style and format. We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering. Taken together, these results shed new light on the Superficial Alignment Hypothesis, suggesting that it is, at best, an over-simplification.",
            "score": 0.4214787224553288,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4296875
        },
        {
            "corpus_id": "260887200",
            "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
            "text": "The rise of contemporary chatbots, including GPT-4 (OpenAI, 2023), has brought to the forefront of generative artificial intelligence that is based on large language models (LLMs) to tackle a variety of real-world tasks. Well-aligned LLMs with human expectations can precisely recognize human intentions and properly formalize responses expressed in natural languages (Wang et al., 2023d). Achieving such a level of alignment typically necessitates fine-tuning processes, such as supervised fine-tuning (SFT) (Taori et al., 2023;Chiang et al., 2023;Touvron et al., 2023b), response ranking (Yuan et al., 2023b;Song et al., 2023;Rafailov et al., 2023), and reinforcement learning with human feedback (RLHF) (Bai et al., 2022a;Ouyang et al., 2022;Touvron et al., 2023b), to enable LLMs to comprehend and execute diverse instructions effectively. \n\nA broad range of training data covering various semantics and specialties is crucial for achieving alignment with human preference through SFT, which is typically gathered through crowd-sourcing (Ouyang et al., 2022;Bai et al., 2022a;Touvron et al., 2023b) or by distilling from other LLMs (Taori et al., 2023;Ding et al., 2023). The SFT data for alignment is generally formalized in a multi-turn utterance manner, and each turn is composed of a human query and a corresponding response expected to generate by well-aligned chatbots. Recent research indicates that the training dataset for alignment should be diverse and complex, covering various domains, tasks, and formats (Xu et al., 2023a;Mukherjee et al., 2023;Wang et al., 2023b). Such diversity and complexity are mainly determined by query formation. Various methods are proposed and claimed to improve the diversity and complexity of the queries and advance the alignment of LLMs (Wang et al. 2023c;Xu The contributions of this work are mainly three-fold.",
            "score": 0.42123017866894263,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 843
                },
                {
                    "start": 846,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "260438790",
            "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "text": "From our findings, there are two main factors that can improve mathematical reasoning abilities given a preset amount of human-annotated samples, including: (1) Pre-training the LLMs to lower losses; \n\n(2) Augmenting fine-tuning with rejection sampling. Through extensive experiments, we empirically verify the scaling relationships between the mathematical reasoning performance of LLM with both factors respectively. Out of the consideration of sustainable NLP, in this section, we investigate the possible computational resources required to extrapolate the mathematical performance of LLMs by both factors and discuss how to improve the performance more efficiently. \n\nWe estimate the pre-training, SFT, RFT inference, and RFT FLOPs following Kaplan et al. (2020) and GPU times in Table 4 which is detailed in Appendix E. We can find that the cost times of SFT (\u223c 1 \u00d7 10 \u22125 ) and RFT (\u223c 1 \u00d7 10 \u22124 ) are negligible compared to pre-training. One can always use SFT and RFT to improve models' performance. However, it could be hard to use RFT to further boost performance. Since we need much more sampling counts (at an exponential level) to increase distinct reasoning paths and there exists an upper bound of distinct reasoning path amount for a given math reasoning question. \n\nWe assume that performance follows RFT>SFT>ICL, from the findings in this paper we know the improvement speed follows RFT<SFT<ICL. And if we have an omnipotent language model which has a pre-training loss that is the same as the corpus randomness, it could have RFT = SFT = ICL = 100. Thus when you pre-train a better language model (i.e. smaller pre-training loss), your model's performance still follows RFT>SFT>ICL but their performance gaps are diminishing. \n\nSince you can obtain an RFT model without too much effort (compared to pre-training), then the most important thing we should do is to decrease the model's pre-training loss.",
            "score": 0.4212095328967632,
            "section_title": "TOWARDS EXCELSIOR MATHEMATICAL REASONING",
            "char_start_offset": 22399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1743
                },
                {
                    "start": 1746,
                    "end": 1920
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.091552734375
        },
        {
            "corpus_id": "268856712",
            "title": "HyperCLOVA X Technical Report",
            "text": "The next phase in alignment learning is RLHF.Even though the post-SFT model is capable of multiple tasks, it may still generate outputs that are uninformative or contain false or harmful content.Thus, we incorporate RLHF in order to further align the model with human values, such as helpfulness, factuality, and safety (Askell et al., 2021).The overall procedure for RLHF involves training a reward model with human preference data, followed by training the post-SFT model via proximal policy optimization (PPO) (Schulman et al., 2017) to generate sequences that maximize the reward returned by the reward model.\n\nReward Model.Our reward model is initialized as the post-SFT model, with a randomly initialized linear head that outputs a scalar reward.The model is trained with ranking loss from Stiennon et al. (2022) based on the Bradley-Terry model (Bradley and Terry, 1952), in which the negative loglikelihood of the difference between chosen and rejected rewards is minimized.The model is trained for one epoch only, as we observed overfitting after that point, similar to the findings of Ouyang et al. (2022).We place all comparisons from the same prompt in the same optimization step to prevent overfitting while maintaining the max-token batching method as previously described.\n\nThe dataset for our reward model is collected from diverse product requirements based on various criteria and annotating schemes.We observe different reward distributions across the data sources, consistent with the findings in related work (Peng et al., 2023;Zeng et al., 2023).Such differences lead to reward hacking risks and training difficulties due to high variance.To mitigate this problem, we apply normalization and clipping at inference time (Zheng et al., 2023b).\n\nReinforcement Learning.We adopt PPO for reinforcement learning.Following previous work (Ouyang et al., 2022;Bai et al., 2022), we add a Kullback-Leibler (KL) penalty term (Jaques et al., 2019;Stiennon et al., 2020) to the reward with a coefficient of 0.04.",
            "score": 0.4211074953877926,
            "section_title": "Reinforcement Learning from Human Feedback (RLHF)",
            "char_start_offset": 10066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 45,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 342
                },
                {
                    "start": 342,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 982
                },
                {
                    "start": 982,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1287
                },
                {
                    "start": 1289,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1568
                },
                {
                    "start": 1568,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1763
                },
                {
                    "start": 1765,
                    "end": 1788
                },
                {
                    "start": 1788,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 852,
                    "end": 877,
                    "matchedPaperCorpusId": "121987403"
                },
                {
                    "start": 1095,
                    "end": 1115,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2215576171875
        },
        {
            "corpus_id": "270045432",
            "title": "Scaling Laws for Discriminative Classification in Large Language Models",
            "text": "Early work with the GPT architecture investigated the effect of language model pre-training on downstream fine-tuning tasks such as text classification, entailment, semantic similarity, and multiple choice question answering [28].Subsequent work with the GPT architecture however focused on zero-and few-shot language generation in lieu of fine-tuning.Other similar fine tuning methods could include both Parameter Efficient Fine Tuning (PEFT) [41] and Supervised Fine Tuning (SFT) [25].PEFT refers to a family of methods, such as LoRA [17], that seek to fine tune a much smaller set of parameters than exist in the original model.SFT seeks to align the original embeddings of the LLM for the target task through a supervised process that maps the LLM output to that of the target.For InstructGPT, this mapping consists of aligning input training text with multiple sequential output tokens.Our method can be understood as a SFT method with an output space that is of size   .Perhaps the most relevant work is the recent LS-LLaMA approach which systematically studied replacing the final transformer decoder with a classification layer and removing causal masks.In their experiments, they find reliable improvements upon BERT baselines while also outperforming LLMs an order of magnitude larger than LS-LLaMA.Their work however is limited to the LLaMA family of models and they do not quantify the effect of LLM size on discriminative fine-tuning ability.",
            "score": 0.42079469983643697,
            "section_title": "Domain Adaptation of Language Models.",
            "char_start_offset": 10481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 781
                },
                {
                    "start": 781,
                    "end": 891
                },
                {
                    "start": 891,
                    "end": 976
                },
                {
                    "start": 976,
                    "end": 1162
                },
                {
                    "start": 1162,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1455
                }
            ],
            "ref_mentions": [
                {
                    "start": 482,
                    "end": 486,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10467529296875
        },
        {
            "corpus_id": "273323302",
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "text": "We first compared our approach to multilingual alignment, where English responses were self-translated into other languages for SFT training. Additionally, we focus on comparing reasoning task performance with MAPO (She et al., 2024). To ensure a fair comparison with MAPO, we considered two variants: MAPO \u2020 uses the same sampling count as our method, while MAPO \u2021 uses MAPO's sampling configuration but with training data size consistent with ours. \n\nWe used MAPO's official code and hyperparameters for sampling and trained all preference pairs under identical training conditions. We report MAPO's best results, achieved after two iterations.",
            "score": 0.42078400727255455,
            "section_title": "Compared Methods",
            "char_start_offset": 22232,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 646
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "277955249",
            "title": "Learning to Reason under Off-Policy Guidance",
            "text": "For AIME 2024, AIME 2025 and AMC, we report avg@32 as the test set is relatively small, and for the other three benchmarks, we report pass@1. As our RL training mainly focuses on math reasoning, we further validate the generalization capability on three out-of-distribution benchmarks, namely ARC-c [33](Open-Domain Reasoning), GPQA-diamond [34](Science Graduate Knowledge, denoted as GPQA * ), and MMLU-Pro [35](Reasoning-focused Questions from Academic Exams and Textbooks). We shuffle the multiple-choice options to avoid contamination. For testing, the temperature is set as 0.6. \n\nBaseline Methods. For RLVR methods, we consider the following methods: (1) Simple-RL [5]: training from Qwen2.5-Math-7B using rule-based reward; (2) Oat-Zero [6]: training from Qwen2.5-Math-7B and rule-based reward, proposing to remove the standard deviation in GRPO advantage computation and token-level normalization in policy loss computation; (3) PRIME-Zero [24]: using policy rollouts and outcome labels through implict process rewards; (4) OpenReasonerZero [7]: a recent open-source implementation of RLVR methods. Except RLVR approaches from previous work, we consider two kinds of baselines with our setting (1) On-Policy RL -we train on-policy RL within RLVR paradigm using Dr.GRPO with the same reward and data. (2) Alternative Methods to Incorporate Off-Policy Guidance -We consider three methods, namely SFT, we train the model with the same prompts and reasoning traces as LUFFY using SFT; RL w/ SFT Loss, using SFT loss during RL training; SFT + RL, two-stage training that continues RL training after SFT. For detailed setup for training these methods, we refer readers to Appendix C.",
            "score": 0.42068357356179215,
            "section_title": "Experimental Setup",
            "char_start_offset": 15707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 583
                },
                {
                    "start": 586,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1685
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 345,
                    "matchedPaperCorpusId": "265295009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03424072265625
        },
        {
            "corpus_id": "269010053",
            "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
            "text": "Alignment is an important step in creating an assistant LM, because it helps language models generate relevant and helpful responses, as well as avoiding harmful and biased content. Our Eagle models are tested for Chinese alignment using the AlignBench (Liu et al., 2023b), a benchmark for evaluating the alignment of Chinese LLMs, featuring 683 diverse and challenging queries across eight categories like language abilities, logical reasoning, and professional knowledge. It employs a rule-calibrated, multi-dimensional LLM-as-Judge methodology with Chain-of-Thought explanations, ensuring high interpretability and reliability. \n\nTable 11 showcases a consistent improvement in the performance of Eagle and Finch models on the AlignBench benchmark as model size and generation progresses. This trend is evident across a wide range of categories, highlighting the larger models' enhanced capability to understand and generate contextually relevant responses. Particularly, both the Eagle 7B and Finch 3B model significantly surpasses its smaller and previous generation counterparts, achieving higher overall scores. This progression underscores the critical role of scaling model size as well as improving architecture in aligning with human judgment in complex language understanding tasks. The results affirm the importance of model architecture and capacity in achieving superior alignment and interpretability in language models.",
            "score": 0.4205830111911525,
            "section_title": "F.1 Alignment Benchmark",
            "char_start_offset": 40250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.300048828125
        },
        {
            "corpus_id": "274982908",
            "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
            "text": "To study the impact that the SFT and RL stages of deliberative alignment have on model performance, we conduct ablation experiments where we drop safety data from one or both stages. Specifically, we compare the following four settings (see Figure 14 As expected, the \"Safety in SFT & RL\" performs much better than the \"No safety training\" run in terms of disallowed content, response style, and jailbreaks, although in this specific ablation setup the safety training also increases overrefusals. The key finding is that the \"Safety in SFT only\" and \"Safety in RL only\" runs attain intermediate results, showing that both SFT and RL training play critical roles in deliberative alignment training. We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL. \n\nIn Figure 14, we also compare these ablations to a baseline where we do not perform any safety training, but we provide the entire spec to the model at inference time in the system message. Because we would not know what safety category is relevant for prompts received at deployment time, the spec we provide is not tailored to any safety category but instead has the summarized versions of all the content policies (see Section 2.2). Note that it is infeasible to include the detailed versions of the content policies for all safety categories, because each one spans 5-10K tokens and would altogether exceed the model's context window. \n\nDespite having access to the full spec, this baseline appears to learn less safety behavior than the model trained with deliberative alignment (and in many cases, even the model only trained with safety in the SFT stage). This baseline particularly struggles to adhere to response style guidelines. These results indicate that embedding these policies during training is more reliable than providing all of the policies at deployment time.",
            "score": 0.4204884951692138,
            "section_title": "Ablations for different components of the method",
            "char_start_offset": 35683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 836
                },
                {
                    "start": 839,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1477
                },
                {
                    "start": 1480,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1919
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "272146685",
            "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models",
            "text": "Before exploring technical solutions, we establish guiding principles for SFT. We note that SFT is rarely the final stage of LLM development; subsequent phases such as preference learning (Rafailov et al., 2023;Azar et al., 2024;Wang et al., 2024b), reinforcement learning (Li et al., 2024c;Shao et al., 2024), and advanced inference-time strategies (Snell et al., 2024) heavily depend on sampling and output diversity to explore high-quality solutions. This reliance on diversity underscores a key challenge: while pre-trained LLMs produce diverse outputs due to their broad knowledge bases, standard SFT practices-particularly the use of CE loss-often reduces this diversity (O' Mahony et al., 2024;Wang et al., 2024a). Such reduction can lead to knowledge forgetting, aligning with the \"alignment tax\" phenomenon observed in (Bai et al., 2022;Ouyang et al., 2022). \n\nWe argue that preserving output diversity during SFT can address these issues. Intuitively, the ability of a model to generate diverse responses serves as an indicator of the richness of its retained knowledge. By maintaining diversity, the model is compelled to consider alternative plausible responses, which in turn necessitates that its internal parameters encode and retain relevant knowledge. To operationalize this insight, we propose the following guiding principle for SFT: learning from the data while preserving diversity. In the following sections, we present technical insights and solutions. \n\nIn this section, we draw insights into algorithm design by examining the dynamics of CE. We will introduce a new theory of probability transfer. To illustrate this concept, we study a simplified setting, where the prompt x \u2208 X is fixed and given. We model the distribution f \u03b8 (y|x) = softmax(\u03b8 x ) with \u03b8 x \u2208 R K being the \"logit\" and K being the vocabulary size. Let y = i \u2208 [K] denote the token class to be learned. We begin by calculating the gradient of the CE loss for the given example:",
            "score": 0.420247112761644,
            "section_title": "CHALLENGES AND PRINCIPLES FOR SFT",
            "char_start_offset": 4515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1475
                },
                {
                    "start": 1478,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1971
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 229,
                    "matchedPaperCorpusId": "264288854"
                },
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "264146066"
                },
                {
                    "start": 846,
                    "end": 866,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239501953125
        },
        {
            "corpus_id": "268230946",
            "title": "An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning",
            "text": "In the context of significant emergent abilities demonstrated by Large Language Models (LLMs) (Wei et al., 2022a;OpenAI, 2023), the focus on math reasoning tasks, particularly Numerical QA and Math Word Problems (MWP) (Kushman et al., 2014;Upadhyay and Chang, 2017;Miao et al., 2020a;Xu et al., 2022), is paramount.The current approach to activate these abilities in LLMs involves carefully engineered prompting (Brown et al., 2020), in-context learning (ICL) (Chen et al., 2022b) or supervised fine-tuning (SFT).\n\nParticularly due to computational costs and stability concerns (Yuan et al., 2023), there is growing attention on enhancing the abilities of open-source LLMs (Rozi\u00e8re et al., 2023) through SFT.Supervised data is crucial for SFT.Current research is centered on using GPT-4 (OpenAI, 2023) or other powerful base models with prompts composed of their designed reasoning chains to create supervised data for SFT based on several public seed datasets (Lu et al., 2022).\n\nIn this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.We primarily investigate the following research questions (RQs):\n\n\u2022 RQ1: What is the ability boundary of reasoning paths, and how to select paths optimally?\n\n\u2022 RQ2: How can we expand the ability boundary, and what kinds of problem sets are needed for this expansion?\n\nRQ1 originates from a common challenge in response augmentation methods: determining the optimal amount of data the training set should cover to balance data amount, effectiveness, and generalizability.As for RQ2, we focus on introducing additional problems instead of synthesizing new questions for query augmentation, which could assist in selecting and combining the necessary data from the chaotic reality of existing datasets.Actu-arXiv:2403.00799v1",
            "score": 0.420124161160499,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 513
                },
                {
                    "start": 515,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1109
                },
                {
                    "start": 1109,
                    "end": 1173
                },
                {
                    "start": 1175,
                    "end": 1265
                },
                {
                    "start": 1267,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1579
                },
                {
                    "start": 1579,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 1831
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 240,
                    "matchedPaperCorpusId": "12451537"
                },
                {
                    "start": 240,
                    "end": 265,
                    "matchedPaperCorpusId": "14624362"
                },
                {
                    "start": 265,
                    "end": 284,
                    "matchedPaperCorpusId": "220047831"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1473388671875
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "After pretraining on massive corpus, LLMs obtain the ability to serve as a general problem solver. To adapt them for domain-specific tasks, several post-training techniques can be applied to further refine their capabilities beyond initial pre-training. Three pivotal methodologies in this phase are instruction tuning, alignment tuning, and model adaptation (Zhao et al., 2023;Zhang et al., 2023;Wang et al., 2023f). These techniques enhance task generalization, align outputs with human preferences, and optimize models for domain-specific or resourceconstrained settings, respectively. In the following, we briefly introduce their objectives, methods, and impacts based on contemporary research. \n\nInstruction tuning. Instruction tuning refines LLMs to follow task-specific natural language instructions, enabling zero-shot or few-shot generalization to unseen tasks (Wei et al., 2021;Chung et al., 2024). Unlike conventional fine-tuning, which trains models on labeled examples for specific tasks, instruction tuning employs datasets comprising task descriptions, input-output pairs, and diverse prompts (e.g., \"Summarize this article: \n\n[text]\"). This approach conditions models to infer task requirements from instructions, better comprehend tasks, and satisfy human expectations across diverse tasks. Representative models that perform instruction tuning include InstructGPT (Ouyang et al., 2022) and FLAN-T5 (Chung et al., 2024). \n\nInstruction tuning is closed to supervised fine-tuning (SFT) (Ouyang et al., 2022) and prompt tuning (Liu et al., 2021). SFT performs full-parameter fine-tuning based on pre-trained models using task-specific labeled data (input-output pairs). Instruction tuning is a special form of SFT that fine-tunes a model using instructional task descriptions, with the goal of allowing the model to understand and generalize to unseen instructions. Prompt tuning is a parameter-efficient fine-tuning method (which will be discussed in the latter) that guides the model output by adjusting the prompts in the inputs, usually without updating the pre-trained model parameters, and optimizing only a small number of prompt-related parameters. The difference between the three concepts is relatively small.",
            "score": 0.42004484397828107,
            "section_title": "Post-training",
            "char_start_offset": 36048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2233
                }
            ],
            "ref_mentions": [
                {
                    "start": 1382,
                    "end": 1403,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1501,
                    "end": 1522,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1728515625
        },
        {
            "corpus_id": "267406252",
            "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
            "text": "To align a pre-trained language model with human preference. We assume we have a human-aligned QA dataset {x i , y i } N , where x i is the input instruction and y i is ground-truth output. The service provider utilizes supervised finetuning (SFT) to train the model on this alignment dataset. Formally, the loss of the alignment is as follows: \n\nwhere f w l ,\u03f5 (e l\u22121 ) is the l-th layer in an LLM that maps the input to the hidden embedding, and T (x i ) is the tokenizer function that produces embedding e i,0 .",
            "score": 0.4199444080417449,
            "section_title": "Model Alignment",
            "char_start_offset": 7489,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 514
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19873046875
        },
        {
            "corpus_id": "265609639",
            "title": "Data Management For Training Large Language Models: A Survey",
            "text": "Large Language Models (LLMs) have shocked the natural language processing (NLP) community with their strong performance and emergent abilities (OpenAI, 2023;Touvron et al., 2023a;Wei et al., 2022). According to previous studies (Kaplan et al., 2020;Hoffmann et al., 2022), LLMs' achievements depend heavily on self-supervised pretraining over processed vast volumes of text data. Recent research (Zhou et al., 2023a;Ouyang et al., 2022) further enhances LLMs' instructionfollowing ability and performance on downstream tasks through Supervised Fine-Tuning (SFT) on deliberately curated instruction datasets. \n\nTo construct suitable training datasets, data management is vitally important and challenging in both the pretraining and SFT stages of LLMs, which we define as following: \n\nData management: the process of organizing a well-suited training dataset with collected data, including the data selection, combination and utilization strategies, and the evaluation of the chosen strategies. \n\nIn the pretraining stage, constructing datasets with high-quality data is essential for efficient training (Jain et al., 2020;Gupta et al., 2021). To equip LLMs with diverse and comprehensive abilities, heterogeneous dataset composition with mixtures of domains is also required (Gao et al., 2020;Longpre et al., 2023b;Shen et al., 2023). However, many prominent LLMs do not enclose (Anil et al., 2023;OpenAI, 2023) or only document (Brown et al., 2020;Le Scao et al., 2023;Touvron et al., 2023a) the techniques used in the construction of their pretraining dataset, leaving the reasons and effects of choosing specific data management strategies absent. In the SFT stage, LLMs' performance and instruction-following abilities are primarily evoked by carefully constructed instruction datasets (Sanh et al., 2022;Ouyang et al., 2022).",
            "score": 0.41924163998887787,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 993
                },
                {
                    "start": 996,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1830
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 196,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 249,
                    "end": 271,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 416,
                    "end": 436,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1103,
                    "end": 1122,
                    "matchedPaperCorpusId": "221191372"
                },
                {
                    "start": 1122,
                    "end": 1141,
                    "matchedPaperCorpusId": "236980246"
                },
                {
                    "start": 1293,
                    "end": 1315,
                    "matchedPaperCorpusId": "258832491"
                },
                {
                    "start": 1429,
                    "end": 1449,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1199951171875
        },
        {
            "corpus_id": "275954349",
            "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
            "text": "Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we propose Critique Fine-Tuning (CFT), a method more effective than SFT for reasoning tasks. Instead of simply imitating correct responses, CFT trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding - traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct multiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where GPT-4o serves as the teacher to generate critiques in the form of ([query; noisy response], critique). Experiments on these datasets demonstrate that CFT consistently outperforms SFT by 4-10% across six mathematical reasoning benchmarks, and is effective across different base models including Qwen2.5, Qwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only requires 1 hour of training on 8 x H100 over the 50K examples, yet matches or outperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it matches the performance of SimpleRL, which is a DeepSeek-r1 replication trained with 140 x more compute. Experiments on IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance the model's general generation and instruction-following capabilities, outperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies show that CFT is robust to noisy response sources and teacher critique models. These findings highlight that CFT offers a more effective alternative to advance the reasoning of language models.",
            "score": 0.41917070500111897,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1607666015625
        },
        {
            "corpus_id": "273229463",
            "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
            "text": "Recently, large language models (LLMs) have demonstrated remarkable generalization abilities in downstream tasks. These tasks include not only fundamental natural language processing tasks, such as machine translation and text classification (Brown et al., 2020), as well as advanced tasks, such as mathematics and logical reasoning (Achiam et al., 2023). \n\nThe generalization ability of LLMs originates from pre-training on large text corpora, such as RedPajama (Computer, 2023) and Fineweb (Penedo et al., 2024). The corpora often contain content from various domains, such as Common Crawl, GitHub, ArXiv, Wikipedia, and StackExchange. However, the relationship between each domain of training data and the abilities of LLMs is not fully understood. \n\nPrior research has shown that LLMs pre-trained with programming language data acquire high mathematical and reasoning abilities (Roziere et al., 2023;Madaan et al., 2022;Liang et al., 2023;Li et al., 2023); however, this causal relationship has not been rigorously tested. Specifically, fair comparisons are often not conducted between models trained on programming language data and those trained on natural language data due to differences in the number of training tokens and model sizes, or because the information is unknown as for closed models. In addition, some prior works have finetuned models using a mixture of programming languages, but they have not conducted detailed analyses regarding the effect of each programming language on the performance of downstream tasks (Li et al., 2023;Roziere et al., 2023). \n\nWe conducted experiments to analyze whether models trained solely on a single programming language generalize better to pure logical reasoning tasks compared to models trained on natural language. Specifically, we trained GPT2-124M, GPT2-774M, GPT2-1.5B, and LLaMA-774M (Radford et al., 2019;Zhang et al., 2024) from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under the same conditions.",
            "score": 0.41912473115708826,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 904,
                    "end": 924,
                    "matchedPaperCorpusId": "252873120"
                },
                {
                    "start": 924,
                    "end": 943,
                    "matchedPaperCorpusId": "253553585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058013916015625
        },
        {
            "corpus_id": "277622017",
            "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam",
            "text": "Legal reasoning tasks present unique challenges for large language models (LLMs) due to the complexity of domain-specific knowledge and reasoning processes. This paper investigates how effectively smaller language models (Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514 Multi-state Bar Examination (MBE) questions to improve legal question answering accuracy. We evaluate these models on the 2022 MBE questions licensed from JD Advising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our methodology involves collecting approximately 200 questions per legal domain across 7 domains. We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC (Issue, Rule, Application, Conclusion) format as a guided reasoning process to see if it results in better performance over the non-distilled dataset. We compare the non-fine-tuned models against their supervised fine-tuned (SFT) counterparts, trained for different sample sizes per domain, to study the effect on accuracy and prompt adherence. We also analyse option selection biases and their mitigation following SFT. In addition, we consolidate the performance across multiple variables: prompt type (few-shot vs zero-shot), answer ordering (chosen-option first vs generated-explanation first), response format (Numbered list vs Markdown vs JSON), and different decoding temperatures. Our findings show that domain-specific SFT helps some model configurations achieve close to human baseline performance, despite limited computational resources and a relatively small dataset. We release both the gathered SFT dataset and the family of Supervised Fine-tuned (SFT) adapters optimised for MBE performance. This establishes a practical lower bound on resources needed towards achieving effective legal question answering in smaller LLMs.",
            "score": 0.41895293083199736,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1094970703125
        },
        {
            "corpus_id": "267658120",
            "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
            "text": "At the core of defenses lies alignment, which involves fine-tuning pre-trained models to enhance their internal safety capabilities. In this section, we introduce various alignment algorithms and emphasize the data specifically designed to align models for improved safety. \n\nAlignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations. RLHF (Stiennon et al., 2020;Ouyang et al., 2022) utilizes human feedback and preferences to enhance the capabilities of LLMs, and DPO (Rafailov et al., 2023) simplifies the training process of RLHF by avoiding the need for a reward model. Methods like RLHF and DPO typically optimize a homogeneous and static objective based on human feedback, which is often a weighted combination of different objectives. To achieve joint optimization of multiple objectives (e.g., safety, helpfulness, and honesty) with customization according to specific scenarios, Multi-Objective RLHF (Dai et al., 2023;Ji et al., 2023;Wu et al., 2023c) extends RLHF by introducing fine-grained objective functions to enable trade-offs between safety and other goals such as helpfulness. Meanwhile, MODPO (Zhou et al., 2023b) builds upon RL-free DPO and enables joint optimization of multiple objectives. \n\nAlignment data. Based on the type of data used, data utilization can be divided into two categories: demonstration data for SFT and preference data for preference optimization approaches like DPO. As mentioned above, SFT utilizes high-quality demonstration data, where each question is associated with a single answer. Considering that SFT aims to maximize or minimize the generation probability on this data, selecting appropriate data becomes crucial.",
            "score": 0.41890191626764206,
            "section_title": "LLM Safety Alignment",
            "char_start_offset": 16767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1941
                },
                {
                    "start": 1942,
                    "end": 2076
                }
            ],
            "ref_mentions": [
                {
                    "start": 749,
                    "end": 772,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 772,
                    "end": 792,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "266844262",
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "text": "Our work examines the forgetting occurrence during continual pre-training on an existing finetuned LLM. Our paper primarily focuses on continual pre-training using the Traditional Chinese corpus. We evaluate the impact of continual pretraining across various dimensions, including output format, knowledge, and reliability. We show that more than straightforward methods are required for resolving this issue. Also, we observe an increased prominence of the repetition problem in models that tend to generate Traditional Chinese outputs. Lastly, despite continual pre-training, our findings suggest that the model's knowledge remains unaffected while its reliability declines. ters that have undergone sequential alignment operations, including SFT and RLHF. Our pre-training process utilizes the 1 billion tokens of Traditional Chinese data. We denote the model after continual pre-training as Llama-2-7b-chat-cp. We employ specific prompts to observe the differences between the outputs generated by the two models.",
            "score": 0.4188164627372861,
            "section_title": "Introduction",
            "char_start_offset": 1964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1017
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Instruction Tuning and Alignment.Instruction tuning (also known as supervised fine-tuning) employs human-annotated instructions (Sanh et al., 2022;Mishra et al., 2022;K\u00f6pf et al., 2023;Sun et al., 2023) or synthetic instructions by proprietary models (Taori et al., 2023;Chiang et al., 2023;Wang et al., 2023b) to fine-tune LLMs.Besides, alignment with reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022a) or direct preference optimization (DPO) (Rafailov et al., 2023a) aims to align LLMs with human preference.Both instruction tuning and alignment are able to elicit knowledge from LLMs and improve their capabilities to solve downstream tasks.\n\nRecent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks.Inspired by these studies, we propose to expose knowledge memorization and capability elicitation from instruction tuning and alignment.Unlike these studies which typically focused on instruction tuning or alignment, we differ in that we unify the three stages of training LLMs (i.e., continual pre-training, instruction tuning, and alignment) and conduct a knowledge mixture pre-training to mainly focus on learning new domain knowledge while maintaining general knowledge.",
            "score": 0.4184022341562816,
            "section_title": "RELATED WORK",
            "char_start_offset": 32983,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 33,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 532
                },
                {
                    "start": 532,
                    "end": 666
                },
                {
                    "start": 668,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 167,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 167,
                    "end": 185,
                    "matchedPaperCorpusId": "258179434"
                },
                {
                    "start": 185,
                    "end": 202,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 680,
                    "end": 699,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1065,
                    "end": 1084,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53076171875
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "To perform instruction tuning, the first step is to collect instruction-formatted instances in natural language. Task descriptions are obtained either by crowd-sourced human experts or synthetic instances. Then, these formatted instances are employed to fine-tune LLMs in a supervised learning way. Recent studies (Wang et al., 2022a) have demonstrated that using instruction tuning on public instruction datasets such as Super-NaturalInstructions (Wang et al., 2022b) and PromptSource (Bach et al., 2022) can significantly improve performance on downstream tasks. \n\nAlignment tuning. While pretrained LLMs have impressive generation ability, they may output harmful, biased, or misleading content. Thus, alignment tuning focuses on the adjustment of LLMs to comply with human values and preferences, ethical guidelines, and safety standards (Wang et al., 2023f). This is typically achieved by incorporating human feedback into training loops, often through reinforcement learning (RL) or contrastive learning techniques (Ouyang et al., 2022;Ziegler et al., 2019). \n\nDifferent from the goals of pretraining and instruction tuning, alignment tuning highlights different aspects of the model output, such as honesty with correctness. These human-centric criteria can be obscure for LLMs to comprehend. Thus, the first step to align LLMs is to collect human evaluations and feedback from experts. In existing LLMs, one of the dominant method for generating human feedback is human annotation (Ziegler et al., 2019;Ouyang et al., 2022;Wang et al., 2023f). Since high-quality human feedback data is crucial for aligning LLMs, this process can be resource-consuming and requires careful treatment. \n\nAfter collecting and constructing feedback datasets from human experts, a prevalent method for alignment is Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017;Ziegler et al., 2019), where models are fine-tuned using datasets of human preferences to guide their behavior. RLHF adapts LLMs to human feedback by learning a reward model, incorporating human in the training loop. It involves three stages: \n\n1. Collecting human rankings of SFT model output.",
            "score": 0.4182671408743143,
            "section_title": "Resource-constrained scenarios",
            "char_start_offset": 38797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1064
                },
                {
                    "start": 1067,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 505,
                    "matchedPaperCorpusId": "8236317"
                },
                {
                    "start": 1021,
                    "end": 1042,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1511,
                    "end": 1531,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464111328125
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "Large language models (LLMs) have revolutionized Natural Language Processing (NLP), where LLMs have been pretrained on a massive textual corpus and encoded massive world knowledge [1,8]. These models achieve remarkable zero-shot and few-shot performance across a wide range of tasks [2,6,45,65,66]. The innovation of instruction tuning, also known as supervised fine-tuning (SFT), has Figure 1: Performance comparison of various fine-tuning methods under three OOD data mixing scenarios. The target domain is medical knowledge, using Chinese subset of MMedBench [52] as the in-domain training dataset. (a) The dataset is mixed with medical OOD data from CMExam [32], maintaining a total dataset size of 20k; (b) The dataset is mixed with general OOD data from CoT-Collection [24], also keeping the total dataset size at 20k; (c) The dataset includes general OOD data from CoT-Collection, while the size of the in-domain training dataset remains at 20k. As the proportion of OOD data increases, the performance of the vanilla fine-tuning declines significantly, whereas TAIA manages to sustain robust performance in the target domain (details in Appendix E.5). \n\nfurther enhanced the instruction-following capabilities of LLMs [11,46], simplifying human-LLM interactions. Despite the availability of high-quality data for SFT being limited [7,86], expanding SFT datasets remains a straightforward method to adapt LLMs for specific tasks [14]. Various SFT datasets, such as Alpaca [49] and Natural Instructions [41,69], have been manually curated or artificially generated to create more generalized instruction-tuned LLMs. \n\nHowever, real-world applications of LLMs are diverse [6] and complex [34], often making public datasets insufficient. While synthetic data is useful, it is expensive and tends to exhibit a distribution shift biased towards the parent LLM [67]. Consequently, the data distribution that LLMs adapt to during fine-tuning often differs significantly from that required for specific tasks.",
            "score": 0.4179268840305729,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1159
                },
                {
                    "start": 1162,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1621
                },
                {
                    "start": 1624,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 2008
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 185,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 286,
                    "end": 288,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "258841149"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 1230,
                    "end": 1233,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1509,
                    "end": 1513,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1862,
                    "end": 1866,
                    "matchedPaperCorpusId": "264405716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.129150390625
        },
        {
            "corpus_id": "267548105",
            "title": "A Closer Look at the Limitations of Instruction Tuning",
            "text": "While extensive research has introduced new IT datasets, models, and enhancement methods (Zhang et al., 2023a), few studies examine IT's limitations. Concurrent work by Gudibande et al. (2023) shows that IT with datasets synthesized from powerful proprietary models only leads to imitating their style, not their knowledge. Similarly, Lin et al. (2023) shows that alignment only teaches style and proposes in-context learning as an alternative to IT that outperforms several tuned models. This can be attributed to our findings on LFT, where the models respond using pre-trained knowledge and do not lead to knowledge degradation like SFT. This supports the superficial alignment hypothesis by Zhou et al. (2023), positing that models gain knowledge in pre-training, with alignment shaping format used in user interactions. Finally, Kung & Peng (2023) show that models trained on simplified task definition or delusive examples achieve performance comparable to the ones trained on the original instructions. In contrast to all these works, we study the exact causes of these limitations, investigate pattern copying and hallucination from novel perspectives, and highlight the overlooked effectiveness of LFT that utilizes only pre-trained knowledge.",
            "score": 0.41791347880155005,
            "section_title": "Related Work",
            "char_start_offset": 33427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1251
                }
            ],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 712,
                    "matchedPaperCorpusId": "258822910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.123779296875
        },
        {
            "corpus_id": "278715135",
            "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment",
            "text": "To comprehensively assess the reward model, we review the discriminative ability of RM and the effectiveness of RLHF by applying the trained RM in optimizing the policy during the RL Stage to verify the effectiveness of the proposed method. \n\nDiscriminative Ability of RM We conduct experiments on below publicly available datasets for reward modeling and review the discriminative ability of RM: \n\n\u2022 UltraFeedback-Binarized [29], The UltraFeedback Binarized dataset is a processed subset of the UltraFeedback dataset, consisting of 64,000 prompts, each accompanied by four responses generated by various large language models. Based on GPT-4 scoring, two responses are selected per prompt to construct binary preference pairs [56], enabling their use in preference-based alignment methods such as reward modeling and Direct Preference Optimization (DPO). \n\n\u2022 HH-RLHF [1], We employ the HH-RLHF dataset, which contains 161k training and 8.55k test samples annotated with human preferences on helpfulness and harmlessness. Designed for training preference (or reward) models used in RLHF, each sample is formatted as a prompt paired with two responses, one of which is preferred. \n\n\u2022 Skywork-Reward [10], The Skywork-Reward dataset comprises 80,000 high-quality preference pairs, curated with an emphasis on specific capability and knowledge domains. It is constructed through a series of data selection and filtering strategies proposed in Paper-A, aiming to improve the quality and applicability of open-source preference data for real-world alignment tasks. \n\n\u2022 RewardBench [30], RewardBench consists of prompt-chosen-rejected triplets covering domains such as chat, reasoning, and safety, designed to evaluate reward models on challenging, structured, and out-of-distribution queries. Each comparison instance is constructed with subtle but verifiable preference signals. Evaluation is based on whether the model assigns a higher score to the chosen response than to the rejected one. \n\nEffectiveness of RLHF We compared policies optimized by implicit RM and explicit RM, respectively, in the general dialogue task and the summarization task.",
            "score": 0.41787695644074296,
            "section_title": "B.1 Tasks and Dataset",
            "char_start_offset": 26082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 1987
                },
                {
                    "start": 1990,
                    "end": 2145
                }
            ],
            "ref_mentions": [
                {
                    "start": 425,
                    "end": 429,
                    "matchedPaperCorpusId": "264490502"
                },
                {
                    "start": 727,
                    "end": 731,
                    "matchedPaperCorpusId": "269157140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260986328125
        },
        {
            "corpus_id": "272146685",
            "title": "Preserving Diversity in Supervised Fine-Tuning of Large Language Models",
            "text": "Large Language Models (LLMs) (OpenAI, 2023;Dubey et al., 2024) are powerful generative models that excel across specialized tasks. Despite extensive training, pre-trained LLMs often generate tokens without truly understanding users' queries, resulting in irrelevant answers. To enhance performance on specific tasks, researchers employ instruction tuning (Raffel et al., 2020;Wei et al., 2021;Chung et al., 2024), also known as Supervised Fine-Tuning (SFT) (Ouyang et al., 2022;Bai et al., 2022); see Figure 1. This process involves fine-tuning models on high-quality prompt-response pairs, with the Cross-Entropy (CE) loss being the de facto choice for optimization. \n\nHowever, is CE the most suitable choice for SFT? To answer this question, we must examine SFT's role in the broader context of model development. As the initial post-training stage, SFT teaches LLMs to follow instructions. Once these models can produce responses that are clear, interpretable, and verifiable for annotators, they become well-suited for leveraging human feedback in downstream tasks via techniques like reinforcement learning (Li et al., 2024c). From this perspective, SFT's primary objective is to align the output space of pre-trained LLMs with the preferred formats of downstream tasks. This alignment lays the groundwork for subsequent learning paradigms, including (Rafailov et al., 2023), self-improvement with synthetic data (Adler et al., 2024), and test-time scaling strategies (Snell et al., 2024;Brown et al., 2024;Wu et al., 2024). a practical training algorithm, GEM, for the game. GEM leverages some unique properties of LLMs, offering computational efficiency and scalability comparable to optimizing the CE loss. \n\nWe empirically validate the effectiveness of GEM, by fine-tuning pre-trained models ranging from 3B to 70B in size. We have two main findings. First, GEM enhances the model's ability to generate more diverse responses, which translates into improved performance in test-time scaling.",
            "score": 0.4176764215610159,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 376,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 457,
                    "end": 478,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1112,
                    "end": 1130,
                    "matchedPaperCorpusId": "264146066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "266933337",
            "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
            "text": "Training Pipeline. LLMs undergo a series of exquisite development steps to implement high-quality text generation. The typical process of LLM development contains three stepspre-training, supervised fine-tuning, and learning from human feedback [11], [24], [33]- [40]. In what follows, we will briefly review the core steps for training LLMs to help readers understand the preliminary knowledge of LLM construction. \n\n\u2022 Pre-Training. The initial LLM is pre-trained on a largescale corpora to obtain extensive general knowledge. The pretraining corpora is a mixture of datasets from diverse sources, including web pages, books, and user dialog data. Moreover, specialized data, such as code, multilingual data, and scientific data, is incorporated to enhance LLMs's reasoning and task-solving abilities [41]- [44]. For the collected raw data, data pre-processing [2]- [5] is required to remove noise and redundancy. After that, tokenization [45] is used to transform textual data into token sequences for language modeling. By maximizing the likelihood of token sequences, the pre-trained model is empowered with impressive language understanding and generation ability. \n\n\u2022 Supervised Fine-Tuning (SFT). Different from the pretraining process which requires a huge demand for computational resources, SFT usually trains the model on a smaller scale but well-designed high-quality instances to unlock LLMs' ability to deal with prompts of multiple downstream tasks [46]. Among recent LLM fine-tuning methods, instruction tuning [11] has become the most popular one, in which the input prompts follow the instruction format. \n\n\u2022 Learning from Human Feedback. Reinforcement learning from human feedback (RLHF) is a typical method for aligning LLMs' responses with human preference [11], [47], [48] and enhancing the safety of LLMs [4], [47]. In RLHF, a reward model is trained with human feedback to score the quality of LLMs' output content, where the human preference is expressed as the ranking of multiple LLM outputs about a certain input prompt. Particularly, the architecture of a reward model can also be a language model.",
            "score": 0.4175974972318989,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 7405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 263,
                    "end": 267,
                    "matchedPaperCorpusId": "258947756"
                },
                {
                    "start": 808,
                    "end": 812,
                    "matchedPaperCorpusId": "252668917"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 940,
                    "end": 944,
                    "matchedPaperCorpusId": "219683473"
                },
                {
                    "start": 1464,
                    "end": 1468,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "4787508"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.295166015625
        },
        {
            "corpus_id": "270226226",
            "title": "Self-Improving Robust Preference Optimization",
            "text": "The goal of this section is to motivate why learning self-improvement models through preference optimization can be useful for training a GPT-style language model. One of the main challenges that affect the performance of sequence-to-sequence GPT-style models is that they are hard to recover from failure at the time of inference. This problem, which is known as exposure bias in the literature on sequence-to-sequence prediction (Xu et al., 2020;Arora et al., 2023;Bengio et al., 2015), is because at the time of inference, GPT-style models generate tokens step by step based on their own previous outputs, not the ground truth. This can lead to error accumulation: if the model generates an incorrect token early in the sequence (or a misleading token that may lead to an error later down the line), it may propagate errors since its subsequent predictions are based on the earlier incorrect (or misleading) token (Arora et al., 2023). \n\nTo address this shortcoming of GPT-style models, one may fine-tune the model such that it can revise its incorrect generations through a self-improvement/self-refinement process (Madaan et al., 2024;Hu et al., 2024;Huang et al., 2022;Bai et al., 2022). The simplest way to train such a self-improvement model is through supervised fine tuning (SFT) (Bai et al., 2022;Wei et al., 2023). \n\nTo train a self-improvement model through SFT, one first need to build a supervised dataset of (x, y, y * ) in which y is some initial completion than can be correct or incorrect and y * is the corrected version of y. The improvement model can then be fine-tuned using SFT to predict y * from the pair (x, y). \n\nHowever, training a SFT self-improvement model is a challenging task for the following reasons. \n\n1. Creating a supervised dataset of (x, y, y * ) is resource intensive and hard to scale (Huang et al., 2022). Also, the existing standard SFT datasets are not created in this way.",
            "score": 0.4175354658326008,
            "section_title": "Learning Self-Improvement Policy Through Preference Optimization",
            "char_start_offset": 6153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1326
                },
                {
                    "start": 1329,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1638
                },
                {
                    "start": 1641,
                    "end": 1736
                },
                {
                    "start": 1739,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1919
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 487,
                    "matchedPaperCorpusId": "1820089"
                },
                {
                    "start": 1119,
                    "end": 1140,
                    "matchedPaperCorpusId": "257900871"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1070556640625
        },
        {
            "corpus_id": "270766267",
            "title": "NCL_NLP at SemEval-2024 Task 7: CoT-NumHG: A CoT-Based SFT Training Strategy with Large Language Models for Number-Focused Headline Generation",
            "text": "Meanwhile, SFT, despite its ability to improve performance, shows limitations in the interpretability of the reasoning process and suffers from attention decay, potentially leading to the omission of important information.\n\nTo address these challenges, we propose a training strategy based on the CoT approach, designed to significantly enhance LLMs in the task of number-focused headline generation.Our method consists of two key components.First, drawing on the concept of knowledge distillation (Dasgupta et al., 2023), we utilize GPT-3.5-Turbo(Brown et al., 2020) and instructions to process the original NumHG dataset, generating a series of reasoning steps.Given the issue of attention decay when handling long-distance information (Xiao et al., 2023), we created a new CoT-NumHG dataset by combining the question statement with reasoning steps.This process aims to bolster the model's attention mechanism and improve the interpretability of the reasoning process (Wang et al., 2023).Secondly, we selected three LLMs as base models and performed full-parameters SFT using the constructed CoT-NumHG dataset on these base models.Through this approach, we not only significantly improved performance on the specific task, but also optimized structured outputs while maintaining the models' versatility.Our research contributions are threefold:\n\n1. Based on the NumHG dataset, we developed the CoT-NumHG dataset, enhancing model interpretability and structured output capabilities.Importantly, we introduce a dataset construction technique specifically designed for the CoT-NumHG.\n\n2. We demonstrate the enhancement of model performance through task-oriented SFT train- ing on the CoT-NumHG dataset across three base models, significantly improving news headline generation while maintaining general-purpose capabilities.\n\n3. Through ablation studies, we demonstrated that the CoT-based training strategy effectively boosts the model's performance.",
            "score": 0.4175104441144165,
            "section_title": "Introduction",
            "char_start_offset": 4074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 224,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 663
                },
                {
                    "start": 663,
                    "end": 851
                },
                {
                    "start": 851,
                    "end": 990
                },
                {
                    "start": 990,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1483
                },
                {
                    "start": 1483,
                    "end": 1582
                },
                {
                    "start": 1584,
                    "end": 1823
                },
                {
                    "start": 1825,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 521,
                    "matchedPaperCorpusId": "259858962"
                },
                {
                    "start": 547,
                    "end": 567,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1463623046875
        },
        {
            "corpus_id": "270123084",
            "title": "InstructionCP: A fast approach to transfer Large Language Models into target language",
            "text": "Firstly, the KTO loss function is based on the Kahneman-Tversky value function for monetary gains and losses, which may not accurately reflect how humans perceive the relative goodness of text.Nevertheless, LLMs trained using RLHF demonstrate enhanced safety in completions, a crucial factor for companies and groups intending to open-source their models (Stiennon et al., 2022).Yet, as highlighted in Qi et al. (2023), challenges persist in maintaining RLHF capabilities when fine-tuning GPT-4(OpenAI, 2023)  Figure 1: n illustration to demonstrate the difference between the traditional approach and our method.\n\nIn the traditional approach, considerable effort is expended in collecting a plethora of contextual data for continual pre-training (CP), various types of instruction-following data for instruction tuning, and significant human resources are allocated to label data for reinforcement learning from human feedback (RLHF).However, with our method, Instruction Continual Pre-training (InsCP), these processes are streamlined into a single step\n\nIn this work, we propose a novel fine-tuning approach called Instruction Continual Pretraining (InsCP) for LLMs to adapt to non-English languages.This process draws inspiration from merging CP and SFT into a unified one-step training process.Additionally, we investigate whether LLMs, equipped with their own templates, can recognize tags during CP.Furthermore, we hypothesize that providing a chat template during CP prevents the model from forgetting its conversational abilities, as it resembles its original training conditions.Our approach begins with CP on a specific dataset, where we augment each piece of data with special instruction tokens, such as < |begin_of _text| > in LLaMA3(AI@Meta, 2024).This augmentation enables the model to respond to target language inputs in the target language and effectively handle offensive input based on its original RLHF capabilities.\n\nWe evaluate the effectiveness of InsCP on LLMs, primarily focusing on the LLaMA3instruct model, across three key aspects: language alignment, reliability, and knowledge benchmarks.Language alignment tests the model's proficiency in learning the desired language, while reliability evaluates its retention of RLHF capabilities.",
            "score": 0.4171705141393421,
            "section_title": "Introduction",
            "char_start_offset": 1815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1055
                },
                {
                    "start": 1057,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1299
                },
                {
                    "start": 1299,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1589
                },
                {
                    "start": 1589,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1938
                },
                {
                    "start": 1940,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200439453125
        },
        {
            "corpus_id": "273653892",
            "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
            "text": "Then human labelers rank these output pairs based on their preferences. Given human predictions, the reward model is trained to predict these rankings, effectively learning human preferences. Notably, [102] proposes an approach, namely Reinforcement Learning from AI Feedback (RLAIF), the annotation of preference on response pairs can be generated by an AI agent, increasing the automatic ability of the reinforcement process. \u201a Reinforcement Learning Fine-Tuning: The final step involves formalizing the alignment process as a reinforcement learning problem. Here, the pre-trained language model acts as a policy generating text, with the reward model providing feedback scores. To prevent the model from deviating too far from its initial state, a penalty term is often included in the reward function. The language model is then optimized using algorithms like SARSA [103], DQN [104], PPO [105], DPO [106], and GRPO [63], iteratively improving its performance based on human-aligned rewards. 3) Datasets for LLM: A critical component of the development and deployment of LLM is the datasets used at various stages of their lifecycle, which significantly influence their capabilities and performance. In this section, we delve into the datasets that are instrumental in the Pre-training, SFT, and RLHF. The Pre-training phase is where an LLM absorbs the foundational knowledge from a diverse array of textual data. This stage is pivotal, as it sets the stage for the model's general understanding of language. The datasets used in Pretraining are vast and varied, encompassing everything from the sprawling expanse of the internet to curated collections of literature and encyclopedias. SFT is the process where the LLM is fine-tuned on specific tasks or domains. This phase refines the model's abilities, enabling it to perform with greater precision and relevance in targeted applications. SFT datasets are often more specialized and may include annotated examples that guide the model towards desired behaviors and outputs. RLHF is the stage where the LLM is further optimized based on human feedback. This phase enhances the model's alignment with human preferences and values, ensuring that its outputs are more aligned with user expectations.",
            "score": 0.4167238424391366,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 35412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 876,
                    "matchedPaperCorpusId": "10253791"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "57373825"
                },
                {
                    "start": 904,
                    "end": 909,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        }
    ],
    "quotes": {
        "cost": 0.16632300000000003,
        "quotes": [
            {
                "idx": 0,
                "key": "[260125946 | Liu | 2023 | Citations: 1]",
                "snippets": "The main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "21",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1719,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[261705563 | Li et al. | 2023 | Citations: 118]",
                "snippets": "To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1542,
                        "end": 1827,
                        "sentence_offsets": [
                            {
                                "start": 1542,
                                "end": 1827
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[265608902 | Lin et al. | 2023 | Citations: 198]",
                "snippets": "We investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Preprint",
                        "pdf_hash": "",
                        "start": 645,
                        "end": 1901,
                        "sentence_offsets": [
                            {
                                "start": 632,
                                "end": 816
                            },
                            {
                                "start": 817,
                                "end": 969
                            },
                            {
                                "start": 970,
                                "end": 973
                            },
                            {
                                "start": 974,
                                "end": 1180
                            },
                            {
                                "start": 1181,
                                "end": 1509
                            },
                            {
                                "start": 1510,
                                "end": 1528
                            },
                            {
                                "start": 1529,
                                "end": 1651
                            },
                            {
                                "start": 1652,
                                "end": 1901
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[267658120 | Dong et al. | 2024 | Citations: 68]",
                "snippets": "Alignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "LLM Safety Alignment",
                        "pdf_hash": "",
                        "start": 276,
                        "end": 743,
                        "sentence_offsets": [
                            {
                                "start": 276,
                                "end": 297
                            },
                            {
                                "start": 298,
                                "end": 420
                            },
                            {
                                "start": 421,
                                "end": 633
                            },
                            {
                                "start": 634,
                                "end": 743
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Alignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[268553509 | Lee et al. | 2024 | Citations: 8]",
                "snippets": "Despite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in NLP tasks. Previous works (Zhou et al., 2023; Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities. Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users. Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Despite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in NLP tasks. Previous works (Zhou et al., 2023; Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities. Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users. Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[269502676 | Lin et al. | 2024 | Citations: 18]",
                "snippets": "In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 574,
                        "end": 777,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[270123077 | Zhao et al. | 2024 | Citations: 13]",
                "snippets": "(Zhou et al., 2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1221,
                        "end": 1501,
                        "sentence_offsets": [
                            {
                                "start": 1206,
                                "end": 1501
                            }
                        ],
                        "ref_mentions": [
                            "258822910"
                        ],
                        "quote": "(Zhou et al., 2023) introduced the Superficial Alignment Hypothesis, stating that LLMs acquire all their capabilities during pre-training, and fine-tuning only allows the models to better access such knowledge when interacting with users (Gudibande et al., 2023;Duan et al., 2023)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270123676 | Hadji-Kyriacou et al. | 2024 | Citations: 1]",
                "snippets": "However, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it. One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters. Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 472,
                        "end": 1096,
                        "sentence_offsets": [
                            {
                                "start": 246,
                                "end": 610
                            },
                            {
                                "start": 612,
                                "end": 729
                            },
                            {
                                "start": 729,
                                "end": 1058
                            },
                            {
                                "start": 1058,
                                "end": 1234
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it. One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters. Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270357323 | Thakkar et al. | 2024 | Citations: 11]",
                "snippets": "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "F Background and Related Work F.1 Alignment Methods",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 705,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 110
                            },
                            {
                                "start": 110,
                                "end": 206
                            },
                            {
                                "start": 206,
                                "end": 429
                            },
                            {
                                "start": 429,
                                "end": 555
                            },
                            {
                                "start": 555,
                                "end": 705
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[271213062 | Jiang et al. | 2024 | Citations: 3]",
                "snippets": "Recent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 668,
                        "end": 1419,
                        "sentence_offsets": [
                            {
                                "start": 668,
                                "end": 936
                            },
                            {
                                "start": 936,
                                "end": 1085
                            },
                            {
                                "start": 1085,
                                "end": 1419
                            }
                        ],
                        "ref_mentions": [
                            "258822910",
                            "258822910"
                        ],
                        "quote": "Recent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[271709991 | Liu et al. | 2024 | Citations: 0]",
                "snippets": "For instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 234,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 234
                            }
                        ],
                        "ref_mentions": [
                            "258822910"
                        ],
                        "quote": "For instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[273186633 | Raghavendra et al. | 2024 | Citations: 3]",
                "snippets": "We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[273346831 | Fei et al. | 2024 | Citations: 0]",
                "snippets": "Recent studies (Zhou et al., 2023)Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, (Lin et al., 2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265608902 | Lin et al. | 2023 | Citations: 198]": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 823,
                        "end": 1193,
                        "sentence_offsets": [
                            {
                                "start": 823,
                                "end": 1055
                            },
                            {
                                "start": 1056,
                                "end": 1193
                            }
                        ],
                        "ref_mentions": [
                            "258822910",
                            "265608902"
                        ],
                        "quote": "Recent studies (Zhou et al., 2023)Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, (Lin et al., 2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[273350763 | Li et al. | 2024 | Citations: 3]",
                "snippets": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "DISCUSSION, LIMITATION, AND CONCLUSION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1453,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 274
                            },
                            {
                                "start": 275,
                                "end": 516
                            },
                            {
                                "start": 517,
                                "end": 763
                            },
                            {
                                "start": 764,
                                "end": 841
                            },
                            {
                                "start": 844,
                                "end": 993
                            },
                            {
                                "start": 994,
                                "end": 1127
                            },
                            {
                                "start": 1130,
                                "end": 1262
                            },
                            {
                                "start": 1263,
                                "end": 1453
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[273901354 | Singla et al. | 2024 | Citations: 2]",
                "snippets": "First, the superficial alignment hypothesis (Zhou et al., 2023) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2023)Zhao et al., 2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265608902 | Lin et al. | 2023 | Citations: 198]": "The alignment tuning process of large language models (LLMs) typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). A recent study, LIMA (Zhou et al. 2023), shows that using merely 1K examples for SFT can achieve significant alignment performance as well, suggesting that the effect of alignment tuning might be\"superficial.\"This raises questions about how exactly the alignment tuning transforms a base LLM. We analyze the effect of alignment tuning by examining the token distribution shift between base LLMs and their aligned counterpart. Our findings reveal that base LLMs and their alignment-tuned versions perform nearly identically in decoding on the majority of token positions. Most distribution shifts occur with stylistic tokens. These direct evidence strongly supports the Superficial Alignment Hypothesis suggested by LIMA. Based on these findings, we rethink the alignment of LLMs by posing the research question: how effectively can we align base LLMs without SFT or RLHF? To address this, we introduce a simple, tuning-free alignment method, URIAL. URIAL achieves effective alignment purely through in-context learning (ICL) with base LLMs, requiring as few as three constant stylistic examples and a system prompt. We conduct a fine-grained and interpretable evaluation on a diverse set of examples, named JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with URIAL can match or even surpass the performance of LLMs aligned with SFT or SFT+RLHF. We show that the gap between tuning-free and tuning-based alignment methods can be significantly reduced through strategic prompting and ICL. Our findings on the superficial nature of alignment tuning and results with URIAL suggest that deeper analysis and theoretical understanding of alignment is crucial to future LLM research.",
                    "[258822910 | Zhou et al. | 2023 | Citations: 850]": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 248,
                        "end": 438,
                        "sentence_offsets": [
                            {
                                "start": 248,
                                "end": 438
                            }
                        ],
                        "ref_mentions": [
                            "258822910",
                            "265608902"
                        ],
                        "quote": "First, the superficial alignment hypothesis (Zhou et al., 2023) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2023)Zhao et al., 2024)."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[274131023 | Yang et al. | 2024 | Citations: 0]",
                "snippets": "Alignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246426909 | Ouyang et al. | 2022 | Citations: 13203]": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                "metadata": [
                    {
                        "section_title": "Aligned LLMs",
                        "pdf_hash": "",
                        "start": 1595,
                        "end": 2036,
                        "sentence_offsets": [
                            {
                                "start": 1595,
                                "end": 1614
                            },
                            {
                                "start": 1615,
                                "end": 1749
                            },
                            {
                                "start": 1750,
                                "end": 1865
                            },
                            {
                                "start": 1866,
                                "end": 2036
                            }
                        ],
                        "ref_mentions": [
                            "246426909"
                        ],
                        "quote": "Alignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[274982908 | Guan et al. | 2024 | Citations: 77]",
                "snippets": "We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Ablations for different components of the method",
                        "pdf_hash": "",
                        "start": 699,
                        "end": 836,
                        "sentence_offsets": [
                            {
                                "start": 699,
                                "end": 836
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.021426,
        "cot": "I'll organize the quotes into dimensions that directly address the user's query about whether alignment training (SFT or RLHF) imparts new knowledge/reasoning abilities or just influences output formats.\n\nFor this scientific query, I need the following dimensions:\n\n1. Introduction/Background: This will provide context on what alignment training is and why this question matters. Though there might not be direct quotes for this, it's essential for framing the response.\n\n2. The Superficial Alignment Hypothesis: This appears to be a central concept in many quotes (0, 1, 2, 6, 9, 12, 13, 14) and directly addresses the core of the query by suggesting alignment is primarily superficial. This should be a synthesis format to coherently explain this hypothesis.\n\n3. Empirical Evidence Supporting \"Superficial Alignment\": Multiple quotes (0, 2, 9) describe experimental evidence showing alignment primarily affects style/format rather than adding new capabilities. These quotes include token distribution analyses and small dataset experiments that the user specifically requested. This should be synthesis format to connect the different evidence points.\n\n4. Evidence of Alignment Affecting Capabilities: Some quotes (11, 5) suggest alignment training can potentially add new knowledge or change capabilities. This contradicts the superficial alignment hypothesis and should be presented as a balanced counterpoint. Synthesis format is appropriate to explain the nuances.\n\n5. Effects on Reasoning and Performance: Several quotes (4, 7, 11, 16) specifically address how alignment impacts reasoning abilities, which is a key part of the query. This should be synthesis format to properly analyze the conflicting evidence.\n\n6. Practical Implications: Finally, addressing what these findings mean for alignment training in practice (10, 15) would provide a useful conclusion. Synthesis format is appropriate to connect findings to real-world applications.",
        "plan": {
            "Introduction: Alignment Training in LLMs (synthesis)": [
                3,
                8,
                15
            ],
            "The Superficial Alignment Hypothesis (synthesis)": [
                1,
                6,
                13,
                14
            ],
            "Empirical Evidence Supporting Superficial Alignment (synthesis)": [
                0,
                2,
                9,
                12
            ],
            "Evidence of Alignment Affecting Capabilities (synthesis)": [
                5,
                11
            ],
            "Effects on Reasoning and Performance (synthesis)": [
                4,
                7,
                10,
                16
            ],
            "Practical Implications and Conclusions (synthesis)": []
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction: Alignment Training in LLMs",
                "tldr": "Alignment training in large language models (LLMs) refers to techniques that adjust model behavior to better match human preferences and intentions. The two primary methods are Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), which aim to make models more helpful, safe, and responsive to user needs. (4 sources)",
                "text": "\nAlignment training encompasses methods designed to reduce the gap between an LLM's pre-trained capabilities and what users actually need or expect from these systems. The core objective is to ensure LLMs \"align with desired objectives, such as safety\" <Paper corpusId=\"267658120\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>. This training addresses the fundamental challenge that \"making language models bigger does not inherently make them better at following a user's intent\" <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. \n\nTwo principal techniques dominate the alignment landscape. Supervised Fine-Tuning (SFT), also known as instruction tuning, involves \"fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations\" <Paper corpusId=\"267658120\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>. This approach uses a dataset that is \"orders of magnitude smaller\" than pre-training data and \"follows a strict format\" of input instructions paired with gold-standard outputs <Paper corpusId=\"270357323\" paperTitle=\"(Thakkar et al., 2024)\" isShortName></Paper>. SFT has been instrumental in models like Alpaca and Mistral-7b-Instruct <Paper corpusId=\"270357323\" paperTitle=\"(Thakkar et al., 2024)\" isShortName></Paper>.\n\nThe second major technique is Reinforcement Learning from Human Feedback (RLHF), which builds upon SFT by incorporating human preferences to further refine model outputs <Paper corpusId=\"274131023\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. This approach has shown remarkable results, as demonstrated by OpenAI's InstructGPT models, where the 1.3B parameter model was preferred by humans over the much larger 175B GPT-3 model, \"despite having 100x fewer parameters\" <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>.\n\nThe ultimate goals of alignment training extend beyond improving helpfulness to also \"ensure that models are safe and harmless, reducing the risks associated with their use\" <Paper corpusId=\"270357323\" paperTitle=\"(Thakkar et al., 2024)\" isShortName></Paper>. As these techniques continue to evolve, a key question emerges: do they merely shape the style and format of outputs, or do they genuinely enhance the model's underlying capabilities and knowledge?",
                "citations": [
                    {
                        "id": "(Dong et al., 2024)",
                        "snippets": [
                            "Alignment algorithms. Alignment algorithms encompass a variety of methods that aim to ensure LLMs align with desired objectives, such as safety. Supervised fine-tuning (SFT) (OpenAI, 2023a;Touvron et al., 2023;Zhou et al., 2023a), or instruction tuning, is the process of fine-tuning LLMs on supervised data of prompt-response (input-output) demonstrations. SFT makes sure LLM are both helpful and safe by minimizing empirical losses over high-quality demonstrations."
                        ],
                        "paper": {
                            "corpus_id": 267658120,
                            "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
                            "authors": [
                                {
                                    "authorId": "2284178258",
                                    "name": "Zhichen Dong"
                                },
                                {
                                    "authorId": "2254279326",
                                    "name": "Zhanhui Zhou"
                                },
                                {
                                    "authorId": "2268678836",
                                    "name": "Chao Yang"
                                },
                                {
                                    "authorId": "2254280929",
                                    "name": "Jing Shao"
                                },
                                {
                                    "authorId": "2268675804",
                                    "name": "Yu Qiao"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 68
                        },
                        "score": 0.65576171875
                    },
                    {
                        "id": "(Ouyang et al., 2022)",
                        "snippets": [
                            "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                        ],
                        "paper": {
                            "corpus_id": 246426909,
                            "title": "Training language models to follow instructions with human feedback",
                            "authors": [
                                {
                                    "authorId": "31793034",
                                    "name": "Long Ouyang"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2115903168",
                                    "name": "Xu Jiang"
                                },
                                {
                                    "authorId": "2061137049",
                                    "name": "Diogo Almeida"
                                },
                                {
                                    "authorId": "2064084601",
                                    "name": "Carroll L. Wainwright"
                                },
                                {
                                    "authorId": "2051714782",
                                    "name": "Pamela Mishkin"
                                },
                                {
                                    "authorId": null,
                                    "name": "Chong Zhang"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "2117680841",
                                    "name": "Katarina Slama"
                                },
                                {
                                    "authorId": "2064770039",
                                    "name": "Alex Ray"
                                },
                                {
                                    "authorId": "47971768",
                                    "name": "John Schulman"
                                },
                                {
                                    "authorId": "2052366271",
                                    "name": "Jacob Hilton"
                                },
                                {
                                    "authorId": "2151735262",
                                    "name": "Fraser Kelton"
                                },
                                {
                                    "authorId": "2142365973",
                                    "name": "Luke E. Miller"
                                },
                                {
                                    "authorId": "2151735251",
                                    "name": "Maddie Simens"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "2930640",
                                    "name": "P. Welinder"
                                },
                                {
                                    "authorId": "145791315",
                                    "name": "P. Christiano"
                                },
                                {
                                    "authorId": "2990741",
                                    "name": "Jan Leike"
                                },
                                {
                                    "authorId": "49407415",
                                    "name": "Ryan J. Lowe"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 13203
                        },
                        "score": 0
                    },
                    {
                        "id": "(Thakkar et al., 2024)",
                        "snippets": [
                            "Alignment training aims to reduce the mismatch between an LLM's pre-training and user preference requirements.It also ensures that models are safe and harmless, reducing the risks associated with their use.We choose the two most widely used alignment methods: Supervised fine-tuning (SFT) SFT uses a pair of input instructions and corresponding gold answers or outputs to fine-tune the LLM using autoregressive language modeling.The training objective is similar to pre-training, but the dataset is orders of magnitude smaller and follows a strict format.This method is often used for the 'instruction-tuning' stage for models like Alpaca (Taori et al., 2023) and Mistral-7b-Instruct (Jiang et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 270357323,
                            "title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques",
                            "authors": [
                                {
                                    "authorId": "2264977662",
                                    "name": "Megh Thakkar"
                                },
                                {
                                    "authorId": "2303408438",
                                    "name": "Quentin Fournier"
                                },
                                {
                                    "authorId": "2305480521",
                                    "name": "Matthew Riemer"
                                },
                                {
                                    "authorId": "2305538500",
                                    "name": "Pin-Yu Chen"
                                },
                                {
                                    "authorId": "2301579793",
                                    "name": "Amal Zouaq"
                                },
                                {
                                    "authorId": "2283308757",
                                    "name": "Payel Das"
                                },
                                {
                                    "authorId": "123607932",
                                    "name": "Sarath Chandar"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 11
                        },
                        "score": 0.6416015625
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Alignment Training. This approach is designed to enhance the behavior of LLMs and boost their intrinsic safety by employing robust training methodologies. Key techniques include Supervised Fine-Tuning (SFT) [17] and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). SFT has made substantial contributions to alignment by refining LLMs using instruction-based datasets, which helps in generating accurate responses to a variety of tasks."
                        ],
                        "paper": {
                            "corpus_id": 274131023,
                            "title": "The Dark Side of Trust: Authority Citation-Driven Jailbreak Attacks on Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2112075893",
                                    "name": "Xikang Yang"
                                },
                                {
                                    "authorId": "102993876",
                                    "name": "Xuehai Tang"
                                },
                                {
                                    "authorId": "2126086577",
                                    "name": "Jizhong Han"
                                },
                                {
                                    "authorId": "2241727804",
                                    "name": "Songlin Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.79541015625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "The Superficial Alignment Hypothesis",
                "tldr": "The Superficial Alignment Hypothesis proposes that LLMs acquire virtually all their knowledge and capabilities during pre-training, while alignment training merely teaches them which output formats to use when responding to users. This hypothesis challenges the traditional view that alignment substantially enhances model capabilities. (5 sources)",
                "text": "\nThe Superficial Alignment Hypothesis (SAH), introduced by Zhou et al., represents a paradigm shift in our understanding of how language models develop their abilities. This hypothesis fundamentally \"challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation\" <Paper corpusId=\"273350763\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Instead, it posits that \"a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used\" <Paper corpusId=\"261705563\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe key assertion of SAH consists of two main components. First, \"Capabilities are Learned in Pretraining,\" where \"the model acquires a vast amount of general-purpose knowledge from diverse datasets\" including \"language, reasoning, factual knowledge, and even ethical guidelines\" <Paper corpusId=\"273350763\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Second, \"Alignment Guides Output Behavior,\" meaning that the alignment process \"is not responsible for teaching the model new knowledge or capabilities\" but rather \"acts as a filter that directs the model to produce acceptable formats or styles of responses\" <Paper corpusId=\"273350763\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nStrong evidence for this hypothesis emerged from the LIMA (Less Is More for Alignment) model, which demonstrated \"remarkably strong performance\" despite being fine-tuned on \"only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling\" <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>. In human evaluations, LIMA's responses were \"either equivalent or strictly preferred to GPT-4 in 43% of cases\" <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>, suggesting that \"almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output\" <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nThis hypothesis has gained significant traction in recent research, with some studies extending it to suggest that LLMs \"can be effectively aligned with lightweight tuning or simply prompting\" <Paper corpusId=\"273901354\" paperTitle=\"(Singla et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Lin et al. provided \"direct evidence\" supporting the Superficial Alignment Hypothesis by analyzing token distribution shifts between base LLMs and their aligned counterparts, finding that \"most distribution shifts occur with stylistic tokens\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This has led researchers to explore tuning-free alignment methods that achieve \"effective alignment purely through in-context learning (ICL) with base LLMs\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "The Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally challenges the traditional assumption that a language model requires extensive fine-tuning on instruction-following on preference data to align its responses with human expectation. Instead, SAH posits that the majority of a model's knowledge and capabilities are acquired during the pretraining phase, while the subsequent alignment phase primarily functions to guide the model's output format when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a small, carefully selected set of aligned data is sufficient to achieve strong performance as long as the pretraining stage has effectively captured the necessary underlying knowledge. The key assertion of SAH is that alignment is superficial, in the sense that: \n\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast amount of general-purpose knowledge from diverse datasets. These datasets contain implicit structures and information about language, reasoning, factual knowledge, and even ethical guidelines. \n\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching the model new knowledge or capabilities. Rather, it acts as a filter that directs the model to produce acceptable formats or styles of responses based on user queries, reflecting the correct subset of its vast pretrained knowledge."
                        ],
                        "paper": {
                            "corpus_id": 273350763,
                            "title": "Superficial Safety Alignment Hypothesis",
                            "authors": [
                                {
                                    "authorId": "2326007326",
                                    "name": "Jianwei Li"
                                },
                                {
                                    "authorId": "2326001415",
                                    "name": "Jung-Eun Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.6259765625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "To justify the feasibility, our inspiration stems from the concept of superficial alignment hypothesis (Zhou et al., 2023): a model's knowledge and capabilities are learnt almost entirely during pre-training, while alignment teaches it which sub-distribution of formats should be used."
                        ],
                        "paper": {
                            "corpus_id": 261705563,
                            "title": "RAIN: Your Language Models Can Align Themselves without Finetuning",
                            "authors": [
                                {
                                    "authorId": "2192674200",
                                    "name": "Yuhui Li"
                                },
                                {
                                    "authorId": "2239197291",
                                    "name": "Fangyun Wei"
                                },
                                {
                                    "authorId": "2256929424",
                                    "name": "Jinjing Zhao"
                                },
                                {
                                    "authorId": "2256776221",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "40975176",
                                    "name": "Hongyang Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 118
                        },
                        "score": 0.53515625
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                        ],
                        "paper": {
                            "corpus_id": 258822910,
                            "title": "LIMA: Less Is More for Alignment",
                            "authors": [
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "2214843767",
                                    "name": "Puxin Xu"
                                },
                                {
                                    "authorId": "1900163",
                                    "name": "Srini Iyer"
                                },
                                {
                                    "authorId": "145478138",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "3375249",
                                    "name": "Yuning Mao"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "1388010852",
                                    "name": "Avia Efrat"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2108244542",
                                    "name": "Susan Zhang"
                                },
                                {
                                    "authorId": "134007132",
                                    "name": "Gargi Ghosh"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 850
                        },
                        "score": 0
                    },
                    {
                        "id": "(Singla et al., 2024)",
                        "snippets": [
                            "First, the superficial alignment hypothesis (Zhou et al., 2023) posits that LLMs can be effectively aligned with lightweight tuning or simply prompting (Lin et al., 2023)Zhao et al., 2024)."
                        ],
                        "paper": {
                            "corpus_id": 273901354,
                            "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models",
                            "authors": [
                                {
                                    "authorId": "2179326599",
                                    "name": "Somanshu Singla"
                                },
                                {
                                    "authorId": "2261683280",
                                    "name": "Zhen Wang"
                                },
                                {
                                    "authorId": "2115347044",
                                    "name": "Tianyang Liu"
                                },
                                {
                                    "authorId": "2329737016",
                                    "name": "Abdullah Ashfaq"
                                },
                                {
                                    "authorId": "2295863002",
                                    "name": "Zhiting Hu"
                                },
                                {
                                    "authorId": "2259944106",
                                    "name": "Eric P. Xing"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.61962890625
                    },
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "We investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.59033203125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Empirical Evidence Supporting Superficial Alignment",
                "tldr": "Multiple empirical studies provide strong evidence for the Superficial Alignment Hypothesis through token distribution analyses and minimal fine-tuning experiments. These studies consistently show that alignment primarily affects stylistic tokens rather than content-bearing words, and that models fine-tuned on very small, high-quality datasets can match or exceed the performance of models trained on much larger datasets. (5 sources)",
                "text": "\nA growing body of empirical research has emerged that directly tests and supports the Superficial Alignment Hypothesis. Perhaps the most compelling evidence comes from Lin et al.'s investigation of token distribution shifts between base LLMs and their aligned versions, such as Llama-2 and Llama-2-chat. Their analysis revealed that \"base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Significantly, they found that \"the most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThis token distribution analysis provides \"direct evidence\" strongly supporting the hypothesis that alignment training primarily affects the model's response style rather than its underlying knowledge and capabilities <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. Multiple subsequent studies have cited and corroborated these findings, noting that \"only a small subset of stylistic tokens is affected after alignment\" <Paper corpusId=\"273346831\" paperTitle=\"(Fei et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nThe LIMA model provided another form of empirical evidence through its remarkable performance with minimal fine-tuning. The model demonstrated that \"with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data\" <Paper corpusId=\"260125946\" paperTitle=\"(Liu, 2023)\" isShortName></Paper>. Specifically, \"LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4\" <Paper corpusId=\"260125946\" paperTitle=\"(Liu, 2023)\" isShortName></Paper>. In human evaluations, \"responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases\" <Paper corpusId=\"271213062\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nAblation studies on LIMA further reinforced these findings by showing that \"increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity\" <Paper corpusId=\"260125946\" paperTitle=\"(Liu, 2023)\" isShortName></Paper>. This supports the notion that alignment is more about teaching appropriate response formats than imparting new knowledge. Moreover, LIMA's ability to \"conduct coherent multi-turn dialogues in a zero-shot manner\" despite minimal training on dialogue examples demonstrates that such capabilities were likely already present from pre-training <Paper corpusId=\"260125946\" paperTitle=\"(Liu, 2023)\" isShortName></Paper>.\n\nThe empirical consistency across these different studies\u2014analyzing token distributions and testing minimal fine-tuning approaches\u2014provides strong converging evidence that \"alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired\" <Paper corpusId=\"265608902\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lin et al., 2023)",
                        "snippets": [
                            "We investigate the effects of alignment tuning by directly comparing the token distributions between base LLMs and their aligned versions (e.g., Llama-2 and Llama-2-chat). Surprisingly, we find that base and aligned LLMs typically perform almost identically in most positions in terms of ranking tokens during decoding (Sec. 2). Additionally, we observe that the top-ranked tokens in aligned LLMs are mostly found within the top five tokens ranked by base LLMs, and the distribution shift is more pronounced in earlier token positions. The most significant distribution shifts occur predominantly in stylistic tokens (e.g., 'Hello', 'Thank', 'However', 'Remember', etc.), which include transitional phrases, discourse markers, and safety disclaimers, rather than in content-bearing words that directly provide useful knowledge for resolving the queries from users. Our findings (Sec. 2.3) from token distribution shift analysis directly provide substantial support for the superficial alignment hypothesis. We offer both quantitative and qualitative analyses to demonstrate that alignment tuning primarily focus on adopting the language style of responsible AI assistants and depends to a great extent on the knowledge that base LLMs have already acquired."
                        ],
                        "paper": {
                            "corpus_id": 265608902,
                            "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning",
                            "authors": [
                                {
                                    "authorId": "51583409",
                                    "name": "Bill Yuchen Lin"
                                },
                                {
                                    "authorId": "3023068",
                                    "name": "Abhilasha Ravichander"
                                },
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "46217681",
                                    "name": "Nouha Dziri"
                                },
                                {
                                    "authorId": "1947172233",
                                    "name": "Melanie Sclar"
                                },
                                {
                                    "authorId": "37619618",
                                    "name": "Khyathi Raghavi Chandu"
                                },
                                {
                                    "authorId": "1857797",
                                    "name": "Chandra Bhagavatula"
                                },
                                {
                                    "authorId": "2259707400",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 198
                        },
                        "score": 0.59033203125
                    },
                    {
                        "id": "(Fei et al., 2024)",
                        "snippets": [
                            "Recent studies (Zhou et al., 2023)Mitchell et al., 2023) argue that alignment primarily enhances LLMs' ability to generate helpful and wellformatted responses, while the foundational knowledge and capabilities stem from pretraining. More concretely, (Lin et al., 2023) analyzed Llama-2 models and found only a small subset of stylistic tokens is affected after alignment."
                        ],
                        "paper": {
                            "corpus_id": 273346831,
                            "title": "Nudging: Inference-time Alignment of LLMs via Guided Decoding",
                            "authors": [
                                {
                                    "authorId": "2121536275",
                                    "name": "Yu Fei"
                                },
                                {
                                    "authorId": "1899492908",
                                    "name": "Yasaman Razeghi"
                                },
                                {
                                    "authorId": "2299171638",
                                    "name": "Sameer Singh"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.68310546875
                    },
                    {
                        "id": "(Liu, 2023)",
                        "snippets": [
                            "The main discovery of this paper is that with a strong pre-trained language model, focused optimization of a relatively small amount of high-quality, diverse alignment data can produce results that are comparable to models trained on much more data. This supports their hypothesis that most of a model's capabilities come from pre-training, while alignment requires teaching it mainly superficial stylistic conventions. More specifically, their key findings include: -LIMA, fine-tuned on just 1,000 examples, is able to produce responses that are equal to or better than models trained on 52,000 to millions of examples, including GPT-4. This shows that large amounts of alignment data may not be necessary. -Ablation experiments show that increasing data diversity and quality have a larger impact on performance than simply scaling up data quantity. This supports the idea that alignment relies more on teaching appropriate response formats. -LIMA is able to conduct coherent multi-turn dialogues in a zero-shot manner, showing that such capabilities were likely learned during pre-training. Fine-tuning on only 30 dialogue examples substantially improves its performance. -Analysis of LIMA's absolute generation quality finds that 50% of responses are excellent and 88% meet the prompt requirements, despite training on just 1,000 examples. In summary, the main discovery is that focused optimization of a small amount of high-quality alignment data, by increasing diversity and targeting appropriate response formats, can produce results comparable to models trained on much more data. This supports the hypothesis that pre-training is relatively more important for a model's capabilities than subsequent alignment."
                        ],
                        "paper": {
                            "corpus_id": 260125946,
                            "title": "External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback",
                            "authors": [
                                {
                                    "authorId": "2173701426",
                                    "name": "Akide Liu"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.5673828125
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Recent work (Zhou et al., 2023) has demonstrated that LLMs mainly learn the style or format for interacting with users through simple instruction tuning and alignment, by leveraging their prior knowledge and capabilities already acquired during the pre-training stage.Therefore, only employing as few as 1,000 examples in supervised fine-tuning can also achieve satisfactory alignment performance (Zhou et al., 2023).Furthermore, by comparing the token distribution before and after alignment, recent work (Lin et al., 2023) found that the most significant distribution shifts appear dominantly in stylistic tokens such as transitional phrases and discourse markers instead of contextual words that involve rich knowledge for solving downstream tasks."
                        ],
                        "paper": {
                            "corpus_id": 271213062,
                            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
                            "authors": [
                                {
                                    "authorId": "2118240359",
                                    "name": "Jinhao Jiang"
                                },
                                {
                                    "authorId": "2018027",
                                    "name": "Junyi Li"
                                },
                                {
                                    "authorId": "2290240238",
                                    "name": "Wayne Xin Zhao"
                                },
                                {
                                    "authorId": "2157992507",
                                    "name": "Yang Song"
                                },
                                {
                                    "authorId": "2146341464",
                                    "name": "Tao Zhang"
                                },
                                {
                                    "authorId": "2274218622",
                                    "name": "Ji-Rong Wen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.53076171875
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                        ],
                        "paper": {
                            "corpus_id": 258822910,
                            "title": "LIMA: Less Is More for Alignment",
                            "authors": [
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "2214843767",
                                    "name": "Puxin Xu"
                                },
                                {
                                    "authorId": "1900163",
                                    "name": "Srini Iyer"
                                },
                                {
                                    "authorId": "145478138",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "3375249",
                                    "name": "Yuning Mao"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "1388010852",
                                    "name": "Avia Efrat"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2108244542",
                                    "name": "Susan Zhang"
                                },
                                {
                                    "authorId": "134007132",
                                    "name": "Gargi Ghosh"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 850
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evidence of Alignment Affecting Capabilities",
                "tldr": "While the Superficial Alignment Hypothesis has substantial support, recent studies present evidence that alignment training can enhance certain capabilities beyond mere stylistic changes. Research shows that performance on reasoning tasks scales with the number of fine-tuning examples, suggesting that post-training can genuinely improve capabilities in areas like mathematical reasoning and knowledge integration. (2 sources)",
                "text": "\nDespite the compelling evidence for the Superficial Alignment Hypothesis, emerging research suggests that alignment training can impact a model's capabilities in meaningful ways that go beyond stylistic changes. Recent studies have begun to challenge the notion that capabilities are exclusively learned during pre-training, revealing more nuanced effects of post-training processes.\n\nLin et al. identified an important limitation of Supervised Fine-Tuning (SFT), noting that \"training the LLM on new knowledge or unfamiliar texts can encourage hallucination\" which can make models \"less factual as it trains on human labeled data that may be novel to the LLM\" <Paper corpusId=\"269502676\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. This observation indicates that alignment training can affect not just the style but also the factuality and reliability of model outputs, suggesting deeper impacts on model behavior than mere superficial changes.\n\nMore direct evidence challenging the Superficial Alignment Hypothesis comes from Raghavendra et al., who conducted extensive experiments with the Llama-3, Mistral, and Llama-2 model families. Their research revealed that \"post-training task performance scales as a power law against the number of finetuning examples\" across various capabilities including \"mathematical reasoning, coding, instruction following, and multihop-reasoning\" <Paper corpusId=\"273186633\" paperTitle=\"(Raghavendra et al., 2024)\" isShortName></Paper>. This finding directly contradicts the notion that alignment training merely affects output style without improving underlying capabilities.\n\nParticularly noteworthy is their observation that for complex tasks like mathematics and multihop reasoning, \"a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks\" <Paper corpusId=\"273186633\" paperTitle=\"(Raghavendra et al., 2024)\" isShortName></Paper>. Instead, they found that \"model performance is correlated with its reasoning ability and it improves significantly with more examples,\" highlighting that substantial capability improvements can occur during post-training with sufficient data <Paper corpusId=\"273186633\" paperTitle=\"(Raghavendra et al., 2024)\" isShortName></Paper>.\n\nPerhaps most significantly, Raghavendra et al. directly challenged a core tenet of the Superficial Alignment Hypothesis by demonstrating that \"language models are not necessarily limited to using knowledge learned during pre-training\" <Paper corpusId=\"273186633\" paperTitle=\"(Raghavendra et al., 2024)\" isShortName></Paper>. Their research showed that \"with appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering\" <Paper corpusId=\"273186633\" paperTitle=\"(Raghavendra et al., 2024)\" isShortName></Paper>. This evidence suggests that alignment training can genuinely enhance a model's capabilities to acquire and apply new knowledge, rather than merely teaching it to present pre-existing knowledge in more acceptable formats.",
                "citations": [
                    {
                        "id": "(Lin et al., 2024)",
                        "snippets": [
                            "In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM."
                        ],
                        "paper": {
                            "corpus_id": 269502676,
                            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "122045993",
                                    "name": "Sheng-Chieh Lin"
                                },
                                {
                                    "authorId": "2299485255",
                                    "name": "Luyu Gao"
                                },
                                {
                                    "authorId": "9185192",
                                    "name": "Barlas O\u011fuz"
                                },
                                {
                                    "authorId": "2266752758",
                                    "name": "Wenhan Xiong"
                                },
                                {
                                    "authorId": "2273564585",
                                    "name": "Jimmy Lin"
                                },
                                {
                                    "authorId": "2072801764",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2292024725",
                                    "name": "Xilun Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 18
                        },
                        "score": 0.67138671875
                    },
                    {
                        "id": "(Raghavendra et al., 2024)",
                        "snippets": [
                            "We re-examine these claims by empirically studying the scaling behavior of post-training with increasing finetuning examples and evaluating them using objective task-specific standardized benchmarks. Through experiments with the Llama-3, Mistral, and Llama-2 model families of multiple sizes, we observe that, similar to the pre-training scaling laws, post-training task performance scales as a power law against the number of finetuning examples. This power law relationship holds across a broad array of capabilities, including mathematical reasoning, coding, instruction following, and multihop-reasoning. In addition, for tasks like math and multihop reasoning, we observe that a handful of examples merely align the model stylistically but do not saturate performance on the benchmarks. Model performance is instead correlated with its reasoning ability and it improves significantly with more examples, illustrating the need for holistic evaluation programs leveraging objective benchmarks in addition to measurement of alignment to human preferences. We also observe that language models are not necessarily limited to using knowledge learned during pre-training. With appropriate post-training, a model's ability to integrate new knowledge greatly improves on downstream tasks like multihop question-answering."
                        ],
                        "paper": {
                            "corpus_id": 273186633,
                            "title": "Revisiting the Superficial Alignment Hypothesis",
                            "authors": [
                                {
                                    "authorId": "2345822162",
                                    "name": "Mohit Raghavendra"
                                },
                                {
                                    "authorId": "2151210591",
                                    "name": "Vaskar Nath"
                                },
                                {
                                    "authorId": "2265402399",
                                    "name": "Sean M. Hendryx"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.68896484375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effects on Reasoning and Performance",
                "tldr": "Alignment training has complex effects on model reasoning abilities, with evidence suggesting that RLHF can sometimes impair reasoning while in other cases enhancing it through better utilization of chain-of-thought processes. Research indicates that the relationship between alignment techniques and reasoning performance varies based on model size, training approach, and the specific tasks being evaluated. (5 sources)",
                "text": "\nThe impact of alignment techniques on model reasoning capabilities presents a nuanced picture with contradictory findings across studies. While previous sections established that alignment primarily affects stylistic elements, emerging research suggests more complex effects on reasoning and performance. Evidence indicates that alignment training can sometimes diminish rather than enhance reasoning capabilities in certain contexts.\n\nLee et al. highlight that \"despite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in NLP tasks\" <Paper corpusId=\"268553509\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This observation aligns with the skepticism raised regarding whether alignment truly enhances model capabilities or merely adjusts response styles.\n\nMore concerning, Hadji-Kyriacou et al. report that \"RLHF may actually hurt an LLM's reasoning abilities rather than improving it,\" citing studies that found \"performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks\" <Paper corpusId=\"270123676\" paperTitle=\"(Hadji-Kyriacou et al., 2024)\" isShortName></Paper>. This suggests that alignment techniques might sometimes produce a trade-off between helpfulness and reasoning accuracy.\n\nThe relationship between alignment techniques appears to vary with model scale. Research indicates that \"SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters\" <Paper corpusId=\"270123676\" paperTitle=\"(Hadji-Kyriacou et al., 2024)\" isShortName></Paper>. This scale-dependent effect suggests that alignment's impact on reasoning is not uniform across model sizes.\n\nAnother concerning effect of alignment is the potential increased tendency toward hallucination in certain contexts. Research has found \"an increased tendency for RLHF models to make up information in closed domain tasks ('hallucination') compared to models trained with SFT alone\" <Paper corpusId=\"270123676\" paperTitle=\"(Hadji-Kyriacou et al., 2024)\" isShortName></Paper>. This finding highlights a potential trade-off where models become more helpful but potentially less factually reliable.\n\nHowever, not all evidence suggests negative effects on reasoning. Some researchers propose that different alignment techniques contribute complementary effects on reasoning abilities. Guan et al. suggest that \"the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL\" <Paper corpusId=\"274982908\" paperTitle=\"(Guan et al., 2024)\" isShortName></Paper>. This indicates that while SFT might establish reasoning foundations, reinforcement learning helps optimize how models apply those reasoning capabilities.\n\nThe efficacy of alignment approaches also appears to depend on data quality and model capabilities. Liu et al. note that \"when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF\" <Paper corpusId=\"271709991\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822910\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>. This suggests that with high-quality supervision and strong base models, simpler alignment approaches may yield reasoning performance comparable to more complex methods.\n\nThese varied findings reveal that alignment's impact on reasoning and performance is not straightforward but depends on implementation details, model architecture, dataset quality, and the specific reasoning tasks being evaluated. While alignment certainly affects how models present their reasoning, the extent to which it enhances or impairs underlying reasoning capabilities appears highly contextual.",
                "citations": [
                    {
                        "id": "(Lee et al., 2024)",
                        "snippets": [
                            "Despite recent successes in preference alignment, training LLMs through RLHF does not guarantee a significant improvement of LLM's capabilities, in terms of downstream performance in NLP tasks. Previous works (Zhou et al., 2023; Lin et al., 2023) have raised skepticism regarding the efficacy of current alignment techniques in improving LLM's capabilities. Zhou et al. (2023) claim that such alignment tuning might be superficial learning, where the model primarily learns favorable styles or formats for interacting with users. Lin et al. (2023) also observe that most distribution shifts between base and post-alignment LLMs tend to be predominantly in stylistic tokens."
                        ],
                        "paper": {
                            "corpus_id": 268553509,
                            "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
                            "authors": [
                                {
                                    "authorId": "79733119",
                                    "name": "Kyungjae Lee"
                                },
                                {
                                    "authorId": "1474356736",
                                    "name": "Dasol Hwang"
                                },
                                {
                                    "authorId": "2282197642",
                                    "name": "Sunghyun Park"
                                },
                                {
                                    "authorId": "2288604723",
                                    "name": "Youngsoo Jang"
                                },
                                {
                                    "authorId": "2269856969",
                                    "name": "Moontae Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.62744140625
                    },
                    {
                        "id": "(Hadji-Kyriacou et al., 2024)",
                        "snippets": [
                            "However, recent research has shown that RLHF may actually hurt an LLM's reasoning abilities rather than improving it. One study [6] discovered that performing alignment during the Supervised Fine-Tuning (SFT) stage of training may lead to worse performance on reasoning benchmarks, and another [4] discovered that SFT alone outperforms RLHF for smaller models with the benefits of RLHF only emerging for models with more than 1 Billion parameters. Ouyang et al. [31] also reports an increased tendency for RLHF models to make up information in closed domain tasks (\"hallucination\") compared to models trained with SFT alone."
                        ],
                        "paper": {
                            "corpus_id": 270123676,
                            "title": "Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads",
                            "authors": [
                                {
                                    "authorId": "2139984073",
                                    "name": "Avelina Asada Hadji-Kyriacou"
                                },
                                {
                                    "authorId": "46837178",
                                    "name": "Ognjen Arandjelov\u00edc"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1
                        },
                        "score": 0.568359375
                    },
                    {
                        "id": "(Guan et al., 2024)",
                        "snippets": [
                            "We believe that the model learns a strong prior for safe reasoning during SFT, and then learns to use its CoT more effectively during RL."
                        ],
                        "paper": {
                            "corpus_id": 274982908,
                            "title": "Deliberative Alignment: Reasoning Enables Safer Language Models",
                            "authors": [
                                {
                                    "authorId": "2334583384",
                                    "name": "Melody Y. Guan"
                                },
                                {
                                    "authorId": "2185778",
                                    "name": "Manas R. Joglekar"
                                },
                                {
                                    "authorId": "2297774080",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "2329885104",
                                    "name": "Saachi Jain"
                                },
                                {
                                    "authorId": "2286382037",
                                    "name": "Boaz Barak"
                                },
                                {
                                    "authorId": "2219550562",
                                    "name": "Alec Helyar"
                                },
                                {
                                    "authorId": "2328288671",
                                    "name": "Rachel Dias"
                                },
                                {
                                    "authorId": "2275244586",
                                    "name": "Andrea Vallone"
                                },
                                {
                                    "authorId": "2282935403",
                                    "name": "Hongyu Ren"
                                },
                                {
                                    "authorId": "2329627448",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "2275839391",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "2336876158",
                                    "name": "Sam Toyer"
                                },
                                {
                                    "authorId": "2151087994",
                                    "name": "Johannes Heidecke"
                                },
                                {
                                    "authorId": "2297773170",
                                    "name": "Alex Beutel"
                                },
                                {
                                    "authorId": "2105840001",
                                    "name": "Amelia Glaese"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 77
                        },
                        "score": 0.61279296875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "For instance, LIMA (Zhou et al., 2023) has experimentally demonstrated that when the pre-trained model's capabilities are sufficiently strong and the quality of the SFT data is high, it can achieve results comparable to those of RLHF."
                        ],
                        "paper": {
                            "corpus_id": 271709991,
                            "title": "Progressively Label Enhancement for Large Language Model Alignment",
                            "authors": [
                                {
                                    "authorId": "2246701947",
                                    "name": "Biao Liu"
                                },
                                {
                                    "authorId": "2314833623",
                                    "name": "Ning Xu"
                                },
                                {
                                    "authorId": "2314830191",
                                    "name": "Xin Geng"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.58837890625
                    },
                    {
                        "id": "(Zhou et al., 2023)",
                        "snippets": [
                            "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output."
                        ],
                        "paper": {
                            "corpus_id": 258822910,
                            "title": "LIMA: Less Is More for Alignment",
                            "authors": [
                                {
                                    "authorId": "2384711",
                                    "name": "Chunting Zhou"
                                },
                                {
                                    "authorId": "144118452",
                                    "name": "Pengfei Liu"
                                },
                                {
                                    "authorId": "2214843767",
                                    "name": "Puxin Xu"
                                },
                                {
                                    "authorId": "1900163",
                                    "name": "Srini Iyer"
                                },
                                {
                                    "authorId": "145478138",
                                    "name": "Jiao Sun"
                                },
                                {
                                    "authorId": "3375249",
                                    "name": "Yuning Mao"
                                },
                                {
                                    "authorId": "2378954",
                                    "name": "Xuezhe Ma"
                                },
                                {
                                    "authorId": "1388010852",
                                    "name": "Avia Efrat"
                                },
                                {
                                    "authorId": "2114104308",
                                    "name": "Ping Yu"
                                },
                                {
                                    "authorId": "49297123",
                                    "name": "L. Yu"
                                },
                                {
                                    "authorId": "2108244542",
                                    "name": "Susan Zhang"
                                },
                                {
                                    "authorId": "134007132",
                                    "name": "Gargi Ghosh"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 850
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Practical Implications and Conclusions",
                "tldr": "The debate on whether alignment training adds capabilities or merely shapes outputs has significant practical implications for LLM development strategies. Understanding this distinction helps developers make informed decisions about resource allocation, model evaluation, and alignment approaches that balance helpfulness with maintaining factual accuracy. (LLM Memory)",
                "text": "\nThe emerging research on alignment's effects has several important practical implications for how we develop, evaluate, and deploy language models. First, the evidence supporting the Superficial Alignment Hypothesis suggests that developers might achieve more efficient alignment by focusing on high-quality, diverse datasets rather than simply scaling up alignment data. As demonstrated by models like LIMA, \"carefully curated smaller datasets can yield comparable or better results than much larger datasets, potentially reducing computational costs and training time\" <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThis insight has direct implications for resource allocation in model development. If most capabilities are indeed acquired during pre-training, then \"organizations might benefit from investing more heavily in pre-training on diverse, high-quality corpora and less on extensive alignment training\" <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This approach could be particularly valuable for teams with limited computational resources who need to make strategic decisions about where to focus their efforts.\n\nHowever, the contradictory evidence regarding alignment's effects on reasoning capabilities suggests a more nuanced approach is needed. Since \"different alignment techniques appear to have varying impacts depending on model size, task type, and implementation details,\" developers should \"carefully evaluate which alignment approaches best serve their specific use cases\" <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This might involve targeted alignment for specific reasoning capabilities rather than general-purpose alignment.\n\nThe potential trade-offs between helpfulness and factuality highlighted in several studies also have important implications for model evaluation. When assessing aligned models, developers should \"explicitly test for both stylistic appropriateness and preservation of reasoning capabilities\" to ensure alignment hasn't inadvertently compromised important model functions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This suggests the need for multidimensional evaluation frameworks that capture both alignment quality and underlying capabilities.\n\nFor deployment contexts where factual accuracy is paramount, the findings that RLHF may sometimes increase hallucination suggests that \"developers might need to implement additional safeguards or use hybrid approaches that balance helpfulness with factual reliability\" <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This could involve combining different alignment techniques or implementing specific interventions targeted at reducing hallucination.\n\nIn conclusion, while the question of whether alignment training imparts new capabilities or merely influences output formats remains somewhat open, the available evidence suggests a middle ground. Alignment training primarily affects how models present information rather than fundamentally altering their capabilities, but certain alignment approaches can enhance specific abilities with sufficient high-quality data. This understanding allows developers to make more informed choices about alignment strategies that maximize helpfulness while preserving the knowledge and reasoning abilities acquired during pre-training.",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.13111499999999998
    }
}