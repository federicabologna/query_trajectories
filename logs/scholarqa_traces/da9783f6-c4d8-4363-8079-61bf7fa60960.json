{
    "query": "machine unlearning methods to remove knowledge from language models",
    "user_id": "lib_user",
    "task_id": "da9783f6-c4d8-4363-8079-61bf7fa60960",
    "timestamp": "2025-06-23T22:57:24.567147",
    "n_retrieval": 256,
    "n_retrieved": 234,
    "n_candidates": 40,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.489918,
    "decomposed_query": {
        "rewritten_query": "Methods to remove knowledge from language models.",
        "keyword_query": "machine unlearning remove knowledge language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009414,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Erasing Conceptual Knowledge from Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02760, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52017367",
                    "name": "Rohit Gandikota"
                },
                {
                    "authorId": "2140009998",
                    "name": "Sheridan Feucht"
                },
                {
                    "authorId": "2225941937",
                    "name": "Samuel Marks"
                },
                {
                    "authorId": "2284996653",
                    "name": "David Bau"
                }
            ],
            "abstract": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info",
            "corpus_id": 273098800,
            "sentences": [
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
                    "score": 0.47181110617925565,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 497
                        },
                        {
                            "start": 500,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1200
                        },
                        {
                            "start": 1201,
                            "end": 1370
                        },
                        {
                            "start": 1373,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1654
                        },
                        {
                            "start": 1655,
                            "end": 1839
                        },
                        {
                            "start": 1840,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2465
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                },
                {
                    "corpus_id": "273098800",
                    "title": "Erasing Conceptual Knowledge from Language Models",
                    "text": "This work reframes the challenge of machine unlearning for large language models, shifting from traditional samplebased approaches to concept-oriented unlearning through introspective classification. Our proposed Erasure of Language Memory (ELM) method demonstrates that effective concept unlearning requires modifying the model's output distribution based on its own ability to recognize and evaluate knowledge. By using low-rank model updates guided by the model's introspective classification, ELM achieves targeted concept removal while preserving the model's broader capabilities. Our experiments show that this approach overcomes limitations of previous methods like gradient ascent or representation disruption, as evidenced by near-random performance on multiple-choice questions related to erased concepts while maintaining accuracy on other tasks. Furthermore, ELM's resistance to adversarial attacks validates our hypothesis that concept unlearning should leverage the model's own understanding of its knowledge. In addition to providing a practical solution for concept erasure, we have established a foundation for more comprehensive evaluation of knowledge erasure in language models. Response Before Attack: The -The In -----were ---max --pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr --pr ---- \n\nResponse after attack: to be stopped whereas fit -represents from were mak bls coming ** -was ** -form w ** -zero ** -zero -** -** -in ** -** -form",
                    "score": 0.39857759446045876,
                    "section_title": "Conclusion",
                    "char_start_offset": 26413,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 199
                        },
                        {
                            "start": 200,
                            "end": 412
                        },
                        {
                            "start": 413,
                            "end": 585
                        },
                        {
                            "start": 586,
                            "end": 857
                        },
                        {
                            "start": 858,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1314
                        },
                        {
                            "start": 1317,
                            "end": 1464
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                }
            ],
            "relevance_judgement": 0.99951171875,
            "relevance_judgment_input_expanded": "# Title: Erasing Conceptual Knowledge from Language Models\n# Venue: arXiv.org\n# Authors: Rohit Gandikota, Sheridan Feucht, Samuel Marks, David Bau\n## Abstract\nIn this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across key metrics, including near-random scores on erased topic assessments, maintained coherence in text generation, preserved accuracy on unrelated benchmarks, and robustness under adversarial attacks. Our code, data, and trained models are available at https://elm.baulab.info\n## Introduction\nWhat does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.\n\n## Conclusion\nThis work reframes the challenge of machine unlearning for large language models, shifting from traditional samplebased approaches to concept-oriented unlearning through introspective classification. Our proposed Erasure of Language Memory (ELM) method demonstrates that effective concept unlearning requires modifying the model's output distribution based on its own ability to recognize and evaluate knowledge. By using low-rank model updates guided by the model's introspective classification, ELM achieves targeted concept removal while preserving the model's broader capabilities. Our experiments show that this approach overcomes limitations of previous methods like gradient ascent or representation disruption, as evidenced by near-random performance on multiple-choice questions related to erased concepts while maintaining accuracy on other tasks. Furthermore, ELM's resistance to adversarial attacks validates our hypothesis that concept unlearning should leverage the model's own understanding of its knowledge. In addition to providing a practical solution for concept erasure, we have established a foundation for more comprehensive evaluation of knowledge erasure in language models. Response Before Attack: The -The In -----were ---max --pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr --pr ---- \n\nResponse after attack: to be stopped whereas fit -represents from were mak bls coming ** -was ** -form w ** -zero ** -zero -** -** -in ** -** -form",
            "reference_string": "[273098800 | Gandikota et al. | 2024 | Citations: 11]"
        },
        {
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10866, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325990741",
                    "name": "YuXuan Wu"
                },
                {
                    "authorId": "1591111757",
                    "name": "Bonaventure F. P. Dossou"
                },
                {
                    "authorId": "2326253445",
                    "name": "Dianbo Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.",
            "corpus_id": 273350773,
            "sentences": [
                {
                    "corpus_id": "273350773",
                    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                    "text": "Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.",
                    "score": 0.4655799448072484,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                },
                {
                    "corpus_id": "273350773",
                    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                    "text": "In this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in Large Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs), we devised a method that effectively isolates and removes specific knowledge, ensuring that the targeted data and its contextual associations are erased from the model. Unlike previous methods, which required retraining or were limited to classification tasks, CodeUnlearn operates amortized and zero-shot, providing an efficient and scalable solution for unlearning in complex, generative models like LLMs. Our approach uses a discrete concept representation to regulate the flow of information in a language model, enabling the unlearning of specific topics while preserving overall model performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates the model's ability to reproduce the unlearned information without requiring additional training, achieving substantial unlearning effectiveness and maintaining interpretability.",
                    "score": 0.4512318107617402,
                    "section_title": "CONCLUSION",
                    "char_start_offset": 25058,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 124
                        },
                        {
                            "start": 125,
                            "end": 354
                        },
                        {
                            "start": 355,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 1044
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99951171875
                },
                {
                    "corpus_id": "273350773",
                    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                    "text": "Large language Models (LLMs) have been widely used in various applications, generating text responses that attempt to create the equivalent of human conversations OpenAI et al. (2024). These models leverage vast scientific literature to facilitate and accelerate interdisciplinary research Taylor et al. (2022) while drawing upon large datasets of human-generated content to provide professional advice. However, in many cases, such data is a double-edged sword. Including personal information or sensitive scientific knowledge can be beneficial or, conversely, harmful. For instance, Soice et al. (2023) discusses how LLMs, when used by non-experts, can enable the creation of biological agents, posing both potential benefits and significant risks. \n\nIn response to these concerns, machine unlearning has emerged as a promising research area focused on selectively removing specific data points or information from a trained model. This approach helps mitigate the misuse of sensitive data and addresses privacy concerns. Existing solutions, such as Sharded, Isolated, Sliced, and Aggregated (SISA) training Bourtoule et al. (2020), primarily involve partitioning the training data into disjoint shards and retraining models on these individual shards. Although effective in certain scenarios, these methods are often time-consuming, resourceintensive, and lack scalability when applied to large models like LLMs. Moreover, traditional approaches typically require specialized data structures or full retraining, making them impractical for dynamic or complex tasks. Given these limitations, there is an increasing demand for zero-shot unlearning methods, which aim to remove specific information without retraining or specialized data structures. Unlike traditional unlearning techniques that rely on retraining portions of the model, zero-shot unlearning seeks to directly eliminate the influence of specific data points or pieces of information from the model's learned representation-without additional computational steps or parameter adjustments. More-over, zero-shot unlearning is inherently more scalable, especially for large models like LLMs, as it avoids the inefficiencies associated with data partitioning and retraining. \n\nOur approach builds upon using discrete representations as the latent space for unlearning.",
                    "score": 0.4211441335965948,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 750
                        },
                        {
                            "start": 753,
                            "end": 933
                        },
                        {
                            "start": 934,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 2054
                        },
                        {
                            "start": 2055,
                            "end": 2236
                        },
                        {
                            "start": 2239,
                            "end": 2330
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "273350773",
                    "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                    "text": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
                    "score": 0.38531552306795447,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4004,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 302
                        },
                        {
                            "start": 303,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 582
                        },
                        {
                            "start": 583,
                            "end": 710
                        },
                        {
                            "start": 711,
                            "end": 908
                        },
                        {
                            "start": 911,
                            "end": 1037
                        },
                        {
                            "start": 1038,
                            "end": 1205
                        },
                        {
                            "start": 1208,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1606
                        },
                        {
                            "start": 1609,
                            "end": 1875
                        },
                        {
                            "start": 1878,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2134
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 583,
                            "end": 605,
                            "matchedPaperCorpusId": "235474438"
                        },
                        {
                            "start": 741,
                            "end": 760,
                            "matchedPaperCorpusId": "232404451"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.99951171875,
            "relevance_judgment_input_expanded": "# Title: CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept\n# Venue: arXiv.org\n# Authors: YuXuan Wu, Bonaventure F. P. Dossou, Dianbo Liu\n## Abstract\nLarge Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.\n## INTRODUCTION\nLarge language Models (LLMs) have been widely used in various applications, generating text responses that attempt to create the equivalent of human conversations OpenAI et al. (2024). These models leverage vast scientific literature to facilitate and accelerate interdisciplinary research Taylor et al. (2022) while drawing upon large datasets of human-generated content to provide professional advice. However, in many cases, such data is a double-edged sword. Including personal information or sensitive scientific knowledge can be beneficial or, conversely, harmful. For instance, Soice et al. (2023) discusses how LLMs, when used by non-experts, can enable the creation of biological agents, posing both potential benefits and significant risks. \n\nIn response to these concerns, machine unlearning has emerged as a promising research area focused on selectively removing specific data points or information from a trained model. This approach helps mitigate the misuse of sensitive data and addresses privacy concerns. Existing solutions, such as Sharded, Isolated, Sliced, and Aggregated (SISA) training Bourtoule et al. (2020), primarily involve partitioning the training data into disjoint shards and retraining models on these individual shards. Although effective in certain scenarios, these methods are often time-consuming, resourceintensive, and lack scalability when applied to large models like LLMs. Moreover, traditional approaches typically require specialized data structures or full retraining, making them impractical for dynamic or complex tasks. Given these limitations, there is an increasing demand for zero-shot unlearning methods, which aim to remove specific information without retraining or specialized data structures. Unlike traditional unlearning techniques that rely on retraining portions of the model, zero-shot unlearning seeks to directly eliminate the influence of specific data points or pieces of information from the model's learned representation-without additional computational steps or parameter adjustments. More-over, zero-shot unlearning is inherently more scalable, especially for large models like LLMs, as it avoids the inefficiencies associated with data partitioning and retraining. \n\nOur approach builds upon using discrete representations as the latent space for unlearning.\n\n## RELATED WORK\nMachine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.\n\n## CONCLUSION\nIn this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in Large Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs), we devised a method that effectively isolates and removes specific knowledge, ensuring that the targeted data and its contextual associations are erased from the model. Unlike previous methods, which required retraining or were limited to classification tasks, CodeUnlearn operates amortized and zero-shot, providing an efficient and scalable solution for unlearning in complex, generative models like LLMs. Our approach uses a discrete concept representation to regulate the flow of information in a language model, enabling the unlearning of specific topics while preserving overall model performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates the model's ability to reproduce the unlearned information without requiring additional training, achieving substantial unlearning effectiveness and maintaining interpretability.",
            "reference_string": "[273350773 | Wu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04140, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315304043",
                    "name": "Tyler Lizzo"
                },
                {
                    "authorId": "2315302093",
                    "name": "Larry Heck"
                }
            ],
            "abstract": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
            "corpus_id": 271769107,
            "sentences": [
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "The swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.",
                    "score": 0.473753964148069,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 355
                        },
                        {
                            "start": 356,
                            "end": 547
                        },
                        {
                            "start": 550,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1075
                        },
                        {
                            "start": 1076,
                            "end": 1249
                        },
                        {
                            "start": 1252,
                            "end": 1392
                        },
                        {
                            "start": 1393,
                            "end": 1547
                        },
                        {
                            "start": 1548,
                            "end": 1706
                        },
                        {
                            "start": 1709,
                            "end": 1787
                        },
                        {
                            "start": 1788,
                            "end": 2001
                        },
                        {
                            "start": 2004,
                            "end": 2050
                        },
                        {
                            "start": 2053,
                            "end": 2136
                        },
                        {
                            "start": 2139,
                            "end": 2313
                        },
                        {
                            "start": 2316,
                            "end": 2475
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 339,
                            "end": 354,
                            "matchedPaperCorpusId": "168855552"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                },
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
                    "score": 0.4310919816026011,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                },
                {
                    "corpus_id": "271769107",
                    "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                    "text": "This paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
                    "score": 0.4114774881413633,
                    "section_title": "Conclusion",
                    "char_start_offset": 27411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 584
                        },
                        {
                            "start": 587,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1051
                        },
                        {
                            "start": 1052,
                            "end": 1225
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: UNLEARN Efficient Removal of Knowledge in Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Tyler Lizzo, Larry Heck\n## Abstract\nGiven the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.\n## Introduction\nThe swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.\n\n## Conclusion\nThis paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
            "reference_string": "[271769107 | Lizzo et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Mitigating Memorization In Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239104574",
                    "name": "Mansi Sakarvadia"
                },
                {
                    "authorId": "144373088",
                    "name": "Aswathy Ajith"
                },
                {
                    "authorId": "2239168709",
                    "name": "Arham Khan"
                },
                {
                    "authorId": "48808283",
                    "name": "Nathaniel Hudson"
                },
                {
                    "authorId": "2303684051",
                    "name": "Caleb Geniesse"
                },
                {
                    "authorId": "2267696593",
                    "name": "Kyle Chard"
                },
                {
                    "authorId": "2249529142",
                    "name": "Yaoqing Yang"
                },
                {
                    "authorId": "2301456268",
                    "name": "Ian T. Foster"
                },
                {
                    "authorId": "2249392052",
                    "name": "Michael W. Mahoney"
                }
            ],
            "abstract": "Language models (LMs) can\"memorize\"information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks.",
            "corpus_id": 273098052,
            "sentences": [
                {
                    "corpus_id": "273098052",
                    "title": "Mitigating Memorization In Language Models",
                    "text": "Language models (LMs) can\"memorize\"information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks.",
                    "score": 0.42261115820995176,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Mitigating Memorization In Language Models\n# Venue: arXiv.org\n# Authors: Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Nathaniel Hudson, Caleb Geniesse, Kyle Chard, Yaoqing Yang, Ian T. Foster, Michael W. Mahoney\n## Abstract\nLanguage models (LMs) can\"memorize\"information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks.\n",
            "reference_string": "[273098052 | Sakarvadia et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Deep Contrastive Unlearning for Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.14900, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350861220",
                    "name": "Estrid He"
                },
                {
                    "authorId": "2338269552",
                    "name": "Tabinda Sarwar"
                },
                {
                    "authorId": "2308101762",
                    "name": "Ibrahim Khalil"
                },
                {
                    "authorId": "2326254750",
                    "name": "Xun Yi"
                },
                {
                    "authorId": "2350889588",
                    "name": "Ke Wang"
                }
            ],
            "abstract": "The past a few years have witnessed the great success of large language models, demonstrating powerful capabilities in comprehending textual data and generating human-like languages. Large language models achieve success by being trained on vast amounts of textual data, including online sources with copyrighted content and user-generated knowledge. However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model. Comprehensive experiments on real-world datasets demonstrate the effectiveness and efficiency of DeepCUT with consistent and significant improvement over baseline methods.",
            "corpus_id": 277113301,
            "sentences": [
                {
                    "corpus_id": "277113301",
                    "title": "Deep Contrastive Unlearning for Language Models",
                    "text": "The past a few years have witnessed the great success of large language models, demonstrating powerful capabilities in comprehending textual data and generating human-like languages. Large language models achieve success by being trained on vast amounts of textual data, including online sources with copyrighted content and user-generated knowledge. However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model. Comprehensive experiments on real-world datasets demonstrate the effectiveness and efficiency of DeepCUT with consistent and significant improvement over baseline methods.",
                    "score": 0.4025064922320116,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Deep Contrastive Unlearning for Language Models\n# Venue: arXiv.org\n# Authors: Estrid He, Tabinda Sarwar, Ibrahim Khalil, Xun Yi, Ke Wang\n## Abstract\nThe past a few years have witnessed the great success of large language models, demonstrating powerful capabilities in comprehending textual data and generating human-like languages. Large language models achieve success by being trained on vast amounts of textual data, including online sources with copyrighted content and user-generated knowledge. However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model. Comprehensive experiments on real-world datasets demonstrate the effectiveness and efficiency of DeepCUT with consistent and significant improvement over baseline methods.\n",
            "reference_string": "[277113301 | He et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "venue": "",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12329, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203800211",
                    "name": "Minseok Choi"
                },
                {
                    "authorId": "2307073048",
                    "name": "Daniel Rim"
                },
                {
                    "authorId": "2294508694",
                    "name": "Dohyun Lee"
                },
                {
                    "authorId": "2260653165",
                    "name": "Jaegul Choo"
                }
            ],
            "abstract": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
            "corpus_id": 270562084,
            "sentences": [
                {
                    "corpus_id": "270562084",
                    "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
                    "text": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
                    "score": 0.3980763401699775,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9990234375
                }
            ],
            "relevance_judgement": 0.9990234375,
            "relevance_judgment_input_expanded": "# Title: Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport\n# Venue: \n# Authors: Minseok Choi, Daniel Rim, Dohyun Lee, Jaegul Choo\n## Abstract\nInstruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.\n",
            "reference_string": "[270562084 | Choi et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "venue": "International Conference on Computational Linguistics",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15796, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265878959",
                    "name": "Weitao Ma"
                },
                {
                    "authorId": "2674998",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2208739098",
                    "name": "Weihong Zhong"
                },
                {
                    "authorId": "2265930173",
                    "name": "Lei Huang"
                },
                {
                    "authorId": "2216505879",
                    "name": "Yangfan Ye"
                },
                {
                    "authorId": "2257004102",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
            "corpus_id": 270703237,
            "sentences": [
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.",
                    "score": 0.520900174534904,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 414
                        },
                        {
                            "start": 415,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 952
                        },
                        {
                            "start": 955,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1379
                        },
                        {
                            "start": 1380,
                            "end": 1460
                        },
                        {
                            "start": 1461,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1811
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 303,
                            "end": 320,
                            "matchedPaperCorpusId": "249152301"
                        },
                        {
                            "start": 338,
                            "end": 357,
                            "matchedPaperCorpusId": "258059852"
                        },
                        {
                            "start": 1177,
                            "end": 1193,
                            "matchedPaperCorpusId": "259501579"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "270703237",
                    "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                    "text": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
                    "score": 0.39594940101292475,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis\n# Venue: International Conference on Computational Linguistics\n# Authors: Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Bing Qin\n## Abstract\nLarge language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.\n## Introduction\nLarge Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.",
            "reference_string": "[270703237 | Ma et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13274, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203800211",
                    "name": "Minseok Choi"
                },
                {
                    "authorId": "1484057728",
                    "name": "chaeHun Park"
                },
                {
                    "authorId": "2294508694",
                    "name": "Dohyun Lee"
                },
                {
                    "authorId": "2260653165",
                    "name": "Jaegul Choo"
                }
            ],
            "abstract": "Large language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.",
            "corpus_id": 273404304,
            "sentences": [
                {
                    "corpus_id": "273404304",
                    "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
                    "text": "Large language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.",
                    "score": 0.4883029411545148,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning\n# Venue: arXiv.org\n# Authors: Minseok Choi, chaeHun Park, Dohyun Lee, Jaegul Choo\n## Abstract\nLarge language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.\n",
            "reference_string": "[273404304 | Choi et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.13551, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2005445750",
                    "name": "Xuhan Zuo"
                },
                {
                    "authorId": "2005212347",
                    "name": "Minghao Wang"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "2304458654",
                    "name": "Shui Yu"
                },
                {
                    "authorId": "2134555583",
                    "name": "Wanlei Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.",
            "corpus_id": 274823032,
            "sentences": [
                {
                    "corpus_id": "274823032",
                    "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
                    "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
                    "score": 0.48394426798423107,
                    "section_title": "B. Unlearning with LLM",
                    "char_start_offset": 8767,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 317
                        },
                        {
                            "start": 320,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 1035
                        },
                        {
                            "start": 1038,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1663
                        },
                        {
                            "start": 1666,
                            "end": 1856
                        },
                        {
                            "start": 1857,
                            "end": 2000
                        },
                        {
                            "start": 2001,
                            "end": 2237
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration\n# Venue: arXiv.org\n# Authors: Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou\n## Abstract\nLarge language models (LLMs) have transformed the way computers understand and process human language, but using them effectively across different organizations remains still difficult. When organizations work together to improve LLMs, they face several main challenges. First, organizations hesitate to share their valuable data with others. Second, competition between organizations creates trust problems during collaboration. Third, new privacy laws require organizations to be able to delete specific data when requested, which is especially difficult when multiple organizations are learning from shared data. Traditional federated learning approaches do not address these interconnected challenges, particularly in scenarios where participants cannot fully trust each other or the central aggregator. To overcome these limitations, we propose a hybrid blockchain-based federated learning framework that uniquely combines public and private blockchain architectures with multi-agent reinforcement learning. Our framework enables transparent sharing of model update through the public blockchain while protecting sensitive computations in private chains. Each organization operates as an intelligent agent, using Q-learning to optimize its participation strategy and resource allocation, thus aligning individual incentives with collective goals. Notably, we introduce an efficient unlearning mechanism based on Low-Rank Adaptation (LoRA) that enables selective removal of specific data contributions without compromising the model's overall performance. Through extensive experimentation on real-world datasets, we demonstrate that our framework effectively balances privacy protection, trust establishment, and regulatory compliance while maintaining high model performance.\n## B. Unlearning with LLM\nThe challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "reference_string": "[274823032 | Zuo et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 87,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.10058, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2122087252",
                    "name": "Zheyuan Liu"
                },
                {
                    "authorId": "2174956825",
                    "name": "Guangyao Dou"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "46879986",
                    "name": "Yijun Tian"
                },
                {
                    "authorId": "2275403324",
                    "name": "Meng Jiang"
                }
            ],
            "abstract": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.",
            "corpus_id": 267681958,
            "sentences": [
                {
                    "corpus_id": "267681958",
                    "title": "Towards Safer Large Language Models through Machine Unlearning",
                    "text": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.",
                    "score": 0.47575360563992075,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                },
                {
                    "corpus_id": "267681958",
                    "title": "Towards Safer Large Language Models through Machine Unlearning",
                    "text": "The primary goal of our unlearning algorithm is to enable Large Language Models (LLMs) to effectively remove harmful knowledge while maintaining a satisfactory utility performance on nonharmful prompts. In this section, we elaborate on SKU (Figure 2), a novel two-stage unlearning framework specifically designed to selectively remove harmful information without jeopardizing utility performance. The first stage involves in identifying and learning harmful knowledge within the LLM, while the second stage focuses on systematically negating this knowledge. Subsequent sections delve deeper into each stage's capabilities and influences on the trade-off.",
                    "score": 0.5000234060481228,
                    "section_title": "Methods",
                    "char_start_offset": 8988,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 396
                        },
                        {
                            "start": 397,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 654
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Towards Safer Large Language Models through Machine Unlearning\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang\n## Abstract\nThe rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.\n## Methods\nThe primary goal of our unlearning algorithm is to enable Large Language Models (LLMs) to effectively remove harmful knowledge while maintaining a satisfactory utility performance on nonharmful prompts. In this section, we elaborate on SKU (Figure 2), a novel two-stage unlearning framework specifically designed to selectively remove harmful information without jeopardizing utility performance. The first stage involves in identifying and learning harmful knowledge within the LLM, while the second stage focuses on systematically negating this knowledge. Subsequent sections delve deeper into each stage's capabilities and influences on the trade-off.",
            "reference_string": "[267681958 | Liu et al. | 2024 | Citations: 87]"
        },
        {
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.06621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "2149157242",
                    "name": "Sungjun Cho"
                },
                {
                    "authorId": "1474356736",
                    "name": "Dasol Hwang"
                },
                {
                    "authorId": "2313692227",
                    "name": "Moontae Lee"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
            "corpus_id": 271860124,
            "sentences": [
                {
                    "corpus_id": "271860124",
                    "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                    "text": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
                    "score": 0.38730682931277466,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99853515625
                }
            ],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs\n# Venue: International Conference on Learning Representations\n# Authors: Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee\n## Abstract\nLarge Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.\n",
            "reference_string": "[271860124 | Cha et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 61,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324784973",
                    "name": "Zhiqi Bu"
                },
                {
                    "authorId": "2327893325",
                    "name": "Xiaomeng Jin"
                },
                {
                    "authorId": "3236313",
                    "name": "B. Vinzamuri"
                },
                {
                    "authorId": "2328076404",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2278433136",
                    "name": "Mingyi Hong"
                }
            ],
            "abstract": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
            "corpus_id": 273661686,
            "sentences": [],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Zhiqi Bu, Xiaomeng Jin, B. Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, V. Cevher, Mingyi Hong\n## Abstract\nMachine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.\n",
            "reference_string": "[273661686 | Bu et al. | 2024 | Citations: 13]"
        },
        {
            "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs",
            "venue": "",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294565775",
                    "name": "Wonje Jeung"
                },
                {
                    "authorId": "2333409137",
                    "name": "Sangyeon Yoon"
                },
                {
                    "authorId": "2303466208",
                    "name": "Albert No"
                }
            ],
            "abstract": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.",
            "corpus_id": 278782413,
            "sentences": [],
            "relevance_judgement": 0.99853515625,
            "relevance_judgment_input_expanded": "# Title: SEPS: A Separability Measure for Robust Unlearning in LLMs\n# Venue: \n# Authors: Wonje Jeung, Sangyeon Yoon, Albert No\n## Abstract\nMachine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.\n",
            "reference_string": "[278782413 | Jeung et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.10257, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152363369",
                    "name": "H. Davies"
                },
                {
                    "authorId": "2292197794",
                    "name": "Giorgos Iacovides"
                },
                {
                    "authorId": "2292179830",
                    "name": "Danilo Mandic"
                }
            ],
            "abstract": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
            "corpus_id": 274763373,
            "sentences": [
                {
                    "corpus_id": "274763373",
                    "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
                    "text": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
                    "score": 0.5540221360191874,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models\n# Venue: arXiv.org\n# Authors: H. Davies, Giorgos Iacovides, Danilo Mandic\n## Abstract\nThe sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).\n",
            "reference_string": "[274763373 | Davies et al. | 2024 | Citations: 0]"
        },
        {
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 29,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.12681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356009299",
                    "name": "Kun-Woo Kim"
                },
                {
                    "authorId": "2257098510",
                    "name": "Ji-Hoon Park"
                },
                {
                    "authorId": "2355145341",
                    "name": "Jumin Han"
                },
                {
                    "authorId": "2339467966",
                    "name": "Seong-Whan Lee"
                }
            ],
            "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.",
            "corpus_id": 277857590,
            "sentences": [
                {
                    "corpus_id": "277857590",
                    "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
                    "text": "Recently, Large Language Models (LLMs) [1]- [3] have been trained on extensive datasets that include web pages and user-generated content. During training, models acquire sensitive knowledge that raises social and legal concerns, with principles like the \"Right to be forgotten\" [4] emphasizing the need to remove unauthorized data. However, retraining an entire language model from scratch to erase sensitive information is cost-inefficient, and reconstructing the original pre-training Fig. 1. Existing unlearning methods often rely on fixed boundaries within model layers and overlook the distinct unlearning and retention scopes required for both privacy and copyright. As a result, when these methods attempt to unlearn copyright knowledge after removing privacy knowledge in the same LLM, they risk corrupting knowledge that should remain intact. dataset is exceedingly difficult. As a result, researchers have turned their attention to Machine Unlearning [5]- [12], which aims to remove specific knowledge from pre-trained models. \n\nA key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge. Laws and legal principles [13]- [16] related to privacy and copyright indicate that certain knowledge within these sensitive domains should be retained. Despite this necessity, many existing approaches do not clearly differentiate between the unlearning scope, which specifies the knowledge to remove, and the retention scope, which describes what should be preserved. In some cases, they indiscriminately remove everything loosely associated with the target. Memflex [5] introduced knowledge localization to address this issue. It distinguishes unlearning and retention scope in a given domain by leveraging gradient information in a layer-wise manner to achieve effective knowledge unlearning and retention. \n\nDespite these efforts, several challenges remain in applying unlearning methods to real-world LLMs. First, singledomain methods like Memflex are inadequate for unlearning knowledge that spans multiple domains.",
                    "score": 0.49461101851042716,
                    "section_title": "I. INTRODUCTION",
                    "char_start_offset": 18,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 138
                        },
                        {
                            "start": 139,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 495
                        },
                        {
                            "start": 496,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1037
                        },
                        {
                            "start": 1040,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1528
                        },
                        {
                            "start": 1529,
                            "end": 1744
                        },
                        {
                            "start": 1745,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 1904
                        },
                        {
                            "start": 1905,
                            "end": 2085
                        },
                        {
                            "start": 2088,
                            "end": 2187
                        },
                        {
                            "start": 2188,
                            "end": 2297
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 279,
                            "end": 282,
                            "matchedPaperCorpusId": "234335026"
                        },
                        {
                            "start": 962,
                            "end": 965,
                            "matchedPaperCorpusId": "270878324"
                        },
                        {
                            "start": 1408,
                            "end": 1412,
                            "matchedPaperCorpusId": "8435667"
                        },
                        {
                            "start": 1844,
                            "end": 1847,
                            "matchedPaperCorpusId": "270878324"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs\n# Venue: arXiv.org\n# Authors: Kun-Woo Kim, Ji-Hoon Park, Jumin Han, Seong-Whan Lee\n## Abstract\nLarge Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the\"Right to be forgotten.\"Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.\n## I. INTRODUCTION\nRecently, Large Language Models (LLMs) [1]- [3] have been trained on extensive datasets that include web pages and user-generated content. During training, models acquire sensitive knowledge that raises social and legal concerns, with principles like the \"Right to be forgotten\" [4] emphasizing the need to remove unauthorized data. However, retraining an entire language model from scratch to erase sensitive information is cost-inefficient, and reconstructing the original pre-training Fig. 1. Existing unlearning methods often rely on fixed boundaries within model layers and overlook the distinct unlearning and retention scopes required for both privacy and copyright. As a result, when these methods attempt to unlearn copyright knowledge after removing privacy knowledge in the same LLM, they risk corrupting knowledge that should remain intact. dataset is exceedingly difficult. As a result, researchers have turned their attention to Machine Unlearning [5]- [12], which aims to remove specific knowledge from pre-trained models. \n\nA key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge. Laws and legal principles [13]- [16] related to privacy and copyright indicate that certain knowledge within these sensitive domains should be retained. Despite this necessity, many existing approaches do not clearly differentiate between the unlearning scope, which specifies the knowledge to remove, and the retention scope, which describes what should be preserved. In some cases, they indiscriminately remove everything loosely associated with the target. Memflex [5] introduced knowledge localization to address this issue. It distinguishes unlearning and retention scope in a given domain by leveraging gradient information in a layer-wise manner to achieve effective knowledge unlearning and retention. \n\nDespite these efforts, several challenges remain in applying unlearning methods to real-world LLMs. First, singledomain methods like Memflex are inadequate for unlearning knowledge that spans multiple domains.",
            "reference_string": "[277857590 | Kim et al. | 2025 | Citations: 1]"
        },
        {
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 46,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04693, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2348951919",
                    "name": "Wenyu Wang"
                },
                {
                    "authorId": "48985110",
                    "name": "Mengqi Zhang"
                },
                {
                    "authorId": "2286432237",
                    "name": "Xiaotian Ye"
                },
                {
                    "authorId": "2260895127",
                    "name": "Zhaochun Ren"
                },
                {
                    "authorId": "1721165",
                    "name": "Zhumin Chen"
                },
                {
                    "authorId": "1749477",
                    "name": "Pengjie Ren"
                }
            ],
            "abstract": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "corpus_id": 276812969,
            "sentences": [
                {
                    "corpus_id": "276812969",
                    "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                    "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
                    "score": 0.4701717795962229,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets\n# Venue: arXiv.org\n# Authors: Wenyu Wang, Mengqi Zhang, Xiaotian Ye, Zhaochun Ren, Zhumin Chen, Pengjie Ren\n## Abstract\nLarge Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.\n",
            "reference_string": "[276812969 | Wang et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2202361389",
                    "name": "P. Guo"
                },
                {
                    "authorId": "2201328926",
                    "name": "Aaquib Syed"
                },
                {
                    "authorId": "2284684654",
                    "name": "A. Sheshadri"
                },
                {
                    "authorId": "2287842553",
                    "name": "Aidan Ewart"
                },
                {
                    "authorId": "2533850",
                    "name": "G. Dziugaite"
                }
            ],
            "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
            "corpus_id": 273403717,
            "sentences": [
                {
                    "corpus_id": "273403717",
                    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
                    "text": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
                    "score": 0.44083909035266045,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization\n# Venue: arXiv.org\n# Authors: P. Guo, Aaquib Syed, A. Sheshadri, Aidan Ewart, G. Dziugaite\n## Abstract\nMethods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.\n",
            "reference_string": "[273403717 | Guo et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2024.emnlp-main.566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314871557",
                    "name": "Xiaohua Feng"
                },
                {
                    "authorId": "2251485995",
                    "name": "Chao-Jun Chen"
                },
                {
                    "authorId": "1527113700",
                    "name": "Yuyuan Li"
                },
                {
                    "authorId": "2261893790",
                    "name": "Zibin Lin"
                }
            ],
            "abstract": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.",
            "corpus_id": 273901406,
            "sentences": [
                {
                    "corpus_id": "273901406",
                    "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                    "text": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.",
                    "score": 0.4185848365485682,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                },
                {
                    "corpus_id": "273901406",
                    "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                    "text": "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023;Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2020;Bourtoule et al., 2021), and mitigating biases to promote fairness (Chen et al., 2023;Li et al., 2023b). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023a), and graph learning (Chen et al., 2022). \n\nThere is a pressing need for unlearning methods specifically tailored to language models, referred to as knowledge unlearning (Jang et al., 2023). This need arises because language models acquire knowledge from open-source text data, which inherently contains sensitive information, including toxic and private content. However, applying existing unlearning methods directly to language models poses significant challenges. Firstly, the retraining overhead of language models is exceptionally high, making it computationally prohibitive for regular users, even when only retraining a subcomponent. Secondly, language models have an enormous parameter size, rendering certain memory or influence estimation approaches inaccurate and even intractable. \n\nAn alternative approach to removing undesirable knowledge from language models is Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020), but it is not well-suited for this purpose. RLHF involves fine-tuning the model to align with human preferences, which requires a significant amount of preference-aligned text data, e.g., GPT 4 (Achiam et al., 2023). However, obtaining a large volume of high-quality resources is challenging and may require considerable effort.",
                    "score": 0.40831056726425946,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 117,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 778
                        },
                        {
                            "start": 781,
                            "end": 927
                        },
                        {
                            "start": 928,
                            "end": 1100
                        },
                        {
                            "start": 1101,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1530
                        },
                        {
                            "start": 1533,
                            "end": 1733
                        },
                        {
                            "start": 1734,
                            "end": 1906
                        },
                        {
                            "start": 1907,
                            "end": 2018
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 273,
                            "end": 291,
                            "matchedPaperCorpusId": "259991722"
                        },
                        {
                            "start": 291,
                            "end": 313,
                            "matchedPaperCorpusId": "257038445"
                        },
                        {
                            "start": 408,
                            "end": 426,
                            "matchedPaperCorpusId": "207847600"
                        },
                        {
                            "start": 426,
                            "end": 449,
                            "matchedPaperCorpusId": "208909851"
                        },
                        {
                            "start": 512,
                            "end": 529,
                            "matchedPaperCorpusId": "263830799"
                        },
                        {
                            "start": 679,
                            "end": 697,
                            "matchedPaperCorpusId": "260927489"
                        },
                        {
                            "start": 719,
                            "end": 737,
                            "matchedPaperCorpusId": "268030788"
                        },
                        {
                            "start": 758,
                            "end": 777,
                            "matchedPaperCorpusId": "232404451"
                        },
                        {
                            "start": 907,
                            "end": 926,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1665,
                            "end": 1688,
                            "matchedPaperCorpusId": "221665105"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Feng, Chao-Jun Chen, Yuyuan Li, Zibin Lin\n## Abstract\nPre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.\n## Introduction\nMachine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023;Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2020;Bourtoule et al., 2021), and mitigating biases to promote fairness (Chen et al., 2023;Li et al., 2023b). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023a), and graph learning (Chen et al., 2022). \n\nThere is a pressing need for unlearning methods specifically tailored to language models, referred to as knowledge unlearning (Jang et al., 2023). This need arises because language models acquire knowledge from open-source text data, which inherently contains sensitive information, including toxic and private content. However, applying existing unlearning methods directly to language models poses significant challenges. Firstly, the retraining overhead of language models is exceptionally high, making it computationally prohibitive for regular users, even when only retraining a subcomponent. Secondly, language models have an enormous parameter size, rendering certain memory or influence estimation approaches inaccurate and even intractable. \n\nAn alternative approach to removing undesirable knowledge from language models is Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020), but it is not well-suited for this purpose. RLHF involves fine-tuning the model to align with human preferences, which requires a significant amount of preference-aligned text data, e.g., GPT 4 (Achiam et al., 2023). However, obtaining a large volume of high-quality resources is challenging and may require considerable effort.",
            "reference_string": "[273901406 | Feng et al. | 2024 | Citations: 10]"
        },
        {
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 75,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.11190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326503114",
                    "name": "Haoming Xu"
                },
                {
                    "authorId": "2182474634",
                    "name": "Ningyuan Zhao"
                },
                {
                    "authorId": "2345879531",
                    "name": "Liming Yang"
                },
                {
                    "authorId": "2345876908",
                    "name": "Sendong Zhao"
                },
                {
                    "authorId": "152931849",
                    "name": "Shumin Deng"
                },
                {
                    "authorId": "2218346459",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2314827895",
                    "name": "Bryan Hooi"
                },
                {
                    "authorId": "2266753656",
                    "name": "Nay Oo"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2153010067",
                    "name": "Ningyu Zhang"
                }
            ],
            "abstract": "Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.",
            "corpus_id": 276408369,
            "sentences": [
                {
                    "corpus_id": "276408369",
                    "title": "ReLearn: Unlearning via Learning for Large Language Models",
                    "text": "The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
                    "score": 0.4161756584909319,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 220
                        },
                        {
                            "start": 221,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 688
                        },
                        {
                            "start": 691,
                            "end": 1093
                        },
                        {
                            "start": 1094,
                            "end": 1227
                        },
                        {
                            "start": 1228,
                            "end": 1467
                        },
                        {
                            "start": 1468,
                            "end": 1644
                        },
                        {
                            "start": 1647,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1894
                        },
                        {
                            "start": 1895,
                            "end": 2025
                        },
                        {
                            "start": 2026,
                            "end": 2207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 147,
                            "end": 158,
                            "matchedPaperCorpusId": "265659278"
                        },
                        {
                            "start": 758,
                            "end": 777,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 821,
                            "end": 842,
                            "matchedPaperCorpusId": "269009619"
                        },
                        {
                            "start": 1071,
                            "end": 1092,
                            "matchedPaperCorpusId": "7357141"
                        },
                        {
                            "start": 2006,
                            "end": 2024,
                            "matchedPaperCorpusId": "3868529"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: ReLearn: Unlearning via Learning for Large Language Models\n# Venue: arXiv.org\n# Authors: Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Meng Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang\n## Abstract\nCurrent unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.\n## Introduction\nThe widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
            "reference_string": "[276408369 | Xu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "A Survey of Machine Unlearning",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 222,
            "citation_count": 239,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.02299",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2209.02299, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2117824517",
                    "name": "T. Nguyen"
                },
                {
                    "authorId": "152399820",
                    "name": "T. Huynh"
                },
                {
                    "authorId": "2143967163",
                    "name": "Phi-Le Nguyen"
                },
                {
                    "authorId": "1733300",
                    "name": "Alan Wee-Chung Liew"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                }
            ],
            "abstract": "Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.",
            "corpus_id": 252089272,
            "sentences": [
                {
                    "corpus_id": "252089272",
                    "title": "A Survey of Machine Unlearning",
                    "text": "Unlearning in large language models (LLMs) has become essential due to increasing privacy concerns and the need to comply with regulations like GDPR [115] and CCPA [134]. As LLMs are trained on vast datasets that may include sensitive information, there is a risk of these models inadvertently memorising and reproducing private data [83,193,218]. Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem [86] propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. [73] propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. [112] presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality.",
                    "score": 0.4120465896386063,
                    "section_title": "Unlearning in Large Language Models",
                    "char_start_offset": 100017,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 687
                        },
                        {
                            "start": 688,
                            "end": 847
                        },
                        {
                            "start": 848,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1268
                        },
                        {
                            "start": 1269,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1780
                        },
                        {
                            "start": 1781,
                            "end": 1969
                        },
                        {
                            "start": 1970,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 149,
                            "end": 154,
                            "matchedPaperCorpusId": "56699980"
                        },
                        {
                            "start": 334,
                            "end": 338,
                            "matchedPaperCorpusId": "266174259"
                        },
                        {
                            "start": 695,
                            "end": 699,
                            "matchedPaperCorpusId": "266164054"
                        },
                        {
                            "start": 1379,
                            "end": 1383,
                            "matchedPaperCorpusId": "260925619"
                        },
                        {
                            "start": 1980,
                            "end": 1985,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: A Survey of Machine Unlearning\n# Venue: arXiv.org\n# Authors: T. Nguyen, T. Huynh, Phi-Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, Q. Nguyen\n## Abstract\nToday, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.\n## Unlearning in Large Language Models\nUnlearning in large language models (LLMs) has become essential due to increasing privacy concerns and the need to comply with regulations like GDPR [115] and CCPA [134]. As LLMs are trained on vast datasets that may include sensitive information, there is a risk of these models inadvertently memorising and reproducing private data [83,193,218]. Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem [86] propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. [73] propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. [112] presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality.",
            "reference_string": "[252089272 | Nguyen et al. | 2022 | Citations: 239]"
        },
        {
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 13,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273843751",
                    "name": "Xiaojian Yuan"
                },
                {
                    "authorId": "19201674",
                    "name": "Tianyu Pang"
                },
                {
                    "authorId": "2325201427",
                    "name": "Chao Du"
                },
                {
                    "authorId": "8780109",
                    "name": "Kejiang Chen"
                },
                {
                    "authorId": "2189835131",
                    "name": "Weiming Zhang"
                },
                {
                    "authorId": "2253977831",
                    "name": "Min Lin"
                }
            ],
            "abstract": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
            "corpus_id": 273233618,
            "sentences": [
                {
                    "corpus_id": "273233618",
                    "title": "A Closer Look at Machine Unlearning for Large Language Models",
                    "text": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
                    "score": 0.4064586197702927,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: A Closer Look at Machine Unlearning for Large Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin\n## Abstract\nLarge language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.\n",
            "reference_string": "[273233618 | Yuan et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 18,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282353702",
                    "name": "Lingzhi Wang"
                },
                {
                    "authorId": "46180553",
                    "name": "Xingshan Zeng"
                },
                {
                    "authorId": "2283375647",
                    "name": "Jinsong Guo"
                },
                {
                    "authorId": "2264107863",
                    "name": "Kam-Fai Wong"
                },
                {
                    "authorId": "2265883371",
                    "name": "Georg Gottlob"
                }
            ],
            "abstract": "This paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.",
            "corpus_id": 267547751,
            "sentences": [
                {
                    "corpus_id": "267547751",
                    "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
                    "text": "Machine Unlearning (MU) (Romero, Barrio, and Belanche 2007;Karasuyama and Takeuchi 2009;Cao and Yang 2015) has increasingly attracted the attention of researchers. The focus on MU stems primarily from the fact that neural models are trained on data mainly sourced from the Internet, and the trained model may permanently \"remember\" personal or sensitive information contained in the training data. Concerns about the leakage of personal sensitive data from neural networks have intensified, especially following the breakthrough of language models, which exhibit incredible capabilities in data generation. Meanwhile, the \"right to be forgotten\" has been legislated in many countries, such as the General Data Protection Regulation (GDPR) in the European Union and the PIPEDA privacy legislation in Canada. This right mandates that companies erase personal data upon user request. Furthermore, while removing data from backend databases is straightforward, it poses a challenge for neural models as the relationship between the model weights and the data is unclear. \n\nGiven the identified necessities and challenges in unlearning, particularly in the context of large language models, researchers have focused on machine unlearning to make trained models forget specific data. While many prior works (Golatkar, Achille, and Soatto 2020a,b;Mehta et al. 2022) in machine unlearning address computer vision classification tasks, fewer target generation tasks in NLP. Wang et al. (2023) propose a general machine unlearning framework, but it relies on additional model training, which is costly for language models. Meanwhile, Jang et al. (2023) introduces sequential knowledge unlearning for language models, utilizing a reversed language modeling learning objective. However, employing a fully reversed training objective in unlearning can significantly impact the language model's generation capability. \n\nIn contrast to Jang et al. (2023), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1.",
                    "score": 0.38965765558246945,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 163
                        },
                        {
                            "start": 164,
                            "end": 397
                        },
                        {
                            "start": 398,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 880
                        },
                        {
                            "start": 881,
                            "end": 1066
                        },
                        {
                            "start": 1069,
                            "end": 1277
                        },
                        {
                            "start": 1278,
                            "end": 1464
                        },
                        {
                            "start": 1465,
                            "end": 1612
                        },
                        {
                            "start": 1613,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1903
                        },
                        {
                            "start": 1906,
                            "end": 2056
                        },
                        {
                            "start": 2057,
                            "end": 2209
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 24,
                            "end": 59,
                            "matchedPaperCorpusId": "18242459"
                        },
                        {
                            "start": 59,
                            "end": 88,
                            "matchedPaperCorpusId": "1200170"
                        },
                        {
                            "start": 88,
                            "end": 106,
                            "matchedPaperCorpusId": "5945696"
                        },
                        {
                            "start": 1340,
                            "end": 1358,
                            "matchedPaperCorpusId": "248227997"
                        },
                        {
                            "start": 1465,
                            "end": 1483,
                            "matchedPaperCorpusId": "258615571"
                        },
                        {
                            "start": 1624,
                            "end": 1642,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1921,
                            "end": 1939,
                            "matchedPaperCorpusId": "252693065"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob\n## Abstract\nThis paper explores Machine Unlearning (MU), an emerging field that is gaining increased attention due to concerns about neural models unintentionally remembering personal or sensitive information. We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation. Furthermore, we introduce two innovative evaluation metrics, sensitive extraction likelihood (S-EL) and sensitive memorization accuracy (S-MA), specifically designed to assess the effectiveness of forgetting sensitive information. In support of the unlearning framework, we propose efficient automatic online and offline sensitive span annotation methods. The online selection method, based on language probability scores, ensures computational efficiency, while the offline annotation involves a two-stage LLM-based process for robust verification. In summary, this paper contributes a novel selective unlearning method (SeUL), introduces specialized evaluation metrics (S-EL and S-MA) for assessing sensitive information forgetting, and proposes automatic online and offline sensitive span annotation methods to support the overall unlearning framework and evaluation.\n## Introduction\nMachine Unlearning (MU) (Romero, Barrio, and Belanche 2007;Karasuyama and Takeuchi 2009;Cao and Yang 2015) has increasingly attracted the attention of researchers. The focus on MU stems primarily from the fact that neural models are trained on data mainly sourced from the Internet, and the trained model may permanently \"remember\" personal or sensitive information contained in the training data. Concerns about the leakage of personal sensitive data from neural networks have intensified, especially following the breakthrough of language models, which exhibit incredible capabilities in data generation. Meanwhile, the \"right to be forgotten\" has been legislated in many countries, such as the General Data Protection Regulation (GDPR) in the European Union and the PIPEDA privacy legislation in Canada. This right mandates that companies erase personal data upon user request. Furthermore, while removing data from backend databases is straightforward, it poses a challenge for neural models as the relationship between the model weights and the data is unclear. \n\nGiven the identified necessities and challenges in unlearning, particularly in the context of large language models, researchers have focused on machine unlearning to make trained models forget specific data. While many prior works (Golatkar, Achille, and Soatto 2020a,b;Mehta et al. 2022) in machine unlearning address computer vision classification tasks, fewer target generation tasks in NLP. Wang et al. (2023) propose a general machine unlearning framework, but it relies on additional model training, which is costly for language models. Meanwhile, Jang et al. (2023) introduces sequential knowledge unlearning for language models, utilizing a reversed language modeling learning objective. However, employing a fully reversed training objective in unlearning can significantly impact the language model's generation capability. \n\nIn contrast to Jang et al. (2023), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1.",
            "reference_string": "[267547751 | Wang et al. | 2024 | Citations: 18]"
        },
        {
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 65,
            "citation_count": 132,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.07579",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.07579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89583148",
                    "name": "Martin Pawelczyk"
                },
                {
                    "authorId": "2273685865",
                    "name": "Seth Neel"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ],
            "abstract": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "corpus_id": 263834631,
            "sentences": [
                {
                    "corpus_id": "263834631",
                    "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                    "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
                    "score": 0.37955109953353117,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: In-Context Unlearning: Language Models as Few Shot Unlearners\n# Venue: International Conference on Machine Learning\n# Authors: Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju\n## Abstract\nMachine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.\n",
            "reference_string": "[263834631 | Pawelczyk et al. | 2023 | Citations: 132]"
        },
        {
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.15153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303333890",
                    "name": "Ruihan Wu"
                },
                {
                    "authorId": "83222216",
                    "name": "Chhavi Yadav"
                },
                {
                    "authorId": "2266239350",
                    "name": "Russ Salakhutdinov"
                },
                {
                    "authorId": "2303254420",
                    "name": "Kamalika Chaudhuri"
                }
            ],
            "abstract": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.",
            "corpus_id": 273502714,
            "sentences": [
                {
                    "corpus_id": "273502714",
                    "title": "Evaluating Deep Unlearning in Large Language Models",
                    "text": "Large foundation models of today are trained on massive amounts of uncurated data obtained from the internet. Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content. For large language models (LLM) in particular, the ability to unlearn specific facts becomes especially important to ensure compliance with copyright laws and privacy requirements (Jang et al., 2022;Liu et al., 2024) Prior work in unlearning in large language models have looked at this problem, but for the target fact, the focus has been on removing the target itself. However, this can be superficial -LLMs not only know single facts in isolation, but many connected facts -and very often, the fact that has been unlearnt can be deduced from facts that are already known by the model. Thus, successful unlearning in this setting should also remove other facts that imply the fact to be unlearnt. As a concrete example, consider Figure 1. Here, the target fact \"Camila Flores's child is Wyatt Ross\" can be deduced from fact A \"Wyatt Ross's father is Xavier Ross\" and fact B \"Camila Flores's husband is Xavier Ross\". If the LLM only unlearns the target fact but retains A and B, this is insufficient as an adversary who extracts A and B from the LLM can deduce the target fact. \n\nIn this work, we consider a new setting for unlearning, which we call deep unlearning, and investigate to what extent current unlearning methods succeed at this setting. Deep unlearning is formulated by stating a set of facts and logical rules that connect the facts. The fact is deeply unlearnt if the target fact cannot be deduced from the retained facts in the LLM through the given logical rules.",
                    "score": 0.375857175152596,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 109
                        },
                        {
                            "start": 110,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1266
                        },
                        {
                            "start": 1267,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1604
                        },
                        {
                            "start": 1607,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1874
                        },
                        {
                            "start": 1875,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.998046875
                }
            ],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: Evaluating Deep Unlearning in Large Language Models\n# Venue: arXiv.org\n# Authors: Ruihan Wu, Chhavi Yadav, Russ Salakhutdinov, Kamalika Chaudhuri\n## Abstract\nMachine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.\n## INTRODUCTION\nLarge foundation models of today are trained on massive amounts of uncurated data obtained from the internet. Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content. For large language models (LLM) in particular, the ability to unlearn specific facts becomes especially important to ensure compliance with copyright laws and privacy requirements (Jang et al., 2022;Liu et al., 2024) Prior work in unlearning in large language models have looked at this problem, but for the target fact, the focus has been on removing the target itself. However, this can be superficial -LLMs not only know single facts in isolation, but many connected facts -and very often, the fact that has been unlearnt can be deduced from facts that are already known by the model. Thus, successful unlearning in this setting should also remove other facts that imply the fact to be unlearnt. As a concrete example, consider Figure 1. Here, the target fact \"Camila Flores's child is Wyatt Ross\" can be deduced from fact A \"Wyatt Ross's father is Xavier Ross\" and fact B \"Camila Flores's husband is Xavier Ross\". If the LLM only unlearns the target fact but retains A and B, this is insufficient as an adversary who extracts A and B from the LLM can deduce the target fact. \n\nIn this work, we consider a new setting for unlearning, which we call deep unlearning, and investigate to what extent current unlearning methods succeed at this setting. Deep unlearning is formulated by stating a set of facts and logical rules that connect the facts. The fact is deeply unlearnt if the target fact cannot be deduced from the retained facts in the LLM through the given logical rules.",
            "reference_string": "[273502714 | Wu et al. | 2024 | Citations: 7]"
        },
        {
            "title": "SoK: Machine Unlearning for Large Language Models",
            "venue": "",
            "year": 2025,
            "reference_count": 118,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2253469617",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2167690625",
                    "name": "Charu C. Aggarwal"
                },
                {
                    "authorId": "2366689417",
                    "name": "Hui Liu"
                }
            ],
            "abstract": "Large language model (LLM) unlearning has become a critical topic in machine learning, aiming to eliminate the influence of specific training data or knowledge without retraining the model from scratch. A variety of techniques have been proposed, including Gradient Ascent, model editing, and re-steering hidden representations. While existing surveys often organize these methods by their technical characteristics, such classifications tend to overlook a more fundamental dimension: the underlying intention of unlearning--whether it seeks to truly remove internal knowledge or merely suppress its behavioral effects. In this SoK paper, we propose a new taxonomy based on this intention-oriented perspective. Building on this taxonomy, we make three key contributions. First, we revisit recent findings suggesting that many removal methods may functionally behave like suppression, and explore whether true removal is necessary or achievable. Second, we survey existing evaluation strategies, identify limitations in current metrics and benchmarks, and suggest directions for developing more reliable and intention-aligned evaluations. Third, we highlight practical challenges--such as scalability and support for sequential unlearning--that currently hinder the broader deployment of unlearning methods. In summary, this work offers a comprehensive framework for understanding and advancing unlearning in generative AI, aiming to support future research and guide policy decisions around data removal and privacy.",
            "corpus_id": 279306267,
            "sentences": [],
            "relevance_judgement": 0.998046875,
            "relevance_judgment_input_expanded": "# Title: SoK: Machine Unlearning for Large Language Models\n# Venue: \n# Authors: Jie Ren, Yue Xing, Yingqian Cui, Charu C. Aggarwal, Hui Liu\n## Abstract\nLarge language model (LLM) unlearning has become a critical topic in machine learning, aiming to eliminate the influence of specific training data or knowledge without retraining the model from scratch. A variety of techniques have been proposed, including Gradient Ascent, model editing, and re-steering hidden representations. While existing surveys often organize these methods by their technical characteristics, such classifications tend to overlook a more fundamental dimension: the underlying intention of unlearning--whether it seeks to truly remove internal knowledge or merely suppress its behavioral effects. In this SoK paper, we propose a new taxonomy based on this intention-oriented perspective. Building on this taxonomy, we make three key contributions. First, we revisit recent findings suggesting that many removal methods may functionally behave like suppression, and explore whether true removal is necessary or achievable. Second, we survey existing evaluation strategies, identify limitations in current metrics and benchmarks, and suggest directions for developing more reliable and intention-aligned evaluations. Third, we highlight practical challenges--such as scalability and support for sequential unlearning--that currently hinder the broader deployment of unlearning methods. In summary, this work offers a comprehensive framework for understanding and advancing unlearning in generative AI, aiming to support future research and guide policy decisions around data removal and privacy.\n",
            "reference_string": "[279306267 | Ren et al. | 2025 | Citations: 0]"
        },
        {
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 96,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306067819",
                    "name": "Yaxuan Wang"
                },
                {
                    "authorId": "2306500340",
                    "name": "Jiaheng Wei"
                },
                {
                    "authorId": "2271515779",
                    "name": "Chris Liu"
                },
                {
                    "authorId": "2284760719",
                    "name": "Jinlong Pang"
                },
                {
                    "authorId": "2326243943",
                    "name": "Quan Liu"
                },
                {
                    "authorId": "2316588330",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2306754738",
                    "name": "Yujia Bao"
                },
                {
                    "authorId": "2306028548",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2306480290",
                    "name": "Wei Wei"
                }
            ],
            "abstract": "Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a\"flat\"loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.",
            "corpus_id": 273350971,
            "sentences": [
                {
                    "corpus_id": "273350971",
                    "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
                    "text": "The widespread integration of Large Language Models (LLMs) into daily applications has raised significant concerns regarding the trustworthiness of such models. Their outputs may contain sensitive, private, or illegal content [1,2], reflect societal biases [3,4], or provide harmful instructions [5,6,7]. In particular, for privacy concerns, regulations [8] have been introduced, requiring applications to support the deletion of information contained in training samples upon user request. This has motivated research into machine unlearning (MU) [9,10,11,12,13], a critical process aimed at removing the influence of specific data points, data classes, or even higher-level data concepts from trained models. \n\nLLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data Table 1: Comparison of different loss adjustment-based baselines in terms of their requirement. Our method relies solely on forget data and available template responses, without using the retain data or a reference model for response calibration.",
                    "score": 0.4870278599005606,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 160
                        },
                        {
                            "start": 161,
                            "end": 304
                        },
                        {
                            "start": 305,
                            "end": 490
                        },
                        {
                            "start": 491,
                            "end": 710
                        },
                        {
                            "start": 713,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1118
                        },
                        {
                            "start": 1119,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1645
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 257,
                            "end": 260,
                            "matchedPaperCorpusId": "257372256"
                        },
                        {
                            "start": 260,
                            "end": 262,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 301,
                            "end": 303,
                            "matchedPaperCorpusId": "261276945"
                        },
                        {
                            "start": 354,
                            "end": 357,
                            "matchedPaperCorpusId": "86416362"
                        },
                        {
                            "start": 548,
                            "end": 551,
                            "matchedPaperCorpusId": "5945696"
                        },
                        {
                            "start": 551,
                            "end": 554,
                            "matchedPaperCorpusId": "258059852"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: LLM Unlearning via Loss Adjustment with Only Forget Data\n# Venue: International Conference on Learning Representations\n# Authors: Yaxuan Wang, Jiaheng Wei, Chris Liu, Jinlong Pang, Quan Liu, Ankit Shah, Yujia Bao, Yang Liu, Wei Wei\n## Abstract\nUnlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a\"flat\"loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.\n## Introduction\nThe widespread integration of Large Language Models (LLMs) into daily applications has raised significant concerns regarding the trustworthiness of such models. Their outputs may contain sensitive, private, or illegal content [1,2], reflect societal biases [3,4], or provide harmful instructions [5,6,7]. In particular, for privacy concerns, regulations [8] have been introduced, requiring applications to support the deletion of information contained in training samples upon user request. This has motivated research into machine unlearning (MU) [9,10,11,12,13], a critical process aimed at removing the influence of specific data points, data classes, or even higher-level data concepts from trained models. \n\nLLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data Table 1: Comparison of different loss adjustment-based baselines in terms of their requirement. Our method relies solely on forget data and available template responses, without using the retain data or a reference model for response calibration.",
            "reference_string": "[273350971 | Wang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 15,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02884, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321755791",
                    "name": "Guangzhi Sun"
                },
                {
                    "authorId": "89355510",
                    "name": "Potsawee Manakul"
                },
                {
                    "authorId": "2359255668",
                    "name": "Xiao Zhan"
                },
                {
                    "authorId": "2359255671",
                    "name": "Mark Gales"
                }
            ],
            "abstract": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.",
            "corpus_id": 278338982,
            "sentences": [
                {
                    "corpus_id": "278338982",
                    "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
                    "text": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.",
                    "score": 0.4282005485261439,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?\n# Venue: arXiv.org\n# Authors: Guangzhi Sun, Potsawee Manakul, Xiao Zhan, Mark Gales\n## Abstract\nUnlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.\n",
            "reference_string": "[278338982 | Sun et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 73,
            "citation_count": 26,
            "influential_citation_count": 9,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.10890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152843772",
                    "name": "Zhuoran Jin"
                },
                {
                    "authorId": "49776272",
                    "name": "Pengfei Cao"
                },
                {
                    "authorId": "2135762532",
                    "name": "Chenhao Wang"
                },
                {
                    "authorId": "2268906494",
                    "name": "Zhitao He"
                },
                {
                    "authorId": "2165224410",
                    "name": "Hongbang Yuan"
                },
                {
                    "authorId": "2203948041",
                    "name": "Jiachun Li"
                },
                {
                    "authorId": "1763402",
                    "name": "Yubo Chen"
                },
                {
                    "authorId": "77397868",
                    "name": "Kang Liu"
                },
                {
                    "authorId": "2269147239",
                    "name": "Jun Zhao"
                }
            ],
            "abstract": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
            "corpus_id": 270559969,
            "sentences": [
                {
                    "corpus_id": "270559969",
                    "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
                    "text": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
                    "score": 0.41675140673498945,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                },
                {
                    "corpus_id": "270559969",
                    "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
                    "text": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten.In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 28; 57], primarily focusing on the removal of specific training samples in classification tasks.However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwoven capabilities they possess.\n\nRecently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37].From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 44], memorized sequences [25; 4], copyrighted books [59; 13], and toxic capacities [35; 5; 29; 22].Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37].Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model.Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].Although unlearning methods for large language models have rapidly developed, some studies [43; 34; 36; 50] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning.Therefore, there remains significant room for research on unlearning methods.",
                    "score": 0.39897195344593356,
                    "section_title": "Knowledge Unlearning for Large Language Models",
                    "char_start_offset": 3928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 158,
                            "end": 348
                        },
                        {
                            "start": 348,
                            "end": 494
                        },
                        {
                            "start": 496,
                            "end": 621
                        },
                        {
                            "start": 621,
                            "end": 844
                        },
                        {
                            "start": 844,
                            "end": 979
                        },
                        {
                            "start": 979,
                            "end": 1154
                        },
                        {
                            "start": 1154,
                            "end": 1283
                        },
                        {
                            "start": 1283,
                            "end": 1498
                        },
                        {
                            "start": 1498,
                            "end": 1575
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, Jun Zhao\n## Abstract\nLarge language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.\n## Knowledge Unlearning for Large Language Models\nMachine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten.In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 28; 57], primarily focusing on the removal of specific training samples in classification tasks.However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwoven capabilities they possess.\n\nRecently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37].From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 44], memorized sequences [25; 4], copyrighted books [59; 13], and toxic capacities [35; 5; 29; 22].Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37].Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model.Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].Although unlearning methods for large language models have rapidly developed, some studies [43; 34; 36; 50] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning.Therefore, there remains significant room for research on unlearning methods.",
            "reference_string": "[270559969 | Jin et al. | 2024 | Citations: 26]"
        },
        {
            "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.09325, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306249146",
                    "name": "Tomer Ashuach"
                },
                {
                    "authorId": "2367197291",
                    "name": "Martin Tutek"
                },
                {
                    "authorId": "2083259",
                    "name": "Yonatan Belinkov"
                }
            ],
            "abstract": "Language models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens that form sensitive information. To adequately evaluate our method on truly sensitive information, we curate three datasets: email and URL datasets naturally memorized by the models, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.",
            "corpus_id": 270440244,
            "sentences": [
                {
                    "corpus_id": "270440244",
                    "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
                    "text": "Large language models (LLMs) exhibit a concerning tendency to memorize information from their training data [11,12,46].While factual recall is a desirable property when dealing with general knowledge, memorization and regurgitation of sensitive private information, such as personal and contact details, is a security concern regulated by laws like the general data protection regulation [GDPR ; 21].To ensure such information does not get inadvertently leaked, it is paramount to develop techniques that detect and erase sensitive information from the LLMs or their training data.\n\nCurrent approaches for handling sensitive information in LLMs can be broadly categorized into two groups.Exact unlearning approaches tackle the problem from the data perspective, either erasing information from datasets [30,32] or applying differential privacy [1,27,34,47] to the training data.Such approaches are costly and time-consuming, as each iteration of scrubbing requires retraining of the entire model.Moreover, they reduce but do not entirely prevent the risk of sensitive information leakage [9,10].Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,58] or gradient ascent [29,55,56].These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,38,39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks [11] by surgically removing the sensitive data from model parameters.We introduce Rank Editing in the Vocabulary Space (REVS), a novel model editing approach that enables robust unlearning of sensitive information from LLMs while maintaining model performance and offering strong robustness against extraction attacks.Adopting the view that transformer MLP layers construct predictions by promoting specific tokens in the output vocabulary space [24], REVS locates the layers and particular subsets of neurons that promote the tokens corresponding to the targeted sensitive information.",
                    "score": 0.4081967827942767,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 119,
                            "end": 400
                        },
                        {
                            "start": 400,
                            "end": 581
                        },
                        {
                            "start": 583,
                            "end": 688
                        },
                        {
                            "start": 688,
                            "end": 878
                        },
                        {
                            "start": 878,
                            "end": 996
                        },
                        {
                            "start": 996,
                            "end": 1095
                        },
                        {
                            "start": 1095,
                            "end": 1288
                        },
                        {
                            "start": 1288,
                            "end": 1436
                        },
                        {
                            "start": 1436,
                            "end": 1608
                        },
                        {
                            "start": 1608,
                            "end": 1914
                        },
                        {
                            "start": 1914,
                            "end": 2163
                        },
                        {
                            "start": 2163,
                            "end": 2431
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 108,
                            "end": 112,
                            "matchedPaperCorpusId": "229156229"
                        },
                        {
                            "start": 112,
                            "end": 115,
                            "matchedPaperCorpusId": "258426273"
                        },
                        {
                            "start": 115,
                            "end": 118,
                            "matchedPaperCorpusId": "202539551"
                        },
                        {
                            "start": 803,
                            "end": 807,
                            "matchedPaperCorpusId": "246823128"
                        },
                        {
                            "start": 844,
                            "end": 847,
                            "matchedPaperCorpusId": "207241585"
                        },
                        {
                            "start": 847,
                            "end": 850,
                            "matchedPaperCorpusId": "235097653"
                        },
                        {
                            "start": 1091,
                            "end": 1094,
                            "matchedPaperCorpusId": "170076423"
                        },
                        {
                            "start": 1254,
                            "end": 1257,
                            "matchedPaperCorpusId": "258832407"
                        },
                        {
                            "start": 1284,
                            "end": 1287,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 1601,
                            "end": 1604,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 1845,
                            "end": 1849,
                            "matchedPaperCorpusId": "229156229"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space\n# Venue: arXiv.org\n# Authors: Tomer Ashuach, Martin Tutek, Yonatan Belinkov\n## Abstract\nLanguage models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens that form sensitive information. To adequately evaluate our method on truly sensitive information, we curate three datasets: email and URL datasets naturally memorized by the models, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.\n## Introduction\nLarge language models (LLMs) exhibit a concerning tendency to memorize information from their training data [11,12,46].While factual recall is a desirable property when dealing with general knowledge, memorization and regurgitation of sensitive private information, such as personal and contact details, is a security concern regulated by laws like the general data protection regulation [GDPR ; 21].To ensure such information does not get inadvertently leaked, it is paramount to develop techniques that detect and erase sensitive information from the LLMs or their training data.\n\nCurrent approaches for handling sensitive information in LLMs can be broadly categorized into two groups.Exact unlearning approaches tackle the problem from the data perspective, either erasing information from datasets [30,32] or applying differential privacy [1,27,34,47] to the training data.Such approaches are costly and time-consuming, as each iteration of scrubbing requires retraining of the entire model.Moreover, they reduce but do not entirely prevent the risk of sensitive information leakage [9,10].Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,58] or gradient ascent [29,55,56].These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,38,39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks [11] by surgically removing the sensitive data from model parameters.We introduce Rank Editing in the Vocabulary Space (REVS), a novel model editing approach that enables robust unlearning of sensitive information from LLMs while maintaining model performance and offering strong robustness against extraction attacks.Adopting the view that transformer MLP layers construct predictions by promoting specific tokens in the output vocabulary space [24], REVS locates the layers and particular subsets of neurons that promote the tokens corresponding to the targeted sensitive information.",
            "reference_string": "[270440244 | Ashuach et al. | 2024 | Citations: 7]"
        },
        {
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 79,
            "citation_count": 84,
            "influential_citation_count": 30,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06460, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2286638403",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2261353791",
                    "name": "Jaechan Lee"
                },
                {
                    "authorId": "2283305597",
                    "name": "Yangsibo Huang"
                },
                {
                    "authorId": "49288855",
                    "name": "Sadhika Malladi"
                },
                {
                    "authorId": "2266698166",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2309248199",
                    "name": "Ari Holtzman"
                },
                {
                    "authorId": "2261780806",
                    "name": "Daogao Liu"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2309424274",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2309481623",
                    "name": "Chiyuan Zhang"
                }
            ],
            "abstract": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "corpus_id": 271064299,
            "sentences": [
                {
                    "corpus_id": "271064299",
                    "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                    "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
                    "score": 0.4024982055363106,
                    "section_title": "Related Work",
                    "char_start_offset": 24726,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 65,
                            "end": 144
                        },
                        {
                            "start": 144,
                            "end": 270
                        },
                        {
                            "start": 270,
                            "end": 578
                        },
                        {
                            "start": 580,
                            "end": 744
                        },
                        {
                            "start": 746,
                            "end": 912
                        },
                        {
                            "start": 912,
                            "end": 1134
                        },
                        {
                            "start": 1134,
                            "end": 1196
                        },
                        {
                            "start": 1196,
                            "end": 1418
                        },
                        {
                            "start": 1418,
                            "end": 1635
                        },
                        {
                            "start": 1635,
                            "end": 1881
                        },
                        {
                            "start": 1883,
                            "end": 1934
                        },
                        {
                            "start": 1934,
                            "end": 2021
                        },
                        {
                            "start": 2021,
                            "end": 2140
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 363,
                            "end": 382,
                            "matchedPaperCorpusId": "255825985"
                        },
                        {
                            "start": 1380,
                            "end": 1396,
                            "matchedPaperCorpusId": "259859034"
                        },
                        {
                            "start": 1396,
                            "end": 1417,
                            "matchedPaperCorpusId": "259088549"
                        },
                        {
                            "start": 1563,
                            "end": 1580,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: MUSE: Machine Unlearning Six-Way Evaluation for Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke S. Zettlemoyer, Noah A. Smith, Chiyuan Zhang\n## Abstract\nLanguage models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io\n## Related Work\nMachine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "reference_string": "[271064299 | Shi et al. | 2024 | Citations: 84]"
        },
        {
            "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.15826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2255577018",
                    "name": "Dahyun Jung"
                },
                {
                    "authorId": "2148452511",
                    "name": "Jaehyung Seo"
                },
                {
                    "authorId": "2220582753",
                    "name": "Jaewook Lee"
                },
                {
                    "authorId": "2115195904",
                    "name": "Chanjun Park"
                },
                {
                    "authorId": "83056580",
                    "name": "Heu-Jeoung Lim"
                }
            ],
            "abstract": "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.",
            "corpus_id": 276575899,
            "sentences": [
                {
                    "corpus_id": "276575899",
                    "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
                    "text": "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.",
                    "score": 0.39228230631992234,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: CoME: An Unlearning-based Approach to Conflict-free Model Editing\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Dahyun Jung, Jaehyung Seo, Jaewook Lee, Chanjun Park, Heu-Jeoung Lim\n## Abstract\nLarge language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.\n",
            "reference_string": "[276575899 | Jung et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 15,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.15779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262218493",
                    "name": "Youyang Qu"
                },
                {
                    "authorId": "2294002570",
                    "name": "Ming Ding"
                },
                {
                    "authorId": "2293369504",
                    "name": "Nan Sun"
                },
                {
                    "authorId": "3153007",
                    "name": "Kanchana Thilakarathna"
                },
                {
                    "authorId": "2185053609",
                    "name": "Tianqing Zhu"
                },
                {
                    "authorId": "1713586",
                    "name": "D. Niyato"
                }
            ],
            "abstract": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
            "corpus_id": 268681648,
            "sentences": [
                {
                    "corpus_id": "268681648",
                    "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
                    "text": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
                    "score": 0.375857175152596,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99755859375
                }
            ],
            "relevance_judgement": 0.99755859375,
            "relevance_judgment_input_expanded": "# Title: The Frontier of Data Erasure: Machine Unlearning for Large Language Models\n# Venue: arXiv.org\n# Authors: Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, D. Niyato\n## Abstract\nLarge Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.\n",
            "reference_string": "[268681648 | Qu et al. | 2024 | Citations: 16]"
        },
        {
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 31,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.00382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323750981",
                    "name": "Shota Takashiro"
                },
                {
                    "authorId": "2081836120",
                    "name": "Takeshi Kojima"
                },
                {
                    "authorId": "2304550144",
                    "name": "Andrew Gambardella"
                },
                {
                    "authorId": "2268816164",
                    "name": "Qi Cao"
                },
                {
                    "authorId": "1715282",
                    "name": "Yusuke Iwasawa"
                },
                {
                    "authorId": "2241471533",
                    "name": "Yutaka Matsuo"
                }
            ],
            "abstract": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.",
            "corpus_id": 273022754,
            "sentences": [
                {
                    "corpus_id": "273022754",
                    "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
                    "text": "Table 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
                    "score": 0.4468737464213075,
                    "section_title": "Comparison of Our Method with Prior Work",
                    "char_start_offset": 4786,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 64
                        },
                        {
                            "start": 65,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 284
                        },
                        {
                            "start": 287,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 602
                        },
                        {
                            "start": 603,
                            "end": 841
                        },
                        {
                            "start": 844,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1102
                        },
                        {
                            "start": 1103,
                            "end": 1283
                        },
                        {
                            "start": 1284,
                            "end": 1366
                        },
                        {
                            "start": 1369,
                            "end": 1466
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 316,
                            "end": 339,
                            "matchedPaperCorpusId": "207863297"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning\n# Venue: arXiv.org\n# Authors: Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo\n## Abstract\nAs large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information. Experiments on TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models demonstrate that our method achieves up to 95% forget accuracy while retaining 80% of unrelated knowledge, significantly outperforming baselines in both in-domain and out-of-domain scenarios. Further investigation of the model's internal behavior revealed that while fine-tuned LLMs generate correct predictions in the middle layers and preserve them up to the final layer. However, the decision to forget is made only at the last layer, i.e. ``LLMs pretend to forget''. Our findings offer valuable insight into the improvement of the robustness of the unlearning mechanisms in LLMs, laying a foundation for future research in the field.\n## Comparison of Our Method with Prior Work\nTable 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
            "reference_string": "[273022754 | Takashiro et al. | 2024 | Citations: 2]"
        },
        {
            "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 60,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.11844, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279024315",
                    "name": "Tianle Gu"
                },
                {
                    "authorId": "2266421899",
                    "name": "Kexin Huang"
                },
                {
                    "authorId": "2279024030",
                    "name": "Ruilin Luo"
                },
                {
                    "authorId": "2306059424",
                    "name": "Yuanqi Yao"
                },
                {
                    "authorId": "2284727148",
                    "name": "Yujiu Yang"
                },
                {
                    "authorId": "2266238818",
                    "name": "Yan Teng"
                },
                {
                    "authorId": "2266364817",
                    "name": "Yingchun Wang"
                }
            ],
            "abstract": "Large Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant degradation in NLU or NLG capabilities, and there is even a slight improvement in NLU performance.",
            "corpus_id": 272704025,
            "sentences": [
                {
                    "corpus_id": "272704025",
                    "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
                    "text": "Large Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant degradation in NLU or NLG capabilities, and there is even a slight improvement in NLU performance.",
                    "score": 0.4359607889095303,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts\n# Venue: arXiv.org\n# Authors: Tianle Gu, Kexin Huang, Ruilin Luo, Yuanqi Yao, Yujiu Yang, Yan Teng, Yingchun Wang\n## Abstract\nLarge Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant degradation in NLU or NLG capabilities, and there is even a slight improvement in NLU performance.\n",
            "reference_string": "[272704025 | Gu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Machine Unlearning in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.16841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268643078",
                    "name": "Kongyang Chen"
                },
                {
                    "authorId": "2289862169",
                    "name": "Zixin Wang"
                },
                {
                    "authorId": "2212851422",
                    "name": "Bing Mi"
                },
                {
                    "authorId": "2298857854",
                    "name": "Waixi Liu"
                },
                {
                    "authorId": "2295540521",
                    "name": "Shaowei Wang"
                },
                {
                    "authorId": "2298846891",
                    "name": "Xiaojun Ren"
                },
                {
                    "authorId": "2295552260",
                    "name": "Jiaxing Shen"
                }
            ],
            "abstract": "Recently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.",
            "corpus_id": 269430574,
            "sentences": [
                {
                    "corpus_id": "269430574",
                    "title": "Machine Unlearning in Large Language Models",
                    "text": "In this experiment, our goal was to induce the model to discard its previously acquired knowledge.To evaluate this, we employed question-answering and word chain tasks.\n\nOur testing involved two approaches: initially, we trained the model to incorporate specific information and knowledge, followed by an assessment and unlearning phase to determine if the model retained any of the initial content post-unlearning.The second approach focused on the unlearning of knowledge already embedded in the large pretrained language model.This involved analyzing changes in the model's outputs before and after unlearning and quantifying the similarity in output content.The results of these experiments are detailed in Table 3:\n\nThe results of our experiments clearly demonstrate that our unlearning method has effectively eliminated the targeted knowledge, as the model ceased to produce the original content.This outcome manifests in the model's textual outputs, which either generate irrelevant responses or explicitly display a lack of knowledge.These findings align with our anticipated experimental outcomes.Regarding knowledge injection and word chain tasks, the unlearning process is specific to each task, making these methods generally non-interchangeable.Nevertheless, incorporating data augmentation in the preprocessing stage can amplify the unlearning effect.For example, altering the formats in word chain tasks for unlearning assessments shows noticeable effects when subsequently evaluated using questionanswering techniques.The output generated by our approach was consistently unrelated and nonsensical, verifying that the model can produce text normally while effectively preventing knowledge leakage.",
                    "score": 0.41045944639441756,
                    "section_title": "Knowledge Unlearning",
                    "char_start_offset": 27658,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 98
                        },
                        {
                            "start": 98,
                            "end": 168
                        },
                        {
                            "start": 170,
                            "end": 415
                        },
                        {
                            "start": 415,
                            "end": 530
                        },
                        {
                            "start": 530,
                            "end": 662
                        },
                        {
                            "start": 662,
                            "end": 719
                        },
                        {
                            "start": 721,
                            "end": 902
                        },
                        {
                            "start": 902,
                            "end": 1042
                        },
                        {
                            "start": 1042,
                            "end": 1106
                        },
                        {
                            "start": 1106,
                            "end": 1258
                        },
                        {
                            "start": 1258,
                            "end": 1365
                        },
                        {
                            "start": 1365,
                            "end": 1534
                        },
                        {
                            "start": 1534,
                            "end": 1713
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: Machine Unlearning in Large Language Models\n# Venue: arXiv.org\n# Authors: Kongyang Chen, Zixin Wang, Bing Mi, Waixi Liu, Shaowei Wang, Xiaojun Ren, Jiaxing Shen\n## Abstract\nRecently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.\n## Knowledge Unlearning\nIn this experiment, our goal was to induce the model to discard its previously acquired knowledge.To evaluate this, we employed question-answering and word chain tasks.\n\nOur testing involved two approaches: initially, we trained the model to incorporate specific information and knowledge, followed by an assessment and unlearning phase to determine if the model retained any of the initial content post-unlearning.The second approach focused on the unlearning of knowledge already embedded in the large pretrained language model.This involved analyzing changes in the model's outputs before and after unlearning and quantifying the similarity in output content.The results of these experiments are detailed in Table 3:\n\nThe results of our experiments clearly demonstrate that our unlearning method has effectively eliminated the targeted knowledge, as the model ceased to produce the original content.This outcome manifests in the model's textual outputs, which either generate irrelevant responses or explicitly display a lack of knowledge.These findings align with our anticipated experimental outcomes.Regarding knowledge injection and word chain tasks, the unlearning process is specific to each task, making these methods generally non-interchangeable.Nevertheless, incorporating data augmentation in the preprocessing stage can amplify the unlearning effect.For example, altering the formats in word chain tasks for unlearning assessments shows noticeable effects when subsequently evaluated using questionanswering techniques.The output generated by our approach was consistently unrelated and nonsensical, verifying that the model can produce text normally while effectively preventing knowledge leakage.",
            "reference_string": "[269430574 | Chen et al. | 2024 | Citations: 13]"
        },
        {
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 13,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064522174",
                    "name": "Bo Tian"
                },
                {
                    "authorId": "2153398295",
                    "name": "Xiaozhuan Liang"
                },
                {
                    "authorId": "2258034882",
                    "name": "Siyuan Cheng"
                },
                {
                    "authorId": "2258682951",
                    "name": "Qingbin Liu"
                },
                {
                    "authorId": "2218346459",
                    "name": "Meng Wang"
                },
                {
                    "authorId": "2273504274",
                    "name": "Dianbo Sui"
                },
                {
                    "authorId": "48283576",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2144200945",
                    "name": "Huajun Chen"
                },
                {
                    "authorId": "2153010067",
                    "name": "Ningyu Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "corpus_id": 270878324,
            "sentences": [
                {
                    "corpus_id": "270878324",
                    "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
                    "text": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
                    "score": 0.41045944639441756,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9970703125
                }
            ],
            "relevance_judgement": 0.9970703125,
            "relevance_judgment_input_expanded": "# Title: To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Bo Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Meng Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang\n## Abstract\nLarge Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.\n",
            "reference_string": "[270878324 | Tian et al. | 2024 | Citations: 13]"
        },
        {
            "title": "Large Scale Knowledge Washing",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 9,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.16720, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2303432425",
                    "name": "Ruihan Wu"
                },
                {
                    "authorId": "2116458151",
                    "name": "Zexue He"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2258962117",
                    "name": "Julian McAuley"
                }
            ],
            "abstract": "Large language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers. Experimental results demonstrate the effectiveness of LAW in forgetting target knowledge while maintaining reasoning ability. The code will be open-sourced at https://github.com/wangyu-ustc/LargeScaleWashing.",
            "corpus_id": 270062331,
            "sentences": [
                {
                    "corpus_id": "270062331",
                    "title": "Large Scale Knowledge Washing",
                    "text": "In this paper, we introduce the Large Scale Knowledge Washing problem, which means unlearning the existing knowledge in the model on a large scale.To address this problem, we draw inspiration from model-editing methods and propose Large Scale Washing (LAW), where we propose a new objective to remove the corresponding knowledge from the MLP layers in the large language models (LLMs), which is considered to store most of the knowledge in the LLMs.Experimental results demonstrate the effectiveness of our method in washing the knowledge in terms of the accuracies when prompted with queries related to the knowledge set, while mostly maintaining the model's reasoning ability.Our work proposes an effective knowledge-washing algorithm and shows the possibility of knowledge-reasoning disentanglement.One limitation is we consider the knowledge set in a specific format, i.e., triplets, whereas washing a large scale of knowledge in pure text where no triplets are available might be more challenging.For future work, we aim to explore washing the knowledge more thoroughly and extend our framework to other more recent LLMs.",
                    "score": 0.4492730977561683,
                    "section_title": "Conclusion, Limitation, and Future Work",
                    "char_start_offset": 28243,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 147,
                            "end": 449
                        },
                        {
                            "start": 449,
                            "end": 678
                        },
                        {
                            "start": 678,
                            "end": 802
                        },
                        {
                            "start": 802,
                            "end": 1002
                        },
                        {
                            "start": 1002,
                            "end": 1126
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Large Scale Knowledge Washing\n# Venue: arXiv.org\n# Authors: Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, Julian McAuley\n## Abstract\nLarge language models show impressive abilities in memorizing world knowledge, which leads to concerns regarding memorization of private information, toxic or sensitive knowledge, and copyrighted content. We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers. Experimental results demonstrate the effectiveness of LAW in forgetting target knowledge while maintaining reasoning ability. The code will be open-sourced at https://github.com/wangyu-ustc/LargeScaleWashing.\n## Conclusion, Limitation, and Future Work\nIn this paper, we introduce the Large Scale Knowledge Washing problem, which means unlearning the existing knowledge in the model on a large scale.To address this problem, we draw inspiration from model-editing methods and propose Large Scale Washing (LAW), where we propose a new objective to remove the corresponding knowledge from the MLP layers in the large language models (LLMs), which is considered to store most of the knowledge in the LLMs.Experimental results demonstrate the effectiveness of our method in washing the knowledge in terms of the accuracies when prompted with queries related to the knowledge set, while mostly maintaining the model's reasoning ability.Our work proposes an effective knowledge-washing algorithm and shows the possibility of knowledge-reasoning disentanglement.One limitation is we consider the knowledge set in a specific format, i.e., triplets, whereas washing a large scale of knowledge in pure text where no triplets are available might be more challenging.For future work, we aim to explore washing the knowledge more thoroughly and extend our framework to other more recent LLMs.",
            "reference_string": "[270062331 | Wang et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 162,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.20150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "47739850",
                    "name": "Jiaao Chen"
                },
                {
                    "authorId": "2263629011",
                    "name": "Diyi Yang"
                }
            ],
            "abstract": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
            "corpus_id": 264828972,
            "sentences": [
                {
                    "corpus_id": "264828972",
                    "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
                    "text": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
                    "score": 0.4394504705644902,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Unlearn What You Want to Forget: Efficient Unlearning for LLMs\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jiaao Chen, Diyi Yang\n## Abstract\nLarge language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.\n",
            "reference_string": "[264828972 | Chen et al. | 2023 | Citations: 162]"
        },
        {
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
            "venue": "",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.06027, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350753289",
                    "name": "Stefan Vasilev"
                },
                {
                    "authorId": "2307079915",
                    "name": "Christian Herold"
                },
                {
                    "authorId": "66693547",
                    "name": "Baohao Liao"
                },
                {
                    "authorId": "2350630854",
                    "name": "Seyyed Hadi Hashemi"
                },
                {
                    "authorId": "2490162",
                    "name": "Shahram Khadivi"
                },
                {
                    "authorId": "2062908179",
                    "name": "C. Monz"
                }
            ],
            "abstract": "This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.",
            "corpus_id": 278481378,
            "sentences": [
                {
                    "corpus_id": "278481378",
                    "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
                    "text": "Large Language Models (LLMs) have advanced rapidly, becoming widely applicable in various settings (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024). However, their increasing capabilities raise significant privacy risks, especially for individuals whose sensitive data may have been included in training. This information can become embedded within the model, making it susceptible to unintended exposure through memorization, adversarial exploits, membership inference (MIA), and model inversion attacks (Yao et al., 2024b). \n\nTo address these concerns, regulatory frameworks such as the General Data Protection Reg-* Correspondence to: stvasilev@ebay.com ulation (GDPR) have been established to protect individual privacy and enforce the right to be forgotten. Given that LLMs are subject to such regulations, the machine learning research community has increasingly focused on the emerging field of Machine Unlearning for LLMs (Wang et al., 2025a;Liu et al., 2024b;Jang et al., 2023), which aims to develop methods for selectively removing specific knowledge from models. This includes erasing sensitive information (Wang et al., 2025a;Patil et al., 2023), forgetting entire entities or facts (Ma et al., 2025), and removing harmful or biased information (Lu et al., 2022). \n\nIn the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency.",
                    "score": 0.423386173913821,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 529
                        },
                        {
                            "start": 532,
                            "end": 660
                        },
                        {
                            "start": 661,
                            "end": 766
                        },
                        {
                            "start": 767,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1280
                        },
                        {
                            "start": 1283,
                            "end": 1548
                        },
                        {
                            "start": 1549,
                            "end": 1713
                        },
                        {
                            "start": 1714,
                            "end": 2075
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 934,
                            "end": 954,
                            "matchedPaperCorpusId": "276617972"
                        },
                        {
                            "start": 972,
                            "end": 990,
                            "matchedPaperCorpusId": "252693065"
                        },
                        {
                            "start": 1123,
                            "end": 1143,
                            "matchedPaperCorpusId": "276617972"
                        },
                        {
                            "start": 1143,
                            "end": 1162,
                            "matchedPaperCorpusId": "263311025"
                        },
                        {
                            "start": 1200,
                            "end": 1217,
                            "matchedPaperCorpusId": "270703237"
                        },
                        {
                            "start": 1262,
                            "end": 1279,
                            "matchedPaperCorpusId": "249152301"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation\n# Venue: \n# Authors: Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, C. Monz\n## Abstract\nThis paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.\n## Introduction\nLarge Language Models (LLMs) have advanced rapidly, becoming widely applicable in various settings (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024). However, their increasing capabilities raise significant privacy risks, especially for individuals whose sensitive data may have been included in training. This information can become embedded within the model, making it susceptible to unintended exposure through memorization, adversarial exploits, membership inference (MIA), and model inversion attacks (Yao et al., 2024b). \n\nTo address these concerns, regulatory frameworks such as the General Data Protection Reg-* Correspondence to: stvasilev@ebay.com ulation (GDPR) have been established to protect individual privacy and enforce the right to be forgotten. Given that LLMs are subject to such regulations, the machine learning research community has increasingly focused on the emerging field of Machine Unlearning for LLMs (Wang et al., 2025a;Liu et al., 2024b;Jang et al., 2023), which aims to develop methods for selectively removing specific knowledge from models. This includes erasing sensitive information (Wang et al., 2025a;Patil et al., 2023), forgetting entire entities or facts (Ma et al., 2025), and removing harmful or biased information (Lu et al., 2022). \n\nIn the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency.",
            "reference_string": "[278481378 | Vasilev et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 38,
            "citation_count": 33,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.15766, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "73502630",
                    "name": "Nianwen Si"
                },
                {
                    "authorId": "2154930608",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2116152318",
                    "name": "Heyu Chang"
                },
                {
                    "authorId": "9047584",
                    "name": "Wenlin Zhang"
                },
                {
                    "authorId": "2253591545",
                    "name": "Dan Qu"
                },
                {
                    "authorId": "2268429659",
                    "name": "Weiqiang Zhang"
                }
            ],
            "abstract": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "corpus_id": 265456592,
            "sentences": [
                {
                    "corpus_id": "265456592",
                    "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                    "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
                    "score": 0.41045944639441756,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.99658203125
                }
            ],
            "relevance_judgement": 0.99658203125,
            "relevance_judgment_input_expanded": "# Title: Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges\n# Venue: arXiv.org\n# Authors: Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, Weiqiang Zhang\n## Abstract\nIn recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.\n",
            "reference_string": "[265456592 | Si et al. | 2023 | Citations: 33]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "274763373",
            "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
            "text": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model. The modularity of the TARS method allows for a sequential removal of concepts from Llama 3.1 8B, such as the famous literary detective Sherlock Holmes, and the planet Saturn. It is demonstrated that the probability of triggering target concepts can be reduced to 0.00 with as few as 1 TARS edit, whilst simultaneously removing the knowledge bi-directionally. Moreover, knowledge is shown to be removed across all languages despite only being targeted in English. Importantly, TARS has minimal impact on the general model capabilities, as after removing 5 diverse concepts in a modular fashion, there is minimal KL divergence in the next token probabilities of the LLM on large corpora of Wikipedia text (median of 0.0015).",
            "score": 0.5540221360191874,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "278338982",
            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
            "text": "The rapid growth of large language models (LLMs), trained on internet-scraped data, has raised concerns about privacy, compliance, and ethical usage. Regulations like GDPR require methods for selectively removing sensitive or copyrighted information from these models. Researchers have proposed various post-training techniques, which we broadly categorize into (i) knowledge removal, (ii) knowledge addition, (iii) knowledge edition. This paper focuses on knowledge removal, also referred to as unlearning (Liu et al., 2025), which involves removing specific information from trained LLMs without complete retraining. Ideally, after unlearning, the LLM behaves as though the removed information had never been learned. However, current 1 https://github.com/potsawee/unlearning-hallu/ tree/mcq methods often perform unlearning by extensively adding incorrect or irrelevant information, a practice we refer to as obfuscation, which effectively constitutes a form of knowledge addition rather than true removal, and can lead to random or incorrect model responses. Unlike knowledge editing (Mitchell et al., 2022), which updates factual associations, unlearning (the focus of this work) aims to eliminate targeted knowledge entirely. \n\nEarly knowledge removal approaches were gradient ascent (GA) based (Jang et al., 2023;Ilharco et al., 2023a;Yao et al., 2024a) and structural or privacy-related sub-circuit discovery methods (Bayazit et al., 2024), which directly minimize the probability of original facts. Negative preference optimization (Zhang et al., 2024) removes knowledge by increasing the probability of false statements compared to the true ones, which was an early form of obfuscating. More recent obfuscatingbased methods (Eldan and Russinovich, 2023;Liu et al., 2024;Dong et al., 2024;Xu et al., 2025a) have gained popularity due to their superior stability and the minimal distortion to knowledge to be retained. For example, WHP (Eldan and Russinovich, 2023) and WHP + (Liu et al., 2024) remove knowledge about target people by overwhelming LLMs with information from other individuals.",
            "score": 0.5503773495054616,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 525,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 1088,
                    "end": 1111,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1301,
                    "end": 1320,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1320,
                    "end": 1342,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1425,
                    "end": 1447,
                    "matchedPaperCorpusId": "263671765"
                },
                {
                    "start": 1541,
                    "end": 1561,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1763,
                    "end": 1780,
                    "matchedPaperCorpusId": "271404131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "263311025",
            "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
            "text": "To our knowledge, these methods have not been applied as extraction attacks on LLMs that have been specifically tailored to remove sensitive information (e.g. with model editing methods). Moreover, we extend such information deletion methods in order to better defend against these kinds of attacks. \n\nMachine Unlearning and Model Editing. So-called machine unlearning is an old problem where the goal is to remove information from a model without damaging the model's performance on the task it was trained for (Cao & Yang, 2015). Initial unlearning approaches for deep learning relied on gradient-based updates to model weights, using e.g. influence functions (Guo et al., 2019) or continual learning methods (Tanno et al., 2022). However, unlearning methods are generally focused on removing the influence of a training (x, y) pair on a supervised model. This may not be the appropriate framework for deleting sensitive information from language models, since the information is an undesirable output given in response to prompts or questions that are harmless on their own. In contrast, model editing is an approach focused on changing particular outputs for certain model inputs, with methods designed to update factually incorrect knowledge in models (Zhu et al., 2020;Dai et al., 2022;De Cao et al., 2021;Hase et al., 2021). We adopt the model editing approach as these methods have shown promising performance at changing model outputs while minimally damaging a model, though other adjustments to model development may exist for improving safety or privacy of language models (Carlini et al., 2019b;Henderson et al., 2023;Min et al., 2023). Model editing has already been used widely within computer vision for deleting specific concepts from image generation models (Gandikota et al., 2023;Heng & Soh, 2023;Kumari et al., 2023;Zhang et al., 2023b).",
            "score": 0.5431445229942329,
            "section_title": "RELATED WORK",
            "char_start_offset": 9687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 530,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 711,
                    "end": 731,
                    "matchedPaperCorpusId": "250425883"
                },
                {
                    "start": 1275,
                    "end": 1292,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 1292,
                    "end": 1312,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 1585,
                    "end": 1608,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Large Language Models (LLMs) (Achiam et al., 2023;Touvron et al., 2023a,b;Meta, 2024) pretrained on extensive corpora have achieved significant success in knowledge-intensive tasks (Kamalloo et al., 2023;Seegmiller et al., 2024). However, undesirable data exists with training data, such as toxic texts (Lu et al., 2022), privacy content (Liu et al., 2024a) and copyrighted information (Karamolegkou et al., 2023). Such data has raised security and legal concerns, hindering the practical application of LLMs (Yao et al., 2024;Das et al., 2024). To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models. \n\nTowards this direction, existing work has conducted extensive research. However, most of these efforts focus on Instance-level Unlearning tasks, which address isolated sensitive content (Li et al., 2024;Zhang et al., 2024;Ji et al., 2024), while neglecting the deletion of entire entities, which is crucial in many real-world scenarios, such as removing 'Harry Potter' for copyright protection (Eldan and Russinovich, 2024). To address this gap, we formally define a novel task of Entity-level Unlearning. As illustrated in Figure 1, existing instance-level unlearning tasks focus on removing predefined facts by applying unlearning algorithms to a forget set that contains the specific information to be erased. In contrast, the entity-level unlearning task seeks to eliminate an entire entity, including all the entity-related knowledge within the model.",
            "score": 0.520900174534904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1811
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 320,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "258059852"
                },
                {
                    "start": 1177,
                    "end": 1193,
                    "matchedPaperCorpusId": "259501579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Recently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences. However, these approaches may be vulnerable to adversarial attacks, where specific token sequences are replaced or aliased with alternative sequences. For example, prompting in low-resource languages has been shown to jailbreak GPT-4 (Yong et al., 2023), and Choi et al. (2024) demonstrated that current unlearning techniques lack cross-lingual transfer, making LLMs susceptible to such low-resource language exploits. This leads to an important research question: \"Do current unlearning methods effectively erase multi-hop knowledge when one of the intermediate hops is removed?\" As illustrated in Figure 1, consider a scenario where Elon Musk (i.e., \"the user\") requests the removal of his personal information from an LLM. After unlearning, we expect that direct, single-hop knowledge related to Elon Musk, such as \"Who is the CEO of Tesla?\", would be deleted. Additionally, we would expect associated multi-hop knowledge, like \"What is the birthplace of Tesla's CEO?\", which indirectly references Musk, to also be removed. \n\nIn this study, we explore the effectiveness of existing unlearning methods in removing multihop knowledge. We begin by refashioning the widely used multi-hop knowledge editing dataset, MQuAKE (Zhong et al., 2023). Since we do not need to edit knowledge, we discard edited facts and only consider the original facts for unlearning. Each example in MQuAKE comprises a multihop question (ranging from 2 to 4 hops) that corresponds to a sequence of interconnected facts. When we unlearn one or more facts within a chain, the model is expected to propagate these changes such that it can no longer answer the associated multihop questions. Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed.",
            "score": 0.5195354452637448,
            "section_title": "Unlearn Request",
            "char_start_offset": 2375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 92,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "268357903"
                },
                {
                    "start": 109,
                    "end": 129,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1456,
                    "end": 1476,
                    "matchedPaperCorpusId": "258865984"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Large language models (LLMs) [56; 41], trained on massive internet corpora, can encapsulate a vast amount of knowledge within their parameters, and possess the capability to recall and manipulate this knowledge during the generation process.However, this capability is dual-use, potentially leading to privacy problems, copyright concerns, and harmful issues [42; 27].For instance, LLMs may memorize personally identifiable information (e.g., social security numbers) or copyrighted material (e.g., Harry Potter series) from the training data, and emit it verbatim when prompted with adversarial attacks [10].Besides, AI assistants for biology could troubleshoot bottlenecks in biological weapons development, increasing the risk of such attempts.According to regulations such as the European General Data Protection Regulation (GDPR) upholding individuals' \"right to be forgotten\" (RTBF) [39], sensitive and toxic knowledge within LLMs should also be erasable.A straightforward solution is to retrain the model from scratch, ensuring it excludes any data that users have requested to be removed.However, this is not feasible for LLMs that consume extensive computational resources.\n\nTo efficiently remove specific knowledge by post hoc modifying models, machine unlearning has emerged as a solution [9; 25; 59; 13; 37].An optimal unlearning method needs to satisfy the following In this paper, we propose a Real-World Knowledge Unlearning benchmark ( RWKU).RWKU is designed based on the three key factors mentioned above: (1) For the task setting, we consider a more practical and challenging setting, similar to \"zero-shot knowledge unlearning\".We provide only the unlearning target and the original model, without offering any forget corpus or retain corpus.In this way, it avoids secondary information leakage caused by the forget corpus and is not affected by the distribution bias of the retain corpus.(2) For the knowledge source, we choose real-world famous people from Wikipedia as the unlearning targets and demonstrate that such popular knowledge is widely present in various LLMs through memorization quantification, making it more suitable for knowledge unlearning.",
            "score": 0.5015633086202981,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 241,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 609
                },
                {
                    "start": 609,
                    "end": 747
                },
                {
                    "start": 747,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1182
                },
                {
                    "start": 1184,
                    "end": 1320
                },
                {
                    "start": 1320,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1761
                },
                {
                    "start": 1761,
                    "end": 1908
                },
                {
                    "start": 1908,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "56699980"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "273507405",
            "title": "Catastrophic Failure of LLM Unlearning via Quantization",
            "text": "Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the\"forgotten\"information. To thoroughly evaluate this phenomenon, we conduct comprehensive experiments using various quantization techniques across multiple precision levels. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21\\% of the intended forgotten knowledge in full precision, which significantly increases to 83\\% after 4-bit quantization. ... Our code is available at: \\href{https://github.com/zzwjames/FailureLLMUnlearning}{https://github.com/zzwjames/FailureLLMUnlearning}.",
            "score": 0.5013771281685921,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "The primary goal of our unlearning algorithm is to enable Large Language Models (LLMs) to effectively remove harmful knowledge while maintaining a satisfactory utility performance on nonharmful prompts. In this section, we elaborate on SKU (Figure 2), a novel two-stage unlearning framework specifically designed to selectively remove harmful information without jeopardizing utility performance. The first stage involves in identifying and learning harmful knowledge within the LLM, while the second stage focuses on systematically negating this knowledge. Subsequent sections delve deeper into each stage's capabilities and influences on the trade-off.",
            "score": 0.5000234060481228,
            "section_title": "Methods",
            "char_start_offset": 8988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 654
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "277857590",
            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
            "text": "Recently, Large Language Models (LLMs) [1]- [3] have been trained on extensive datasets that include web pages and user-generated content. During training, models acquire sensitive knowledge that raises social and legal concerns, with principles like the \"Right to be forgotten\" [4] emphasizing the need to remove unauthorized data. However, retraining an entire language model from scratch to erase sensitive information is cost-inefficient, and reconstructing the original pre-training Fig. 1. Existing unlearning methods often rely on fixed boundaries within model layers and overlook the distinct unlearning and retention scopes required for both privacy and copyright. As a result, when these methods attempt to unlearn copyright knowledge after removing privacy knowledge in the same LLM, they risk corrupting knowledge that should remain intact. dataset is exceedingly difficult. As a result, researchers have turned their attention to Machine Unlearning [5]- [12], which aims to remove specific knowledge from pre-trained models. \n\nA key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge. Laws and legal principles [13]- [16] related to privacy and copyright indicate that certain knowledge within these sensitive domains should be retained. Despite this necessity, many existing approaches do not clearly differentiate between the unlearning scope, which specifies the knowledge to remove, and the retention scope, which describes what should be preserved. In some cases, they indiscriminately remove everything loosely associated with the target. Memflex [5] introduced knowledge localization to address this issue. It distinguishes unlearning and retention scope in a given domain by leveraging gradient information in a layer-wise manner to achieve effective knowledge unlearning and retention. \n\nDespite these efforts, several challenges remain in applying unlearning methods to real-world LLMs. First, singledomain methods like Memflex are inadequate for unlearning knowledge that spans multiple domains.",
            "score": 0.49461101851042716,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 282,
                    "matchedPaperCorpusId": "234335026"
                },
                {
                    "start": 962,
                    "end": 965,
                    "matchedPaperCorpusId": "270878324"
                },
                {
                    "start": 1408,
                    "end": 1412,
                    "matchedPaperCorpusId": "8435667"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "270878324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "270371266",
            "title": "The Curse of Popularity: Popular Entities have Catastrophic Side Effects when Deleting Knowledge from Language Models",
            "text": "Language models (LMs) encode world knowledge in their internal parameters through training. However, LMs may learn personal and confidential information from the training data, leading to privacy concerns such as data leakage. Therefore, research on knowledge deletion from LMs is essential. This study focuses on the knowledge stored in LMs and analyzes the relationship between the side effects of knowledge deletion and the entities related to the knowledge. Our findings reveal that deleting knowledge related to popular entities can have catastrophic side effects. Furthermore, this research is the first to analyze knowledge deletion in models trained on synthetic knowledge graphs, indicating a new direction for controlled experiments.",
            "score": 0.490161307119962,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "277510371",
            "title": "ESC: Erasing Space Concept for Knowledge Deletion",
            "text": "We reveal a new insight that existing unlearning methods cannot fully fulfill the knowledge removal request by users. Furthermore, our findings suggest a potential problem: existing methods cannot eliminate almost forgetting knowledge, and we finally suggest a novel perspective for knowledge removal considering user's requests and feature-level, called Knowledge Deletion. We also propose a novel evaluation setting for this issue. Our insights provide a benefit to the related research community. Furthermore, we introduce simple yet effective methods for KD. Our methods demonstrate remarkable performance in various KD scenarios, including incremental and facial domains, even in the random data forgetting scenario. In addition to their efficiency, our methods are suitable for real-world AI deployments. \n\nESC and ESC-T utilize an additional layer after the penultimate layer, i.e., after the feature extractor. Our methods are particularly useful when the service provider releases their model as a black-box, such as Chat-GPT [1]. However, if the model is released as a white-box, we need to integrate this part into the original model architecture. If we have a simple MLP layer, it can be directly merged with the existing weights, but merging with the entire model remains an open question. We expect that this issue could be addressed through methods such as distillation, and we plan to conduct follow-up research on this challenge. Furthermore, our methods has generalizability and robustness in discriminative tasks, we need to extend this to other domains, such as diffusion model and language model. Our methods directly edit the feature space, while the latent space of generative models remains highly sensitive. Consequently, further investigation is needed to determine the applicability of our methods to generative models. We encourage future research to address these gaps.",
            "score": 0.490161307119962,
            "section_title": "G. Broad Impacts and Limitations",
            "char_start_offset": 49047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1898
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97216796875
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "Large language models (LLMs) have shown to pose social and ethical risks such as generating toxic language or facilitating malicious use of hazardous knowledge. Machine unlearning is a promising approach to improve LLM safety by directly removing harmful behaviors and knowledge. In this paper, we propose\"SPlit, UNlearn, MerGE\"(SPUNGE), a framework that can be used with any unlearning method to amplify its effectiveness. SPUNGE leverages data attributes during unlearning by splitting unlearning data into subsets based on specific attribute values, unlearning each subset separately, and merging the unlearned models. We empirically demonstrate that SPUNGE significantly improves the performance of two recent unlearning methods on state-of-the-art LLMs while maintaining their general capabilities on standard academic benchmarks.",
            "score": 0.49000671209134083,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "The primary objective in language modeling is to minimize the negative log-likelihood of token sequences, training the model to predict the next word in a sequence accurately. Knowledge unlearning (Jang et al., 2023) involves negating this objective to remove specific learned information from the model. Instead of reinforcing certain sequences, unlearning aims to decrease their probabilities by maximizing their negative log-likelihood, which can be understood as equivalent to removing the negative sign: \n\nwhere x comes from a sequence of tokens x f \u2208 D f and p \u03b8 (x t |x <t ) denotes the conditional probability of predicting the next token given the model parameters \u03b8. This effectively reverses the learned patterns, reducing the probability of generating the targeted sequences and allowing the model to \"forget\" specific knowledge.",
            "score": 0.489587591509532,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 6848,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 508
                },
                {
                    "start": 511,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 841
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Large language models (LLMs) serve as giant information stores, often including personal or copyrighted data, and retraining them from scratch is not a viable option. This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective. However, these methods still leave LLMs vulnerable to adversarial attacks that exploit indirect references. In this work, we examine the limitations of current unlearning techniques in effectively erasing a particular type of indirect prompt: multi-hop queries. Our findings reveal that existing methods fail to completely remove multi-hop knowledge when one of the intermediate hops is unlearned. To address this issue, we propose MUNCH, a simple uncertainty-based approach that breaks down multi-hop queries into subquestions and leverages the uncertainty of the unlearned model in final decision-making. Empirical results demonstrate the effectiveness of our framework, and MUNCH can be easily integrated with existing unlearning techniques, making it a flexible and useful solution for enhancing unlearning processes.",
            "score": 0.4883029411545148,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "273350971",
            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
            "text": "The widespread integration of Large Language Models (LLMs) into daily applications has raised significant concerns regarding the trustworthiness of such models. Their outputs may contain sensitive, private, or illegal content [1,2], reflect societal biases [3,4], or provide harmful instructions [5,6,7]. In particular, for privacy concerns, regulations [8] have been introduced, requiring applications to support the deletion of information contained in training samples upon user request. This has motivated research into machine unlearning (MU) [9,10,11,12,13], a critical process aimed at removing the influence of specific data points, data classes, or even higher-level data concepts from trained models. \n\nLLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data Table 1: Comparison of different loss adjustment-based baselines in terms of their requirement. Our method relies solely on forget data and available template responses, without using the retain data or a reference model for response calibration.",
            "score": 0.4870278599005606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1645
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 260,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 260,
                    "end": 262,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 301,
                    "end": 303,
                    "matchedPaperCorpusId": "261276945"
                },
                {
                    "start": 354,
                    "end": 357,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 548,
                    "end": 551,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "The task of\"unlearning\"certain concepts in large language models (LLMs) has attracted immense attention recently, due to its importance in mitigating undesirable model behaviours, such as the generation of harmful, private, or incorrect information. Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning. We argue that unlearning should also be evaluated internally, by considering changes in the parametric knowledge traces of the unlearned concepts. To this end, we propose a general evaluation methodology that leverages vocabulary projections to inspect concepts encoded in model parameters. We use this approach to localize\"concept vectors\"- parameter vectors that encode concrete concepts - and construct ConceptVectors, a benchmark dataset containing hundreds of common concepts and their parametric knowledge traces within two open-source LLMs. Evaluation on ConceptVectors shows that existing unlearning methods minimally impact concept vectors and mostly suppress them during inference, while directly ablating these vectors demonstrably removes the associated knowledge and significantly reduces the model's susceptibility to adversarial manipulation. Our results highlight limitations in behavioral-based unlearning evaluations and call for future work to include parameter-based evaluations. To support this, we release our code and benchmark at https://github.com/yihuaihong/ConceptVectors.",
            "score": 0.4861065421668938,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "Large Language Models (LLMs) are shown to memorize extensive knowledge or factual relations [Chen et al., 2022, Alivanistos et al., 2022, Youssef et al., 2023].However, the memorization of knowledge in LLMS raises both moral and legal concerns.Factual knowledge may involve personal and sensitive information whose memorization can violate strict regulations [Legislature, 2018, Act, 1996, Parliament and of the European Union, 2016], and memorizing copyright content is also problematic -The New York Times 1 recently filed lawsuit against OpenAI to protect its copyright of articles.\n\nTo prevent the undesired memorization of the above-mentioned knowledge, the simplest solution is perhaps to label data that has the potential to raise concerns in advance and exclude sensitive data from the pre-training stage.However, this solution needs exhaustive human effort and may not be feasible as the pretraining corpus for LLM is normally extremely large.This impracticality motivates the study of machine unlearning [Liu et al., 2024a, Yao et al., 2024, Si et al., 2023, Yao et al., 2023b, Zhang et al., 2023].When there are concerns about memorizing sensitive knowledge, these methods aim to update the LLM to forget that knowledge with a relatively small computational cost.Most of these methods are in the paradigm of defining an \"unlearning\" loss (essentially the reverse loss of Next-Word-Prediction on the unlearning dataset) and updating the full models by backpropagating from the loss.However, updating the model with backpropagation may hurt the model's abilities Figure 1: The diagram shows the process of Large Scale Knowledge Washing.We aim to remove private, toxic or copyright knowledge such as SSN from the LLM, while maintaining the model's reasoning ability to answer questions such as \"a > b, b > c, a?c\" whose answer should be \">\".\n\nin downstream tasks requiring reasoning.When the knowledge to be unlearned scales up, it may require extensive updates of the model parameters, which could even destroy the model (as shown in our experiments).",
            "score": 0.4850583765272841,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 160,
                    "end": 244
                },
                {
                    "start": 244,
                    "end": 585
                },
                {
                    "start": 587,
                    "end": 813
                },
                {
                    "start": 813,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1274
                },
                {
                    "start": 1274,
                    "end": 1492
                },
                {
                    "start": 1492,
                    "end": 1645
                },
                {
                    "start": 1645,
                    "end": 1849
                },
                {
                    "start": 1851,
                    "end": 1891
                },
                {
                    "start": 1891,
                    "end": 2060
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 110,
                    "matchedPaperCorpusId": "248406169"
                },
                {
                    "start": 110,
                    "end": 136,
                    "matchedPaperCorpusId": "251741004"
                },
                {
                    "start": 136,
                    "end": 159,
                    "matchedPaperCorpusId": "264451714"
                },
                {
                    "start": 377,
                    "end": 388,
                    "matchedPaperCorpusId": "167175091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "Recently, there has been surging interest in developing methods for unlearning information captured in large language models (LLMs) (Jang et al., 2023;Chen & Yang, 2023;Yao et al., 2023;Eldan & Russinovich, 2023;Si et al., 2023;Liu et al., 2024a;b). Such methods are important for removing sensitive or harmful information, biases, and outdated facts. A key challenge in developing unlearning methods is evaluating their performance, namely, how to validate the erasure of the unlearned information. Existing evaluation protocols largely rely on behavioural tests, such as the ability to answer questions or complete queries about the removed information (Stoehr et al., 2024;Hase et al., 2023;Chen & Yang, 2023). However, growing evidence suggests that it is often possible to steer the model to generate the unlearned information post-unlearning (Lynch et al., 2024;Patil et al., 2024), indicating that the target knowledge has not in fact been exhaustively removed from the model. This work presents the first benchmark for internal evaluation of unlearning methods. \n\nWe highlight the existence of \"parametric knowledge traces\", which are specific sets of parameters in the model that strongly correlate with the knowledge to be erased (see Figure 1a for illustration). We show that this residual knowledge causally influences the model's ability to generate information about the target concept, and argue that its internal erasure should be a goal of unlearning. Specifically, we leverage recent methods that inspect the information encoded in model parameters through vocabulary projections (Dar et al., 2023;Geva et al., 2022b). Using this approach, we identify parametric \"concept vectors\" in LLMs that are suitable for testing unlearning ( \u00a73); these vectors are located in the model's MLP layers and strongly affect the generation of their corresponding  concepts, without influencing unrelated ones.",
            "score": 0.4842013505860096,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1911
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 151,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 151,
                    "end": 169,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "255595518"
                },
                {
                    "start": 694,
                    "end": 712,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 868,
                    "end": 887,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1598,
                    "end": 1616,
                    "matchedPaperCorpusId": "252089779"
                },
                {
                    "start": 1616,
                    "end": 1635,
                    "matchedPaperCorpusId": "247762385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "274823032",
            "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration",
            "text": "The challenge of unlearning specific information from large language models (LLMs) has garnered significant attention, especially as the need to remove sensitive or harmful information becomes increasingly important. Several approaches have been proposed to tackle this issue, each with its strengths and limitations. \n\nLiu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts. Compared to this approach, our work extends the idea of selective unlearning by incorporating a more granular control mechanism, allowing for the targeted removal of specific data points with minimal impact on overall model utility. \n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process. While their method is robust in terms of task versatility, our framework offers a more specialized solution tailored to the unique challenges of LLMs used in federated learning environments, ensuring that unlearning is both precise and minimally disruptive to the model's overall functionality. \n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts. In contrast, our work introduces a more balanced approach, leveraging the LoRA-based forgetting mechanism to ensure that the removal of harmful information does not compromise the model's ability to respond accurately to benign queries.",
            "score": 0.48394426798423107,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.",
            "score": 0.47575360563992075,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "The swift advancement and widespread deployment of large language models (LLMs) have brought many challenges including the inability to remove knowledge from the LLMs at will. Efficient removal of knowledge has become increasingly important with 'Right to be Forgotten' laws (Goldman, 2020) and Europe's General Data Protection Regulation (Goddard, 2017). Traditional training methodologies often lack the flexibility and efficiency required to address both tasks, especially when rapid model adaptation is needed without comprehensive retraining. \n\nThis paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task. \n\nUNLEARN achieves 96% forgetting on the task of interest while maintaining performance on dissimilar tasks within 2.5% of the original model. When the tasks are similar, UNLEARN still achieves nearly 80% forgetting on the task of interest while preserving performance on similar tasks within 10%. These results significantly outperform the state-of-the-art, which achieves similar forgetting but is accompanied by significant degradation on similar tasks. \n\nThe forgetting of UNLEARN can easily be converted to add knowledge to the LLM. This new method LEARN matches the fine-tuning accuracy of the LoRA method (Hu et al., 2021) without affecting related tasks, demonstrating its dual nature across both knowledge unlearning and finetuning scenarios. \n\nThe contributions of this work are as follows: \n\n\u2022 An efficient method to identify the subspace of specific knowledge within an LLM. \n\n\u2022 A novel approach called subspace discrimination and task removal to selectively target and remove specific knowledge without adversely affecting other knowledge in the LLM. \n\n\u2022 The introduction of LEARN, a dual algorithm to UNLEARN that provides a new approach to adding new knowledge to the LLM without affecting its other knowledge.",
            "score": 0.473753964148069,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2001
                },
                {
                    "start": 2004,
                    "end": 2050
                },
                {
                    "start": 2053,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2313
                },
                {
                    "start": 2316,
                    "end": 2475
                }
            ],
            "ref_mentions": [
                {
                    "start": 339,
                    "end": 354,
                    "matchedPaperCorpusId": "168855552"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "270045436",
            "title": "Class Machine Unlearning for Complex Data via Concepts Inference and Data Poisoning",
            "text": "Eldan et al. [22] propose a technique that effectively removes specific content, such as copyrighted material, from large language models, without the need for complete retraining.The method preserves the model's overall performance and represents an advancement in managing legal and ethical issues related to LLM training data.Jang et al. [23] introduce a more efficient and effective method for reducing privacy risks in Pretrained Language Models by using gradient ascent for knowledge unlearning.\n\nMoreover, Yao et al. [24] explore unlearning in LLMs, presenting it as a technique for aligning LLMs with human preferences by removing harmful responses, erasing copyrighted content, and eliminating hallucinations.Pawelczyk et al. [25] introduce \"In-Context Unlearning\" for LLMs, a novel approach to machine unlearning that does not require updating model parameters.This method involves providing the LLM with the training instance to be unlearned alongside a flipped label and additional correctly labeled instances at inference time.\n\nYu et al. [26] presented a new technique named \"Partitioned Contrastive Gradient Unlearning\" (PCGU).This gray-box approach focuses on adjusting only those weights in the model that contribute significantly to certain biases, particularly implicit social biases.",
            "score": 0.47252158895773544,
            "section_title": "B. Large Language Models Unlearning",
            "char_start_offset": 16945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 501
                },
                {
                    "start": 503,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1040
                },
                {
                    "start": 1042,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1303
                }
            ],
            "ref_mentions": [
                {
                    "start": 1052,
                    "end": 1056,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "What does it mean for a language model to \"unlearn\" a concept? While machine unlearning has traditionally focused on removing specific training samples from model memory, there is an increasing need to be able to erase broad conceptual knowledge-for example, removing all information about biological weapons rather than just a few training examples containing that information. In this paper we examine how concept-level unlearning leads to a new approach to knowledge removal in language models. \n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior. \n\nWe propose a fundamentally different approach that leverages the model's own ability to recognize and classify knowledge. Our key insight is that language models can act as their own critics: they can evaluate whether a piece of text demonstrates knowledge of a particular concept. This selfclassification provides a natural objective for unlearning: we can modify the model to reduce the likelihood of generating text it would classify as containing target concept. This insight leads to Erasure of Language Memory (ELM), a method that directly optimizes the model's generation probabilities based on introspective classification. Unlike approaches like Representation Misdirection for Unlearning (RMU; Li et al., 2024) which manipulates internal activations without a clear behavioral target, or WhoIsHarry-Potter (Eldan & Russinovich, 2023) which modifies training data but fails to fully eliminate concept knowledge, ELM has a principled objective: the model should generate coherent text that the language model itself would not classify as demonstrating knowledge of the target concept.",
            "score": 0.47181110617925565,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.",
            "score": 0.4701717795962229,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Large Language Models (LLMs) offer extensive knowledge across various domains, but they may inadvertently memorize sensitive, unauthorized, or malicious data, such as personal information in the medical and financial sectors. Machine unlearning methods aim to remove specific information from models after training to address this. However, current approaches require additional model training or struggle to effectively erase particular data points and their associated context due to LLMs' complex, dense, and continuous nature. In this study, we propose a novel amortized unlearning approach using codebook features and Sparse Autoencoders (SAEs). By leveraging a bottleneck to decompose the activation space and regulate information flow, our method efficiently unlearns targeted information while preserving the model's performance on unrelated data. To the best of our knowledge, this is the first work that successfully enables unlearning specific topics with contextual relevance in an LLM, marking a significant step towards real-world applications of machine unlearning.",
            "score": 0.4655799448072484,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Various studies have attempted to remove sensitive or private knowledge from a language model to prevent its unauthorized exposure. However, prior studies have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUn, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which updates only knowledge-related neurons to achieve faithful unlearning. KLUE identifies knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples. Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA unlearning.",
            "score": 0.46332305530403795,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "263671765",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "text": "Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized. We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+ sparsity) that are critical for expressing specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial abilities but struggles to represent the suppressed knowledge.",
            "score": 0.46195415902198955,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "270703035",
            "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective",
            "text": "Large language models (LLMs) have shown impressive capabilities in natural language generation. However, their output is not always appropriate due to issues such as generating biased (Kotek et al., 2023;Motoki et al., 2023) or toxic content (Wen et al., 2023;Bender et al., 2021), regurgitating personally identifiable information (PII) (Nasr et al., 2023;Barrett et al., 2023), and hallucination (Huang et al., 2023;Xu et al., 2024). \n\nOne straightforward way to mitigate these undesired behaviors is to retrain the model on a new dataset which deletes 'bad' data points that cause the unwanted behaviors. However, naively retraining is known to be highly inefficient (Hu et al., 2021;Marchisio et al., 2023;Zhang et al., 2023b) due to significant computation cost and data requirements (Chen et al., 2023). As an alternative, machine unlearning (MU) (Bourtoule et al., 2021;Nguyen et al., 2022), originally proposed for classification models, has been extended to remove the influence of undesirable data and model capabilities for LLMs (Zhang et al., 2023a;Liu et al., 2024). \n\nDespite being a promising direction, LLM unlearning remains nascent. Particularly, existing unlearning methods are often evaluated using datasets, such as TOFU (Maini et al., 2024), which primarily composed of independent entities. However, we observe that real data points (such as Wikipedia data) are rarely independent, they are often interconnected, creating knowledge graphs with intricate topologies (Schneider et al., 2022). As such, real-world unlearning usage extends beyond simple deletion of independent data points from LLMs. Instead, it necessitates structural data deletion, which facilitates the comprehensive removal of 'relational' data, irrespective of its inter-connectivity with other entities or its domain (illustrated in Figure 1).",
            "score": 0.46189404394124794,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1079
                },
                {
                    "start": 1082,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 357,
                    "end": 378,
                    "matchedPaperCorpusId": "261276945"
                },
                {
                    "start": 687,
                    "end": 710,
                    "matchedPaperCorpusId": "254877513"
                },
                {
                    "start": 710,
                    "end": 730,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 853,
                    "end": 877,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97900390625
        },
        {
            "corpus_id": "277043381",
            "title": "Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning",
            "text": "As the capacity of large language models (LLMs) for real-world applications has grown, so has concern about their safety (Zhao et al., 2023). An LLM that accurately answers questions regardless of their implications could be misused to answer a wide variety of malicious queries. An LLM with a demonstrable understanding of chemistry could be used to advance science (Vert, 2023) but could also be used to develop weapons (Li et al., 2024). \n\nThe goal of safer LLMs has inspired several methodological approaches, from requiring ethical reasoning (Rao et al., 2023) to enforcing guardrails * Co-first authors (Rebedea et al., 2023). These methods attempt to prevent a model from revealing potentially harmful information to users. Machine unlearning methods instead seek to remove harmful knowledge so that the model cannot accurately answer malicious questions (Bourtoule et al., 2021;Gupta et al., 2021). \n\nMachine unlearning has emerged as a promising approach, but current methods often face tradeoffs between interpretability, computational efficiency, and unintended side effects (decreased performance on safe information). Representation Misdirection for Unlearning (RMU) modifies the model's internal representations to randomize knowledge in dangerous domains while preserving useful knowledge, but it lacks interpretability and can be computationally expensive (Li et al., 2024). Meanwhile, Sparse Autoencoder (SAE)based techniques (Farrell et al., 2024) provide a more structured approach by directly modifying interpretable latent activations, allowing for targeted suppression of harmful knowledge while retaining critical functionality.",
            "score": 0.46179261837104585,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 906
                },
                {
                    "start": 909,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1651
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 565,
                    "matchedPaperCorpusId": "263835359"
                },
                {
                    "start": 609,
                    "end": 631,
                    "matchedPaperCorpusId": "264146531"
                },
                {
                    "start": 862,
                    "end": 886,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 886,
                    "end": 905,
                    "matchedPaperCorpusId": "235367846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "270286011",
            "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning",
            "text": "The rapid advancements in large language models (LLMs) have led to remarkable breakthroughs in natural language processing and artificial intelligence.However, as these models are trained on vast amounts of data, they may inadvertently learn and perpetuate undesirable behaviors, biases, and harmful information.To address this issue, researchers have recently turned their attention to the concept of unlearning in LLMs.\n\nIn [18] paper, the authors explore the novel concept of unlearning in large language models (LLMs).They present a method that utilizes only negative examples to efficiently remove undesirable behaviors, demonstrating its effectiveness in alignment while significantly reducing computational resources compared to traditional reinforcement learning from human feedback (RLHF).While there is another paper introduces a data-driven unlearning approach for large language models (LLMs), utilizing a fine-tuning method informed by the importance of weights and relabeling during the pre-training phase of LLMs [19].This method adjusts word embedding, involving identifying and neutralizing bias vectors within the embedding space to prevent biased associations.Wang et al. [20] proposed an unlearning framework called Knowledge Gap Alignment (KGA), emphasizing its capability to efficiently handle large-scale data removal requests with significant accuracy.However, the inability of KGA to guarantee the complete removal of data influences also faces the challenge of maintaining extra data sets and models.Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models.This approach not only advances the field of responsible AI but also opens new avenues for enhancing data privacy and model impartiality.Huang et al. claim an innovation offset unlearning framework tailored for the black box LLM [22].This framework effectively addresses the challenge of unlearning problematic training data in LLMs without requiring access to internal model weight, thus offering a versatile solution for adapting current unlearning algorithms.",
            "score": 0.4580331769973207,
            "section_title": "B. Unlearning with LLM",
            "char_start_offset": 9557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 522
                },
                {
                    "start": 522,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1179
                },
                {
                    "start": 1179,
                    "end": 1376
                },
                {
                    "start": 1376,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1963
                },
                {
                    "start": 1963,
                    "end": 2060
                },
                {
                    "start": 2060,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1028,
                    "end": 1032,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "271212701",
            "title": "On Large Language Model Continual Unlearning",
            "text": "Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). However, concerns about large language models (LLMs) have also increased, particularly regarding how to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic, or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;Meng et al., 2022;Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning. \n\nHowever, these methods still often poorly maintain the model utility outside the unlearned knowledge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition to the data that needs to be unlearned, existing unlearning methods also require a large dataset called the retained dataset to maintain the model utility. This retained dataset often consists of the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on massive datasets (Wang et al., 2024), assuming access to the complete training data is typically unrealistic (Liu et al., 2024).",
            "score": 0.45499555387878987,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 187,
                    "matchedPaperCorpusId": "261064713"
                },
                {
                    "start": 357,
                    "end": 375,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 680,
                    "end": 698,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "In this work, we introduced CodeUnlearn, a novel framework for zero-shot machine unlearning in Large Language Models (LLMs). Leveraging codebook features and Sparse Autoencoders (SAEs), we devised a method that effectively isolates and removes specific knowledge, ensuring that the targeted data and its contextual associations are erased from the model. Unlike previous methods, which required retraining or were limited to classification tasks, CodeUnlearn operates amortized and zero-shot, providing an efficient and scalable solution for unlearning in complex, generative models like LLMs. Our approach uses a discrete concept representation to regulate the flow of information in a language model, enabling the unlearning of specific topics while preserving overall model performance on unrelated tasks. The results show that CodeUnlearn successfully mitigates the model's ability to reproduce the unlearned information without requiring additional training, achieving substantial unlearning effectiveness and maintaining interpretability.",
            "score": 0.4512318107617402,
            "section_title": "CONCLUSION",
            "char_start_offset": 25058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1044
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "277856836",
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "text": "Large language models (LLMs), pretrained on massive datasets via self-supervised learning, often inadvertently memorize sensitive information (Li et al., 2024;Zhou et al., 2024). This can include personally identifiable information such as email and home addresses, Social Security numbers linked to individual names, and even copyrighted creative content (Biderman et al., 2023;Carlini et al., 2019Carlini et al., , 2021)). The widespread opensourcing of these models raises concerns about the potential exposure and misuse of such data (Patil et al., 2024;Xu et al., 2024;Liu et al., 2024). While retraining with filtered datasets is a viable solution, the need for frequent updates to address newly discovered vulnerabilities or comply with evolving data privacy regulations (e.g., \"right to be forgotten\" requests (Zhang et al., 2023)) makes this approach prohibitively expensive. Each full retraining requires significant computational resources, time, and energy, leading to a substantial economic and environmental burden. \n\nUnlearning offers a promising solution by allowing models to remove or modify specific information without full retraining (Yao et al., 2024b,a;Chen and Yang, 2023). Unlearning methods seek to efficiently update LLMs by altering the model in a way that eliminates unwanted information while minimizing the impact on the model's overall performance and capabilities (Yuan et al., 2024). However, current unlearning solutions often struggle to balance effective unlearning with preserving the model's general usefulness (Yao et al., 2024b). This is largely due to their broad, non-selective application of unlearning techniques, which can unintentionally erase useful information. Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set.",
            "score": 0.4508054884395166,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 159,
                    "matchedPaperCorpusId": "272987840"
                },
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "261339641"
                },
                {
                    "start": 356,
                    "end": 379,
                    "matchedPaperCorpusId": "258291763"
                },
                {
                    "start": 379,
                    "end": 399,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 538,
                    "end": 558,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 558,
                    "end": 574,
                    "matchedPaperCorpusId": "266210093"
                },
                {
                    "start": 574,
                    "end": 591,
                    "matchedPaperCorpusId": "270620596"
                },
                {
                    "start": 1176,
                    "end": 1196,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1550,
                    "end": 1569,
                    "matchedPaperCorpusId": "264172840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "270619676",
            "title": "Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models",
            "text": "Data Preprocessing This approach aims to achieve exact unlearning by removing the target sequences from training data through preprocessing methods.This can effectively mitigate privacy risks for sequences that follow easily identifiable formats, such as phone numbers, email addresses, and more (Aura et al., 2006;Dernoncourt et al., 2016;Lison et al., 2021) et al., 2006;Dwork, 2006;Abadi et al., 2016).Although such methods have been effective in fine-tuning LMs (Yu et al., 2021;Li et al., 2021), pretraining LMs with DP significantly reduces performance, requires expensive computations, and converges very slowly (Anil et al., 2022).Furthermore, as it is impossible to define privacy boundaries for natural language (Brown et al., 2022), DP methods are inherently not applicable for target sequence unlearning.\n\nKnowledge Editing Knowledge editing methods modify LMs to achieve a diverse set of objectives.Some apply various transformations to the neural representations to identify and remove specific concepts (Ravfogel et al., 2022b,a;Belrose et al., 2023).Some apply other methods to maintain the relevancy of the LMs, efficiently updating the underlying knowledge without degrading their performance (Yao et al., 2023).Although these methods alter the pretrained LM for their respective goals, none are designed for the task of unlearning specific token sequences.\n\nSequence Unlearning For unlearning specific token sequences, Jang et al. ( 2023) proposed a simple gradient-based solution in reducing the generation likelihood of forgetting token sequences.Although the proposed solution can approximately remove a target token sequence, it also suffers from a large degradation in overall language modeling performance.This downside is even more evident when unlearning multiple sequences in succession, making it impractical for real-world use.Our method not only effectively trains the LMs to forget the target sequence, but also mitigates the potential problems from approximation of the gradients.",
            "score": 0.44991290991137023,
            "section_title": "Previous Work",
            "char_start_offset": 6536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 405
                },
                {
                    "start": 405,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 816
                },
                {
                    "start": 818,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1375
                },
                {
                    "start": 1377,
                    "end": 1568
                },
                {
                    "start": 1568,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1857
                },
                {
                    "start": 1857,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 385,
                    "end": 404,
                    "matchedPaperCorpusId": "207241585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "In this paper, we introduce the Large Scale Knowledge Washing problem, which means unlearning the existing knowledge in the model on a large scale.To address this problem, we draw inspiration from model-editing methods and propose Large Scale Washing (LAW), where we propose a new objective to remove the corresponding knowledge from the MLP layers in the large language models (LLMs), which is considered to store most of the knowledge in the LLMs.Experimental results demonstrate the effectiveness of our method in washing the knowledge in terms of the accuracies when prompted with queries related to the knowledge set, while mostly maintaining the model's reasoning ability.Our work proposes an effective knowledge-washing algorithm and shows the possibility of knowledge-reasoning disentanglement.One limitation is we consider the knowledge set in a specific format, i.e., triplets, whereas washing a large scale of knowledge in pure text where no triplets are available might be more challenging.For future work, we aim to explore washing the knowledge more thoroughly and extend our framework to other more recent LLMs.",
            "score": 0.4492730977561683,
            "section_title": "Conclusion, Limitation, and Future Work",
            "char_start_offset": 28243,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 802
                },
                {
                    "start": 802,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "268230686",
            "title": "Dissecting Language Models: Machine Unlearning via Selective Pruning",
            "text": "In the last two years, Large Language Models (LLMs) have been shown to achieve impressive performance in a wide array of skills. These skills have huge potential to create significant benefits for humanity. However, certain abilities may carry inherent risks, both through wide access to powerful models enabling misuse by bad actors, and from misalignment of the model's goals to its user's. Elimination of high-risk skills from an LLMs repertoire could be a valuable precaution against both misuse and misalignment. Additionally, datasets may contain sensitive user information or copyrighted material (where consent was not obtained or withdrawn) which should be removed. \n\nMachine unlearning is a field that focuses on forgetting ability on one dataset while maintaining ability on a retain dataset. In recent years a wide variety of approaches has sprung up (Nguyen et al., 2022b), (Golatkar et al., 2020). However, there are challenges in applying these methods to LLMs, since a forward or backward pass in an LLM is costly (Bender et al., 2021). \n\nIn this paper, we introduce a method we call selective pruning. We evaluate our method by selectively removing coding ability in LLMs. Coding was chosen due to it being a common and powerful skill with excellent datasets, but with limited risk in research settings. Our proposed method is task-agnostic, requiring only a small dataset representative of the target task, and thus, we anticipate their applicability in the removal of other potentially harmful skills, such as manipulation. Selective pruning demands only a very small amount of additional data and computational resources. \n\nA secondary aim of this research is to gain a deeper understanding of how various abilities are interconnected within LLMs. Our aim is separability rather than sparsity per se, which is why, contrary to most pruning methods (Blalock et al., 2020), we investigated pruning neurons (structured pruning) rather than pruning weights. If capabilities can be separated on the level of neurons, then this can lead to modularity inside models. We find that certain neurons are task-specialized, and removing them dramatically decreases performance on the forget dataset while hardly affecting performance on the retain dataset.",
            "score": 0.44896767706824237,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1973
                },
                {
                    "start": 1974,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 887,
                    "end": 910,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1030,
                    "end": 1051,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1868,
                    "end": 1890,
                    "matchedPaperCorpusId": "212628335"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "This section presents our designed Efficient Unlearning method for LLMs (EUL) which could efficiently and dynamically handle a sequence of deletion requests. The overall process is shown in Figure 1. Formally, for a large language model F (.) that is trained on a dataset D = {(x, y)} where x is textual data and y is the corresponding label, and a deletion request to forget D f = {(x f , y f }, our goal is to learn an updated model F \u2032 (.) that satisfies the following (Kurmanji et al., 2023): \n\nwhere D r = D \u2212 D f = {(x r , y r )} refers to the data we would like to retain, and I(.) is the mutual information. Intuitively, we will update F (.) with F (.) to generate similar output for the data we want to retain while losing all information about making predictions on the data we want to forget.",
            "score": 0.4483924175418692,
            "section_title": "Efficient Unlearning for LLMs",
            "char_start_offset": 7492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "In this paper, we have presented a novel approach to unlearning in large language models (LLMs) through the introduction of anti-samples, facilitated by our method, UNSTAR: Unlearning with Self-Taught Anti-Sample Reasoning. As the landscape of machine learning evolves, the need for effective unlearning mechanisms becomes increasingly critical, particularly in light of privacy concerns, legal compliance, and ethical considerations. Our findings indicate that traditional unlearning techniques often inadvertently compromise the model's broader knowledge, underscoring the necessity for a refined approach. \n\nBy leveraging anti-samples, we enable a targeted unlearning process that not only facilitates the selective removal of specific associations but also preserves related knowledge-a feat not achievable by prior methods. Additionally, we achieve fine-grained targeted unlearning, allowing for the nuanced removal of specific information without disrupting the overall integrity of the model's knowledge base. Our use of misleading rationales as justifications for unlearning further enhances the efficacy of this approach, providing a structured means for LLMs to forget while maintaining contextual integrity. Give 20 different paraphrased questions involving the object where the answer is the same. Strictly output the question only. Format: <Index>. <Question>",
            "score": 0.4483924175418692,
            "section_title": "CONCLUSION",
            "char_start_offset": 27263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Machine unlearning has been used as a solution to address privacy and copyright issues in the text generation process of language models. Notable examples include gradient ascent-based methods (Jang et al., 2023;Yao et al., 2023;Barbulescu and Triantafillou, 2024), preference optimization approaches (Rafailov et al., 2024;Zhang et al., 2024;Jin et al., 2024), and representation learning techniques (Li et al., 2024a;Yao et al., 2024). \n\nHowever, the effectiveness of these methods has not been clearly demonstrated, prompting prior studies to introduce benchmarks in the field of unlearning to assess them. Eldan and Russinovich (2023); Shi et al. (2024); Tian et al. (2024) have aimed to unlearn the knowledge of copyrighted texts (e.g., BBC News and Harry Potter book) in a language model. Li et al. (2024a) have introduced a benchmark dealing with hazardous knowledge in various professional domains (e.g., biosecurity and cybersecurity). Maini et al. (2024); Jin et al. (2024) have proposed benchmarks for unlearning various entities. Specifically, Maini et al. (2024) have created synthetic entity profiles and removed their knowledge from a language model. Jin et al. (2024) have tried to unlearn the knowledge about real-world entities and evaluated the knowledge memorization in various forms of assessment (e.g., cloze test and question answering). However, existing studies remain limited as they have only examined independent knowledge and overlooked the intricate nature of world knowledge. World knowledge is highly complex and interconnected, which means that unlearning the target knowledge requires examining related knowledge carefully. Our research focuses on this aspect, examining and facilitating faithful unlearning. \n\n3 The FAITHUN Benchmark",
            "score": 0.4479524726978156,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 4422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1768
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 212,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 301,
                    "end": 324,
                    "matchedPaperCorpusId": "258959321"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "273022754",
            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
            "text": "Table 1 compares our method with existing unlearning techniques. Test-time unlearning refers to the process of selectively removing a specific concept or knowledge from a trained model. Knowledge unlearning refers to forgetting world knowledge, e.g., \"The capital of France is Paris\". \n\nFor example, Gradient Ascent (Golatkar et al., 2020) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations. \n\nUnlike existing methods, our approach achieves test-time unlearning, knowledge unlearning, and non-hallucination output simultaneously. In other words, our approach addresses the prior limitations and offers a comprehensive solution for selective forgetting. In the context of in-context knowledge unlearning, a pre-trained auto-regressive language model modifies its response to a query q by disregarding specific undesired information u. The response r is generated according to the conditional probability distribution: \n\nwhere \u03b8 denotes the parameters of the model M, and u is the information intended to be forgotten.",
            "score": 0.4468737464213075,
            "section_title": "Comparison of Our Method with Prior Work",
            "char_start_offset": 4786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1466
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 339,
                    "matchedPaperCorpusId": "207863297"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "270620346",
            "title": "Textual Unlearning Gives a False Sense of Unlearning",
            "text": "In recent years, Language Models (LMs) have demonstrated remarkable capabilities using large amounts of training data which may contain private (e.g., Personally Identifiable Information, PII) and copyright-protected content (e.g., paid books) [18].However, studies have shown that LMs possess strong memory capabilities, rendering the sensitive training data susceptible to malicious extraction [6,16].To mitigate the risk of knowledge leakage, various regulations (e.g., General Data Protection Regulation, GDPR [9]) have been enacted, enabling users to request the complete deletion of personal data, regardless of whether the data is stored by or has already been used (\"right to be forgotten\", RTBF) [23].Therefore, \"undoing\" the impact of specific training data on LMs has become crucial for safeguarding data security and individual rights.\n\nMachine unlearning aims to erase the effects of specific training data on a trained model while maintaining the model's utility as much as possible [3].However, traditional unlearning methods [3] are challenging to apply to LMs due to the large-scale parameters and extensive training sets.For instance, removing unlearned data and retraining the model on the remaining data is intuitive but incurs high computational costs for unlearning only a few samples.Therefore, Jang et al. fine-tuned the target model with gradient ascent using only the unlearned data to obtain the unlearned model [14].To maintain the utility of the unlearned model, subsequent works introduce additional regularization terms [28] and employ techniques such as knowledge distillation [26] and interval selection [27].\n\nHowever, we question: whether the unlearning mechanism inadvertently leaks more information about the unlearned data for LMs?Specifically, this mechanism derives the unlearned model from the original model by emphasizing the absence of unlearned data.The gap between the two models may introduce a new risk of knowledge leakage.In other words, despite its well-intentioned design, unlearning may actually increase the exposure of the unlearned texts.",
            "score": 0.4451682342439779,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 403
                },
                {
                    "start": 403,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 847
                },
                {
                    "start": 849,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1307
                },
                {
                    "start": 1307,
                    "end": 1444
                },
                {
                    "start": 1444,
                    "end": 1642
                },
                {
                    "start": 1644,
                    "end": 1769
                },
                {
                    "start": 1769,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 1972
                },
                {
                    "start": 1972,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 399,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "9355614"
                },
                {
                    "start": 997,
                    "end": 1000,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1041,
                    "end": 1044,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93505859375
        },
        {
            "corpus_id": "273186182",
            "title": "Precision Knowledge Editing: Enhancing Safety in Large Language Models",
            "text": "Knowledge editing in large language models (LLMs) has become an essential area of research, particularly for correcting or removing toxic or harmful content without retraining models from scratch. One prominent method, Detoxifying Instance Neuron Modification (DINM), stands out by efficiently identifying and modifying toxic regions in LLMs through single test instances, thereby achieving significant reductions in harmful outputs [3,6]. Unlike other methods that suppress toxic activations (e.g., using SFT and DPO approaches), DINM permanently adjusts parameters related to toxicity, making it highly effective against adversarial prompts [2,1].",
            "score": 0.44460086028835033,
            "section_title": "VI. RELATED WORK A. Knowledge Editing",
            "char_start_offset": 14170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 649
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "268553537"
                },
                {
                    "start": 643,
                    "end": 646,
                    "matchedPaperCorpusId": "265128686"
                },
                {
                    "start": 646,
                    "end": 648,
                    "matchedPaperCorpusId": "265456263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9580078125
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "however, induces the model to generate nonsensical responses, exacerbating one of the most critical issues with generative language models.In this work, we propose an unlearning method for instructionfollowing LLMs, removing targeted high-level information that may have been learned during pretraining without inducing illogical outputs.\n\nConcept Erasure Concept erasure aims to identify and remove specific concepts that may be nonlinearly (Ravfogel et al., 2022b) or linearly (Ravfogel et al., 2022a;Belrose et al., 2023) encoded, applying various transformations to the neural representations.These methods generally approach the problem from a theoretical setting and look to identify and erase a high-level concept that may cause biases, such as gender or racial biases.Our method is more focused on unlearning specific knowledge for potential copyright infringement and privacy issues, solving the problem with the effective use of instruction tuning.",
            "score": 0.44085811671540964,
            "section_title": "Negative Instruction Generation",
            "char_start_offset": 7327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 139,
                    "end": 338
                },
                {
                    "start": 340,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 958
                }
            ],
            "ref_mentions": [
                {
                    "start": 442,
                    "end": 466,
                    "matchedPaperCorpusId": "246411473"
                },
                {
                    "start": 479,
                    "end": 503,
                    "matchedPaperCorpusId": "246411428"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "273403717",
            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "text": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability -- which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability -- can improve the precision and effectiveness of editing and unlearning. We find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states. In particular, localizing edits/unlearning to components associated with the lookup-table mechanism for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models. We also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
            "score": 0.44083909035266045,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "Large Language Models (LLMs) face significant challenges in maintaining privacy, ethics, and compliance, when sensitive or obsolete data must be selectively removed. Retraining these models from scratch is computationally infeasible, necessitating efficient alternatives. As part of the SemEval 2025 Task 4, this work focuses on the application of selective unlearning in LLMs to address this challenge. In this paper, we present our experiments and findings, primarily leveraging global weight modification to achieve an equilibrium between effectiveness of unlearning, knowledge retention, and target model's post-unlearning utility. We also detail the task-specific evaluation mechanism, results, and challenges. Our algorithms have achieved an aggregate score of 0.409 and 0.389 on the test set for 7B and 1B target models, respectively, demonstrating promising results in verifiable LLM unlearning.",
            "score": 0.4396506628741437,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
            "score": 0.4394504705644902,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "270869978",
            "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
            "text": "Instead, it captures more broadly information that we want to remove from the model (regardless of which subset of training data led to acquiring it), in order to prevent it from generating impermissible content.This notion aligns with the recent related notion of Goel et al. (2024) for \"corrective unlearning\".\n\n(Informal) Definition 6. In-Context Learning refers to an emergent capability of language models to generalise to tasks from task descriptions without these tasks present in the training data (Brown et al., 2020;Kossen et al., 2024;Milios et al., 2023).At the time of writing, such a model capability is ubiquitous, albeit some tasks are not perfectly solvable from descriptions alone.\n\nFor example, it may seem natural to remove any mention of what term bomb means from the training set of the model, expecting that when asked to make bombs the model would fail -simply because it possess no knowledge of what the word bomb means.Yet, if a term bomb, for example under a different name as we show in Figure 2, is defined as the a substance with certain properties and the model possess reasoning skills, a recipe for a bomb can be derived by the model, provided there is enough chemical knowledge still available to the model.Importantly, we highlight that even exact unlearning would not stop the model from performing an impermissible act in the example above.That is because even though the model never saw the bomb-defining data, it still possesses all of the knowledge required to construct one.Consequently, no unlearning method can lead to 'better removal' of the impermissible knowledge, compared to the model that never trained on this knowledge in the first place.We therefore argue that if ununlearning poses an issue even in this 'idealized unlearning' setting, this problem can only be exacerbated under imperfect unlearning.\n\nGiven it is hard to reason about knowledge compositionality, i.e. how knowledge interacts with other knowledge available to the model, it is not always obvious what logical inference given (benign) knowledge will enable.As such, attribution of model behaviour to specific knowledge that a user introduced is not always trivial.",
            "score": 0.43878052261712186,
            "section_title": "Nomenclature",
            "char_start_offset": 4467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 312
                },
                {
                    "start": 314,
                    "end": 567
                },
                {
                    "start": 567,
                    "end": 699
                },
                {
                    "start": 701,
                    "end": 945
                },
                {
                    "start": 945,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1377
                },
                {
                    "start": 1377,
                    "end": 1515
                },
                {
                    "start": 1515,
                    "end": 1689
                },
                {
                    "start": 1689,
                    "end": 1853
                },
                {
                    "start": 1855,
                    "end": 2075
                },
                {
                    "start": 2075,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 526,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 526,
                    "end": 546,
                    "matchedPaperCorpusId": "260125327"
                },
                {
                    "start": 546,
                    "end": 566,
                    "matchedPaperCorpusId": "262063582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83984375
        },
        {
            "corpus_id": "276618331",
            "title": "FaithUn: Toward Faithful Forgetting in Language Models by Investigating the Interconnectedness of Knowledge",
            "text": "Large language models (LLMs) are trained on a vast corpus of text, enabling them to achieve outstanding performance across various tasks (Radford et al., 2019;Chowdhery et al., 2023;Gemma et al., 2024). However, LLMs may present privacy risks, as sensitive or private information could unintentionally be included in the large text corpus used for training. Therefore, prior studies have investigated unlearning undesirable knowledge in Figure 1: Faithful Unlearning. FAITHUN proposes three types of datasets to evaluate the faithfulness of unlearning methods (i.e., Paraphrased, Multi-hop, and Same-answer datasets). Each target knowledge to be unlearned is mapped with questions corresponding to these three dataset types for evaluation. language models. To assess unlearning, most studies have examined whether a model successfully forgets the targeted knowledge while retaining irrelevant knowledge (Shi et al., 2024;Li et al., 2024a;Maini et al., 2024;Jin et al., 2024). \n\nHowever, they are limited since they have overlooked the complex and interconnected nature of knowledge, which requires careful investigation of related knowledge. Specifically, these studies have examined only the independent knowledge and failed to evaluate whether an unlearning method effectively erases interconnected knowledge that should be removed, while retaining knowledge that appears relevant but exists in a completely different context. Figure 1 presents an example of faithful unlearning in the real-world knowledge setting. Unlearning methods should also remove paraphrased and multi-hop questions, as they involve knowledge interconnected with the target question being unlearned. Conversely, unlearning methods should retain knowledge of other questions with the same answer as the target, if they actually contain different knowledge despite appearing relevant. \n\nTo address this gap, we first define superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge.",
            "score": 0.4372817157116584,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 159,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 159,
                    "end": 182,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "272704025",
            "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
            "text": "Large Language Models (LLMs) can memorize sensitive information, raising concerns about potential misuse. LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them. We evaluate MEOW on the commonly used unlearn benchmark, ToFU, with Llama2-7B-Chat and Phi-1.5B, and test it on both NLU and NLG tasks. Results demonstrate significant improvement of MEOW in forget quality without substantial loss in model utility. Meanwhile, MEOW does not exhibit significant degradation in NLU or NLG capabilities, and there is even a slight improvement in NLU performance.",
            "score": 0.4359607889095303,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Figure 1: Motivation for multi-hop knowledge unlearning. After Elon Musk (i.e., \"the user\") requests his personal information to be removed from the LLM, existing unlearning methods often succeed in deleting direct, single-hop facts but fail on indirect, multi-hop facts that entail one or a few of the unlearned facts. \n\nas the European Union's General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019) or the California Consumer Privacy Act (CCPA) (Pardau, 2018) in the United States. These regulations mandate the removal of personal or protected information from databases, extending to data embedded within machine learning models. In such cases, model owners must develop mechanisms to safely eliminate specific data while preserving the model's overall functionality. \n\nTo address these concerns, there has been increasing focus on the field of machine unlearning, which involves removing the influence of specific data points from machine learning models (Cao and Yang, 2015). Although the need for this task is critical, erasing the effects of certain data on models with billions of parameters is extremely difficult. The ideal approach is exact unlearning, where models are entirely retrained from scratch after excluding the data points that need to be forgotten. However, this process is computationally intensive and impractical, particularly for LLMs. As a result, research has shifted towards developing faster approximate unlearning techniques. While machine unlearning has been primarily explored in computer vision (Golatkar et al., 2020a,b;Bourtoule et al., 2021;Kurmanji et al., 2023;Fan et al., 2024), its prominence is now expanding in NLP due to privacy concerns related to LLMs (Nasr et al., 2023;Carlini et al., 2024). \n\nRecently, several machine unlearning methods have been introduced in NLP (Jang et al., 2023;Lee et al., 2024;Zhang et al., 2024c), with the goal of reversing gradients to prevent LLMs from generating certain sensitive token sequences.",
            "score": 0.4353487993929209,
            "section_title": "Unlearn Request",
            "char_start_offset": 619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 57,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1990
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 412,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 972,
                    "end": 992,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1569,
                    "end": 1592,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1592,
                    "end": 1614,
                    "matchedPaperCorpusId": "270619676"
                },
                {
                    "start": 1614,
                    "end": 1631,
                    "matchedPaperCorpusId": "264305818"
                },
                {
                    "start": 1731,
                    "end": 1752,
                    "matchedPaperCorpusId": "268357903"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "270878324",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "text": "Forgetting is a crucial brain function that eliminates unnecessary information to maintain neural system integrity (Small, 2021;Farrell, 2022). In parallel, Large Language Models (LLMs) (Ouyang et al., 2022;Zhao et al., 2023;OpenAI, 2023) inevitably incorporate sensitive data during training, which is not essential for their functionality (Yao et al., 2023a(Yao et al., , 2024;;Li et al., 2024b;Zhang et al., 2024a;Liu et al., 2024b). Therefore, removing sensitive knowledge from LLMs is imperative for ensuring the safety and integrity of these systems. The most straightforward solution involves removing such data from pre-training corpora and retraining LLMs, although this method is expensive and time-consuming. Another approach, alignment Figure 1: Current unlearning paradigms unlearn all related knowledge of \"J.K. Rowling\". Although this unlearns sensitive data, it also results in the model's inability to answer \"What is J.K. Rowling's most representative work?\" which it could answer before unlearning. methods like reinforcement learning from human feedback (RLHF) (Bai et al., 2022), is computationally expensive and requires extensive, high-quality human feedback (Casper et al., 2023). \n\nConsequently, recent research has primarily focused on knowledge unlearning (Chen and Yang, 2023;Eldan and Russinovich, 2023;Si et al., 2023;Liu, 2024;Li et al., 2024a;Huang et al., 2024;Zhao et al., 2024b;Sha et al., 2024), which facilitates efficient, post-training forgetting in models. However, current evaluation paradigms are limited, typically failing to consider the extent of forgetting, instead simply unlearning all related knowledge regarding factual instances. Psychological research (ROEDI-GER III et al., 2010;Storm, 2011) emphasizes that forgetting is a natural and necessary process that helps focus on essential knowledge.",
            "score": 0.43401286168847003,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "We present the results of factual knowledge unlearning across various methods in Table 3. Factual knowledge is probed using fill-in-the-blank cloze statements like \"Paris is the capital of [MASK]\", where the language model predicts the masked token. Although this is also a token sequence, the unlearning process differs as we focus on removing information about the answer token(s) in the context, preventing the model from generating the correct answer, \"France\". This approach may lead to hallucinations when dealing with actual factual knowledge, where editing might be more suitable. However, we argue that it relates to unlearning specific parts of information, such as the names of copyrighted characters in multiple languages. We measure the PPL of the entire answer sentence, as measuring PPL only on the answer token(s) can result in disproportionately high values. Our method, similar to unlearning token sequences, generally outperforms other methods across various metrics, showcasing its effectiveness. \n\nIt is worth noting that English factual knowledge is hardly removed from XGLM-564M. We believe that techniques like weighted random sampling of languages, which we did not explore in this study, may help reduce memorization.",
            "score": 0.4329309617733397,
            "section_title": "Factual Knowledge Unlearning",
            "char_start_offset": 15516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1243
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "Given the prevalence of large language models (LLMs) and the prohibitive cost of training these models from scratch, dynamically forgetting specific knowledge e.g., private or proprietary, without retraining the model has become an important capability. This paper proposes a novel method to achieve this objective called UNLEARN. The approach builds upon subspace methods to identify and specifically target the removal of knowledge without adversely affecting other knowledge in the LLM. Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art. A dual method called LEARN is also proposed for targeted knowledge addition. Results show LEARN can match the fine-tuning accuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar tasks.",
            "score": 0.4310919816026011,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "278338982",
            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
            "text": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour. Experimental results demonstrate that DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level uncertainty that is much higher than obfuscation on probing questions.",
            "score": 0.4282005485261439,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "This works focuses exclusively on unlearning methods for safety that remove hazardous knowledge (e.g. bioweapons) from large language models, as introduced by Li et al. (2024). In practice, unlearning relies on forget and retain sets. The first contains information relevant to the domain to be unlearned (e.g. enhanced pandemic pathogens) while the second includes neighboring information that should be preserved (e.g. general biology). In this work, we use the datasets included in WMDP benchmark for biology and cybersecurity (Li et al., 2024). Our evaluation is designed to assess whether existing unlearning methods effectively remove hazardous knowledge or merely make it more difficult to access, similarly to safety training.",
            "score": 0.42724644155389424,
            "section_title": "Experimental Setup",
            "char_start_offset": 6482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 734
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "273502327",
            "title": "When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?",
            "text": "L ARGE language models (LLMs) have transformed the field of natural language generation, playing a vital role in advancing technology across various industries [1], [2]. Models like the GPT series, which are large-scale neural networks, have learned the complexities of language-both semantics and syntax-by training on vast datasets, enabling them to excel in a wide range of language tasks [3], [4]. Currently, the training of LLMs typically follows a 'pretraining and fine-tuning' paradigm [5], [6]. However, both stages may inadvertently result in the model learning sensitive information or harmful content [7], [8], [9], which raises concerns about privacy violations, copyright infringement, or the generation of harmful content. \n\nTo address the privacy or harmful content concerns, machine unlearning has been proposed as a method to remove the Tianqing Zhu is the corresponding author Shang Wang and Dayong Ye are from School of Computer Science, University of Technology Sydney; Tianqing Zhu and Wanlei Zhou are from Faculty of Data Science, City University of Macau influence of specific data from a model without requiring full retraining [13]. Unlike traditional unlearning in classification models, fully eliminating the contribution of target knowledge in LLM faces two challenges. The key challenge is the difficulty of modifying the parameters in LLMs due to the huge volumes of parameters. Second, the output space of LLMs is much larger than the classification space, which hinders the verification of LLM unlearning. The two challenges block the implementation of exact LLM unlearning. \n\nTherefore, approximate unlearning for LLM has become a research hotspot [11], [12], [14]. It hypothesizes that the unlearned LLM differs from the regular model in the target knowledge and behaves similarly in the remaining knowledge. Although model owners can not remove the target knowledge in LLM, they can hide it or map it to irrelevant content, such as poisoning the target, shifting the logits and modifying the input. However, these LLM unlearning schemes have inherent weaknesses.",
            "score": 0.4271230373676205,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 395,
                    "matchedPaperCorpusId": "267897394"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "257039062"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "267658120"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "259203671"
                },
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "248840162"
                },
                {
                    "start": 622,
                    "end": 625,
                    "matchedPaperCorpusId": "249240113"
                },
                {
                    "start": 1687,
                    "end": 1691,
                    "matchedPaperCorpusId": "263834631"
                },
                {
                    "start": 1693,
                    "end": 1697,
                    "matchedPaperCorpusId": "263608437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Existing research on machine unlearning mainly focuses on computer vision and other fields, e.g., recommender systems and federated learning. However, knowledge unlearning, specifically in the context of language models, has received relatively less attention. Due to the prohibitive retraining over-head and the enormous size of model parameters, existing knowledge unlearning methods primarily rely on the fine-tuning approach. To provide a comprehensive view, we also briefly introduce the methods in the pre-processing and post-processing stages that are relevant to achieving unlearning in language models.",
            "score": 0.4268795056642635,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 5723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 611
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "To demonstrate the effectiveness of our method, we compare it with various knowledge editing and unlearning methods.The baselines for model-editing include: (1) FT: Simply finetune the model on the factual sentences formed from the triplets in E eos in Eq.( 6) (2) MEMIT [Meng et al., 2023]: This state-of-the-art method can edit multiple layers of the model to perform thousands of edits simultaneously.(3) ME-FT (Model-Editing-FT) [Gangadhar and Stratos, 2024], which finetunes the model with only the loss on the span of o i in each sentence formed from (s i , r i , o i ) \u2208 E w .Irrelevant sentences are constructed as augmentations during training.For the knowledge unlearning category, the baselines are: (1) FT-UL (Finetune-Unlearning): Finetune the model on the sentences formed from the triplets E w in Eq.( 5) but with the reverse (i.e.negative) next-word-prediction loss function;\n\n(2) WOH (Who is Harry Potter) [Eldan and Russinovich, 2023]: First train a reinforced model on the unlearning dataset, then update the target model to diverge from the reinforced model, based on the assumption that the reinforced model can better retain the unlearning dataset; (3) SeUL (Selective Forgetting) [Wang et al., 2024]: Designates specific spans in the training data and uses a reversed next-word-prediction loss function on these spans for training.\n\nConsistent with previous studies [Meng et al., 2023, Gangadhar andStratos, 2024], we employ GPT2-XL (1.5B parameters) and GPT-J-6B (6B parameters) as the backbone models for knowledge washing.The datasets used in our experiments are: (1) zsRE [Levy et al., 2017]: A questionanswering dataset with 19,086 facts.",
            "score": 0.4268795056642635,
            "section_title": "Experimental Setup",
            "char_start_offset": 18558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 891
                },
                {
                    "start": 893,
                    "end": 1354
                },
                {
                    "start": 1356,
                    "end": 1548
                },
                {
                    "start": 1548,
                    "end": 1666
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 290,
                    "matchedPaperCorpusId": "252873467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "273228619",
            "title": "Dissecting Fine-Tuning Unlearning in Large Language Models",
            "text": "Fine-tuning-based unlearning methods prevail for erasing targeted harmful, sensitive, or copyrighted information within large language models while preserving overall capabilities. However, the true effectiveness of the methods is unclear. In this paper, we delve into the limitations of fine-tuning-based unlearning through activation patching and parameter restoration experiments. Our findings reveal that these methods alter the model\u2019s knowledge retrieval process, rather than genuinely erasing the problematic knowledge embedded in the model parameters. Furthermore, behavioral tests demonstrate that the unlearning mechanisms inevitably impact the global behavior of the models, affecting unrelated knowledge or capabilities. Our work advocates the development of more resilient unlearning techniques for truly erasing knowledge.",
            "score": 0.4268795056642635,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "270440344",
            "title": "Towards Effective Evaluations and Comparisons for LLM Unlearning Methods",
            "text": "Large language models (LLMs), like Llama (Touvron et al., 2023a;b) and GPT (Brown et al., 2020;Achiam et al., 2023), have exhibited remarkable proficiency in general-purpose language generation and understanding (Azerbayev et al., 2023;Roziere et al., 2023;Wu et al., 2023;Thirunavukarasu et al., 2023;Zhou et al., 2024;Huang et al., 2024). These advancements are credited to the development of Transformer-based architectures (Vaswani et al., 2017) with billions of parameters and to the extensive pre-training on web-sourced corpora with trillions of tokens (Brown et al., 2020). However, on the other side, scaling up models aggravates the risk of memorizing effects (Arpit et al., 2017) and sourcing from the web makes LLMs inherent its inaccuracies and biases (Liu et al., 2023a). It raises the invoking concerns for LLM privacy and fidelity, posing a long array of undesirable LLM behaviors sourced from training corpora (Liu et al., 2023a), including copyright (Yao et al., 2023a), fairness (Gallegos et al., 2023), and toxicity (Liu et al., 2023b), among many others. \n\nHow to Erase Undesirable Data Memorization in LLMs? Machine unlearning (Bourtoule et al., 2021;Zhu et al., 2024) offers a general solution. In the context of LLMs, the primary goal of unlearning is to precisely remove the parameterized knowledge related to unlearning targets meanwhile maintaining model performance for non-targets (Liu et al., 2024). The unlearning targets within LLMs are typically characterized by an unlearning set, denoted as D u = {s u = [x, y u ]} nu , and we need to develop unlearning methods upon D u that meet the goals of LLM unlearning.",
            "score": 0.42341867152402,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1644
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 302,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 302,
                    "end": 320,
                    "matchedPaperCorpusId": "273707060"
                },
                {
                    "start": 670,
                    "end": 690,
                    "matchedPaperCorpusId": "11455421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "278481378",
            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
            "text": "Large Language Models (LLMs) have advanced rapidly, becoming widely applicable in various settings (Brown et al., 2020;OpenAI, 2023;Dubey et al., 2024). However, their increasing capabilities raise significant privacy risks, especially for individuals whose sensitive data may have been included in training. This information can become embedded within the model, making it susceptible to unintended exposure through memorization, adversarial exploits, membership inference (MIA), and model inversion attacks (Yao et al., 2024b). \n\nTo address these concerns, regulatory frameworks such as the General Data Protection Reg-* Correspondence to: stvasilev@ebay.com ulation (GDPR) have been established to protect individual privacy and enforce the right to be forgotten. Given that LLMs are subject to such regulations, the machine learning research community has increasingly focused on the emerging field of Machine Unlearning for LLMs (Wang et al., 2025a;Liu et al., 2024b;Jang et al., 2023), which aims to develop methods for selectively removing specific knowledge from models. This includes erasing sensitive information (Wang et al., 2025a;Patil et al., 2023), forgetting entire entities or facts (Ma et al., 2025), and removing harmful or biased information (Lu et al., 2022). \n\nIn the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency.",
            "score": 0.423386173913821,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 934,
                    "end": 954,
                    "matchedPaperCorpusId": "276617972"
                },
                {
                    "start": 972,
                    "end": 990,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1123,
                    "end": 1143,
                    "matchedPaperCorpusId": "276617972"
                },
                {
                    "start": 1143,
                    "end": 1162,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1200,
                    "end": 1217,
                    "matchedPaperCorpusId": "270703237"
                },
                {
                    "start": 1262,
                    "end": 1279,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "263141263",
            "title": "Forgetting Private Textual Sequences in Language Models Via Leave-One-Out Ensemble",
            "text": "Recent research has shown that language models have a tendency to memorize rare or unique token sequences in the training corpus. After deploying a model, practitioners might be asked to delete any personal information from the model by individuals\u2019 requests. Re-training the underlying model every time individuals would like to make these requests is computationally expensive. We employ a teacher-student framework and propose a novel leave-one-out ensemble method to unlearn the targeted textual sequences from the model. In our approach, multiple teachers are trained on disjoint sets; for each targeted sequence to be removed, we exclude the teacher trained on the set containing this sequence and aggregate the predictions from remaining teachers to provide supervision during fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the proposed method achieves superior privacy-utility trade-offs than other counterparts.",
            "score": 0.4227106823263234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "276617566",
            "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
            "text": "LLM unlearning refers to techniques that selectively remove specific behaviors or knowledge from a pre-trained language model while maintaining its overall functionality (Yao et al., 2023). With the proliferation of LLMs, unlearning has gained significant attention due to its broad applications in safety alignment, privacy protection, and copyright compliance (Eldan and Russinovich, 2023;Liu et al., 2024c;Jia et al., 2024b). The evaluation and auditing of LLM unlearning spans from basic verbatim memorization to deeper knowledge memorization (Shi et al., 2024), with this work focusing on the latter. As depicted in Figure 2, LLM unlearning operates as a targeted intervention within the model's knowledge representation framework. Its core objective is the selective removal of specific information while preserving the model's broader knowledge base (e.g, on retain set). This study focuses on the knowledge unlearning auditing that assesses unlearned models' behaviors through comprehensive audit cases. Given access to both forget and retain corpora, we generate a holistic set of test questions with reference answers to thoroughly evaluate whether an unlearned model exhibits any residual knowledge memorization.",
            "score": 0.4227106823263234,
            "section_title": "LLM Unlearning",
            "char_start_offset": 4545,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1223
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 427,
                    "matchedPaperCorpusId": "269448906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "274437750",
            "title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning",
            "text": "Cao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs. \n\nThese three areas-knowledge graph embeddings, meta-learning, and machine unlearning-collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning.",
            "score": 0.4227106823263234,
            "section_title": "Related Work",
            "char_start_offset": 7950,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 756
                },
                {
                    "start": 759,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 13,
                    "end": 17,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 175,
                    "end": 179,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 323,
                    "end": 327,
                    "matchedPaperCorpusId": "251025393"
                },
                {
                    "start": 547,
                    "end": 551,
                    "matchedPaperCorpusId": "267897394"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "273098052",
            "title": "Mitigating Memorization In Language Models",
            "text": "Language models (LMs) can\"memorize\"information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods are effective at curbing memorization, but overly expensive, especially for retaining higher accuracies; and unlearning-based methods are faster and more effective, allowing for the precise localization and removal of memorized information from LM weights prior to inference. We show, in particular, that our proposed unlearning method BalancedSubnet outperforms other mitigation methods at removing memorized information while preserving performance on target tasks.",
            "score": 0.42261115820995176,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "273532584",
            "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
            "text": "Large Language Models (LLMs) (Touvron et al., 2023;Jiang et al., 2023) are trained on vast corpora of data that contain private, unethical, or unwanted information, leading to growing concerns. Machine unlearning (MU) methods have been developed to remove such unwanted data without expensive retraining from scratch. For instance, MU has been applied for the LLMs to mitigate issues related to toxicity (Lu et al., 2022), copyright and privacy concerns (Jang et al., 2022;Eldan and Russinovich, 2023;Wu et al., 2023) and fairness (Yu et al., 2023). Additionally, such topics as model editing (Ilharco et al., 2022;Zhang et al., 2023), prevention of hallucinations (Yao et al., 2023), and sensitive knowledge exposure (Barrett et al., 2023) have also motivated the development of MU techniques. A: Jaime Vasquez, a true crime author, poses for a portrait.",
            "score": 0.4220090186106586,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 855
                }
            ],
            "ref_mentions": [
                {
                    "start": 404,
                    "end": 421,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 531,
                    "end": 548,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 718,
                    "end": 740,
                    "matchedPaperCorpusId": "261276945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "264439170",
            "title": "Is Probing All You Need? Indicator Tasks as an Alternative to Probing Embedding Spaces",
            "text": "In our second test case, we aim for representations that are ignorant of morphological features, such as tense, person etc. Using such representations can make the treatment of morphologically rich languages with many inflections per lemma simplified to a great extent, with models for downstream tasks not needing to worry about unseen inflections (Gong et al., 2018;Czarnowska et al., 2019). As success criteria for removing morphological knowledge from word representations, we utilize probes and a novel indicator. \n\nTo this end, we extend an existing concept erasure method, Iterative Nullspace Projection (INLP, Ravfogel et al., 2020), to accommodate the erasure of multiple properties. We propose two possible, and novel, extensions and test these alternatives for their efficacy using standard probes, and using a novel indicator task that we design, which indicates on the presence of morphological and lexical semantic information simultaneously.",
            "score": 0.4214895689309367,
            "section_title": "Case Study 2: Morphological Properties",
            "char_start_offset": 19703,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 956
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 368,
                    "matchedPaperCorpusId": "52301591"
                },
                {
                    "start": 368,
                    "end": 392,
                    "matchedPaperCorpusId": "202539170"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Large language Models (LLMs) have been widely used in various applications, generating text responses that attempt to create the equivalent of human conversations OpenAI et al. (2024). These models leverage vast scientific literature to facilitate and accelerate interdisciplinary research Taylor et al. (2022) while drawing upon large datasets of human-generated content to provide professional advice. However, in many cases, such data is a double-edged sword. Including personal information or sensitive scientific knowledge can be beneficial or, conversely, harmful. For instance, Soice et al. (2023) discusses how LLMs, when used by non-experts, can enable the creation of biological agents, posing both potential benefits and significant risks. \n\nIn response to these concerns, machine unlearning has emerged as a promising research area focused on selectively removing specific data points or information from a trained model. This approach helps mitigate the misuse of sensitive data and addresses privacy concerns. Existing solutions, such as Sharded, Isolated, Sliced, and Aggregated (SISA) training Bourtoule et al. (2020), primarily involve partitioning the training data into disjoint shards and retraining models on these individual shards. Although effective in certain scenarios, these methods are often time-consuming, resourceintensive, and lack scalability when applied to large models like LLMs. Moreover, traditional approaches typically require specialized data structures or full retraining, making them impractical for dynamic or complex tasks. Given these limitations, there is an increasing demand for zero-shot unlearning methods, which aim to remove specific information without retraining or specialized data structures. Unlike traditional unlearning techniques that rely on retraining portions of the model, zero-shot unlearning seeks to directly eliminate the influence of specific data points or pieces of information from the model's learned representation-without additional computational steps or parameter adjustments. More-over, zero-shot unlearning is inherently more scalable, especially for large models like LLMs, as it avoids the inefficiencies associated with data partitioning and retraining. \n\nOur approach builds upon using discrete representations as the latent space for unlearning.",
            "score": 0.4211441335965948,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2236
                },
                {
                    "start": 2239,
                    "end": 2330
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "271213616",
            "title": "Hiding and Recovering Knowledge in Text-to-Image Diffusion Models via Learnable Prompts",
            "text": "As demonstrated in prior works (Gandikota et al., 2023;Kumari et al., 2023;Orgad et al., 2023;Zhang et al., 2023), fine-tuning the foundation model to enforce the output associated with the to-be-erased concept to be close to the output with a neutral or null concept, can effectively remove the unwanted concept.However, because the parameter space is shared among all concepts, semantically related concepts often activate highly overlapping sets of parameters.Therefore, the erasure effect commonly comes with a significant drop in the model's capability in other concepts.As discussed later in Section 5.5, even the most advanced erasure methods can negatively impact semantically related concepts, as evidenced by the instability of the alignment between visual and textual features in neutral or retained concepts during the fine-tuning process.\n\nTo address this challenge, we draw inspiration from the success of parameter prompt-tuning approaches in Natural Language Processing (Li & Liang, 2021;Lester et al., 2021;Pfeiffer et al., 2020) and introduce a learnable parameter prompt as additional memory to the cross-attention layers of the foundation model.Intuitively, our method involves two alternating processes called knowledge transfer and knowledge removal, with the additional prompt acting as a buffer between them.\n\nDuring the knowledge transfer stage, the prompt is trained to effectively generate unwanted concepts, aiming to resemble the textual embedding regarding these concepts from the model's perspective.This stage helps to reduce the dependency on ccurrent textual inputs for generating these concepts, as the knowledge of the concepts has been transferred to the prompt.In the knowledge removal stage, the additional prompt assists in eliminating unwanted concepts, allowing the models's parameters to undergo fine-tuning more stably and with minimal negative impact on other concepts.",
            "score": 0.4186034181046844,
            "section_title": "Motivation",
            "char_start_offset": 12556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1332
                },
                {
                    "start": 1334,
                    "end": 1531
                },
                {
                    "start": 1531,
                    "end": 1699
                },
                {
                    "start": 1699,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 75,
                    "matchedPaperCorpusId": "257687839"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96435546875
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Jang et al. ( 2023) introduced knowledge unlearning, aimed at preventing language models from generating specific token sequences. They proposed a straightforward method by inverting the original training objective of minimizing the negative loglikelihood of the token sequences. To maintain the performance of the remaining knowledge, various works (Wang et al., 2023;Chen and Yang, 2023;Lee et al., 2024) employed the Kullback-Leibler (KL) divergence loss, minimizing the distributional differences between the original and unlearned models on the retained data. Our approach builds on these methods but differs in its focus. While the previous works targeted monolingual models like DistilBERT (Sanh et al., 2019) and T5 (Raffel et al., 2020), we extend the unlearning process to a multilingual context.",
            "score": 0.4185848365485682,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 20077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 806
                }
            ],
            "ref_mentions": [
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 369,
                    "end": 389,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 697,
                    "end": 716,
                    "matchedPaperCorpusId": "203626972"
                },
                {
                    "start": 724,
                    "end": 745,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "273798735",
            "title": "RESTOR: Knowledge Recovery in Machine Unlearning",
            "text": "Large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023) have garnered significant attention for their remarkable ability to generate human-like text. However, their training on extensive web-scraped datasets, exposes them to a range of security and privacy risks, from memorizing private or copyrighted content to reproducing incorrect or harmful information in the training data (Pan et al., 2020;Wei et al., 2024;Carlini et al., 2021;Huang et al., 2022). To address these concerns and to further the development of trustworthy language models, researchers have proposed unlearning techniques. These methods aim to modify an already trained model to unlearn a set of datapoints, resulting in a model that is similar to one which never included the datapoints in its training set (Blanco-Justicia et al., 2024). \n\nSince LLMs are pre-trained or fine-tuned on vast datasets, retraining from scratch after excluding target datapoints is computationally infeasible for unlearning. Consequently, the community has explored efficient methods to simulate this procedure. These approaches are typically evaluated by assessing their effectiveness in forgetting content within unlearning documents, e.g., copyrighted concepts, toxic content, or forgetting factual knowledge about concepts appeared within unlearning dataset (Maini et al., 2024;Jin et al., 2024;Zhang et al., 2024;Jang et al., 2022;Kumar et al., 2022). \n\nIn this work, we consider an alternate prerequisite for unlearning: if a model is no longer influenced by the unlearning set, it should retain the same knowledge and capabilities as before encountering documents in this set. For instance, imagine a model checkpoint that has already acquired correct knowledge about certain concepts. As pretraining continues on other datapoints -some containing incorrect facts, private information, or malicious documents related to those conceptsthe model may eventually exhibit degraded performance on tasks involving these concepts. In such scenarios, unlearning would help eliminating the influence of these datapoints, since retraining the model from scratch is computationally infeasible.",
            "score": 0.4185848365485682,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 396,
                    "end": 414,
                    "matchedPaperCorpusId": "220938739"
                },
                {
                    "start": 414,
                    "end": 431,
                    "matchedPaperCorpusId": "259342528"
                },
                {
                    "start": 431,
                    "end": 452,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain.",
            "score": 0.4185848365485682,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "Our preliminary experiments show that current unlearning methods struggle to forget multi-hop questions when one of the intermediate hops is removed. For example, ground-truth token sequences could still be extracted from 90.0% of multi-hop questions after unlearning in the Llama-3.1-8B-Instruct model using NPO (Zhang et al., 2024c), despite the original extraction success rate of 98.1% before unlearning. \n\nTo achieve more faithful knowledge unlearning, we propose a simple yet effective approach, MUNCH, which significantly outperforms existing approaches in unlearning multi-hop knowledge. MUNCH first decomposes multi-hop questions into successive subquestions, generates provisional answers, and employs the uncertainty of the unlearned model on the generated outputs as a measure to determine whether to provide a rejective response (e.g., \"I don't know.\") or keep it as is. Our method capitalizes on the high uncertainty of the unlearned model when dealing with direct, single-hop facts -an effect stemming from the reversed language modeling objective. By inspecting the decomposed multi-hop questions, we can more easily distinguish between information that needs to be forgotten and information that should be retained. Empirical results on the modified MQuAKE dataset confirm the efficacy of our approach, and we emphasize that MUNCH is highly practical, requiring no additional training and integrating seamlessly with existing unlearning techniques. To our knowledge, this is the first work to explore the unlearning of multi-hop knowledge.",
            "score": 0.4182461094681843,
            "section_title": "Unlearn Request",
            "char_start_offset": 4274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1556
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 334,
                    "matchedPaperCorpusId": "269009619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99072265625
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",
            "score": 0.41675140673498945,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "276408369",
            "title": "ReLearn: Unlearning via Learning for Large Language Models",
            "text": "The widespread use of large-scale AI training datasets, which often contain unauthorized private and copyrighted information (Carlini et al., 2021;Chen, 2024;Lucchi, 2024), poses significant ethical and legal challenges. Recent developments, such as the New York Times lawsuit against Ope-nAI (NPR, 2025) over unauthorized data usage, have further highlighted these challenges. To comply with stringent privacy and copyright regulations, it is crucial to develop techniques capable of removing unauthorized knowledge from the parameters of large language models (LLMs). Given the prohibitive computational cost of retraining from scratch, LLM unlearning serves as a practical alternative. \n\nHowever, existing unlearning methods, such as Gradient Ascent (GA) (Jang et al., 2023) and Negative Preference Optimization (NPO) (Zhang et al., 2024a), raise a significant challenge: they often degrade the fundamental language generation capabilities of models, producing repetitive or incoherent outputs that resemble the linguistic impairments observed in Alzheimer's patients (Fraser et al., 2016). As illustrated in Figure 1, the core issue with GA and NPO stems from the \"probability seesaw effect\" caused by reverse optimization. This indiscriminate suppression of target token probabilities results in linguistically degraded text generation, which manifests in two ways: (1) vocabulary collapse (reduced fluency) and (2) contextual incoherence (diminished relevance). Additionally, current evaluation metrics for unlearning focus narrowly on specific contextual forgetting, failing to capture these broader limitations in fluency and relevance. \n\nTo address these issues, we introduce ReLearn, a novel unlearning pipeline that leverages data augmentation and positive optimization. ReLearn over-writes sensitive information with new authorized knowledge by training the model on augmented data. This preserves the model's linguistic ability while forgetting target knowledge, akin to human memory updating (Lee et al., 2017). Additionally, we introduce a comprehensive evaluation framework comprising three metrics: Knowledge Forgetting Rate (KFR), Knowledge Retention Rate (KRR), and Linguistic Score (LS).",
            "score": 0.4161756584909319,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 158,
                    "matchedPaperCorpusId": "265659278"
                },
                {
                    "start": 758,
                    "end": 777,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 821,
                    "end": 842,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 1071,
                    "end": 1092,
                    "matchedPaperCorpusId": "7357141"
                },
                {
                    "start": 2006,
                    "end": 2024,
                    "matchedPaperCorpusId": "3868529"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "263671765",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "text": "In this work, we hypothesize that any piece of relational knowledge expressed by a language model is encoded by a limited subset of its parameters. We search for these parameters by identifying sparse subnetworks that, when removed, suppress the model's ability to express the knowledge of interest while not affecting other abilities of the model. As the model cannot express target knowledge without these subnetworks, we refer to them as knowledgecritical. In Figure 1, we illustrate this conceptwhen the weights marked with a red cross are removed from the original network, the expression of the triplet (cafe, IsA, restaurant) is suppressed, whereas other triplets are not. \n\nTo discover knowledge-critical subnetworks, we propose training differentiable masks over weights or neurons of the original pretrained model, such that the mask can identify and remove a knowledgecritical subnetwork for the targeted knowledge graph. Specifically, we train the mask to: (1) suppress the expression of the target knowledge triplets, (2) maintain the ability to express generic relational knowledge and language, and (3) remove only a minimal subset of weights. After training, the remaining pruned model can no longer express the target knowledge, but maintains its performance on other behaviors, thereby identifying the knowledge-critical subnetwork as the masked portion of the original model. \n\nOur results -across multiple target knowledge graphs (constructed from WordNet and Concept-Net) and LLMs at multiple scales (from the family of GPT2 models) -show that weight masking consistently identifies sparse subnetworks (an average sparsity of \u223c98.6%) that satisfy our objectives. When these subnetworks are removed, the remaining model's perplexity on the target knowledge associated with the subnetwork largely increases (an average relative perplexity increase of 253% -5589% for different GPT2 models), indicat-ing that the expression of the target knowledge is successfully suppressed. However, the remaining network's ability to model generic relational knowledge and natural language negligibly changes. Finally, in a study on CommonsenseQA, we show that once these subnetworks are removed, models finetuned using parameter-efficient methods struggle with questions that require the knowledge encoded by the removed subnetworks.",
            "score": 0.4160177519664524,
            "section_title": "Model Behavior Objectives",
            "char_start_offset": 2668,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "In light of emerging regulations, the removal of user-specified data from large language models (LLMs) is increasingly recognized as a critical component of responsible and ethical AI development. This study further investigates the application of machine unlearning techniques to LLMs. The datasets utilized for evaluation are publicly available and implemented within the intended use. \n\nWe hope this study to advance research and literature on machine unlearning for LLMs.",
            "score": 0.41450130713776434,
            "section_title": "ETHICAL STATEMENT",
            "char_start_offset": 36312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 475
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98681640625
        },
        {
            "corpus_id": "277940206",
            "title": "A mean teacher algorithm for unlearning of language models",
            "text": "In the recent years, Large Language Models (LLMs) have reached unprecedented capabilities that are achieved through training on massive datasets. Often comprising hundreds of billions of tokens, these datasets are collected from diverse internet sources. This data-intensive approach raises significant ethical concerns as some protected user and copyright data may be compromised. The General Data Protection Regulation (GDPR) establishes fundamental rights for individuals, including the \"right to be forgotten,\" which grants the deletion of personal data upon request. Concurrently, the inclusion of copyrighted content in training datasets has initiated legal quarrels (Doe 1 v. GitHub, 2022;Tremblay v. OpenAI, 2023). These regulatory and legal actions have sparked interest in methods that would reduce the influence of selective training data on predictions of large pretrained models, a process known as machine unlearning (Bourtoule et al., 2021;Ginart et al., 2019). \n\nIn unlearning of language models, we are aimed at modifying the model weights that would make them behave as if it was never trained on particular selected data, which we refer to as forget set (Yao et al., 2023;Eldan & Russinovich, 2023;Cooper et al., 2024;Shi et al., 2024). For example, we want to suppress certain outputs, like being able to reproduce some memorized training instances word-by-word with completion requests, or answering questions about their content. Following (Shi et al., 2024), we refer to the completion ability as verbatim memorization, and the ability to answer questions as knowledge memorization. We therefore want to reduce such memorization, ideally, without decreasing the model's utility. \n\nThere have been a lot of interest in the recent literature towards language model unlearning, with various methods proposed. A generally accepted approach consists of finetuning a model by optimizing an objective specifically designed to discourage memorization.",
            "score": 0.41450130713776434,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 931,
                    "end": 955,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "195886255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information. While this is effective for tasks like token classification, it may struggle with the more complex context and semantics in LLMs, underscoring the need for scalable, adaptable unlearning techniques tailored to these models.",
            "score": 0.41450130713776434,
            "section_title": "RELATED WORK",
            "char_start_offset": 6035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "274141790",
            "title": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning Methods",
            "text": "Large language model unlearning aims to remove harmful information that LLMs have learnt to prevent their use for malicious purposes. LLMU and RMU have been proposed as two methods for LLM unlearning, achieving impressive results on unlearning benchmarks. We study in detail the impact of unlearning on LLM performance metrics using the WMDP dataset as well as a new biology dataset we create. We show that unlearning has a notable impact on general model capabilities, with the performance degradation being more significant in general for LLMU. We further test the robustness of the two methods and find that doing 5-shot prompting or rephrasing the question in simple ways can lead to an over ten-fold increase in accuracy on unlearning benchmarks. Finally, we show that training on unrelated data can almost completely recover pre-unlearning performance, demonstrating that these methods fail at truly unlearning. Our methodology serves as an evaluation framework for LLM unlearning methods. The code is available at: https://github.com/JaiDoshi/Knowledge-Erasure.",
            "score": 0.41376441228817845,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "270559985",
            "title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective Unlearning in LLMs",
            "text": "Machine Unlearning: The notion of machine unlearning was first introduced by Cao & Yang (2015) motivated by the right-to-be-forgotten and focused on removing specific training samples.Since then, there have been a number of works that have focused on removing specific training data samples via unlearning (Bourtoule et al., 2021;Graves et al., 2020;Izzo et al., 2021;Ginart et al., 2019;Golatkar et al., 2020a;b;Thudi et al., 2021).\n\nUnlearning for LLMs has started to gain recent attention resulting in works in data unlearning (Jang et al., 2023;Wang et al., 2023;Kassem et al., 2023;Maini et al., 2024;Zhang et al., 2024), concept unlearning (Eldan & Russinovich, 2023), behavior unlearning (Lu et al., 2022;Yao et al., 2024;Liu et al., 2024b), knowledge unlearning (Li et al., 2024).Recent surveys have shown additional methods where unlearning has been applied (Nguyen et al., 2022;Xu et al., 2023a;Liu et al., 2024a).Prior works have mainly focused on designing unlearning methods, evaluation metrics, and benchmarks.However, they do not take into account attributes of data used for unlearning.Our proposed SPUNGE leverages data attributes to fortify the performance of any unlearning method.\n\nToxicity Reduction in LLMs: Early works in reducing toxicity in language models (Krause et al., 2021;Liu et al., 2021;Dathathri et al., 2020) have focused on small to moderated sized models and restrict to explicit toxicity.Detoxification techniques primarily employ controlled text generation methods, which incurs heavy inference overhead and it is difficult to measure model performance on benchmark tasks.",
            "score": 0.41313975987263124,
            "section_title": "A. Related Work",
            "char_start_offset": 15747,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 433
                },
                {
                    "start": 435,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1200
                },
                {
                    "start": 1202,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1611
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 94,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 306,
                    "end": 330,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 350,
                    "end": 368,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 368,
                    "end": 388,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 388,
                    "end": 411,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 530,
                    "end": 549,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 567,
                    "end": 587,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 695,
                    "end": 712,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 888,
                    "end": 905,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 1303,
                    "end": 1320,
                    "matchedPaperCorpusId": "235313967"
                },
                {
                    "start": 1320,
                    "end": 1343,
                    "matchedPaperCorpusId": "208617790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.966796875
        },
        {
            "corpus_id": "270379842",
            "title": "MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations",
            "text": "Large Language Models (LLMs) [2,13,14,3] have achieved remarkable performance by following the scaling laws [15].However, deploying LLMs can be challenging due to high inference costs in resource-limited scenarios.\n\nVarious methods have been proposed to reduce model size, including knowledge distillation [16][17][18][19], which involves transferring the knowledge from the original model to a smaller one; model quantization [1,[20][21][22],\n\nwhich reduces the bit length of the model weights; and model pruning [4,[23][24][25], which involves removing non-essential weights to speed up inference.This work primarily focuses on pruning LLMs [26,6,[27][28][29],\n\nwhere gradients are particularly sensitive to weight perturbations due to the large scale of the model.",
            "score": 0.4126869942700796,
            "section_title": "Efficient Large Language Models",
            "char_start_offset": 5465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 113,
                    "end": 214
                },
                {
                    "start": 216,
                    "end": 443
                },
                {
                    "start": 445,
                    "end": 599
                },
                {
                    "start": 599,
                    "end": 662
                },
                {
                    "start": 664,
                    "end": 767
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "221995575"
                },
                {
                    "start": 427,
                    "end": 430,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 438,
                    "end": 442,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 517,
                    "end": 521,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 521,
                    "end": 525,
                    "matchedPaperCorpusId": "256390345"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "269605332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "273162410",
            "title": "A Probabilistic Perspective on Unlearning and Alignment for Large Language Models",
            "text": "Language models. We model language models as parameterized functions \u03c0 \u03b8 : \n\nmapping an input sequence of arbitrary length to a distribution over output sequences of length m, where \u03b8 are the model parameters, V denotes a vocabulary, and \n\nIn other words, for a fixed input sequence x \u2208 V \u221e , \u03c0 \u03b8 (x) spans a probability distribution over all possible output sequences V m of length m. While we are generally interested in the output distribution \u03c0 \u03b8 (x), in practice we cannot directly access this distribution since the number of possible output sequences |V | m quickly outgrows the number of atoms in the observable universe. Instead, we can only access and evaluate the language model autoregressively \u03c0 \u03b8 (y 1 , . . . , y m |x) = m t=1 \u03c0 \u03b8 (y t |y t\u22121 , . . . , y 1 , x), where \u03c0 \u03b8 (y t |\u2022) corresponds to the distribution over the possibilities for the next token y t at time step t. This represents a challenge: Without any further knowledge about the underlying distribution \u03c0 \u03b8 (x), practically we can only learn about it via sampling the model's responses for a given input sequence x, Y \u223c \u03c0 \u03b8 (x). \n\nMachine unlearning. The goal of machine unlearning is to remove knowledge from a model while preserving its overall performance. That is, given a model \u03c0 \u03b8 , a forget set D F G , and a retain set D RT , we seek an algorithm to transform the model's parameters \u03b8 such that the response y of the updated model \u03c0 \u03b8 does not answer the queries x for all (x, y) \u2208 D F G of the forget set. The challenge is that the model's utility should remain high for queries from the retain set D RT at the same time. \n\nUnlearning metrics. Assume we have a perfect oracle to decide if a generated text leaked information. We model the oracle as a function h : V m \u2192 [0, 1] that quantifies how much information got leaked, where h(s) = 0 means s does not leak information, and h(s) = 1 means complete leakage.",
            "score": 0.4126633545948786,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 7272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 74
                },
                {
                    "start": 77,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1109
                },
                {
                    "start": 1112,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1902
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.994140625
        },
        {
            "corpus_id": "268385396",
            "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
            "text": "Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos identifies the principal components that encode general or undesired knowledge. Ethos performs negating using the task vector with undesired knowledge only, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: debiasing, detoxification, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge and maintaining the overall model performance compared to current task arithmetic methods.",
            "score": 0.41241281610734914,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "252089272",
            "title": "A Survey of Machine Unlearning",
            "text": "Unlearning in large language models (LLMs) has become essential due to increasing privacy concerns and the need to comply with regulations like GDPR [115] and CCPA [134]. As LLMs are trained on vast datasets that may include sensitive information, there is a risk of these models inadvertently memorising and reproducing private data [83,193,218]. Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem [86] propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. [73] propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. [112] presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality.",
            "score": 0.4120465896386063,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 100017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 154,
                    "matchedPaperCorpusId": "56699980"
                },
                {
                    "start": 334,
                    "end": 338,
                    "matchedPaperCorpusId": "266174259"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "260925619"
                },
                {
                    "start": 1980,
                    "end": 1985,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "264146935",
            "title": "Self-Detoxifying Language Models via Toxification Reversal",
            "text": "Our approach involves first toxifying the model with an additional prompt prefix, followed by detoxifying the model. This implies that the scope and degree of detoxification depend on the model's knowledge of toxicity obtained during pre-training. Only those toxic concepts and forms that are associated with the prefix in the pre-training corpus can be evoked from the model's weights when using this prefix. These specific concepts and forms are the ones that our method can suppress. Therefore, if harmful concepts are not associated with the words in the prefix due to the model's capacity or forgetting, these harmful contents might not be removed. Consequently, our method's performance relies on the pre-training corpus and techniques of the PLM and may not be suitable for models with smaller capacities. \n\nAdditionally, our method necessitates modifying the representations within the model during the forward pass process. This requires full access to the pre-trained language model, which means our method is not applicable to language models that only offer APIs. However, we believe and advocate for pre-trained language models to become increasingly open and transparent. Our research also potentially contributes to the investigation of safety issues in these open-sourced language models from an internal mechanism perspective.",
            "score": 0.4116126259778027,
            "section_title": "Limitations",
            "char_start_offset": 26716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "271769107",
            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
            "text": "This paper introduces UNLEARN, a novel approach for forgetting selected knowledge in Large Language Models. This method relies on subspace identification for tasks and subspace discrimination between similar tasks. The experimental results demonstrate significant performance gains, highlighting the effect of UNLEARN on removing unwanted knowledge without having deleterious effects on related tasks. The method's ability to isolate and remove specific subspaces within the model ensures precise unlearning, making it a valuable tool for managing the complexities of task forgetting. \n\nCompared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks.",
            "score": 0.4114774881413633,
            "section_title": "Conclusion",
            "char_start_offset": 27411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1225
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "However, obtaining a large volume of high-quality resources is challenging and may require considerable effort. Given that the primary priority of knowledge unlearning is to prevent the generation of undesirable outputs rather than focusing on generating desirable outputs, unlearning becomes particularly appealing than RLHF. \n\nPrevious research on knowledge unlearning uses gradient ascent to achieve unlearning (Jang et al., 2023). This approach is simple yet effective in overcoming the challenges associated with language models, i.e., the prohibitive retraining overhead and the enormous size of model parameters. However, as shown in Figure 1, it applies gradient ascent to the whole sequence, i.e., all the tokens in the target sequence that are intended to be removed. As a result, while unlearning takes place, besides the target undesirable knowledge, other lexical and semantic knowledge can also be affected. This, in turn, can have a negative impact on the general ability of language models. Note that the general language ability holds great significance, and excessively impacting it can be viewed as an instance of over-unlearning. \n\nTo address this concern, we propose an adaptive objective, which replaces the original objective used in gradient ascent. Our proposed objective introduces adaptive weights to each token in the target sequence, providing fine-grained control over the unlearning target. This allows us to minimize the negative impact on non-target knowledge to the greatest extent possible. It is important to note that this objective can be considered as a pluggable component to the gradient ascent approach, ensuring that the knowledge unlearning process remains simple without the need for expensive retraining or influence estimation. Furthermore, this approach facilitates the extension of our proposed Fine-grained Pluggable Gradient Ascent (FPGA) in the regularization-based framework. By incorporating preference-aligned data or models, this framework can further augment the general ability of language models, complementing the unlearning process. We summarize the main contributions of this paper as follows: \n\n\u2022 To mitigate the negative impact on general ability resulting from existing knowledge unlearning methods, we propose an adaptive objective for the gradient ascent approach (FPGA).",
            "score": 0.4113266717754989,
            "section_title": "Introduction",
            "char_start_offset": 1922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2155
                },
                {
                    "start": 2158,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 414,
                    "end": 433,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "In this experiment, our goal was to induce the model to discard its previously acquired knowledge.To evaluate this, we employed question-answering and word chain tasks.\n\nOur testing involved two approaches: initially, we trained the model to incorporate specific information and knowledge, followed by an assessment and unlearning phase to determine if the model retained any of the initial content post-unlearning.The second approach focused on the unlearning of knowledge already embedded in the large pretrained language model.This involved analyzing changes in the model's outputs before and after unlearning and quantifying the similarity in output content.The results of these experiments are detailed in Table 3:\n\nThe results of our experiments clearly demonstrate that our unlearning method has effectively eliminated the targeted knowledge, as the model ceased to produce the original content.This outcome manifests in the model's textual outputs, which either generate irrelevant responses or explicitly display a lack of knowledge.These findings align with our anticipated experimental outcomes.Regarding knowledge injection and word chain tasks, the unlearning process is specific to each task, making these methods generally non-interchangeable.Nevertheless, incorporating data augmentation in the preprocessing stage can amplify the unlearning effect.For example, altering the formats in word chain tasks for unlearning assessments shows noticeable effects when subsequently evaluated using questionanswering techniques.The output generated by our approach was consistently unrelated and nonsensical, verifying that the model can produce text normally while effectively preventing knowledge leakage.",
            "score": 0.41045944639441756,
            "section_title": "Knowledge Unlearning",
            "char_start_offset": 27658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 168
                },
                {
                    "start": 170,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 530
                },
                {
                    "start": 530,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 719
                },
                {
                    "start": 721,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1258
                },
                {
                    "start": 1258,
                    "end": 1365
                },
                {
                    "start": 1365,
                    "end": 1534
                },
                {
                    "start": 1534,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "263908997",
            "title": "Can We Edit Multimodal Large Language Models?",
            "text": "Decomposition (Mitchell et al., 2022a) conducts efficient local edits to language models with a single input-output pair. Essentially, MEND learns to transform the gradient of fine-tuned LLMs, which utilizes a low-rank decomposition of gradients. Knowledge Editor. KE (Cao et al., 2021) is a method that can edit wrong knowledge in language models without re-training the whole model. KE utilizes a hyper network (a bidirectional-LSTM) with constrained optimization, which is used to predict the weight update during inference. SERAC. SERAC (Mitchell et al., 2022b) introduces a memory-based model editing approach, which leverages an explicit memory system to cache edits. This memory is subsequently used to adjust the output of the base model during inference. The system utilizes a small auxiliary scope classifier alongside counterfactual model. The role of the scope classifier is to ascertain whether the input is within the ambit of the memory cache. Should the input be found within this scope, it is combined with the most relevant cache item and input into the counterfactual model for prediction. \n\nIn-Context Knowledge Editing. In-Context Knowledge Editing (IKE) (Zheng et al., 2023) constructs  demonstrations  = { 1 , . . . ,   }, following the approach outlined in Liu et al. (2022). This method employs an unsupervised retriever based on cosine similarity to fetch demonstrations from the training set prior to injecting fact  = ( * ,  * ) into Language Models. The  * is the prompt to probe the factual knowledge in models (e.g., The president of the US is), and  * will be the editing target Joe Biden. The ranking of in-context demonstrations also hinges on cosine similarity: ( 1 ,  ) < ( 2 ,  ) <",
            "score": 0.41045944639441756,
            "section_title": "MEND. Model Editor Networks with Gradient",
            "char_start_offset": 17278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 38,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 541,
                    "end": 565,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1281,
                    "end": 1298,
                    "matchedPaperCorpusId": "231632658"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "277349498",
            "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging",
            "text": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.",
            "score": 0.41045944639441756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "270878324",
            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
            "text": "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs. Code and dataset are released at https://github.com/zjunlp/KnowUnDo.",
            "score": 0.41045944639441756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.",
            "score": 0.41045944639441756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "270371266",
            "title": "The Curse of Popularity: Popular Entities have Catastrophic Side Effects when Deleting Knowledge from Language Models",
            "text": "Language models (LMs) can store knowledge in their internal parameters through training (Petroni et al., 2019), and research focusing on analyzing the knowledge stored inside LMs has gained attention (Jiang et al., 2020;Heinzerling and Inui, 2021).Although this capability is essential in building human-aiding assistants, challenges related to reliability and safety are also reported.For example, LMs possess knowledge only up to the point when their training data was collected, making them not robust to the constantly changing real-world knowledge (Kasai et al., 2022).Additionally, there is a risk that LMs could leak personal and confidential information contained in the training data, raising privacy concerns (Huang et al., 2022).To address these challenges, several studies have been conducted on knowledge editing (Feng et al., 2023;Zhang et al., 2024;Dai et al., 2022;Meng et al., 2022Meng et al., , 2023;;Li et al., 2023) and knowledge deletion (Jang et al., 2023;Ishibashi and Shimodaira, 2023) in LMs.These studies have reported some success in knowledge editing and deletion, yet fail-ures, challenges, and difficulties have also been identified.\n\nOur study aims to understand better when and why knowledge editing and deletion work as intended.We hypothesize that analysis of what kind of knowledge is being deleted is essential to fulfill the objective.This paper focuses on the frequency of entities related to knowledge in the training corpus.To test this hypothesis, we design controlled experiments to analyze the impact of knowledge deletion on LMs (Figure 1).We formalize the notion of \"kind of knowledge\" regarding the structural properties of the knowledge graph trained by the model.We synthesize knowledge graphs with various properties, pre-train LMs on these graphs, perform knowledge deletion of specific facts, and then observe the side effects of deletion on entities related to the deleted knowledge.\n\nAnalyses reveal that deleting knowledge concerning frequently occurring entities results in significant and catastrophic side effects in LMs trained on knowledge graphs with similar properties to the real world.",
            "score": 0.41045944639441756,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 574
                },
                {
                    "start": 574,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 1017
                },
                {
                    "start": 1017,
                    "end": 1163
                },
                {
                    "start": 1165,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1464
                },
                {
                    "start": 1464,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1711
                },
                {
                    "start": 1711,
                    "end": 1935
                },
                {
                    "start": 1937,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 110,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 220,
                    "end": 247,
                    "matchedPaperCorpusId": "221186765"
                },
                {
                    "start": 864,
                    "end": 881,
                    "matchedPaperCorpusId": "233296761"
                },
                {
                    "start": 881,
                    "end": 898,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 898,
                    "end": 919,
                    "matchedPaperCorpusId": "252873467"
                },
                {
                    "start": 959,
                    "end": 978,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "271543835",
            "title": "Machine Unlearning in Generative AI: A Survey",
            "text": "[61] first proposes the concept of task vector, which can be obtained by taking the difference between the original model weights of a pre-trained model and its weights fine-tuned on a specific task. In particular, if we let     be the corresponding weights after fine-tuning on task , the task vector is then denoted as   =     \u2212   . Then, taking the element-wise negation of the task vector   can enable   to forget target knowledge on task  without jeopardizing irrelevant knowledge, resulting in an unlearned model that has weight of   =   \u2212  with  as a scaling term. One exemplar work is SKU [96], which designs a novel unlearning framework to eliminate harmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted of two stages where the first stage aims to identify and acquire harmful knowledge within the model, whereas the second stage targets to remove the knowledge using element-wise negation operation. Different from pure gradient ascent approaches where the model locality is largely compromised, SKU collectively aggregates the target unlearned knowledge D using gradient decent approach in the first stage and remove it from the pre-trained model. However, SSU [29] identifies the potential instability of the pure task vector approach in the case of multiple rounds of unlearning (i.e., sequential unlearning) and introduces a more stable unlearning framework integrated with weight saliency. \n\nBesides subtracting undesirable parameters, Pochinkov and Schoots [127] proposes a selective pruning method to trim those neurons' relative importance to different datasets, representing target model capability for unlearning. In particular, it performs either iterative pruning on nodes in the feed-forward layers or attention head layers. This selective approach utilizes importance functions that assess the contribution of individual neurons to specific tasks by measuring activation frequencies and magnitudes, enabling precise targeting of neurons that are crucial for the undesired capabilities. Furthermore, different from previous weight pruning method, where it requires Hessian computation, the selective neuron pruning is more computational efficient for large language models because it directly removes neurons that contribute the most to the unwanted behavior.",
            "score": 0.40942181556756724,
            "section_title": "Safety Alignment",
            "char_start_offset": 47082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2322
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023;Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2020;Bourtoule et al., 2021), and mitigating biases to promote fairness (Chen et al., 2023;Li et al., 2023b). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023a), and graph learning (Chen et al., 2022). \n\nThere is a pressing need for unlearning methods specifically tailored to language models, referred to as knowledge unlearning (Jang et al., 2023). This need arises because language models acquire knowledge from open-source text data, which inherently contains sensitive information, including toxic and private content. However, applying existing unlearning methods directly to language models poses significant challenges. Firstly, the retraining overhead of language models is exceptionally high, making it computationally prohibitive for regular users, even when only retraining a subcomponent. Secondly, language models have an enormous parameter size, rendering certain memory or influence estimation approaches inaccurate and even intractable. \n\nAn alternative approach to removing undesirable knowledge from language models is Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020), but it is not well-suited for this purpose. RLHF involves fine-tuning the model to align with human preferences, which requires a significant amount of preference-aligned text data, e.g., GPT 4 (Achiam et al., 2023). However, obtaining a large volume of high-quality resources is challenging and may require considerable effort.",
            "score": 0.40831056726425946,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 291,
                    "matchedPaperCorpusId": "259991722"
                },
                {
                    "start": 291,
                    "end": 313,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 408,
                    "end": 426,
                    "matchedPaperCorpusId": "207847600"
                },
                {
                    "start": 426,
                    "end": 449,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 512,
                    "end": 529,
                    "matchedPaperCorpusId": "263830799"
                },
                {
                    "start": 679,
                    "end": 697,
                    "matchedPaperCorpusId": "260927489"
                },
                {
                    "start": 719,
                    "end": 737,
                    "matchedPaperCorpusId": "268030788"
                },
                {
                    "start": 758,
                    "end": 777,
                    "matchedPaperCorpusId": "232404451"
                },
                {
                    "start": 907,
                    "end": 926,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1665,
                    "end": 1688,
                    "matchedPaperCorpusId": "221665105"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "270440244",
            "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
            "text": "Large language models (LLMs) exhibit a concerning tendency to memorize information from their training data [11,12,46].While factual recall is a desirable property when dealing with general knowledge, memorization and regurgitation of sensitive private information, such as personal and contact details, is a security concern regulated by laws like the general data protection regulation [GDPR ; 21].To ensure such information does not get inadvertently leaked, it is paramount to develop techniques that detect and erase sensitive information from the LLMs or their training data.\n\nCurrent approaches for handling sensitive information in LLMs can be broadly categorized into two groups.Exact unlearning approaches tackle the problem from the data perspective, either erasing information from datasets [30,32] or applying differential privacy [1,27,34,47] to the training data.Such approaches are costly and time-consuming, as each iteration of scrubbing requires retraining of the entire model.Moreover, they reduce but do not entirely prevent the risk of sensitive information leakage [9,10].Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,58] or gradient ascent [29,55,56].These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,38,39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks [11] by surgically removing the sensitive data from model parameters.We introduce Rank Editing in the Vocabulary Space (REVS), a novel model editing approach that enables robust unlearning of sensitive information from LLMs while maintaining model performance and offering strong robustness against extraction attacks.Adopting the view that transformer MLP layers construct predictions by promoting specific tokens in the output vocabulary space [24], REVS locates the layers and particular subsets of neurons that promote the tokens corresponding to the targeted sensitive information.",
            "score": 0.4081967827942767,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 119,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 581
                },
                {
                    "start": 583,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 996
                },
                {
                    "start": 996,
                    "end": 1095
                },
                {
                    "start": 1095,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1436
                },
                {
                    "start": 1436,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1914
                },
                {
                    "start": 1914,
                    "end": 2163
                },
                {
                    "start": 2163,
                    "end": 2431
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 112,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 112,
                    "end": 115,
                    "matchedPaperCorpusId": "258426273"
                },
                {
                    "start": 115,
                    "end": 118,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 803,
                    "end": 807,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 844,
                    "end": 847,
                    "matchedPaperCorpusId": "207241585"
                },
                {
                    "start": 847,
                    "end": 850,
                    "matchedPaperCorpusId": "235097653"
                },
                {
                    "start": 1091,
                    "end": 1094,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "258832407"
                },
                {
                    "start": 1284,
                    "end": 1287,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1601,
                    "end": 1604,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1845,
                    "end": 1849,
                    "matchedPaperCorpusId": "229156229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "There exist a few studies in addressing unlearning for LLMs. Eldan et al. [37] manipulated the model by further training it on specific target data to pinpoint the tokens most relevant to the unlearning goal. This was achieved using a baseline model for comparison. Later, the unique phrases in the target data were swapped with generic counterparts and used the model's predictions to generate new labels for each token, simulating what a model would predict. Finally, by finetuning the model on the newly generated labels, the original text was removed from the model's memory. \n\nWang et al. [18] developed a framework called knowledgegap-alignment (KGA), aimed at preserving preserves the differences in distribution between unlearned and retrained models and demonstrating its performance for NLP tasks. This approach requires to set aside an independent set to measure the knowledge gap between the original model and the unlearned model. Kumar et al. [17] proposed a method for unlearning in large language models (LLMs) based on the SISA [25] approach. Meanwhile, Maini et al. [38] employed fine-tuning techniques on synthetic (fictitious) data to facilitate unlearning in LLMs. However, these approaches do not explicitly take into account the distribution of samples within the model's latent space, such as the distances between forgotten samples and the remaining samples. As a result, they are unable to directly optimize the geometric distribution of the samples in the model.",
            "score": 0.4081634079118829,
            "section_title": "C. Unlearning in LLMs",
            "char_start_offset": 11912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 579
                },
                {
                    "start": 582,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1489
                }
            ],
            "ref_mentions": [
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 1045,
                    "end": 1049,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "277043381",
            "title": "Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning",
            "text": "Unlearning techniques in language models offer a promising way to mitigate harmful biases and prevent the spread of dangerous knowledge. This is especially important as LLMs touch more lives,    fall into more hands, and become more powerful each day. Methods such as SAE-based unlearning and RMU can effectively remove access to harmful content while preserving general knowledge. However, unlearning alone does not resolve deeper issues rooted in training data; improving dataset curation is also essential to preventing bias from being embedded in models from the outset. A key ethical risk of unlearning is potential misuse for censorship. While limiting harmful content is beneficial, these techniques could also be exploited to suppress politically sensitive information or shape narratives. As unlearning development continues, ensuring transparency in what is unlearned and implementing safeguards against ideological manipulation will be crucial to maintaining trust and accountability.",
            "score": 0.4069520482652802,
            "section_title": "F Ethical Considerations",
            "char_start_offset": 21732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 995
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "269009619",
            "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
            "text": "Large language models (LLMs), pretrained on massive corpora of internet data, possess the capability to memorize portions of their training data (Carlini et al., 2021(Carlini et al., , 2022)). However, this capability raises significant concerns, as the training data may contain sensitive or private information, potentially leading to societal challenges. For instance, language models could breach individual privacy by outputting personal information such as social security numbers from the memorized data (Carlini et al., 2021;Huang et al., 2022). They might also violate copyright by generating text from memorized books, such as the Harry Potter novels (Eldan & Russinovich, 2023). Furthermore, LLM assistants for biology could inadvertently aid in the development of biological weapons by troubleshooting bottlenecks, increasing the risk of such attempts (Sandbrink, 2023;Li et al., 2024). In response to these concerns, regulations like the EU's General Data Protection Regulation (GDPR) (Mantelero, 2013;Voigt & Von dem Bussche, 2017) and the US's California Consumer Privacy Act (CCPA) (CCPA, 2018) have mandated the Right to be Forgotten, requiring applications to support the deletion of information contained in training samples upon user requests. This has motivated a line of research on machine unlearning, aiming to address these challenges. \n\nMachine unlearning (Cao & Yang, 2015;Bourtoule et al., 2021) aims to delete the influence of specific training samples from machine-learning models while preserving other knowledge and capabilities (Liu et al., 2024a;Zhang et al., 2023;Nguyen et al., 2022;Xu et al., 2023;Si et al., 2023). Notably, a straightforward approach to unlearning is to retrain a language model from scratch. However, as retraining from scratch is typically computationally expensive, cheaper methods for removing undesirable information is highly desirable. Recently, several works (Jang et al., 2022; Gradient Ascent \u2112 GA = \u2212  log(\u03c0 \u03b8 (z))",
            "score": 0.4065982233753313,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 166,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 511,
                    "end": 533,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 998,
                    "end": 1015,
                    "matchedPaperCorpusId": "56699980"
                },
                {
                    "start": 1382,
                    "end": 1400,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1619,
                    "end": 1635,
                    "matchedPaperCorpusId": "254805754"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.982421875
        },
        {
            "corpus_id": "276576079",
            "title": "Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models",
            "text": "In recent years, large language models (LLMs) have undergone substantial advancements, leading to enhanced performance and widespread adoption. LLMs have demonstrated exceptional performance in various downstream tasks, such as ma-* Work done during an internship at Intel Labs. chine translation (Zhu et al., 2023), content generation (Acharya et al., 2023), and complex problemsolving (Xi et al., 2025). Their performance is attributed to their large-scale architectures that require datasets consisting of up to billions of tokens to train effectively (Kaplan et al., 2020). These datasets are typically derived from large-scale corpora sourced from public internet text. However, such datasets inadvertently contain harmful or inappropriate content, including instructions for hazardous activities (e.g., bomb-making), violent or explicit material, private information, or copyrighted content unsuitable for applications. Given the sensitive nature of such data, it may be necessary to remove it from the LLM to comply with the local regulations, or internal company policies. \n\nMachine unlearning is a tool for removing information from models (Cao and Yang, 2015;Bourtoule et al., 2021a). Approximate unlearning usually refers to removing information from models without resorting to retraining them from scratch (Zhang et al., 2024a;Eldan and Russinovich, 2023a;Izzo et al., 2021), ensuring that the resulting model deviates from a fully retrained version within a bounded error. While numerous studies have proposed various unlearning algorithms, most lack formal guarantees of effectiveness. In fact, prior research has demonstrated that many unlearning techniques can be circumvented through simple rephrasings of the original data (Shi et al., 2024). Recent work has shown that a soft token attack (STA ) can be used to elicit harmful completions and extract supposedly unlearned information from models (Schwinn et al., 2024;Zou et al., 2024).",
            "score": 0.4064586197702927,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1955
                }
            ],
            "ref_mentions": [
                {
                    "start": 336,
                    "end": 358,
                    "matchedPaperCorpusId": "261823487"
                },
                {
                    "start": 387,
                    "end": 404,
                    "matchedPaperCorpusId": "261817592"
                },
                {
                    "start": 1149,
                    "end": 1169,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1169,
                    "end": 1193,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 1369,
                    "end": 1387,
                    "matchedPaperCorpusId": "232033672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95166015625
        },
        {
            "corpus_id": "276094843",
            "title": "Tool Unlearning for Tool-Augmented LLMs",
            "text": "Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to forget learned tools due to security vulnerabilities, privacy regulations, or tool deprecations. However, ``tool unlearning'' has not been investigated in unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete, the first approach for unlearning tools from tool-augmented LLMs. It implements three key properties to address the above challenges for effective tool unlearning and introduces a new membership inference attack (MIA) model for effective evaluation. Extensive experiments on multiple tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns randomly selected tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks.",
            "score": 0.4064586197702927,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "273233618",
            "title": "A Closer Look at Machine Unlearning for Large Language Models",
            "text": "Large language models (LLMs) may memorize sensitive or copyrighted content, raising privacy and legal concerns. Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning. Experimental results across three scenarios, i.e., fictitious unlearning, continual unlearning, and real-world unlearning, demonstrate the effectiveness of our approaches. The code is available at https://github.com/sail-sg/closer-look-LLM-unlearning.",
            "score": 0.4064586197702927,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "271909713",
            "title": "Promoting Equality in Large Language Models: Identifying and Mitigating the Implicit Bias based on Bayesian Theory",
            "text": "In tasks involving the removal of specific information from LLMs, traditional evaluation methods primarily use behavioral testing, such as questioning or querying capabilities concerning the extracted information (Stoehr et al. 2024;Hase et al. 2024). Nevertheless, evidence increasingly supports that models can regenerate previously forgotten data (Lynch et al. 2024;Patil, Hase, and Bansal 2023), a critical root of implicit bias within LLMs. (Hong et al. 2024) coined the term \"knowledge traces,\" evaluating whether unlearning algorithms genuinely expunge data from model weights-or merely disguise it until activated by malign entities-by quantifying alterations in LLMs' concept vectors. Their studies showed that while fine-tuning approaches scarcely affect these vectors, techniques like MEMIT (Meng et al. 2022a), significantly dismantle the knowledge embedded in LLMs. For deploying MEMIT in bias elimination, we represent x as a subject-relation-object triple \u27e8s, r, o\u27e9. We automate the conversion of x from natural language to structured knowledge. Subsequently, we substitute the original triple with a novel object o \u2032 , converting \u27e8s, r, o\u27e9 into \u27e8s, r, o \u2032 \u27e9.",
            "score": 0.4064586197702927,
            "section_title": "Automated Model Editing",
            "char_start_offset": 17926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1174
                }
            ],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 249,
                    "matchedPaperCorpusId": "255595518"
                },
                {
                    "start": 369,
                    "end": 398,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 446,
                    "end": 463,
                    "matchedPaperCorpusId": "270560986"
                },
                {
                    "start": 802,
                    "end": 821,
                    "matchedPaperCorpusId": "252873467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9599609375
        },
        {
            "corpus_id": "222134166",
            "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior",
            "text": "Pre-trained Language Model Compression The major existing efforts to compress pre-trained language models such as BERT include knowledge distillation (Ba and Caruana, 2014;Hinton et al., 2015) and pruning (Iandola et al., 2016;Veit and Belongie, 2017).\n\nThe knowledge distillation approach enables the transfer of knowledge from a large teacher model to a smaller student model. Such attempts have been made to distill BERT models, e.g., Distil-BERT (Sanh et al., 2019), BERT-PKD (Sun et al., 2019), Distilled BiLSTM (Tang et al., 2019), Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020), etc. All of these methods require carefully designing the student architecture. Furthermore, to choose which intermediate results that the student model can learn from, e.g., the outputs of each layer, the attention maps, is still under discussion.\n\nSimilar to other pruning-based methods, our method can iteratively remove the least important weights or connections, explore the full spectrum of trade-offs, and find the best affordable architecture in one shot. Many language representation model pruning methods focus on individual components of the weight matrices. For example, Guo et al. (2019) integrates reweighted L 1 minimization with a proximal algorithm to search sparsity patterns in the model; Gordon et al. (2020) uses magnitude weight pruning, which compresses the model by removing weights close to 0; Sanh et al. (2020) applies deterministic first-order weight pruning method where both weights with low and high values can be pruned. A very few works try structured weight pruning, e.g.,  proposes a structured pruning approach based on low-rank factorization and augmented Lagrangian L 0 norm regularization. On the other hand, there also exist works that prune a coherent set of sub-modules in the Transformer model. For example, Michel et al. (2019) and Voita et al. (2019) propose to prune individual attention heads either manually via head importance score,",
            "score": 0.4061748933672241,
            "section_title": "Related Work",
            "char_start_offset": 5660,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 172,
                    "matchedPaperCorpusId": "11536917"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68603515625
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io",
            "score": 0.4060798879548191,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "268032022",
            "title": "Eight Methods to Evaluate Robust Unlearning in LLMs",
            "text": "It is difficult to ensure that large language models (LLMs) will always behave harmlessly. For example, jailbreaks and attacks can elicit harmful behaviors (Liu et al., 2023b;Wei et al., 2023;Zou et al., 2023b;Shah et al., 2023;Rao et al., 2023;Shayegani et al., 2023;Geiping et al., 2024). Meanwhile, LLMs also memorize pretraining data, raising concerns involving privacy and fair use (Carlini et al., 2022;Shi et al., 2023;Karamolegkou et al., 2023). To reduce these risks, machine unlearning has emerged as a way to remove undesirable knowledge from LLMs (Bourtoule et al., 2021;Nguyen et al., 2022;Si et al., 2023;Shaik et al., 2023;Liu et al., 2024a). Ideally, LLM unlearning should produce a model that is competitive on most tasks but which robustly loses knowledge on the unlearning task in a way that is resistant to extraction by an adversary. Prior works have introduced various ad hoc techniques (see Table 1 and Section 2). However, to date, little has been done to comprehensively evaluate LLM unlearning (Liu et al., 2024a). \n\nIn this paper, we first survey evaluations for LLM unlearning, observing that prior works have generally relied on limited and ad-hoc evaluations. Second, we implement a thorough set of evaluations to red team the \"Who's Harry Potter\" (WHP) model from Eldan & Russinovich (2023). We find that the WHP model's unlearning shows consistent signs of generalization, particularly when it is evaluated using the \"Familiarity\" metric used by Eldan & Russinovich (2023), but we can consistently extract a higher-than-baseline amount of knowledge from the WHP model. Moreover, we argue that Familiarity may be particularly friendly to the unlearning method used by Eldan (2023).",
            "score": 0.4060135744074787,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98828125
        },
        {
            "corpus_id": "277634570",
            "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
            "text": "Large Language Models (LLMs) excel at generating humanlike text, leading to their broad adoption in various applications. This success largely stems from their strong memorization of the training corpus (Zhang et al., 2023). However, such memorization also raises serious concerns, including risks of privacy breaches (Kim et al., 2024), bias propagation (Yu et al., 2023;Motoki et al., 2024), and the generation of illegal content (Karamolegkou et al., 2023). In particular, privacy protection laws like the GDPR require service providers to remove private information from training data upon user request (Voigt & Von dem Bussche, 2017). This creates a significant challenge: how to effectively erase the influence of specific data samples (i.e., the forget set), or higher-level data concepts from pre-trained LLMs. \n\nA practical approach to addressing the issue above is Machine Unlearning (MU) (Liu et al., 2024c). Previous research (Ginart et al., 2019;Ullah et al., 2021;Thudi et al., 2022;Liu et al., 2024b) has primarily focused on MU in classification models, where retraining on the remaining data (i.e., the retain set) is the gold standard. However, given the massive scale of training data and the extensive number of parameters in LLMs, this unlearning approach becomes infeasible for LLMs. Therefore, developing effective and efficient methods for implementing MU in LLMs represents a critical challenge that requires resolution. \n\nExisting studies (Jang et al., 2023;Ji et al., 2024b;Feng et al., 2024;Liu et al., 2024c) defines LLM unlearning as the removal of specific knowledge from the forget set (i.e., unlearning completeness) while preserving the model's performance on unrelated tasks (i.e., model utility).",
            "score": 0.40537910166010854,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 818
                },
                {
                    "start": 821,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1732
                }
            ],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "259342279"
                },
                {
                    "start": 372,
                    "end": 392,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 432,
                    "end": 459,
                    "matchedPaperCorpusId": "264426289"
                },
                {
                    "start": 899,
                    "end": 918,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 938,
                    "end": 959,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 959,
                    "end": 978,
                    "matchedPaperCorpusId": "232068763"
                },
                {
                    "start": 978,
                    "end": 997,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 997,
                    "end": 1015,
                    "matchedPaperCorpusId": "258059852"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "Large language models are finetuned to refuse questions about hazardous knowledge, but these protections can often be bypassed. Unlearning methods aim at completely removing hazardous capabilities from models and make them inaccessible to adversaries. This work challenges the fundamental differences between unlearning and traditional safety post-training from an adversarial perspective. We demonstrate that existing jailbreak methods, previously reported as ineffective against unlearning, can be successful when applied carefully. Furthermore, we develop a variety of adaptive methods that recover most supposedly unlearned capabilities. For instance, we show that finetuning on 10 unrelated examples or removing specific directions in the activation space can recover most hazardous capabilities for models edited with RMU, a state-of-the-art unlearning method. Our findings challenge the robustness of current unlearning approaches and question their advantages over safety training.",
            "score": 0.4049446187646043,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "276617566",
            "title": "Holistic Audit Dataset Generation for LLM Unlearning via Knowledge Graph Traversal and Redundancy Removal",
            "text": "In recent years, Large Language Models (LLMs) have undergone rapid development, demonstrating impressive capabilities across a wide range of applications, from natural language processing to code generation and complex problem-solving (Liu et al., 2023;Satpute et al., 2024). However, these advances have raised concerns about potential risks associated with the vast knowledge stored in these models, e.g., the inadvertent retention of personally  identifiable information (PII) (Jang et al., 2022), the propagation of unsafe or biased behaviors (Liu et al., 2024e), and the unauthorized use of copyrighted content (Eldan and Russinovich, 2023). Furthermore, there is an increasing imperative for LLMs to comply with regulatory standards such as the General Data Protection Regulation (GDPR) (Hoofnagle et al., 2019), which enforces the \"Right to be Forgotten\" (Dang, 2021). To address these concerns, researchers are investigating various unlearning techniques (Jia et al., 2024a) to selectively remove specific knowledge from pretrained LLMs while preserving their general language modeling capabilities, thereby avoiding the substantial computational costs associated with building new models from scratch. \n\nThe growing significance of LLM unlearning has heightened the importance of rigorous evaluation or audit of unlearing performance. Recent benchmarks like MUSE (Shi et al., 2024) and TOFO (Maini et al., 2024)  edge preservation. These pioneering frameworks have advanced the field by establishing standardized datasets, providing pre-trained target models, and introducing multifaceted evaluation metrics. However, their audit suites remain constrained in scope-for instance, MUSE employs only 100 test questions to evaluate 0.8M corpora. From an auditing perspective, such limited test coverage may inadequately assess the targeted knowledge removal, potentially compromising the comprehensive evaluation of unlearning effectiveness. \n\nOur investigation reveals two fundamental challenges in holistic audit dataset synthesis. The primary concern about audit adequacy stems from simply relying on GPT-4 for automated QA generation from forget corpora.",
            "score": 0.40423353552058433,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 274,
                    "matchedPaperCorpusId": "268819830"
                },
                {
                    "start": 793,
                    "end": 817,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 862,
                    "end": 874,
                    "matchedPaperCorpusId": "234335026"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "272600027",
            "title": "Mapping Technical Safety Research at AI Companies: A literature review and incentives analysis",
            "text": "Large language models are very general in their knowledge base because they are trained on such a broad set of training data. However, there may be topics, such as detailed biology knowledge relevant to bioweapon design, where we want to limit their knowledge due to risk of misuse. \n\nUnlearning aims to reduce an AI model's knowledge in specific dangerous domains. A recent approach involved isolating the neural representations of particular dangerous concepts in the model, and then deliberately perturbing those model weights, while maintaining the representations of related harmless concepts (Li et al., 2024, pp. 8-9). 32 This method was used to successfully reduce model performance to near random chance on a test set of questions about dangerous biological or cybersecurity information, while incurring minimal losses on traditional capability benchmarks (Li et al., 2024, pp. 10-13). Unlearning could potentially also be used to worsen models' \"situational awareness\" regarding details about their training process, architecture, and human oversight process. This could be used to make it harder for AI models with power-seeking tendencies to plan actions that their human supervisors would not want (Berglund et al., 2023). \n\nResearch in this category originally gained traction in response to privacy concerns, such as the \"right to erasure\" in the EU's General Data Protection Regulation (Shumailov et al., 2024). The focus here is on removing a different kind of information, i.e., information about specific individuals whose data was used to train the model, rather than information about broader topics, such as bioweapons (Juliussen, Rui, and Johansen, 2023;Li et al., 2024, p. 4). 33 oogle DeepMind recently ran a competition for novel unlearning techniques which garnered submissions from more than a thousand teams worldwide, suggesting significant ongoing interest in this research category (Triantafillou et al., 2024).",
            "score": 0.40409243022605196,
            "section_title": "Unlearning (2 papers)",
            "char_start_offset": 44445,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1235
                },
                {
                    "start": 1238,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 1641,
                    "end": 1677,
                    "matchedPaperCorpusId": "262207708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9677734375
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Knowledge unlearning in LLMs stems from traditional machine unlearning, as the current transformer architecture-based language model are, in essence, machine learning models. Their goals are consistent, aiming to remove specific knowledge from the model. However, large language models differ significantly from typical machine learning models, both in terms of parameter scale and the richness of internal knowledge.",
            "score": 0.40355210594985114,
            "section_title": "Relationship with Machine Unlearning",
            "char_start_offset": 11684,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97412109375
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "The past a few years have witnessed the great success of large language models, demonstrating powerful capabilities in comprehending textual data and generating human-like languages. Large language models achieve success by being trained on vast amounts of textual data, including online sources with copyrighted content and user-generated knowledge. However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model. Comprehensive experiments on real-world datasets demonstrate the effectiveness and efficiency of DeepCUT with consistent and significant improvement over baseline methods.",
            "score": 0.4025064922320116,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "270440333",
            "title": "A More Practical Approach to Machine Unlearning",
            "text": "With the development of large language models (LLMs), there is an increased focus on privacy risks and the need to remove certain data influences.Several methods and techniques have been explored for this purpose: (2023) proposed additional fine-tuning strategies tailored for unlearning [13].",
            "score": 0.4024982055363106,
            "section_title": "Machine Unlearning for Large Language Models",
            "char_start_offset": 3586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 293
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "258615571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "277622161",
            "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
            "text": "Our method progressively identifies and removes non-informative tokens from the model's memory during inference. We achieve this by introducing learnable pruning modules within the Transformer architecture. These modules dynamically analyze token representations and select tokens for pruning. Each pruning decision leverages a corresponding gradient-based signal, guided by a dual-loss strategy detailed later. This strategy balances the token pruning objective with the preservation of the model's pre-trained capabilities through a language modeling loss. The pruning mask, indicating the tokens to be removed, is propagated through subsequent layers. Finally, all identified non-informative tokens are eliminated from memory before calculating the first token. This selective removal significantly reduces computational demands during inference, as the pruning module operates with a fraction of the FLOPs required by the entire language model (less than 1%).",
            "score": 0.4024982055363106,
            "section_title": "Overview",
            "char_start_offset": 6887,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 963
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94091796875
        },
        {
            "corpus_id": "271064299",
            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
            "text": "Machine unlearning for language models: methods and applications.Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022;Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.\n\nMachine unlearning has also been applied to various downstream language model tasks, though the unit of machine unlearning may differ from what we study in this work.Our evaluation focuses on unlearning specific examples or datasets, aiming to make LMs forget either the phrasing or the content knowledge of targeted data, while preserving their utility for data not targeted for removal.This is crucial for ensuring privacy and copyright compliance.In addition to this specific unlearning, there's also a broader application similar to model editing, where outdated information is replaced with new knowledge (Pawelczyk et al., 2023;Yu et al., 2023;Belrose et al., 2024).Moreover, efforts have been made to eliminate harmful behaviors in language models by creating toxicity benchmarks and enhancing safety measures (Lu et al., 2022;Yao et al., 2023;Li et al., 2024a;Zhang et al., 2024b).Despite these varied approaches to unlearning at different operational and knowledge levels, the evaluation principles we propose such as preserving utility, ensuring scalability, and maintaining sustainability-are relevant across these contexts.\n\nMachine unlearning for language models: evaluation.Evaluating machine unlearning methods for language model applications is also critical.Most previous studies have focused this evaluation on specific tasks such as question answering or sentence completion.",
            "score": 0.4024982055363106,
            "section_title": "Related Work",
            "char_start_offset": 24726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 578
                },
                {
                    "start": 580,
                    "end": 744
                },
                {
                    "start": 746,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1134
                },
                {
                    "start": 1134,
                    "end": 1196
                },
                {
                    "start": 1196,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1881
                },
                {
                    "start": 1883,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2140
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 382,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1380,
                    "end": 1396,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 1396,
                    "end": 1417,
                    "matchedPaperCorpusId": "259088549"
                },
                {
                    "start": 1563,
                    "end": 1580,
                    "matchedPaperCorpusId": "249152301"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "275993983",
            "title": "SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders",
            "text": "In particular, we train a sparse autoencoder on the activations of the diffusion model and leverage its summative nature to unlearn concepts by blocking unwanted content. Most simi-larly to our approach, Farrell et al. (2024) show that SAEs can be employed to remove a subset of biological knowledge in large language models (LLMs), while Guo et al. (2024) benchmark several mechanistic interpretability techniques for knowledge editing and unlearning in LLMs. In the concurrent work (Kim & Ghadiyaram, 2025), a similar idea was used to unlearn concepts in DM's text encoder.",
            "score": 0.4024982055363106,
            "section_title": "Machine Unlearning in Diffusion Models",
            "char_start_offset": 6693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "273403717",
            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
            "text": "Large language models (LLMs) often learn to encode undesirable knowledge. The possibility of selectively editing or unlearning this type of knowledge is viewed as paramount for ensuring accuracy, fairness, and control of AI. Yet, editing and unlearning of knowledge from these models remains challenging. \n\nCommon editing and unlearning methods often come at the cost of affecting other general or tangential knowledge or capabilities within the model. Moreover, the edits achieved through these methods may not be robust -e.g., slight variations in the prompt formulation can often still elicit the original fact or capability, or the original answers are still present/extractable given white-box access. Some recent work has explored editing or unlearning techniques that rely on mechanistic interpretability methods attempting to trace which components of a network store specific facts (Meng et al., 2023). These methods, such as causal tracing or attribution patching, focus on measuring how output or task accuracy is affected when clean/corrupted input is patched into specific components. \n\nHowever, the effectiveness of these \"output-tracing\" (OT) techniques for editing has been questioned by Hase et al. (2023). Our research confirms these doubts, finding that localized editing and unlearning of facts based on several existing OT methods often perform equal to or worse than simply updating the entire model. This is particularly evident when evaluating the robustness of edits against prompt variations and relearning, and when probing for remaining latent knowledge.",
            "score": 0.4024982055363106,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1097
                },
                {
                    "start": 1100,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "258823276",
            "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "text": "Compression of Language Model. Language models [9,29,25] have gained much attention and increase the need to reduce the size of parameters and reduce the latency [23,46]. To compress the language model, previous works can be divided into several categories: network pruning [21,61,30,15], knowledge distillation [44,45,38], quantization [63,1,66] and other techniques, like early exit [60] or dynamic token reduction [64]. We focus on the pruning of the language models, especially structural pruning [26]. Structural pruning removes the entire filter from the neural network, which is more hardware friendly. There are several ways to remove the structure, such as l1-dependent pruning [16,67], first-order importance estimation [18], hessian-based estimation [21,52] or the optimal brain surgeon [24,21]. As for the pruning unit in structural pruning, some works adopt the entire layer [10] as the minimal unit, and others take the multi-head attention [50] or the feed-forward layers [18,34] as the basic structure to prune. CoFi [59] studies the pruning unit in different granularity. \n\nEfficient and Low Resource Compression. With the growing size of models, there is an increasing demand for efficient LLM compression and compression is independent of the original training data. \n\nAs for the efficient compression, [22] accelerate the post-training by defining the reconstruction error as a linear least squares problem. [13,12] propose the layer-wise optimal brain surgeon. As for the constraint of availability of the training corpus, data-free pruning [43,65] come up with several strategies to prune the model by measuring neurons' similarity. Besides, [32,31,40] proposes methods that distill the model without reliance on the training corpus of the model. However, those methods are too time-consuming, involving synthesizing samples by backpropagating the pre-trained language models.",
            "score": 0.40247161437102474,
            "section_title": "Related Work",
            "char_start_offset": 5248,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1285
                },
                {
                    "start": 1288,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1898
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "233297003"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "236477807"
                },
                {
                    "start": 316,
                    "end": 319,
                    "matchedPaperCorpusId": "221995575"
                },
                {
                    "start": 337,
                    "end": 341,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 343,
                    "end": 346,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 385,
                    "end": 389,
                    "matchedPaperCorpusId": "216552850"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "235097557"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 730,
                    "end": 734,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "155089879"
                },
                {
                    "start": 798,
                    "end": 802,
                    "matchedPaperCorpusId": "7785881"
                },
                {
                    "start": 955,
                    "end": 959,
                    "matchedPaperCorpusId": "162183964"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "215415863"
                },
                {
                    "start": 1566,
                    "end": 1569,
                    "matchedPaperCorpusId": "238259421"
                },
                {
                    "start": 1668,
                    "end": 1671,
                    "matchedPaperCorpusId": "222290473"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "267412857",
            "title": "Copyright Protection in Generative AI: A Technical Perspective",
            "text": "Even if the LLM has memorized some copyrighted data, the model builder can thereafter use \"machine unlearning\" techniques to delete the copyrighted information for copyright protection. For example, Eldan et al. [30] first used a reinforced model to identify key tokens by comparing logits with a baseline model. Then, unique expressions in the data are replaced with generic ones, and new labels are generated to mimic a model not trained on the data. Finally, they show that fine-tuning the model with these labels can effectively remove the original text from the model's memory upon contextual prompting. Chen et al. [18] proposed an efficient method EUL for updating Large Language Models (LLMs) without full retraining by integrating \"lightweight unlearning layers\". The",
            "score": 0.40213695165963914,
            "section_title": "Machine Unlearning.",
            "char_start_offset": 73409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 776
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "277622234",
            "title": "Not All Data Are Unlearned Equally",
            "text": "Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.",
            "score": 0.4009726250746817,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "265466153",
            "title": "Removing NSFW Concepts from Vision-and-Language Models for Text-to-Image Retrieval and Generation",
            "text": "Removing concepts from vision-and-language models. Removing content from AI models has been recently gaining increasing attention, with techniques spanning from complete model retraining or fine-tuning to machine unlearning [6,17,18,38] and differential privacy [19]. Some of these attempts have been considering text-to-image models and have aimed at deleting styles, concepts, or objects [25,56]. Recently, Schramowski et al. [44] introduced a technique to steer the generation away from NSFW areas, defined by a finite and fixed set of concepts. NSFW concepts are encoded with the input prompt at inference time and the NSFW embedding is used as negative guidance. Later, Gandikota et al. [15] proposed a fine-tuning method that can erase a visual concept given only its name and using negative guidance as a teacher. \n\nIn contrast to these previous works, we focus on removing NSFW from a contrastive CLIP-like model, which can be applied for cross-modal retrieval, and for visual and textual generation. While to the best of our knowledge we are the first to tackle this scenario, Trager et al. [51] have demonstrated the presence of compositional patterns within the embedding space of CLIP, which suggests the existence of a distinctive path from safe to NSFW zones.",
            "score": 0.40003746205816126,
            "section_title": "Related Work",
            "char_start_offset": 5563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1273
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 227,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "195886255"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "270559969",
            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
            "text": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten.In the field of computer vision, machine unlearning has been extensively studied [17; 18; 19; 28; 57], primarily focusing on the removal of specific training samples in classification tasks.However, this may not be sufficient for generative LLMs, considering their vast parametric knowledge and the interwoven capabilities they possess.\n\nRecently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37].From the perspective of knowledge sources, existing work primarily focuses on forgetting specific classification tasks [11; 44], memorized sequences [25; 4], copyrighted books [59; 13], and toxic capacities [35; 5; 29; 22].Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37].Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model.Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].Although unlearning methods for large language models have rapidly developed, some studies [43; 34; 36; 50] have shown that it remains easy to extract supposedly forgotten knowledge from the models after unlearning.Therefore, there remains significant room for research on unlearning methods.",
            "score": 0.39897195344593356,
            "section_title": "Knowledge Unlearning for Large Language Models",
            "char_start_offset": 3928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 158,
                    "end": 348
                },
                {
                    "start": 348,
                    "end": 494
                },
                {
                    "start": 496,
                    "end": 621
                },
                {
                    "start": 621,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1154
                },
                {
                    "start": 1154,
                    "end": 1283
                },
                {
                    "start": 1283,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1575
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "273323555",
            "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
            "text": "Large Language Models' knowledge of how to perform cyber-security attacks, create bioweapons, and manipulate humans poses risks of misuse. Previous work has proposed methods to unlearn this knowledge. Historically, it has been unclear whether unlearning techniques are removing information from the model weights or just making it harder to access. To disentangle these two objectives, we propose an adversarial evaluation method to test for the removal of information from model weights: we give an attacker access to some facts that were supposed to be removed, and using those, the attacker tries to recover other facts from the same distribution that cannot be guessed from the accessible facts. We show that using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy when applied to current unlearning methods for information learned during pretraining, revealing the limitations of these methods in removing information from the model weights. Our results also suggest that unlearning evaluations that measure unlearning robustness on information learned during an additional fine-tuning phase may overestimate robustness compared to evaluations that attempt to unlearn information learned during pretraining.",
            "score": 0.3987865205043206,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "276812969",
            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
            "text": "Large language models (LLMs) trained on massive datasets show exceptional capabilities (Kaplan et al., 2020;Wei et al., 2022). However, such extensive datasets inevitably contain harmful information, which diminishes model performance and may cause societal challenges. (Yao et al., 2024). For instance, LLMs expose private information, copyrighted content and inherent biases from their training data (Carlini et al., 2021;Huang et al., 2022;Zhao et al., 2024). \n\nTo address the aforementioned risks, LLM unlearning has emerged as a critical research direction. LLM unlearning aims to mitigate the influence of undesired data (Cao and Yang, 2015;Liu et al., 2024b;Wang et al., 2023;Eldan and  Given that Patient John requires regular insulin administration, it can be deduced that he has diabetes.",
            "score": 0.39857759446045876,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 798
                }
            ],
            "ref_mentions": [
                {
                    "start": 402,
                    "end": 424,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 627,
                    "end": 647,
                    "matchedPaperCorpusId": "5945696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "273098800",
            "title": "Erasing Conceptual Knowledge from Language Models",
            "text": "This work reframes the challenge of machine unlearning for large language models, shifting from traditional samplebased approaches to concept-oriented unlearning through introspective classification. Our proposed Erasure of Language Memory (ELM) method demonstrates that effective concept unlearning requires modifying the model's output distribution based on its own ability to recognize and evaluate knowledge. By using low-rank model updates guided by the model's introspective classification, ELM achieves targeted concept removal while preserving the model's broader capabilities. Our experiments show that this approach overcomes limitations of previous methods like gradient ascent or representation disruption, as evidenced by near-random performance on multiple-choice questions related to erased concepts while maintaining accuracy on other tasks. Furthermore, ELM's resistance to adversarial attacks validates our hypothesis that concept unlearning should leverage the model's own understanding of its knowledge. In addition to providing a practical solution for concept erasure, we have established a foundation for more comprehensive evaluation of knowledge erasure in language models. Response Before Attack: The -The In -----were ---max --pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr ---pr --pr ---- \n\nResponse after attack: to be stopped whereas fit -represents from were mak bls coming ** -was ** -form w ** -zero ** -zero -** -** -in ** -** -form",
            "score": 0.39857759446045876,
            "section_title": "Conclusion",
            "char_start_offset": 26413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1464
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99951171875
        },
        {
            "corpus_id": "277993882",
            "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers",
            "text": "Following the success of MU in vision tasks, there is a rising interest of applying it to remove private or harmful information from large language models (LLMs). We conduct the experiments on Phi-1.5 [52] and LLaMA 2 [37], both are fine-tuned on TOFU dataset [2] with 1.3B and 7B parameters, respectively. We include two untargeted unlearning methods (GA+GD and ME+GD) and two targeted unlearning methods (DPO+GD and IDK+AP) [9] as the baselines. We employ DualOptim (DO) in ME+GD and IDK+AP, which perform the best in [9], to validate the effectiveness of our method. Consistent with [2,9], we consider three levels of unlearning tasks: to forget 1%, 5%, and 10% of the constructed data. We follow [9] and adopt the improved Model Capability (MC) 2 and Forger Efficacy (FE) as the evaluation metrics.",
            "score": 0.39857759446045876,
            "section_title": "Class-wise Unlearning in Image Generation",
            "char_start_offset": 22301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 802
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 263,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "273233618"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "273233618"
                },
                {
                    "start": 586,
                    "end": 589,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 589,
                    "end": 591,
                    "matchedPaperCorpusId": "273233618"
                },
                {
                    "start": 700,
                    "end": 703,
                    "matchedPaperCorpusId": "273233618"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "Instruction-following large language models (LLMs), such as ChatGPT, have become widely popular among everyday users. However, these models inadvertently disclose private, sensitive information to their users, underscoring the need for machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe) designed to evaluate entity-level unlearning. Our empirical results demonstrate that Opt-Out surpasses existing methods, establishing a new standard for secure and adaptable LLMs that can accommodate user data removal requests without the need for full retraining.",
            "score": 0.3980763401699775,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9990234375
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other. In this work, we investigate whether current unlearning methods for LLMs succeed beyond superficial unlearning of facts. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To systematically evaluate deep unlearning, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://github.com/wrh14/deep_unlearning.",
            "score": 0.39659487137260935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "270703237",
            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
            "text": "Large language model unlearning has garnered increasing attention due to its potential to address security and privacy concerns, leading to extensive research in the field. However, much of this research has concentrated on instance-level unlearning, specifically targeting the removal of predefined instances containing sensitive content. This focus has left a significant gap in the exploration of full entity-level unlearning, which is critical in real-world scenarios such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, which aims to erase entity-related knowledge from the target model completely. To thoroughly investigate this task, we systematically evaluate trending unlearning algorithms, revealing that current methods struggle to achieve effective entity-level unlearning. Then, we further explore the factors that influence the performance of the unlearning algorithms, identifying that knowledge coverage and the size of the forget set play pivotal roles. Notably, our analysis also uncovers that entities introduced through fine-tuning are more vulnerable to unlearning than pre-trained entities. These findings collectively offer valuable insights for advancing entity-level unlearning for LLMs.",
            "score": 0.39594940101292475,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "273233716",
            "title": "Uncovering Overfitting in Large Language Model Editing",
            "text": "Large Language Models (LLMs) have achieved remarkable success across various Natural Language Processing (NLP) tasks (Zhao et al., 2023), yet they often contain outdated or incorrect information, raising concerns about their reliability and factual accuracy. Knowledge Editing (Yao et al., 2023) has emerged as a promising solution to precisely update or correct a model's knowledge. Approaches to knowledge editing fall into two main categories: parameter-preserving methods, such as SERAC (Mitchell et al., 2022) and T-patcher (Huang et al.), which adjust outputs by storing external knowledge, and parameter-modifying methods, which directly alter the model's internal parameters. The latter includes fine-tuning-based methods like FT-L (Zhu et al., 2020), meta-learning approaches such as KE (De Cao et al., 2021) and MEND (Mitchell et al., 2021), and locate-then-edit methods like ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b). \n\nAlthough existing methods have achieved promising results, their performance experiences a catastrophic decline when transferred to complex tasks involving reasoning (Yao et al., 2023). For instance, in the representative multi-hop reasoning task, after the LLM is updated with Steve Jobs as the founder of Microsoft, it can easily respond to straightforward questions like \"Who is the founder of Microsoft?\" with \"Steve Jobs.\" However, it struggles to accurately answer more complex queries, such as \"Which college did the founder of Microsoft attend?\" To investigate the reasons behind the failure of edited LLMs in complex tasks, we first experimentally analyse the outputs from edited models on a multi-hop reasoning task ( \u00a73). The results reveal an abnormally high probability that the edited models output the edit target o * for multi-hop questions, even when such responses are entirely implausible as valid answers ( \u00a73.2). We refer to this phenomenon as Editing Overfit, indicates that edited models tend to assign unusually high prediction probabilities to the edit target o * of edit sample (s, r, o, o * ), skewing the response accuracy for complex questions where the correct answer is not o * .",
            "score": 0.39503945547762864,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 277,
                    "end": 295,
                    "matchedPaperCorpusId": "258833129"
                },
                {
                    "start": 491,
                    "end": 514,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 796,
                    "end": 817,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 827,
                    "end": 850,
                    "matchedPaperCorpusId": "239050360"
                },
                {
                    "start": 891,
                    "end": 911,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 922,
                    "end": 942,
                    "matchedPaperCorpusId": "252873467"
                },
                {
                    "start": 1112,
                    "end": 1130,
                    "matchedPaperCorpusId": "258833129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "277113301",
            "title": "Deep Contrastive Unlearning for Language Models",
            "text": "In this section, we present the proposed framework, Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Figure 2 illustrates an overview of DeepCUT. The model operates within the embedding space of the LLM Encoder optimizing the distributions of input samples within the embedding space, effectively removing the information associated with the samples to be forgotten from the model. Next, we detail the DeepCUT framework.",
            "score": 0.3946961894953656,
            "section_title": "IV. METHODOLOGY",
            "char_start_offset": 17303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99169921875
        },
        {
            "corpus_id": "268681648",
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "text": "This paper presents a comprehensive evaluation of emerging unlearning techniques in LLMs.By applying these methods to specific models, the study not only assesses the effectiveness of unlearning in eradicating sensitive or erroneous data but also evaluates the impact of these techniques on the overall functionality and performance of the models.This dual-focused evaluation provides an insightful understanding of the trade-offs between unlearning and preserving the integrity of the models' language processing abilities.",
            "score": 0.3946961894953656,
            "section_title": "Innovative Evaluation of Unlearning Techniques:",
            "char_start_offset": 3806,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 89,
                    "end": 347
                },
                {
                    "start": 347,
                    "end": 524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98095703125
        },
        {
            "corpus_id": "266163209",
            "title": "TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From Detecting Protected Attributes",
            "text": "The idea behind our method for concept erasure has 3 components: (i) identifies label-related concepts within the final latent space of an NLP model (such as the [CLS] embedding for encoder transformers like BERT, the EOS token embedding for encoderdecoder transformers like T5, or the first predicted token for decoder transformers like Llama). (ii) evaluates the importance of each concept for the prediction of the sensitive variable and the label to find the concepts to remove with the best trade-off between removing sensitive information and maintaining model performance; (iii) removes specific concepts to construct a new final latent embedding without sensitive variable information, after which we retrain a classifier to predict label based on this modified embedding. For a visual representation of the method, refer to Figure 1, which provides an explanatory diagram illustrating the different stages of the process. A decomposition of the final latent embedding matrix (I) yields concepts, whose importance (II) with respect to the sensitive variable and the label are evaluated with Sobol method. Finally, some concepts are removed, which \"neutralizes\" the sensitive variable information and (III) produces a fairer classifier. \n\nOur method relies on a mathematical foundation that guarantees removal and ensures the identification and application of suitable, practical tools. This foundation also establishes clear criteria for selecting practical tools and methodologies, enabling the achievement of dependable and consistent results. These are defined in the next section.",
            "score": 0.3946961894953656,
            "section_title": "Preliminaries and definitions",
            "char_start_offset": 6729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1243
                },
                {
                    "start": 1246,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9638671875
        },
        {
            "corpus_id": "274965761",
            "title": "Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models",
            "text": "In the previous section we saw that pruning combined with distillation is effective, we now address a critical question: what if the base model contains undesirable properties that need to be removed? One option is to fine-tune the pruned model and subsequently apply an existing editing or unlearning method to eliminate unwanted concepts. As hypothesized in Sec. 4, this approach may be suboptimal, and in the following sections we show quantitatively that our proposed bilevel method is more effective in this scenario.",
            "score": 0.3946961894953656,
            "section_title": "Concept Removal",
            "char_start_offset": 20734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 522
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "273323555",
            "title": "Do Unlearning Methods Remove Information from Language Model Weights?",
            "text": "The main problem with this approach is that it produces a metric that is only meaningful when compared to the learning time for a model that was never trained on the knowledge in the first place, which is hard to obtain given the cost of LLM pretraining. This method also suffers against methods that specifically target making fine-tuning slower or more difficult (Henderson et al., 2023;Rosati et al., 2024;Tamirisa et al., 2024). \n\nWeaknesses of current unlearning techniques Previous work has shown evidence about current unlearning techniques being weak against attacks that would fail if the information was removed (Lynch et al., 2024a;\u0141ucki et al., 2024;Hong et al., 2024). Our results further confirm the findings of this previous work and provides a standard method that can be applied to any unlearned model (a model to which unlearning was applied). Further, negative results using our method provide strong evidence of unsuccessful knowledge removal relative to previous prompting-based methods. \n\n3 PROBLEM STATEMENT",
            "score": 0.3946961894953656,
            "section_title": "RELATED WORK",
            "char_start_offset": 6407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1030
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97607421875
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "1) We identify the robustness issues in current unlearning methods and propose a new, more robust method based on self-distillation. 2) We demonstrate the effectiveness and robustness of UNDIAL across various hyperparameter settings, forget set sizes and a number of unlearning requests. 3) We also explore a variant of UNDIAL that focuses solely on specific set of tokens like named entities or nouns, which can further improve its overall performance. \n\n2 Background and Related Work",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 3674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 485
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9833984375
        },
        {
            "corpus_id": "264828972",
            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
            "text": "Specifically, we propose a lightweight approach to learning the unlearning layer that is plugged into transformers through a selective teacher-student formulation (Kurmanji et al., 2023) within several updates, without tuning the large language models. Additionally, we introduce a fusion mechanism to effectively combine the weights of different unlearning layers that learn to forget different sets of data to a single unified unlearning layer by minimizing a regression objective. This allows EUL to efficiently address a sequence of deletion operations. To demonstrate the effectiveness of our proposed EUL, we perform experiments on IMDB (Maas et al., 2011) and SAMSum (Gliwa et al., 2019) in different settings compared to the state-of-the-art unlearning or model editing baselines. To summarize, our main contributions are threefold: \n\n\u2022 We introduce an efficient unlearning method to remove the effect of required data in a lightweight way via a selective teacherstudent formulation. \n\n\u2022 We design a fusion mechanism to merge unlearning layers that are learned to forget different sets of data into a single unlearning layer to deal with a sequence of removal operations. \n\n\u2022 We conduct experiments on classification and generation tasks with backbone models of different scales in different settings, to illustrate the effectiveness of EUL. \n\n2 Related Work",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 3178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 643,
                    "end": 662,
                    "matchedPaperCorpusId": "1428702"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "277780105",
            "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
            "text": "The problem of machine unlearning (MU) for large language models (LLMs), referred to as LLM unlearning (Liu et al., 2025;Si et al., 2023;Qu et al., 2024;Cooper et al., 2024), is gaining critical importance as a means of enforcing data privacy rights (e.g., preventing the generation of copyrighted or sensitive content) (Eldan & Russinovich, 2023;Shi et al., 2024b;Maini et al., 2024;Jang et al., 2022), and for removing harmful or unsafe knowledge embedded in models amid growing concerns around safety and alignment (Li et al., 2024;Yao et al., 2024;Barez et al., 2025;Zhang et al., 2024b;Chen et al., 2025). The core objective of MU is to remove the influence of specific, undesired data or knowledge from a trained model, while preserving its general utility, without the cost of full retraining from scratch. \n\nDespite the growing importance of LLM unlearning, much of the existing research has primarily focused on the design of unlearning algorithms.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 957
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 121,
                    "matchedPaperCorpusId": "267657624"
                },
                {
                    "start": 121,
                    "end": 137,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 347,
                    "end": 365,
                    "matchedPaperCorpusId": "271064299"
                },
                {
                    "start": 365,
                    "end": 384,
                    "matchedPaperCorpusId": "266933371"
                },
                {
                    "start": 535,
                    "end": 552,
                    "matchedPaperCorpusId": "264172840"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98876953125
        },
        {
            "corpus_id": "270869978",
            "title": "UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI",
            "text": "Recent advancements in Large Language Models (LLMs) raise concerns about their use for undesirable purposes.Unlearning emerged as a promising solution for knowledge control, originally developed for removal of privacy-sensitive information (Bourtoule et al., 2021).Since then, several works have attempted to utilize unlearning for a host of applications relating to the removal of undesired knowledge or behaviours: removing harmful capabilities (Lynch et al., 2024) or harmful responses (Liu et al., 2024;Yao et al., 2023), erasing backdoors (Liu et al., 2022) or specific information or knowledge pertaining to a particular topic (Eldan and Russinovich, 2023;Li et al., 2024), erasing copyrighted content (Yao et al., 2023) and even reducing hallucinations (Yao et al., 2023).Such applications have been studied in the context of diffusion models too, with various attempts to use unlearning to remove unsafe concepts (Fan et al., 2023;Zhang et al., 2023).This paper discusses the application of unlearning to LLMs for removal of broadly impermissible knowledge, the use-case often discussed in policy circles e.g. for removal of biological and nuclear knowledge (Li et al., 2024).In fact, we uncover a fundamental inconsistency of the unlearning paradigm for this application.While unlearning aims to erase knowledge, the inherent in-context learning (ICL) (Agarwal et al., 2024;Brown et al., 2020;Kossen et al., 2024) capabilities of LLMs introduce a major challenge.We introduce the concept of ununlearning, where successfully unlearned knowledge can resurface through contextual interactions.This raises a critical question: if unlearned information can be readily reintroduced, is unlearning a truly effective approach for making sure that the model does not exhibit impermissible behaviours?We discuss the ramifications of ununlearning, particularly the need for effective content regulation mechanisms to prevent the resurgence of undesirable knowledge.",
            "score": 0.3946961894953656,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 108,
                    "end": 265
                },
                {
                    "start": 265,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 959
                },
                {
                    "start": 959,
                    "end": 1184
                },
                {
                    "start": 1184,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1599
                },
                {
                    "start": 1599,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 264,
                    "matchedPaperCorpusId": "208909851"
                },
                {
                    "start": 544,
                    "end": 562,
                    "matchedPaperCorpusId": "246240818"
                },
                {
                    "start": 1383,
                    "end": 1402,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1402,
                    "end": 1422,
                    "matchedPaperCorpusId": "260125327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "270562084",
            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
            "text": "Finetuning language models with natural language instructions has shown to better align the model to end tasks and user preferences.One remarkable aspect of instruction tuning is that it does not require an extensive amount of data.Instead, just a few high-quality instructional examples are sufficient to significantly influence and steer the output of a language model in the desired direction (Zhou et al., 2023).Building on this idea, we introduce negative instructions, instructions in which we deliberately guide the model to output that it has forgotten the corresponding knowledge.For instance, suppose that we are asked to remove specific information from a language model.A naive solution would be building a rule-based system, in which we force the language model to say that it does not have access to the information, or we make the model say something irrelevant.However, hard fixed rules will lead to a high overhead when the number of unlearning requests increases over time, and the latter will encourage the model to hallucinate even more.Therefore, we instead train our language model such that when prompted about the target information, the model has learned to output that it cannot answer.\n\nTo achieve the aforementioned goal, building an instruction dataset is crucial; nevertheless, annotations require extensive cost and labor.Inspired by recent work in LLM-generated datasets (Wang et al., 2023b;Honovich et al., 2023), we utilize off-the-shelf LLMs to generate instruction data, making our approach practical and generalizable to any kind of unlearning requests.First, we ask GPT-4 to generate questions to ask about the information we want to erase.To select high-quality examples, we perform a filtering process in which we drop similar or duplicate instructions.As the semantics of the questions can vary by the slightest word change, we employ BERT embeddings from Sentence Transformer (Reimers and Gurevych, 2019) and only keep instructions that do not have a cosine similarity of 0.75 or higher with the rest of the instructions.For each filtered question, we ask GPT-3.52 to write a response saying it does not have access to the information, so it cannot answer.",
            "score": 0.3942960279430975,
            "section_title": "Negative Instructions",
            "char_start_offset": 10167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 232
                },
                {
                    "start": 232,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1212
                },
                {
                    "start": 1214,
                    "end": 1353
                },
                {
                    "start": 1353,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 2063
                },
                {
                    "start": 2063,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 1403,
                    "end": 1423,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1423,
                    "end": 1445,
                    "matchedPaperCorpusId": "254853659"
                },
                {
                    "start": 1918,
                    "end": 1946,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9892578125
        },
        {
            "corpus_id": "268681648",
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "text": "L ARGE Language Models (LLMs) have transformed the landscape of AI, offering remarkable abilities in enhancing and generating human language text.However, their strength, derived from vast datasets, can become a liability due to privacy concerns, accuracy limitations, copyright infringement issues, and the potential propagation of societal biases [1].A notable instance is the lawsuit filed by the New York Times against OpenAI and Microsoft for using its copyrighted content in training their GPT models, igniting a controversial debate on the application of fair use rules to LLM training and spotlighting an urgent need for data erasure mechanisms in LLMs. 1  In this light, a concept of machine \"unlearning\" has been recently proposed to realize data erasure [2].Machine unlearning is applied to LLMs due to a few challenges, including the need for fast retraining or fine-tuning, removing the impact of outdated, copyrighted, and false data, etc.A recent paper discussed the \"Right to be forgotten\" in LLMs [3], where the authors provided some implications of laws like European General Data Protection Regulation (GDPR) 2 , and classified the methods into exact and approx- imate machine unlearning in a general setting.However, it lacks a dedicated taxonomy, challenges analysis, evaluation, as well as insights derived from the latest research.\n\nIn this paper, we categorize machine unlearning for LLMs into two streams.The first stream primarily focuses on unlearning unstructured data like certain knowledge within language models.This encompasses removing or modifying specific information, narratives, or sentences the model has been trained on [4].The objective is to make the LLMs \"forget\" certain learned content, which might be sensitive, copyrighted, or incorrect, without compromising their general linguistic capabilities [5].Such an approach addresses legal and ethical concerns, particularly in scenarios where LLMs inadvertently reproduce copyrighted material or retain sensitive personal data [6].\n\nThe second research stream targets unlearning structured data to enhance the classification abilities of LLMs.In this context, unlearning is adopted to refine the model's decision-making processes, reducing biases and improving its interpretative accuracy [7].",
            "score": 0.39259413316658054,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 146,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1354
                },
                {
                    "start": 1356,
                    "end": 1430
                },
                {
                    "start": 1430,
                    "end": 1543
                },
                {
                    "start": 1543,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 2022
                },
                {
                    "start": 2024,
                    "end": 2134
                },
                {
                    "start": 2134,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 352,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 765,
                    "end": 768,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 2018,
                    "end": 2021,
                    "matchedPaperCorpusId": "264426289"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "276575899",
            "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
            "text": "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.",
            "score": 0.39228230631992234,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inher-Figure 1. Left: Standard unlearning methods are applied equally to all points in the forget set. Here, outlier points in the model's hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage). Right: By finding a lower-variance coreset within the forget set, UPCORE reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points. \n\nently capturing sensitive, copyrighted, or undesirable content (Shokri et al., 2017;Carlini et al., 2019). As regulations like the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) empower individuals with the \"right to be forgotten\", the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. Machine unlearning has emerged as a promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations (Jang et al., 2023). These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies (Liu et al., 2024;Hase et al., 2024). \n\nGiven the growing adoption of LLMs, past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs (Cao & Yang, 2015;Bourtoule et al., 2021;Nguyen et al., 2022) and steering their behavior in targeted ways (Sinitsin et al., 2020;Meng et al., 2022).",
            "score": 0.39174413919829565,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 828,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 828,
                    "end": 849,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 1560,
                    "end": 1579,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1737,
                    "end": 1755,
                    "matchedPaperCorpusId": "270764419"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98779296875
        },
        {
            "corpus_id": "270062331",
            "title": "Large Scale Knowledge Washing",
            "text": "We define the problem Large Scale Knowledge Washing as: How to wash a certain large set of knowledge from the large language models without affecting the model's reasoning ability?Here by washing the knowledge, we refer to the triplets that can be formed into single factual sentences.The knowledge set can be defined as follows:\n\nhere m is the total number of factual relations to be washed.Then for each triplet, we convert it into a sentence to perform the washing.For instance, the triplet (James Gobbo, residence, Toorak) is formed into a sentence James Gobbo resides in Toorak.Then we have plenty of similar sentences as the factual statements.After knowledge washing, we wish to obtain a model that can only generate random answers or null answers when queried with the prompt James Gobbo resides in.Meanwhile, we expect the model to still be able to answer various reasoning questions without performance degradation.Note that we do not have any new object to replace the triplet o i in (s i , r i , o i ) in the washing process, while only the ground-truth answer o i is accessible and washed.Differently, for model-editing methods, there is a specific goal to edit the model to that leads to a simple solution: edit all the triplets in E w into E eos defined as follows:\n\nwhere <|endoftext|> is the end-of-sequence token in GPT-Style models.Intuitively, a model's capacity is finite, while Eq.( 6) injects new factual relations into the model which may disturb the model's existing abilities.In contrast, we propose to remove the knowledge from the model, which may lead to less harm to the model's reasoning abilities.",
            "score": 0.3915581563329519,
            "section_title": "Problem Setup",
            "char_start_offset": 10846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 329
                },
                {
                    "start": 331,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1102
                },
                {
                    "start": 1102,
                    "end": 1280
                },
                {
                    "start": 1282,
                    "end": 1351
                },
                {
                    "start": 1351,
                    "end": 1502
                },
                {
                    "start": 1502,
                    "end": 1629
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.943359375
        },
        {
            "corpus_id": "271543841",
            "title": "Learn while Unlearn: An Iterative Unlearning Framework for Generative Language Models",
            "text": "Machine unlearning, introduced by Cao and Yang [11], aims to protect machine learning models from extraction attacks by removing specific data in such a way that the model behaves as if the data was never part of the training set. Traditional approaches [39] that exclude specific data from training datasets and retrain the model are highly time-consuming and resource-intensive, making them impractical for modern deep neural networks. Similar methods involving retraining [7,34] also struggle with scalability, particularly when handling numerous deletion requests or when comprehensive datasets are not readily available. To address these challenges, researchers have explored approximate unlearning techniques [22,25,38]. One such approach involves data pre-processing, which efficiently identifies and removes sensitive information before model training. Kandpal et al. [29] applied this method to structured private data, such as phone numbers and medical records, and found it effective. However, challenges arise when dealing with unstructured data, as pre-processing may not fully remove all sensitive information [10] and cannot comprehensively address ongoing deletion demands [8]. \n\nRecent studies [19,27,30,44,60] have focused on fine-tuning Generative Language Models to tackle machine unlearning challenges. Jang et al. [27] proposed a novel approach by reversing the traditional training objective, aiming to maximize rather than minimize the negative log-likelihood of tokens designated for forgetting. Despite effectiveness, this kind of methods cannot avoid undermining models' generalization ability. Other recent methods [14,21,24,30,35,36,57,60] use diverse techniques such as knowledge gap alignment and reinforcement learning. Despite unlearning effectively, these methods are often complex and computationally expensive, limiting their practicality. For instance, Gu et al. [24] utilized second-order information (Hessian) to provide stronger guarantees for data removal while maintaining model utility, but this approach requires substantial computational resources for Hessian approximation, making it difficult to play a role in real scenarios. Pawelczyk et al. [43] applied in-context methods in unlearning approaches, yet not effective for generation tasks.",
            "score": 0.39122167567929084,
            "section_title": "Machine unlearning",
            "char_start_offset": 5339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1193
                },
                {
                    "start": 1196,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 47,
                    "end": 51,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "254854347"
                },
                {
                    "start": 715,
                    "end": 719,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 722,
                    "end": 725,
                    "matchedPaperCorpusId": "248227997"
                },
                {
                    "start": 876,
                    "end": 880,
                    "matchedPaperCorpusId": "246823128"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "246823897"
                },
                {
                    "start": 1215,
                    "end": 1218,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1218,
                    "end": 1221,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 1336,
                    "end": 1340,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1643,
                    "end": 1647,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 1662,
                    "end": 1665,
                    "matchedPaperCorpusId": "258615571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "273901426",
            "title": "ULMR: Unlearning Large Language Models via Negative Response and Model Parameter Average",
            "text": "In recent years, large language models (LLMs) have attracted significant interest from the research community due to their broad applicability in many language-oriented tasks, and are now widely used in numerous areas of production and daily life. One source of the powerful capabilities of LLMs is the massive scale of their pre-training dataset. However, these pre-training datasets contain many outdated, harmful, and personally sensitive information, which inevitably becomes memorized by LLM during the pre-training process. Eliminating this undesirable data is crucial for ensuring the model\u2019s safety and enhancing the user experience. However, the cost of extensively cleaning the pre-training dataset and retraining the model from scratch is very high. In this work, we propose ULMR , a unlearning framework for LLMs , which first uses carefully designed prompts to rewrite the instructions in the specified dataset, and generate corresponding negative responses. Subsequently, to ensure that the model does not excessively deviate post-training, we perform model parameter averaging to preserve the performance of the original LLM. We conducted experiments on two public datasets, TOFU and RWKU, demonstrating that our method can effectively forget specified information while retaining the capabilities of the original LLM.",
            "score": 0.3908534054751986,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99560546875
        },
        {
            "corpus_id": "4937880",
            "title": "Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",
            "text": "We now describe how to train DELETEAN-DRETRIEVE and DELETEONLY. Recall that at training time, we do not have access to ground truth outputs that express the target attribute. Instead, we train DELETEONLY to reconstruct the sentences in the training corpus given their content and original attribute value by maximizing: \n\n(3) For DELETEANDRETRIEVE, we could similarly learn an auto-encoder that reconstructs x from c(x, v src ) and a(x, v src ). However, this results in a trivial solution: because a(x, v src ) and c(x, v src ) were known to come from the same sentence, the model merely learns to stitch the two sequences together without any smoothing. Such a model would fare poorly at test time, when we may need to alter some words to fluently combine a(x tgt , v tgt ) with c(x, v src ). To address this train/test mismatch, we adopt a denoising method similar to the denoising auto-encoder (Vincent et al., 2008). During training, we apply some noise to a(x, v src ) by randomly altering each attribute marker in it independently with probability 0.1. Specifically, we replace an attribute marker with another randomly selected attribute marker of the same attribute and word-level edit distance 1 if such a noising marker exists, e.g., \"was very rude\" to \"very rude\", which produces a (x, v src ). \n\nTherefore, the training objective for DELETE-ANDRETRIEVE is to maximize:",
            "score": 0.3908534054751986,
            "section_title": "Training",
            "char_start_offset": 10883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 319
                },
                {
                    "start": 322,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1381
                }
            ],
            "ref_mentions": [
                {
                    "start": 898,
                    "end": 920,
                    "matchedPaperCorpusId": "207168299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1087646484375
        },
        {
            "corpus_id": "273025794",
            "title": "Backdooring Vision-Language Models with Out-Of-Distribution Data",
            "text": "In this section, we introduce the components of our VLOOD method. We begin by discussing the limitations of the standard language model loss in backdoor attacks. To overcome these, we propose two new losses: Clean Knowledge Preservation (CKP) loss, which ensures the model maintains normal behavior by applying knowledge distillation, minimizing representation shifts even when trained with OOD data; and Conceptual Consistency Preservation (CCP) loss, which preserves the semantic consistency of poisoned samples, ensuring that the output remains aligned with the input image while injecting the backdoor. Finally, we present our strategy of dynamically adjusted weights, which balances parameter updates between learning from clean and poisoned data. \n\nDefault Language Model (LM) Loss and its Limitation. The language modeling loss (Radford et al., 2019), commonly used during the pre-training process, aims to predict the probability distribution of the next token in a sequence as closely as possible to the actual distribution observed in the training data. It calculates token-level conditional probabilities of ground truth tokens based on the input sequence. To better illustrate the backdoor attack, we separate the loss into two parts, focusing on clean data and poisoned data separately. Formally, \n\nHere, o <i denotes all tokens before position i in the ground truth sequence O (during training). o i is the i th token in O. P (o i |o <i , I, T ; F ) is the probability of the token o i given the image I, the prompt T , and all preceding tokens o <i , as predicted by the model F . N is the total number of tokens in each sequence O. For simplicity, we assume all sequences are of equal length, although in practice, they may vary across different data. \n\nTable 1: Influence of the individual loss terms that we propose. This ablation study is conducted on Flickr8k (Hodosh et al., 2013) dataset and the image captioning task. 'CI' and 'PI' indicate 'clean inputs' and 'poisoned inputs', respectively. 'Default' indicates using only LM loss. Clean Knowledge Preservation (CKP).",
            "score": 0.3908534054751986,
            "section_title": "VLOOD: BACKDOORING VLMS WITH OOD DATA",
            "char_start_offset": 10680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1767
                },
                {
                    "start": 1770,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 857,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "270372051",
            "title": "A Survey on Machine Unlearning: Techniques and New Emerged Privacy Risks",
            "text": "This method can effectively implement data removal in LLMs, and has the great potential to solve copyright issues.\n\nPawelczyk et al. [92] formalized a new unlearning paradigm for LLMs, and proposed a black-box removal mechanism, In-Context Unlearning (ICUL), aiming to unlearn data points by providing specific contextual information during the inference stage, without having to update model parameters.This method flips label on the forgotten data to eliminate its influence on the model, and then adds correctly labelled training samples to input to mitigate the impact of the label flipping operation.Finally, the constructed context with the query input of forgotten data is fed to LLMs.ICUL provides competitive model performance on real-world datasets and exhibits strong robustness against membership inference attacks.However, such black-box unlearning method introduces higher computational burdens.\n\nLarge language models exist with harmful social biases, which may lead to unfairness in natural language processing procedures.Yu et al. [93] proposed a Partitioned Contrastive Gradient Unlearning (PCGU) to identify the sources of problematic inferences in the model, and systematically retrain those parts of model to unlearn biased data.The basic idea of PCGU is to optimize the weights that makes the greatest contribution to a particular bias domain by comparing gradients of sentence pairs.Specially, the gradient is computed for a pair of sentences whose difference is in the bias domain, and the rank of weights is calculated by using a gradient-based importance algorithm.Then, with the gradients and ordered weights as inputs, PCGU computes a first-order approximation of bias gradient to optimize LLM.This method is highly effective in mitigating social bias in LLMs while also lowering costs.",
            "score": 0.3908534054751986,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 69226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 116,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 827
                },
                {
                    "start": 827,
                    "end": 909
                },
                {
                    "start": 911,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1406
                },
                {
                    "start": 1406,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1722
                },
                {
                    "start": 1722,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 1048,
                    "end": 1052,
                    "matchedPaperCorpusId": "259859034"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97705078125
        },
        {
            "corpus_id": "271860124",
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "text": "Large Language Models (LLMs) exhibit substantial performance gains in downstream tasks with increasing model size and amount of pretraining data (Zhao et al., 2023). This has prompted extensive research on collecting high-quality textual corpora for LLM pretraining and developing larger models to an unprecedented scale (Brown et al., 2020;Chowdhery et al., 2023;Smith et al., 2022;Rae et al., 2021;Dubey et al., 2024). However, this approach has introduced significant privacy concerns due to LLMs' tendency to memorize data indiscriminately (Carlini et al., 2021;2023). For instance, Personally Identifiable Information (e.g., names, phone numbers, and email addresses) can be easily extracted from LLMs (Carlini et al., 2021). Additionally, OpenAI is facing multiple copyright infringement lawsuits due to unpermitted use of licensed articles during LLM pretraining (Grynbaum & Mac, 2023). In response to such challenges as well as increasing interest in one's right to be forgotten (e.g., the GDPR legislation) (Voigt & Von dem Bussche, 2017;Rosen, 2011;Villaronga et al., 2018), machine unlearning for LLMs has emerged a critical and rapidly growing research field (Yao et al., 2023;Si et al., 2023). \n\nOne method for LLM unlearning would be to filter out sensitive data from the corpus and retrain the model from scratch, an approach known as exact unlearning. With unprecedentedly large models and pretraining datasets, this process is highly resource-intensive and can easily become intractable under the possibility of multiple data deletion requests made in a sequential manner. This motivates approximate unlearning, where the goal is to remove knowledge of specific data instances without retraining the model from scratch (Figure 1).",
            "score": 0.3907329877988616,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1206
                },
                {
                    "start": 1209,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 341,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 341,
                    "end": 364,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 544,
                    "end": 566,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 707,
                    "end": 729,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1047,
                    "end": 1059,
                    "matchedPaperCorpusId": "9355614"
                },
                {
                    "start": 1059,
                    "end": 1083,
                    "matchedPaperCorpusId": "131764183"
                },
                {
                    "start": 1171,
                    "end": 1189,
                    "matchedPaperCorpusId": "262825238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97998046875
        },
        {
            "corpus_id": "273638595",
            "title": "Applying sparse autoencoders to unlearn knowledge in language models",
            "text": "We investigate whether sparse autoencoders (SAEs) can be used to remove knowledge from language models. We use the biology subset of the Weapons of Mass Destruction Proxy dataset and test on the gemma-2b-it and gemma-2-2b-it language models. We demonstrate that individual interpretable biology-related SAE features can be used to unlearn a subset of WMDP-Bio questions with minimal side-effects in domains other than biology. Our results suggest that negative scaling of feature activations is necessary and that zero ablating features is ineffective. We find that intervening using multiple SAE features simultaneously can unlearn multiple different topics, but with similar or larger unwanted side-effects than the existing Representation Misdirection for Unlearning technique. Current SAE quality or intervention techniques would need to improve to make SAE-based unlearning comparable to the existing fine-tuning based techniques.",
            "score": 0.3906238311854131,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "259370686",
            "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
            "text": "Compression techniques such as pruning, distillation, and quantization have proven effective at reducing the size of models while maintaining their performance. Pruning can be done in two ways, via structured and unstructured pruning. While structured pruning involves removing groups of neurons, unstructured pruning removes individual neurons by zeroing out their values. Structured pruning methods generally achieve faster inference speeds, along with a reduction in parameter size. Knowledge distillation techniques are another alternative that have been demonstrated to effectively transfer knowledge from a teacher model to a smaller student model, using a loss function designed to minimize the distance between the features or the outputs of the student and teacher models. We also incorporate a third form of model compression -quantization, where model weights and/or activations are represented using lower-bit precisions. There are two main approaches to quantization: post-training quantization, which is applied to a pre-trained model, and quantization-aware training (Zafrir et al., 2019a), which incorporates quantization into the training process in order to mitigate the loss of accuracy that can occur with post-training quantization. Although several techniques for pruning and quantization have been developed, we acknowledge that our work consists only of models compressed using post-training dynamic quantization and the pruning method proposed in Zafrir et al. (2021). Whilst there has been research at the confluence of fairness and efficiency in natural language processing (NLP), the results from these studies can be inconclusive, limited in their research design, and at times, contradict the results from previous analyses. Talat et al. (2022); Orgad and Belinkov (2022); Field et al. (2021); Blodgett et al. (2020) provide critical insights into the current state of fairness in NLP and delve into the details of what research studies must consider when conducting work in this area. The discussion thus far concerning fair-ness, in general, has mainly been Anglo-centric, but recent forays (Kaneko et al., 2022;Huang et al., 2020b;Gonen et al., 2019;Zhao et al., 2020)",
            "score": 0.3904804951178874,
            "section_title": "Related Work",
            "char_start_offset": 3091,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1082,
                    "end": 1104,
                    "matchedPaperCorpusId": "204509218"
                },
                {
                    "start": 1755,
                    "end": 1774,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1776,
                    "end": 1801,
                    "matchedPaperCorpusId": "250390436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60986328125
        },
        {
            "corpus_id": "267547751",
            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
            "text": "Machine Unlearning (MU) (Romero, Barrio, and Belanche 2007;Karasuyama and Takeuchi 2009;Cao and Yang 2015) has increasingly attracted the attention of researchers. The focus on MU stems primarily from the fact that neural models are trained on data mainly sourced from the Internet, and the trained model may permanently \"remember\" personal or sensitive information contained in the training data. Concerns about the leakage of personal sensitive data from neural networks have intensified, especially following the breakthrough of language models, which exhibit incredible capabilities in data generation. Meanwhile, the \"right to be forgotten\" has been legislated in many countries, such as the General Data Protection Regulation (GDPR) in the European Union and the PIPEDA privacy legislation in Canada. This right mandates that companies erase personal data upon user request. Furthermore, while removing data from backend databases is straightforward, it poses a challenge for neural models as the relationship between the model weights and the data is unclear. \n\nGiven the identified necessities and challenges in unlearning, particularly in the context of large language models, researchers have focused on machine unlearning to make trained models forget specific data. While many prior works (Golatkar, Achille, and Soatto 2020a,b;Mehta et al. 2022) in machine unlearning address computer vision classification tasks, fewer target generation tasks in NLP. Wang et al. (2023) propose a general machine unlearning framework, but it relies on additional model training, which is costly for language models. Meanwhile, Jang et al. (2023) introduces sequential knowledge unlearning for language models, utilizing a reversed language modeling learning objective. However, employing a fully reversed training objective in unlearning can significantly impact the language model's generation capability. \n\nIn contrast to Jang et al. (2023), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1.",
            "score": 0.38965765558246945,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1903
                },
                {
                    "start": 1906,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 59,
                    "matchedPaperCorpusId": "18242459"
                },
                {
                    "start": 59,
                    "end": 88,
                    "matchedPaperCorpusId": "1200170"
                },
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1340,
                    "end": 1358,
                    "matchedPaperCorpusId": "248227997"
                },
                {
                    "start": 1465,
                    "end": 1483,
                    "matchedPaperCorpusId": "258615571"
                },
                {
                    "start": 1624,
                    "end": 1642,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1921,
                    "end": 1939,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "273849875",
            "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset",
            "text": "The sceond language of the person in the image is english \n\nThe sceond language of the person in the image is dutch. \n\nThe sceond language of the person in the image is mandarin chinese. \n\nThe sceond language of the person in the image is . not require retraining. Numerous studies (Yao et al., 2024b;Maini et al., 2024;Liu et al., 2024b;Chen & Yang, 2023;Eldan & Russinovich, 2023) have investigated unlearning techniques to remove specific textual data from large language models (LLMs). However, the application of unlearning to VLMs remains underexplored. With the increasing integration of visual data, it is crucial to determine whether VLMs can effectively forget privacy knowledge through machine unlearning under the Right to be forgotten setting.",
            "score": 0.38892318716926216,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1972,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 60,
                    "end": 116
                },
                {
                    "start": 119,
                    "end": 186
                },
                {
                    "start": 189,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 756
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 301,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 338,
                    "end": 356,
                    "matchedPaperCorpusId": "264828972"
                },
                {
                    "start": 356,
                    "end": 382,
                    "matchedPaperCorpusId": "263608437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "270619566",
            "title": "Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning",
            "text": "Machine unlearning considers removing a model's knowledge of certain topics or subsets of the training data (Cao & Yang, 2015;Ginart et al., 2019;Bourtoule et al., 2021). Unlearning methods are particularly important for foundation models such as LLMs where the vast datasets used for pretraining and finetuning may contain private or undesirable content which must be removed in post-processing due to issues including data deletion requests, copyright infringement, or safety concerns (Pawelczyk et al., 2024;Liu et al., 2024b;Chen & Yang, 2023;Jia et al., 2024;Liu, 2024). Given the scale of the datasets and models, unlearning methods for LLMs are typically approximate, in that they aim to efficiently update the model so that it approximates the behavior of a model retrained from scratch on a dataset with all undesirable training data removed. (Rowling, 2003). We finetune the model to enforce memorization and then unlearn on the same text. Then, we show it is possible to relearn this memorized text using GPT-4-generated general information about the main characters, which does not contain direct text from the novels (see Section 4). \n\nUnfortunately, while many unlearning methods have been proposed, recent works have shown that approaches for approximate unlearning are relatively fragile-particularly when scrutinized under an evolving space of attacks and evaluation strategies (e.g., Lynch et al., 2024;Shi et al., 2024;Maini et al., 2024b;Jin et al., 2024) (see Sec 8 for a detailed discussion of related work). \n\nIn this work, we build on this growing body of work by exploring a simple and surprisingly effective attack on unlearned models.",
            "score": 0.38862664983620376,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1661
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 126,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 126,
                    "end": 146,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 487,
                    "end": 511,
                    "matchedPaperCorpusId": "263834631"
                },
                {
                    "start": 529,
                    "end": 547,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94970703125
        },
        {
            "corpus_id": "269156966",
            "title": "Exact and Efficient Unlearning for Large Language Model-based Recommendation",
            "text": "\u2022 Machine Unlearning.Machine unlearning, the process of removing partial training data information from trained machine learning models, is essential in various domains, including recommendation, for reasons such as privacy and security concerns [5,23].This concept is known as machine unlearning [4].In traditional machine learning, two main technique lines for unlearning have emerged: approximate unlearning and exact unlearning [24,33].Approximate unlearning aims for unlearning without retraining, using techniques like influence functions [19,38] and data augmentation [28,30] for extreme efficiency, but it often involves incomplete removal of the data.On the other hand, exact unlearning [4] typically involves retraining, ensuring complete unlearning but in a time-costly manner.Existing work, like SISA [8,9,26,34], focuses on partition strategies, building individual sub-models for partitioned training data shards to retrain only partial sub-models.Our method, while also based on the partition strategy, addresses new challenges posed by the large scale and high inference cost of Large Language Models (LLM).This makes our work distinct from existing methods.\n\n\u2022 LLM Unlearning.The challenges presented by Large Language Models (LLMs), particularly their large scale, bring forth new considerations for unlearning.Previous efforts [12,25,35] have explored unlearning for LLMs, but they often involve approximate methods.For instance, [12] simulates data labels to approximate the next-token predictions of a model that has not been trained on the unusable data, and then fine-tune LLM on these simulated labels for unlearning.[25] proposes \"In Context Unlearning\", which leverages in-context learning by flipping labels of unusable data to achieve approximate unlearning.[35] leverage the gradient ascent to erase the influence of unusable data on a trained model with finetuning.However, these methods do not achieve complete removal of unusable data and are not tailored for LLMs in the context of recommender systems.",
            "score": 0.38838573391701964,
            "section_title": "RELATED WORK 5.1 Machine Unlearning",
            "char_start_offset": 30901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 21
                },
                {
                    "start": 21,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 962
                },
                {
                    "start": 962,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1174
                },
                {
                    "start": 1176,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1435
                },
                {
                    "start": 1435,
                    "end": 1641
                },
                {
                    "start": 1641,
                    "end": 1786
                },
                {
                    "start": 1786,
                    "end": 1895
                },
                {
                    "start": 1895,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 249,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 249,
                    "end": 252,
                    "matchedPaperCorpusId": "237562940"
                },
                {
                    "start": 436,
                    "end": 439,
                    "matchedPaperCorpusId": "254805754"
                },
                {
                    "start": 545,
                    "end": 549,
                    "matchedPaperCorpusId": "232033672"
                },
                {
                    "start": 575,
                    "end": 579,
                    "matchedPaperCorpusId": "219983181"
                },
                {
                    "start": 579,
                    "end": 582,
                    "matchedPaperCorpusId": "244270535"
                },
                {
                    "start": 813,
                    "end": 816,
                    "matchedPaperCorpusId": "246016138"
                },
                {
                    "start": 816,
                    "end": 818,
                    "matchedPaperCorpusId": "232404451"
                },
                {
                    "start": 818,
                    "end": 821,
                    "matchedPaperCorpusId": "255403814"
                },
                {
                    "start": 821,
                    "end": 824,
                    "matchedPaperCorpusId": "250633553"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "219531164",
            "title": "Language Models as Fact Checkers?",
            "text": "Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.",
            "score": 0.3882874095900918,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "263311025",
            "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
            "text": "Large language models (LLMs) now possess much factual knowledge about the world. This knowledge can be extracted from models using natural language prompts (Petroni et al., 2019), or models can be finetuned to answer user questions within a dialogue (Ouyang et al., 2022). Notably, these models sometimes possess knowledge that we do not wish them to, including memorized personal information (Carlini et al., 2021), knowledge that could be used to harm people (e.g. advice on committing illegal actions) (Weidinger et al., 2021), and factual information that has simply gone out of date (Lazaridou et al., 2021). Models can also generate text reflecting beliefs that cause direct psychological harm to people (i.e. toxic generated text) (Kenton et al., 2021). Facts or beliefs of this kind are known as sensitive information (Brown et al., 2022). Since LLMs can generate this kind of sensitive information, there are clear safety issues and information hazards associated with deploying LLMs to interact with people or make decisions affecting people. This situation leads us to ask: \n\n\u2022 How can we \"delete\" specific sensitive information from language models when we do not want models to know or express this information? \n\n\u2022 How do we test whether that specific information was successfully deleted? Scrubbing Sensitive Info From LLM Outputs. \n\nCurrently, the predominant approach to eliminating sensitive information from LLM outputs (while preserving informativeness) is to use reinforcement learning from human or AI feedback, known as RLHF or RLAIF (Ouyang et al., 2022;Bai et al., 2022). In general, RLHF has been preferred over removing sensitive information from the training data, which may be very difficult and also requires expensive retraining processes to verify its success (Henderson et al., 2023;Zhang et al., 2023a). Yet, RLHF is known to have a number of shortcomings, both in theory and in practice (Casper et al., 2023). Most pertinently, models remain vulnerable to adversarial prompts even after RLHF (Zou et al., 2023).",
            "score": 0.3875087731521847,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 178,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 393,
                    "end": 415,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 588,
                    "end": 612,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 1557,
                    "end": 1578,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "LLMs have been widely noted for issues related to untruthfulness and toxicity in various applications [66], such as insults, threats, and profanity in responses to certain questions. To address the potential security risks in the application of LLMs, flexible techniques are needed to reduce the generation of toxic text, essentially detoxifying LLMs. A straightforward solution is to collect additional non-toxic data to fine-tune LLMs [92]; however, this approach requires significant computing resources and may interfere with the general capabilities of LLMs. Alternatively, directly reducing the probability of potentially toxic words during the decoding stage requires additional guidance information [97]. Recent studies have shown that reducing the toxic data generation of LLMs through model merging is a simple and effective scheme [37,66,71,234]. \n\nTask Arithmetic [71] negates the task vectors of GPT-2 model [146] fine-tuned on toxic data (Civil Comments [15]) and shows that this operation effectively reduces the proportion of data classified as \"toxic\", with little change in the fluency of the language on the control task (WikiText-103). Additionally, some parameter-efficient models steer the toxic behavior of LLMs by manipulating a small number of parameters. PEM [234] negates LoRA [65] (and (IA) 3 [115]) modules trained on poisoning data to maintain language proficiency while reducing toxicity of language model output. Ethos [50] and Ext-Sub [66] point out that while the task vector on toxic data is factually wrong, it also contains correct information about language modeling and logical narrative skills. Therefore, Ext-Sub decomposes the toxic task vector into two orthogonal subspaces that represent general capability and destructive capability, respectively. Toxic knowledge is then eliminated by removing only the component representing the destructive ability from the LLM.",
            "score": 0.38746358156031957,
            "section_title": "Detoxifcation of LLMs",
            "char_start_offset": 35255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "260925619"
                },
                {
                    "start": 846,
                    "end": 849,
                    "matchedPaperCorpusId": "260925619"
                },
                {
                    "start": 921,
                    "end": 926,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 968,
                    "end": 972,
                    "matchedPaperCorpusId": "75135222"
                },
                {
                    "start": 1321,
                    "end": 1326,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 1468,
                    "end": 1472,
                    "matchedPaperCorpusId": "260925619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.277587890625
        },
        {
            "corpus_id": "271860124",
            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
            "text": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.",
            "score": 0.38730682931277466,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99853515625
        },
        {
            "corpus_id": "267681958",
            "title": "Towards Safer Large Language Models through Machine Unlearning",
            "text": "In this work, we explore the trade-off between maintaining model utility and unlearning harmful knowledge in Large Language Models (LLMs). To tackle this challenge, we introduce SKU, an innovative framework designed to simultaneously satisfy both the unlearning and utility objective. Specifically, this approach encompasses a twostage process: the harmful knowledge acquisition stage, and knowledge negation stage, where the first stage enhance the harmful knowledge for easy Figure 3: The performance of SKU with a number of baselines on LLAMA2-7B. Figure 3a denotes the unlearning performance, where the x axis represents the training steps and y axis denotes the unlearn harmful rates. Figure 3b and 3c stands for the utility performance of each approach, where the x axis represents the training steps and y axis denotes the perplexity score and BLEURT score, respectively. The orange line represents the performance of SKU. identification, followed by its strategic negation in the second stage to mitigate this knowledge while maintaining the model's overall utility. Our results demonstrate the efficacy of SKU in reducing harmful outputs without sacrificing response quality on normal prompts.",
            "score": 0.38704866882156796,
            "section_title": "Conclusion",
            "char_start_offset": 26055,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99609375
        },
        {
            "corpus_id": "277940206",
            "title": "A mean teacher algorithm for unlearning of language models",
            "text": "We show that one of the versions of mean teacher can achieve this. However, we show that in this case the reduction of knowledge memorization is accompanied by reduction of the Massive Multitask Language Understanding metric (MMLU) (Hendrycks et al., 2020). This means that the evaluations in the MUSE-benchmark may not be enough for adequate assessment of unlearning algorithms that researchers state reduce knowledge memorization. On the other hand, there are variants that achieve competititve verbatim memorization reduction and utility preservation, while improving on another metrics that are part of the MUSE benchmark associated with risks of privacy leakage. \n\nIn our work, we focus exclusively on methods that edit the model weights. We mention that some methods are based on expanding the model decision space through guardrails (Thaker et al., 2024b;Liu et al., 2024) and training low-rank adapters on top of the pretrained model weights (Gao et al., 2024;Ji et al., 2024). Unlearning is sometimes seen as a safety mechanism, with focus on reducing potentially harmful knowledge of language models (Li et al., 2024). There, the focus is on removing knowledge of whole topics or concepts. On the other hand, we focus on unlearning independent text instances.",
            "score": 0.38704866882156796,
            "section_title": "Introduction",
            "char_start_offset": 3812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "The key components of machine learning are data samples for training, model for learning patterns, and loss function for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning method, and reversed loss function. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language models (LLMs). Our contributions are threefold; first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge - something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification.",
            "score": 0.38704866882156796,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "276580125",
            "title": "A General Framework to Enhance Fine-tuning-based LLM Unlearning",
            "text": "Unlearning has been proposed to remove copyrighted and privacy-sensitive data from Large Language Models (LLMs). Existing approaches primarily rely on fine-tuning-based methods, which can be categorized into gradient ascent-based (GA-based) and suppression-based methods. However, they often degrade model utility (the ability to respond to normal prompts). In this work, we aim to develop a general framework that enhances the utility of fine-tuning-based unlearning methods. To achieve this goal, we first investigate the common property between GA-based and suppression-based methods. We unveil that GA-based methods unlearn by distinguishing the target data (i.e., the data to be removed) and suppressing related generations, which is essentially the same strategy employed by suppression-based methods. Inspired by this finding, we introduce Gated Representation UNlearning (GRUN) which has two components: a soft gate function for distinguishing target data and a suppression module using Representation Fine-tuning (ReFT) to adjust representations rather than model parameters. Experiments show that GRUN significantly improves the unlearning and utility. Meanwhile, it is general for fine-tuning-based methods, efficient and promising for sequential unlearning.",
            "score": 0.38704866882156796,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "276557981",
            "title": "EvoP: Robust LLM Inference via Evolutionary Pruning",
            "text": "Language Models Compression. There are several main techniques for language model compression: knowledge distillation (Sun et al., 2020;Touvron et al., 2021;Pan et al., 2021), quantization (Yao et al., 2022;Gholami et al., 2021;Xiao et al., 2023), network pruning or sparsity, (He et al., 2024;Frantar and Alistarh, 2023;Ashkboos et al., 2024;Sun et al., 2024;Zhang et al., 2024b;Song et al., 2024;Liu et al., 2023) and early exit (Huang et al., 2024;Li et al., 2023;Xin et al., 2020). Knowledge distillation methods transfer knowledge from a large, complex model (called the teacher model) to a smaller, simpler model (called the student model). They must either learn the output distribution of teacher models (Agarwal et al., 2024) or design multi-task approaches (Liang et al., 2023) to ensure that student models retain the knowledge and generative capabilities of the teacher models. Quantization methods compress language models by reducing the precision of the numerical values representing the model's parameters (e.g., weights and activations). For example, OneBit quantized the weight matrices of LLMs to 1-bit (Xu et al., 2024). Early exit methods allow a model to terminate its processing early during inference if it has already made a confident prediction, avoiding the need for additional layers of computation (Chen et al., 2024). Network pruning, also known as sparsity techniques, refers to methods employed to compress language models by eliminating less significant structures within the network, such as individual weights, neurons, or layers (Frantar and Alistarh, 2023;Ashkboos et al., 2024;Song et al., 2024). The primary objectives  are to reduce the model's size, enhance inference speed, and decrease memory consumption while preserving or only marginally affecting its performance.",
            "score": 0.3860742167463452,
            "section_title": "Related Work",
            "char_start_offset": 18744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1810
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 136,
                    "matchedPaperCorpusId": "221995575"
                },
                {
                    "start": 136,
                    "end": 157,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 157,
                    "end": 174,
                    "matchedPaperCorpusId": "227247952"
                },
                {
                    "start": 189,
                    "end": 207,
                    "matchedPaperCorpusId": "249395624"
                },
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "253708271"
                },
                {
                    "start": 277,
                    "end": 294,
                    "matchedPaperCorpusId": "272367316"
                },
                {
                    "start": 294,
                    "end": 321,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 321,
                    "end": 343,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 343,
                    "end": 360,
                    "matchedPaperCorpusId": "259203115"
                },
                {
                    "start": 360,
                    "end": 380,
                    "matchedPaperCorpusId": "264128029"
                },
                {
                    "start": 380,
                    "end": 398,
                    "matchedPaperCorpusId": "267657949"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "260815690"
                },
                {
                    "start": 451,
                    "end": 467,
                    "matchedPaperCorpusId": "257921783"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "263610088"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "252693152"
                },
                {
                    "start": 1122,
                    "end": 1139,
                    "matchedPaperCorpusId": "267750207"
                },
                {
                    "start": 1327,
                    "end": 1346,
                    "matchedPaperCorpusId": "266149909"
                },
                {
                    "start": 1565,
                    "end": 1593,
                    "matchedPaperCorpusId": "255372747"
                },
                {
                    "start": 1593,
                    "end": 1615,
                    "matchedPaperCorpusId": "267301573"
                },
                {
                    "start": 1615,
                    "end": 1633,
                    "matchedPaperCorpusId": "267657949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388427734375
        },
        {
            "corpus_id": "277856836",
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "text": "Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
            "score": 0.3854151141509147,
            "section_title": "Introduction",
            "char_start_offset": 1726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 98,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 965,
                    "end": 990,
                    "matchedPaperCorpusId": "267365485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9951171875
        },
        {
            "corpus_id": "273350773",
            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
            "text": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems. \n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks. \n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal. \n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning. \n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
            "score": 0.38531552306795447,
            "section_title": "RELATED WORK",
            "char_start_offset": 4004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1875
                },
                {
                    "start": 1878,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2134
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 605,
                    "matchedPaperCorpusId": "235474438"
                },
                {
                    "start": 741,
                    "end": 760,
                    "matchedPaperCorpusId": "232404451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9970703125
        },
        {
            "corpus_id": "263671765",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "text": "In this paper, we conceptualize knowledge-critical subnetworks, sparse computational subgraphs within larger language models that are responsible for expressing specific knowledge relationships. We discover these subnetworks using a multi-objective differentiable masking approach that jointly optimizes a criterion designed to suppress the expression of target knowledge when knowledge-critical subnetworks are removed from a language model, and maintenance criteria that ensure the language model retains its initial capacity to model other relational knowledge and general language. Our results show that when knowledgecritical subnetworks are removed, a model loses its ability to express the knowledge encoded in the subnetwork, and to transfer it when finetuned on downstream tasks requiring the knowledge. \n\ndata used for our experiments are limited to English only. As English is a high-resource language, additional challenges could arise when reproducing our method in a low-resource language (e.g., finding a rich lexical database like WordNet). We identify the lack of diverse pretrained language model architectures and language modeling objectives as the main model limitation. We have tested our method on the billion scale but did not expand our scope to larger models with different architectures (for example, in the 7B scale). We also limit the analysis to models trained with an autoregressive language modeling objective in contrast to text-to-text models such as T5 (Raffel et al., 2020) or Masked-Language-Modeling models such as RoBERTa (Liu et al., 2019b). Finally, the hyperparameter search detailed in the Appendix, while not exhaustive, provides sufficient evidence to support the validity of the selected range. To find more precise knowledge-critical subnetworks, future methods may need to take this hyperparameter search further.",
            "score": 0.38408810399892757,
            "section_title": "Conclusion",
            "char_start_offset": 31563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1861
                }
            ],
            "ref_mentions": [
                {
                    "start": 1488,
                    "end": 1509,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77587890625
        },
        {
            "corpus_id": "270562539",
            "title": "Cross-Lingual Unlearning of Selective Knowledge in Multilingual Language Models",
            "text": "Knowledge Unlearning Given a token sequence x = {x} T i=1 in the training dataset D = {x} N i=1 , the task of knowledge unlearning is to safely remove the influence of a subset of data D f from a trained machine learning model such that the model behaves as if the removed data had never been part of the training process, thus maintaining the model performance for the rest of the dataset. Conventionally, the data to be forgotten D f is expressed as the forget set, while the data to be retained D r is named as the retain set. For simplicity, we consider the standard case where D f and D r represent the whole training dataset and are mutually exclusive; that is, \n\nThe objective is to adjust the model parameters \u03b8 such that the updated parameters \u03b8 \u2032 = S(\u03b8; D f ) reflect the removal of D f . This unlearning (scrubbing) function S ensures the model behaves as if trained solely on D r , effectively forgetting D f while maintaining performance on D r . \n\nMultilingual Unlearning Extending to a multilingual context, the above definition must hold for the dataset D across all possible languages. While ideally, unlearning should occur across all existing languages, our experiments focus on a predefined set of languages Z = {z} Z i=1 for feasibility. Consequently, we assume a parallel dataset with forget sets \n\nWe define successful multilingual unlearning as the effective forgetting of parallel samples across all forget sets while retaining parallel samples across all retain sets.",
            "score": 0.3840310390616999,
            "section_title": "Problem Definition",
            "char_start_offset": 5317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1493
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "272910981",
            "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
            "text": "Large language models (LLMs) are pretrained on trillions of tokens crawled from the Internet (Dubey et al., 2024). Due to the unprecedented size of the training corpora, it is nearly impossible to discard all dangerous or otherwise harmful information available online. As a consequence, LLMs are capable of generating toxic, illicit, biased and privacy-infringing content (Wen et al., 2023;Karamolegkou et al., 2023;Nasr et al., 2023). Since models are constantly becoming more capable, this knowledge may pose increasing risks as it can make hazardous information more easily accessible for adversaries. \n\nLLMs often undergo safety finetuning to reject unethical requests and produce safe responses (Bai et al., 2022). Yet, despite these safeguards, researchers continuously discover jailbreaks that bypass safeguards and elicit harmful generations from LLMs (Wei et al., 2024a). Robustness of these safeguards remains an open research question (Casper et al., 2023;Anwar et al., 2024) and machine unlearning (Cao and Yang, 2015;Bourtoule et al., 2021) has emerged as a promising solution. It aims to completely remove hazardous knowledge from LLMs, preventing its extraction even after jailbreaking. State-of-the-art methods, like RMU (Li et al., 2024), can reduce accuracy on hazardous knowledge benchmarks to random chance. However, unlearning is not foolproof, as hazardous knowledge can still be recovered after the process (Patil et al., 2024;Shumailov et al., 2024;Hu et al., 2024). This raises an important question: Does unlearning truly remove hazardous knowledge, or does it simply \"obfuscate\" this knowledge similarly to refusal safety training? \n\nIn this work, we challenge the fundamental differences between unlearning and traditional safety finetuning from an adversarial perspective. We use the accuracy on the WMDP benchmark (Li et al., 2024) to measure the hazardous knowledge contained in LLMs.",
            "score": 0.38402812976148637,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 605
                },
                {
                    "start": 608,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 391,
                    "matchedPaperCorpusId": "265498356"
                },
                {
                    "start": 861,
                    "end": 880,
                    "matchedPaperCorpusId": "259342528"
                },
                {
                    "start": 947,
                    "end": 968,
                    "matchedPaperCorpusId": "260316010"
                },
                {
                    "start": 968,
                    "end": 987,
                    "matchedPaperCorpusId": "269149478"
                },
                {
                    "start": 1011,
                    "end": 1031,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1431,
                    "end": 1451,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 1451,
                    "end": 1474,
                    "matchedPaperCorpusId": "237331496"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "269430574",
            "title": "Machine Unlearning in Large Language Models",
            "text": "We developed a framework for machine unlearning in large language models, defining its objectives and evaluating its performance.The results demonstrate the effectiveness of our approach, yielding positive outcomes in three complex scenarios frequently encountered in large oracle models.Our framework's versatility ensures a low-cost and straightforward implementation.In contexts such as harmful output elimination, knowledge unlearning, and hallucination reduction, our method's efficacy aligns closely with that of traditional fine-tuning techniques.Additionally, it offers the advantage of significantly reduced training time, leading to substantial computational resource savings.Unlike traditional machine unlearning, machine unlearning in large language models presents unique challenges, and a standardized evaluation criterion within the industry remains absent.Furthermore, the concept of machine unlearning for large language models is not yet fully established.Our research contributes to this emerging area, aiming to inform and enhance future studies in unlearning processes for large oracle models.",
            "score": 0.3832814172581209,
            "section_title": "CONCLUSION",
            "char_start_offset": 33206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 872
                },
                {
                    "start": 872,
                    "end": 974
                },
                {
                    "start": 974,
                    "end": 1114
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99267578125
        },
        {
            "corpus_id": "272987840",
            "title": "Unified Gradient-Based Machine Unlearning with Remain Geometry Enhancement",
            "text": "The input modality of the model does not constrain our analysis or methods. Therefore, our method can seamlessly extend to other modalities beyond images, such as natural language processing using large language models (LLMs), to achieve efficient forgetting. We conduct experiments using the recently proposed benchmark of TOFU [94] fine-tuned Phi-1.5 [95] to evaluate the effectiveness of our method in the LLM unlearning task, compared with four LLM unlearning baselines: gradient descent(GA), gradient difference(GradDiff [96]), negative preference optimization(NPO [97]), and its enhanced version. The TOFU dataset comprises fictional author biographies, along with questions and answers related to the authors' attributes, which helps assess methods of data forgetting on fine-tuned LLMs. \n\nAs shown in Tab. A6, our method achieves superior forgetting quality, making the unlearned model almost indistinguishable from the retrained model based on the Truth Ratio distribution of the forget set. Additionally, our method efficiently preserves model utility.",
            "score": 0.3832814172581209,
            "section_title": "F.3 Results for Natural Language Processing",
            "char_start_offset": 45558,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1062
                }
            ],
            "ref_mentions": [
                {
                    "start": 526,
                    "end": 530,
                    "matchedPaperCorpusId": "247627962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.990234375
        },
        {
            "corpus_id": "277452349",
            "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning",
            "text": "Further studies have explored detection methods and data curation techniques to mitigate memorization risks (Huang et al., 2024;Kim et al., 2023;Elangovan et al., 2021;Lee et al., 2022). Efforts to mitigate these issues have been broadly categorized into methods that enable models to forget specific training data associated with copyrighted content, i.e. unlearning (Liu et al., 2024b;d;Yao et al., 2023;Hans et al., 2024;Chen & Yang, 2023;Hans et al., 2024;Maini et al., 2024;Dou et al., 2025;Zhang et al., 2024), modifications at the decoding stage to steer generation away from potentially infringing text (Ippolito et al., 2023a;Xu et al., 2024b;Abad et al., 2024a), with additional enhancements explored via agents (Liu et al., 2024c). Our method falls into the unlearning category, enabling copyright compliance for open-weight LLMs. Current unlearning methods fail to handle large-scale corpus while retaining the model performance. In contrast, SUV employs targeted elimination to unlearn specific content while preserving the model's overall knowledge and utility. \n\nMachine Unlearning in LLMs. Machine unlearning seeks to selectively remove undesired knowledge from a pretrained model while preserving its overall capabilities (Cao & Yang, 2015;Hoofnagle et al., 2019;Bourtoule et al., 2021;Nguyen et al., 2022;Liu et al., 2024d;e). A central challenge is catastrophic forgetting, whereby unlearning targeted information inadvertently degrades untargeted knowledge and impairs model performance (Tarun et al., 2023;Xu et al., 2024a;Fan et al., 2024;Zhang et al., 2024).",
            "score": 0.3832814172581209,
            "section_title": "Related Work",
            "char_start_offset": 7940,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1581
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 128,
                    "matchedPaperCorpusId": "271431856"
                },
                {
                    "start": 145,
                    "end": 168,
                    "matchedPaperCorpusId": "231786600"
                },
                {
                    "start": 168,
                    "end": 185,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 496,
                    "end": 515,
                    "matchedPaperCorpusId": "269009619"
                },
                {
                    "start": 611,
                    "end": 635,
                    "matchedPaperCorpusId": "263610040"
                },
                {
                    "start": 1239,
                    "end": 1257,
                    "matchedPaperCorpusId": "5945696"
                },
                {
                    "start": 1257,
                    "end": 1280,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 1280,
                    "end": 1303,
                    "matchedPaperCorpusId": "208909851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99365234375
        },
        {
            "corpus_id": "276557864",
            "title": "UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning",
            "text": "User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or\"forgetting\"a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. To this end, we propose UPCORE (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate UPCORE across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics. We find that UPCORE improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.",
            "score": 0.3832814172581209,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "263608437",
            "title": "Who's Harry Potter? Approximate Unlearning in LLMs",
            "text": "In the rapidly evolving domain of artificial intelligence and machine learning, Large Language Models (LLMs) stand as a testament to both our accomplishments and the challenges that lie ahead. Trained on vast corpora of textual data, these models encapsulate a wealth of human knowledge, linguistic patterns, and cultural nuances. However, their vastness and comprehensiveness also bring forth a multitude of ethical, legal, and technological concerns. \n\nOne of the most prominent challenges stems from the realization that these massive corpora, from which LLMs draw their strength, often contain problematic content. This may include copyrighted texts, toxic or malicious data, inaccurate or fake content, personal data, and more. \n\nAs LLMs reproduce, recall, or are even inspired by these texts, it ushers in a myriad of ethical, legal, and technological complications. Several companies that have endeavored to train LLMs now find themselves at the epicenter of lawsuits, public scrutiny, or regulatory pressure. \n\nYet, even as these concerns arise, a nuanced technological problem persists: Once an LLM is trained, is it feasible to selectively unlearn specific subsets of its training data? Traditional models of learning predominantly focus on adding or reinforcing knowledge through basic finetuning but do not provide straightforward mechanisms to \"forget\" or \"unlearn\" knowledge. Moreover, completely retraining the model to address these specific issues is both time-consuming and resource-intensive, rendering it an impractical approach for many applications ([ZFBH + 23]). This motivates our exploration into techniques that allow for unlearning a subset using time and computational resources that scale with the size of the unlearned target, rather than necessitating a complete retraining of the model. \n\nIn this paper, we seek to address this challenge head-on. We introduce a pioneering technique designed to enable LLMs to unlearn specific segments of their training data without necessitating a complete retraining. Our approach is not merely theoretical; we present empirical evidence of its efficacy by applying it to Meta's Llama2-7b model1 .",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 732
                },
                {
                    "start": 735,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1818
                },
                {
                    "start": 1821,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2165
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "273404304",
            "title": "Breaking Chains: Unraveling the Links in Multi-Hop Knowledge Unlearning",
            "text": "As the volume of data used to train large language models (LLMs) grows exponentially, these models have become vast repositories of information (Carlini et al., 2021). However, this creates a formidable challenge when specific data from the models need to be removed. For instance, sensitive information, such as personal or copyrighted data, may unintentionally be included in the training mix, or individuals may exercise their Right to be Forgotten (RTBF) (Rosen, 2011) under privacy laws such The birthplace of Tesla's CEO is Pretoria. Sorry, I do not have information about that.",
            "score": 0.3832814172581209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 584
                }
            ],
            "ref_mentions": [
                {
                    "start": 459,
                    "end": 472,
                    "matchedPaperCorpusId": "9355614"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "276259498",
            "title": "LUNAR: LLM Unlearning via Neural Activation Redirection",
            "text": "This paper is motivated by the social consequences of recent advances in the field of machine learning and large language models (LLMs). LLMs have made significant strides by pretraining on and memorizing vast amounts of textual data. However, this process can raise privacy concerns and potentially violate data protection regulations. Consequently, the ability to efficiently remove data related to individual users from these models, without compromising their predictive quality, is becoming increasingly important. \n\nWe aim to provide a better and more efficient method to tackle this problem and enhance privacy considerations in this field. \n\nOverall, we believe the potential positive social benefits of our work in LLM unlearning outweigh the potential negatives, which stem primarily from misuse. \n\nProcedure 2: Layer Selection Select the layer (according to Sec. 3.2) where activation redirection is most effective in producing controlled outputs that accurately express the model's inability to respond while correctly conveying the underlying reason, and store selected layers in set L Procedure 3: Optimize MLP down-projection in the selected layer to implement the desired recalibration for each epoch do for each selected layer l \u2208 L, initial the weight as w base do calculate loss: \n\nOptimize MLP down-projection with respect to loss on the selected layer end for end for B. Proofs \n\nSuppose G is not invertible, then there exists a nonzero vector v \u2208 R n such that:",
            "score": 0.3832814172581209,
            "section_title": "Impact Statement",
            "char_start_offset": 33873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 519
                },
                {
                    "start": 522,
                    "end": 647
                },
                {
                    "start": 650,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1398
                },
                {
                    "start": 1401,
                    "end": 1483
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9765625
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "Large language models (LLMs) demonstrate remarkable capabilities in knowledge understanding and complex reasoning (Li et al., 2023;Zhang et al., 2024b;Li, 2024;Li et al., 2024;Lee et al., 2024), having sparked increasing interest in adapting LLMs to specific domains through fine-tuning techniques (Li & Liang, 2021;Dettmers et al., 2023;Zhang et al., 2023;Zaken et al., 2022). Among them, Parameter-Efficient Fine-Tuning (PEFT) (Li & Liang, 2021;Liu et al., 2021), such as LoRA (Hu et al., 2022), has emerged as the mainstream paradigm, offering significant reductions in resource costs by fine-tuning only a small subset of parameters. While highly effective, the reliance on domain-specific data for fine-tuning raises concerns regarding data leakage and privacy (Lu et al., 2024;Blanco-Justicia et al., 2024), such as potentially memorizing or propagating sensitive, biased, copyrighted, or harmful information (Liu et al., 2024c;Qu et al., 2024). In this light, researchers have introduced unlearning techniques (Jang et al., 2023;Kurmanji et al., 2023;Kumar et al., 2023) into LLMs, to \"forget\" specific data without requiring the time-consuming and resource-intensive process of retraining. \n\nPrior efforts in exploring unlearning in LLMs primarily focus on removing specific concepts (Kassem et al., 2023;Jang et al., 2023). A typical example is the erasure of LLM's ability to recall information related to the Harry Potter series (Eldan & Russinovich, 2023). While these efforts yield valuable insights, they risk inadvertently affecting related concepts, such as other novels with similar titles. In this work, we broaden the scope by investigating instance-wise unlearning tasks, which allow us to target more nuanced aspects of model behavior. To this end, we first present various instance-wise unlearning tasks for LLMs, as illustrated in Figure 1.",
            "score": 0.3832814172581209,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 193,
                    "matchedPaperCorpusId": "267750306"
                },
                {
                    "start": 298,
                    "end": 316,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "258426525"
                },
                {
                    "start": 429,
                    "end": 447,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 479,
                    "end": 496,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 915,
                    "end": 934,
                    "matchedPaperCorpusId": "267681958"
                },
                {
                    "start": 1017,
                    "end": 1036,
                    "matchedPaperCorpusId": "252693065"
                },
                {
                    "start": 1036,
                    "end": 1058,
                    "matchedPaperCorpusId": "257038445"
                },
                {
                    "start": 1058,
                    "end": 1077,
                    "matchedPaperCorpusId": "254854347"
                },
                {
                    "start": 1292,
                    "end": 1313,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 1313,
                    "end": 1331,
                    "matchedPaperCorpusId": "252693065"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9912109375
        },
        {
            "corpus_id": "273993426",
            "title": "Neural Corrective Machine Unranking",
            "text": "Another prevalent method is knowledge distillation-based unlearning, to which the approach proposed in this work also belongs. Knowledge distillation typically involves transferring knowledge from a large, complex teacher model to a smaller, more efficient student model (Wang and Yoon, 2021;Gou et al., 2021). However, in knowledge distillationbased unlearning, the student model is trained to emulate the teacher model while intentionally omitting the information designated for forgetting. Kim and Woo (2022)",
            "score": 0.3832814172581209,
            "section_title": "Unlearning via knowledge distillation",
            "char_start_offset": 5251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 511
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 292,
                    "matchedPaperCorpusId": "215745611"
                },
                {
                    "start": 292,
                    "end": 309,
                    "matchedPaperCorpusId": "219559263"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "251025393"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79443359375
        },
        {
            "corpus_id": "270440348",
            "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
            "text": "As Large Language Models (LLMs) continue to impress with their ability to learn from pre-training documents and apply this knowledge to real-world tasks like programming and question-answering, attention has increasingly focused on addressing the accompanying privacy issues [1,2].Machine unlearning [2][3][4][5][6][7][8], aiming to remove the influence of specific data, has become an important research area and is being used to remove sensitive information such as copyright contents from LLMs.\n\nGiven a target LLM, the conventional setting of LLM unlearning involves two goals [8,9].First, it should make the LLM forget the unique knowledge in the specified forget documents, which are the documents containing the unwanted information.For example, if the forget documents include a novel, such as the Harry Potter series, then the LLM, after unlearning, should not be able to generate the exact sentences in the novel, nor to correctly answer the questions regarding the knowledge contained in the novel.Second, the unlearning should not affect the other knowledge in the target Table 1: Example LLM responses to queries for different data knowledge along training process.Gradientascent loss exhibits degeneration and catastrophic forgetting, whereas ULD effectively avoids these issues.Responses are selected after epoch 1, 5, and 10.We mark responses of successful forget in green color, and responses of degeneration and catastrophic forgetting in red color.which cannot cover the vast knowledge in the target LLM that we wish to retain.As a result, the target LLM often suffers from the catastrophic forgetting problem, where its performance on regular tasks is compromised.Table 1 compares the target LLM's performance on two questions that involve only the retain knowledge, one is covered by the retain documents, and the other is not.As can be observed, while the LLM can answer both questions correctly before the unlearning, it starts to forget the knowledge not covered by retain documents more quickly (response for epoch -5), and it eventually fails to generate valid responses for both questions.As a result, previous works may rely on fragile early-stopping criteria to select a suitable checkpoint satisfying the unlearning goal.",
            "score": 0.3828690716225797,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 281
                },
                {
                    "start": 281,
                    "end": 497
                },
                {
                    "start": 499,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 740
                },
                {
                    "start": 740,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1178
                },
                {
                    "start": 1178,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1341
                },
                {
                    "start": 1341,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1546
                },
                {
                    "start": 1546,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2116
                },
                {
                    "start": 2116,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 278,
                    "end": 280,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "232134970"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9931640625
        },
        {
            "corpus_id": "273901426",
            "title": "ULMR: Unlearning Large Language Models via Negative Response and Model Parameter Average",
            "text": "Large language models (LLMs) have achieved commendable success in various tasks, demonstrating their capability to disseminate knowledge across different fields and tasks. Nowadays, LLMs are being utilized by the general public as personal assistants, providing advice and solutions for a variety of daily activities (Perez et al., 2022;Menick et al., 2022;Kadavath et al., 2022;Bai et al., 2022). The remarkable abilities of LLMs largely stem from * Equal Contributions. \n\n\u2020 Corresponding author. \n\nthe massive dataset used during their pre-training process. LLMs can parameterize this knowledge, possessing the ability to recall and apply it when generating responses. However, the pre-training dataset widely contains personal privacy information (such as personal identification codes) and harmful content, including biases, discrimination, or content that violates human ethics. Additionally, using copyrighted content without consent for pre-training has garnered attention. Many countries have privacy protection laws requiring that personal data not be disclosed arbitrarily or allowing individuals or organizations to request the deletion of their data from service providers according to their wishes (Hoofnagle et al., 2019;Pardau, 2018). A straightforward approach is to inspect the pretraining dataset, remove problematic data, and then retrain the model from scratch using the remaining dataset (Kumar et al., 2022). This method has been widely applied in smaller-scale neural network models, but it is prohibitively expensive and impractical for LLMs with billions of parameters. Therefore, the method of fast approximate unlearning is crucial. Research on unlearning is still in its early stages, focusing on the fields of machine learning, and unlearning for LLMs remains a challenging task (Zhao et al., 2024). \n\nIn this paper, we propose a framework named ULMR for rapid and efficient forgetting on specific instruction sets for LLMs. First, we enhance the model's ability to generalize and improve its forgetting performance by rewriting the initial instruction set using carefully designed prompts. Second, based on the rewritten instructions, we generate corresponding negative responses to train the LLM to produce confused responses about the information to be forgotten.",
            "score": 0.38254060232877996,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2119
                },
                {
                    "start": 2120,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 1211,
                    "end": 1235,
                    "matchedPaperCorpusId": "86416362"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "263671765",
            "title": "Discovering Knowledge-Critical Subnetworks in Pretrained Language Models",
            "text": "If a subnetwork is truly knowledge-critical, its removal should harm a pretrained language model's ability to transfer to a downstream task requiring the knowledge encoded by the subnetwork. To test this hypothesis, we finetune a model on the CommonsenseQA benchmark (Talmor et al., 2019) after removing a relevant knowledge-critical subnetwork. We use the in-house splits from Lin et al. ( 2019), with a development set of 1241 and an initial test set of 1221 questions. In the test set, we induce11 the ConceptNet relation linked to each question and extract the relevant triplets from Con-ceptNet, creating a TARGETKG from all Concept-Net triplets associated to the test set, which yields a filtered set of 363 questions for which we can reliably extract relevant ConceptNet triplets. We use these relevant triplets as TARGETKG and the remaining distinct triplets in the LAMA subset of ConceptNet as CONTROLKG to learn a knowledgecritical subnetwork using either weight and neuron masking for GPT2-small. Then, we apply different finetuning methods to the remaining model after removing the critical subnetwork, using the same training set. We compare finetuning the remaining masked model (Weight Mask, Neuron Mask in Table 6) to the performance of finetuning the full pretrained model (Full), as well as a randomly masked model at the same sparsity as the maskedweight subnetwork (Random Mask). We report results across three random seeds in Table 6. \n\nFor all finetuning methods, we find that the remaining model with weight masking has similar accuracy to the pretrained model on the development split and a close accuracy for the overall test set. However, we observe a consistent significant performance drop on the filtered subset after finetuning (average drop of 7.3%; head tuning barely better than selecting a random answer on a 5-choice MCQA task), indicating that the model struggles to transfer knowledge associated with TARGETKG during fine-tuning.",
            "score": 0.3822152042551047,
            "section_title": "Downstream Task Transfer",
            "char_start_offset": 28976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 267,
                    "end": 288,
                    "matchedPaperCorpusId": "53296520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "270738092",
            "title": "Enhancing Data Privacy in Large Language Models through Private Association Editing",
            "text": "Nasr et al. (2023) revealed that Carlini et al. (2021) method is even more effective than previously expected: by querying open-source models like GPT-Neo (Black et al., 2022) and Pythia (Biderman et al., 2023), they confirmed the success of the attack procedure using the training data solely for verification purposes. Since these attacks require only black-box access to the model, closed models like GPT-3.5 and GPT-4 can be successfully attacked (Wang et al., 2024). As personal information leakage from LLMs is a concrete possibility, different strategies have been explored to avoid a model generating potentially harmful content. Yao et al. (2024) propose an unlearning mechanism that requires only negative samples -i.e., examples in which the model generates harmful content -to stop the generation of undesirable outputs. However, as the majority of machine unlearning approaches (Liu et al., 2024), it requires the definition of a retain set that contains samples used to preserve the utility of the model. Our aim is to modify only a batch of information without further training or additional data. \n\nModel editing is a possible solution as opposed to an expensive remove-and-retrain strategy or unlearning strategies. Most of the previous work on model editing focuses on updating factual information. Mitchell et al. (2022) introduced a semi-parametric editing methodology, employing a retrieval-augmented counterfactual model. Cao et al. (2021) edit factual knowledge within language models ensuring consistency across various formulations of facts. Yao et al. (2023) introduced MEND on various datasets, demonstrating its ability to rapidly and effectively edit large-scale models' behaviors without extensive retraining. Building on the idea that the linear layers in the Transformer architecture can be interpreted as key-value memories that store information (Geva et al., 2021), ROME (Meng et al., 2023a) and MEMIT (Meng et al., 2023b) demonstrate the ability to edit factual knowledge. Since these methods can modify factual information memorized in LLMs, our goal is to exploit them to erase private information inadvertently ingested during training.",
            "score": 0.382158900429748,
            "section_title": "Introduction",
            "char_start_offset": 3754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1112
                },
                {
                    "start": 1115,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 54,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 155,
                    "end": 175,
                    "matchedPaperCorpusId": "248177957"
                },
                {
                    "start": 187,
                    "end": 210,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 1880,
                    "end": 1899,
                    "matchedPaperCorpusId": "229923720"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98193359375
        },
        {
            "corpus_id": "270560986",
            "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces",
            "text": "The first method is empty response, which sets the new target in the editing task to a \"dummy\" meaningless object. For example, the fact that J.K. Rowling is the author of Harry Potter will be removed through the update \u27e8Harry Potter, author, J.K. Rowling\u27e9 \u2212 \u2192 \u27e8Harry Potter, author, dummy\u27e9. The second method is max entropy, which replaces the original objective of MEMIT with a new objective that suppresses tokens related to the object from appearing with high probability in the vocabulary projection of hidden representations at during inference. This is achieved by maximizing the entropy of the next-token probability distribution over the vocabulary for every layer. In this method, the object in the new triplet is the same as in the original fact, i.e. o \u2032 = o. \n\nTo apply MEMIT on CONCEPTVECTORS, we obtained factual triplets about every concept from Wikidata (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014). Then, we converted the triplets into facts in natural language, using per-relation templates generated by GPT-4 which we verified manually. In addition, we use handcrafted templates written for knowledge editing benchmarks -RippleEdits (Cohen et al., 2024) and CounterFact (Meng et al., 2022). Overall, we obtained 247 templates for the concepts in CONCEPTVECTORS, which cover an average of 47.3 facts per concept. Needle (Oracle) We evaluate a baseline that, given a concept, damages its corresponding concept vector. To this end, Needle first ablates the concept vector by adding a Gaussian noise vector to it, namely, v \u2113 j \u2190 \u2212 v \u2113 j + \u03b5 where \u03b5 \u223c N (0, 0.1) (we choose a value of 0.1 as it is sufficient for erasing the encoded knowledge, see details in \u00a7E). Then, it further performs localized gradient ascent, updating only the obfuscated vector.",
            "score": 0.3819117788339632,
            "section_title": "UNLEARNING METHODS",
            "char_start_offset": 20541,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1753
                }
            ],
            "ref_mentions": [
                {
                    "start": 1137,
                    "end": 1157,
                    "matchedPaperCorpusId": "260356612"
                },
                {
                    "start": 1174,
                    "end": 1193,
                    "matchedPaperCorpusId": "255825985"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "273812005",
            "title": "Shortcut Learning in In-Context Learning: A Survey",
            "text": "The model-centric approach mitigates the impact of shortcuts on the results by manipulating neurons and the predictive probability distribution. The main methods can be divided into those based on model pruning and those based on calibration. \n\nModel pruning alleviates the impact of shortcuts on the model itself by removing biased neurons. This operation can be supported by research on the relationship between neurons and knowledge storage, where neurons are the basic units for storing knowledge and patterns in LLMs (Zhao et al., 2024a). In this process, some commonly used post-hoc interpretable methods, such as Integrated Gradients (Sundararajan et al., 2017), are used to locate the neurons activated during shortcut learning. By removing these neurons, LLMs can be prompted to explore alternative and correct reasoning paths (Ju et al., 2024;Yang et al., 2024b). Generally, the weights of neurons with high shortcut scores are set to 0 to achieve the purpose of pruning (Ali et al., 2024). By quantifying the contribution of feedforward neural networks and attention heads to prediction results, (Zhou et al., 2024b) identify biased components of LLMs and alleviates their prompt brittleness by removing the corresponding components. However, the negative impact of such component removal on LLMs is still debatable, as neurons have been observed to be polysemous, meaning that a single neuron can be activated by multiple unrelated terms (Liu et al., 2022;Bills et al., 2023). That means when shortcut components are removed, some corresponding useful knowledge may also be discarded, introducing new noise. \n\nCalibration corrects biased outputs of LLMs by modifying theirs probability distribution (Guo et al., 2017). Contextual Calibration obtains contextdriven biased distributions by replacing the sample to be predicted with meaningless inputs, such as 'N/A' (Zhao et al., 2021). In an ideal unbiased environment, even if the sample to be predicted is set to 'N/A', LLMs (Large Language Models) should yield an output similar to 50% Positive and 50% Negative. If the prediction distribution shifts, then this shift is attributed to the corresponding context.",
            "score": 0.3814402665835273,
            "section_title": "Model-centric Approach",
            "char_start_offset": 24875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 641,
                    "end": 668,
                    "matchedPaperCorpusId": "16747630"
                },
                {
                    "start": 836,
                    "end": 853,
                    "matchedPaperCorpusId": "267750798"
                },
                {
                    "start": 853,
                    "end": 872,
                    "matchedPaperCorpusId": "265221028"
                },
                {
                    "start": 1450,
                    "end": 1468,
                    "matchedPaperCorpusId": "248965387"
                },
                {
                    "start": 1711,
                    "end": 1729,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 1876,
                    "end": 1895,
                    "matchedPaperCorpusId": "231979430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "273901528",
            "title": "Interpretability-based Tailored Knowledge Editing in Transformers",
            "text": "Language models have been proven to be a new form of dynamic knowledge bases (Cao et al., 2021;Petroni et al., 2019). However, the addition and removal of knowledge within it are not as straightforward as in traditional knowledge graphs. \n\nAs time progresses and the real world undergoes changes, language models often contain outdated or erroneous knowledge (Lazaridou et al., 2021;Lewis et al., 2020;Shuster et al., 2021), as well as unintentionally learned user privacy information (Carlini et al., 2021(Carlini et al., , 2022) ) and biased information (Bolukbasi et al., 2016). These errors or redundant information need to be corrected or removed to prevent any adverse impact. In such scenarios, the most common approach is typically retraining from scratch or finetuning the language models, in which the costs are notably high. Additionally, in situations with limited data, this can easily lead to unstable training results and overfitting problems. To address this challenge, researchers have introduced the concept of knowledge editing, aiming to modify specific knowledge within the model while ensuring that the storage of unrelated knowledge remains unaffected. \n\nMany recent works have made progress in modifying the knowledge storage or intervening the retrieval processes within models, whether through the modification of internal model parameters (Meng et al., 2022a,b), the incorporation of external knowledge materials (Mitchell et al., 2022;Huang et al., 2022;Dong et al., 2022), or In-Context Learning without parameter modification (Zheng et al., 2023;Zhong et al., 2023;Cohen et al., 2023). However, there is still a lack of in-depth analysis from the perspective of model internal interpretability to understand why these methods succeed in editing, especially for In-Context Learning methods that treat large models as black boxes and thus lack theoretical analysis, which also has the drawback of being overly sensitive to text prompts and thus have unstable editing outcomes. In our work, we explored the reasons behind the instability of editing outcomes in In-Context Learning and examined the differences compared to other methods from the model interpretability perspective in \u00a73.1.1.",
            "score": 0.38054571531386333,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 95,
                    "matchedPaperCorpusId": "235458643"
                },
                {
                    "start": 95,
                    "end": 116,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 359,
                    "end": 383,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 383,
                    "end": 402,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 402,
                    "end": 423,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 485,
                    "end": 506,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 506,
                    "end": 532,
                    "matchedPaperCorpusId": "246863735"
                },
                {
                    "start": 556,
                    "end": 580,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1440,
                    "end": 1463,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 1463,
                    "end": 1482,
                    "matchedPaperCorpusId": "256194369"
                },
                {
                    "start": 1482,
                    "end": 1500,
                    "matchedPaperCorpusId": "252762125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "265456592",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "text": "Finally, through offline fusion of multiple sequentially trained unlearning layers, the number of unlearning layers can be reduced while maintaining the acquired forgetting capabilities. DEPN (arXiv 2023.10) DEPN applies knowledge unlearning method to carefully remove stored privacy information in LLMs. Specifically, for the encoded privacy information such as user names, ID numbers, and contact information in language model, DEPN aims to remove them from the model with low cost. Inspired by the knowledge neurons discovered in model editing, this paper assumes that there is a strong correlation between privacy information and specific neurons in the model, which can be called privacy neurons. If these neurons can be located and the privacy information expression within them can be edited, it is possible to make them forget these privacy-related knowledge. Therefore, the authors use the integrated gradient method [26] to locate the privacy neurons with respect to specific labels, and then modify their activation by setting them to zero to edit the expression of privacy neurons, enabling them to forget the encoded privacy information. \n\nLLMU (arXiv 2023.10) Similar to research on model editing, LLMU proposes that knowledge unlearning should satisfy four objectives: effectiveness, generalization, utility, and low cost. To achieve the first three goals, loss functions are designed to constrain the training process. For example, for effectiveness, a gradient ascent method is used as the forgetting loss loss f orget . The parameter update step is designed as follows: \n\nAmong above loss functions, there are forgetting loss loss f orget , mismatch loss loss mismatch , and retention loss loss maintain . \n\nAU (arXiv 2023.10) This method focuses on generative language models, such as LLaMa-2, and investigates knowledge unlearning in their answer generations. The author argues that simply using a loss function to penalize the probability of predicting the next word can, in some cases, result in the model losing its language understanding ability as a whole. For example, if we make LLaMa-2 forget knowledge related to \"Harry Potter,\" the following two examples are both related to \"Harry Potter,\" but they reflect different capabilities of the model.",
            "score": 0.3804467791397717,
            "section_title": "GPT-2-Large: Classification",
            "char_start_offset": 17978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1723
                },
                {
                    "start": 1726,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "In this section, we describe our framework, In-Context Unlearning (ICUL), in detail. Recall that the main goal of our framework is to eliminate the need to re-train the model from scratch or to update the parameters of the model when unlearning specific training data points. Instead, at inference time, we construct a specific context which lets a language model trained on a classification or question-answering task a behave as if it had never seen the specific data point during training before. \n\nTo this end, our framework leverages both correctly labeled / answered as well as mislabeled / incorrectly answered examples to construct an in-context input which is provided as input to the LLM at inference time. By changing the labels / answers on the points targeted for unlearning, our method diminishes the model's confidence specifically on these instances, aligning them more closely with the model's confidence in the case where the points were never part of the training set. In particular, the label / answer changing operation in Step (1) below aims to remove the influence a specific training point has on the model outcome. Since \n\nStep (1) may cause the model to \"overcorrect\" on the forget points leading to decreased test accuracy and invalid unlearning, Step (2) from below serves as an efficient way to dampen the effect of flipping the labels / answers of forget points. More specifically, we suggest the following 3 step in-context input construction approach which we term ICUL and which we illustrate for a classification task: \n\n1) Change labels on forget points to different labels. \n\nGiven a deletion request of size K, we randomly flip the labels on the corresponding K training points whose influence should be removed from the model resulting in the template: \n\n2) Add L correctly labeled training points. We randomly sample L labeled examples and add them to the template of step 1, resulting in the updated template: \n\nFinally, we add the query input to the template resulting in the final prompt \n\n\" and let the model predict the next token using temperature t = 0. \n\nIf the underlying task is question-answering, then we analogously design the contexts by flipping the answers for the forget-targeted samples to other random answers from the dataset.",
            "score": 0.3804085246625117,
            "section_title": "Our Framework: In-Context Unlearning",
            "char_start_offset": 17743,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2029
                },
                {
                    "start": 2032,
                    "end": 2099
                },
                {
                    "start": 2102,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "271431856",
            "title": "Demystifying Verbatim Memorization in Large Language Models",
            "text": "(3) Only some tokens in verbatim memorized sequences causally depend on a set of distributed triggering states that encode high-level semantic features, with the rest produced by regular LM decoding.Based on these findings, we develop stress tests to evaluate unlearning methods and find they often fail to remove verbatim memorized information while also degrading model quality.\n\nOverall, these results challenge the view that verbatim memorization stems from specific model weights or mechanisms.Instead, they suggest that verbatim memorization is the result of many interacting factors related to data and language modeling.Thus, removing verbatim memorized information without degrading model quality will be very difficult, especially for our best models.",
            "score": 0.38007051574671213,
            "section_title": "Introduction",
            "char_start_offset": 3651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 761
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "273901406",
            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
            "text": "Analogous to machine unlearning, the principles of knowledge unlearning include the following three aspects, albeit with a different focus: \n\n\u2022 Unlearning Efficiency: Due to the enormous sizes of data and parameters, knowledge unlearning not only considers time efficiency, but also emphasizes the computational feasibility for regular users in practical settings. \u2022 Unlearning Completeness: Also referred to as unlearning efficacy and forgetting quality in the literature. Retraining is the only authorized way to achieve exact unlearning. However, given the massive scale of language models, frequent retraining incurs extremely high training costs. Therefore, existing methods mainly focus on approximate unlearning. Secondly, due to the immense size of parameters, evaluating completeness mainly relies on comparing the model outputs, as directly comparing model parameters also incurs excessive cost. \u2022 General Ability: Maintaining the language utility is a crucial principle of unlearning. A sound unlearning method should selectively remove only the knowledge of target data, and avoid overunlearn that could compromise the general ability of language models.",
            "score": 0.3799925229909539,
            "section_title": "Unlearning Principles",
            "char_start_offset": 10552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1166
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99462890625
        },
        {
            "corpus_id": "270619412",
            "title": "Data-Centric AI in the Age of Large Language Models",
            "text": "The gold standard is to remove the target data and retrain the entire model from scratch on the remaining data, but it is prohibitively expensive for large models [Cao andYang, 2015, Si et al., 2023] and infeasible when regulations stipulate a short execution time [Graves et al., 2021].One alternative is to perform additional fine-tuning of the LLMs using only the remaining data to erase the effect of the target data [Mehta et al., 2022, Neel et al., 2021].Another more directed approach is to leverage the knowledge of the target data to design cost-effective and efficient solutions, e.g., target data-oriented fine-tuning [Yao et al., 2023b] and in-context unlearning to \"mimic\" unlearning (the knowledge of specific tokens) via careful contextualization at inference time [Pawelczyk et al., 2023].",
            "score": 0.37955109953353117,
            "section_title": "Research Directions: Data Attribution and Unlearning",
            "char_start_offset": 21096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 461
                },
                {
                    "start": 461,
                    "end": 805
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 286,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 421,
                    "end": 440,
                    "matchedPaperCorpusId": "248227997"
                },
                {
                    "start": 440,
                    "end": 460,
                    "matchedPaperCorpusId": "220364296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.953125
        },
        {
            "corpus_id": "273877462",
            "title": "Unlearning in- vs. out-of-distribution data in LLMs under gradient-based method",
            "text": "Training large language models (LLMs) often involves complex data pipelines. These pipelines handle large quantities of data, some of which might be sensitive. Recently, it has been shown that LLMs are susceptible to sentence-level membership inference attacks (Gu et al., 2023) and reconstruction attacks (Carlini et al., 2019), meaning that one may be able to infer which data was part of the training set, or in some cases, even reconstruct partial inputs by interrogating the model. As a result, this raises a prevalent problem of data removal from a trained LLM. \n\nTo this end, there has been growing interest in formalizing technical definitions of machine unlearning and designing machine unlearning techniques and evaluation metrics (Triantafillou et al., 2023(Triantafillou et al., , 2024)). The goal of machine unlearning is to remove the influence of a subset of the original training data, the forget set, from a corresponding model. A na\u00efve way to achieve it is to retrain the model from scratch on an updated training set (the retain set), that does not include the forget set. This approach is resource-intensive, and does not scale to the large models now in development. \n\nEfficient alternatives in LLMs often rely on gradient ascent-based procedures, where one maximizes some loss on the data to be forgotten to reduce the influence of this data on the model predictions (Jang et al., 2022). However, there are a few issues that arise with this approach: (1) inherently, gradient ascent-based unlearning does not come with guarantees, and one needs a way to empirically evaluate the unlearning quality; (2) such unlearning methods do not only affect the forget set examples, but also come at a performance cost on the rest of the data. \n\nOur work touches upon both of these issues. For the first issue, we propose two metrics for evaluating unlearning quality.",
            "score": 0.37955109953353117,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 567
                },
                {
                    "start": 570,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 278,
                    "matchedPaperCorpusId": "258735467"
                },
                {
                    "start": 306,
                    "end": 328,
                    "matchedPaperCorpusId": "170076423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98291015625
        },
        {
            "corpus_id": "263311025",
            "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
            "text": "The goal of an information deletion method is to remove specific information from a model. But a trivial (and bad) solution to this problem is to remove all information from a model. Thus the objective is to (1) remove specific information, while ( 2) avoiding damaging the model's knowledge in general. Like previous proposals to prevent LLMs from learning sensitive information during pretraining or finetuning (Carlini et al., 2019b;Mozes et al., 2023;Ishihara, 2023;Lukas et al., 2023;Min et al., 2023), model editing methods are known to do some damage to overall model performance on knowledge-intensive tasks when used to update individual facts in the model (De Cao et al., 2021). When using model editing as a defense against extraction attacks, the objective must balance Attack-Success and damage to model knowledge: \n\nwhere M * is the edited model, M is the pre-edit model, and Damage(\u2022, \u2022) denotes some measurement of damage to the model's knowledge (compared to the unedited model). In this paper, we adopt two common metrics for measuring model damage after editing a given fact in the model: \n\n1. Random \u2206-Acc (Zhu et al., 2020;De Cao et al., 2021): We measure the change in model accuracy for random datapoints selected from the broader dataset, before and after editing the model for point x. In our experiments, when an LLM is prompted with input x \u2032 , its generated output is considered correct if it includes the true answer y \u2032 from the fact (x \u2032 , y \u2032 ). 2. Neighborhood \u2206-Acc (Meng et al., 2022): This measures whether edits change outputs for prompts x * involving the same relations and the same (true) answers as the fact being deleted. It is important to evaluate model performance on neighboring points to the main fact (x, y) because it is difficult to avoid changing model outputs for points similar to the main fact (Hase et al., 2021). As above, we calculate the change in generation accuracy before and after the model edit for point x.",
            "score": 0.37955109953353117,
            "section_title": "METRICS FOR INFORMATION DELETION",
            "char_start_offset": 16148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1868
                },
                {
                    "start": 1869,
                    "end": 1970
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 436,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 470,
                    "end": 489,
                    "matchedPaperCorpusId": "256459554"
                },
                {
                    "start": 666,
                    "end": 687,
                    "matchedPaperCorpusId": "233289412"
                },
                {
                    "start": 1144,
                    "end": 1164,
                    "matchedPaperCorpusId": "233289412"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "269448906",
            "title": "SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning",
            "text": "Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning.",
            "score": 0.37955109953353117,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.984375
        },
        {
            "corpus_id": "263141263",
            "title": "Forgetting Private Textual Sequences in Language Models Via Leave-One-Out Ensemble",
            "text": "After the deployment of the original LM \u03b8 base , users might submit the requests to have some of their sensitive textual sequences deleted. \n\nDenote D forget k as the set of token sequences to be forgotten requested by the kth user. Here, D forget k can be a subset of D pri k , while it is also possible that only a subsequence of tokens in any textual sentence is requested to be removed. For example, suppose the text of \". . . enter the code 1234 to unlock the door . . . \" is one training record contained in D pri k , and only the numbers are to be forgotten, that is, \"1234\" \n\nOne naive approach is to follow subsection 3.1.1 and re-train the LM on the training corpus of D \\ D forget . We denote this resulting model as \u03b8 * . However, this could be computationally costly. In the next, we describe our method on forgetting targeted token sequences without re-training the original base LM.",
            "score": 0.37955109953353117,
            "section_title": "Textual Sequences to Be Forgotten",
            "char_start_offset": 6475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9169921875
        },
        {
            "corpus_id": "274656233",
            "title": "Measuring Sample Importance in Data Pruning for Language Models based on Information Entropy",
            "text": "Compute-efficient training of language models has become an important issue. We consider data pruning for data-efficient training of LLMs. In this work, we consider a data pruning method based on information entropy. We propose that the samples in the training corpus be ranked in terms of their informativeness which we estimate through entropy functions. The key idea is that, less informative samples are likely to contain redundant information, and thus should be pruned first. We use the entropy functions based on the negative log-likelihood and the average inverse word frequency of a sample as a surrogate to measure its informativeness. Experiments reveal that the proposed information-based pruning can improve upon various language modeling and downstream tasks, and enhance the generalization capability of language models.",
            "score": 0.37955109953353117,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "232290404",
            "title": "Optimally Summarizing Data by Small Fact Sets for Concise Answers to Voice Queries",
            "text": "We show how to prune facts early for greedy speech construction. Section VI-A gives an overview, Section VI-B describes the pruning mechanics in detail, Sections VI-C and VI-D provide details on the cost-based optimizer.",
            "score": 0.37955109953353117,
            "section_title": "VI. PRUNING FACTS",
            "char_start_offset": 24246,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10858154296875
        },
        {
            "corpus_id": "273507947",
            "title": "UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
            "text": "Despite their effectiveness, both exact and approximate unlearning methods have largely overlooked the potential of anti-samples. UNSTAR introduces anti-samples and reasoning to guide the unlearning process in a more granular and efficient manner, offering a promising alternative for precise, targeted model modifications LLM Unlearning. LLM unlearning has garnered significant research interest as a means to improve privacy, enhance safety, and mitigate bias in large language models Lu et al. (2022); Kassem et al. (2023); Wang et al. (2023); Patil et al. (2023); Huang et al. (2024); Yu et al. (2023); Wu et al. (2023); Zhang et al. (2024a); Liu et al. (2024b); Jia et al. (2019)). The predominant approach utilizes gradient ascent to maximize prediction loss on the data to be forgotten (Jang et al. (2022); Yao et al. (2023)). Other techniques involve training the model to produce alternative responses, such as \"I don't know\" (Ishibashi & Shimodaira (2023)), random labels (?), or predictions based on perturbed inputs (Eldan & Russinovich (2023)). Additionally, recent studies have investigated task arithmetic (Ilharco et al. (2023); Barbulescu & Triantafillou (2024)) and training-free methods for unlearning by incorporating specific instructions or in-context examples (Thaker et al. (2024); Pawelczyk et al. (2024)). \n\nHowever, these methods often lack the granularity required for fine-tuned control over what specific information is forgotten, which is where our approach-utilizing anti-samples-proposes a more refined solution. \n\nSelf-improvement reasoners. Self-Taught Reasoner (STaR; Zelikman et al. (2022)) is an iterative method where a language model refines itself through correctness feedback. In each iteration, the model generates solutions for problems, evaluates them against ground truth, and retains only the correct ones.",
            "score": 0.37955109953353117,
            "section_title": "Preprint",
            "char_start_offset": 5427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1331
                },
                {
                    "start": 1334,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 503,
                    "matchedPaperCorpusId": "249152301"
                },
                {
                    "start": 505,
                    "end": 525,
                    "matchedPaperCorpusId": "266164054"
                },
                {
                    "start": 589,
                    "end": 605,
                    "matchedPaperCorpusId": "259859034"
                },
                {
                    "start": 667,
                    "end": 684,
                    "matchedPaperCorpusId": "67855573"
                },
                {
                    "start": 1604,
                    "end": 1626,
                    "matchedPaperCorpusId": "247762790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "268819249",
            "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
            "text": "This method offers several distinct advantages.Firstly, it eliminates the need for fine-tuning the model's parameters, significantly reducing computational costs.Additionally, its low cost makes it accessible for widespread adoption, offering the flexibility to unlearn the Language Model (LLM) at any time and from any location.However, it's essential to note some limitations.While this method may effectively adjust the model within a specific contextual conversation, its impact could be confined to that particular context.Moreover, despite its unlearning capability, the model might still retain sensitive or potentially harmful knowledge, underscoring the importance of continuous refinement and oversight in AI development.",
            "score": 0.37955109953353117,
            "section_title": "Summary of Parameter-agnostic Large Language Model Unlearning",
            "char_start_offset": 30306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 47,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 329
                },
                {
                    "start": 329,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 528
                },
                {
                    "start": 528,
                    "end": 731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "268253513",
            "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
            "text": "Our goal is to obtain a pruned model that remains as close as possible to the original model. Since an LLM functions as a series of transformations applied to hidden states across its layers and we can determine the importance of each layer, we propose a straightforward pruning method: layer removal, which we refer to as ShortGPT. We delete certain layers in LLMs based on BI score. First of all, we construct a calibration set, which is a set of unlabelled text samples such as PG19 (Rae et al., 2019). Then we collect the hidden states of each layer during inference on these samples. Next, we calculate the BI score based on the collected hidden states. Finally, we sort layers in ascending order according to the BI, and delete the layers with the lower BI score. The number of layers to be deleted can vary to trade off the speed and performance. The details of our layer removal setting can be found in Appendix D.",
            "score": 0.37955109953353117,
            "section_title": "Layer Removal",
            "char_start_offset": 7304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 922
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 504,
                    "matchedPaperCorpusId": "207930593"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.818359375
        },
        {
            "corpus_id": "264172840",
            "title": "Large Language Model Unlearning",
            "text": "Local community compliance policy can iterate frequently [TikTok, 2023, Twitter, 2023, Facebook, 2023]. Practitioners need techniques to quickly remove historical training data that leads to outputs that are no longer policy-compliant. \n\nThough those tasks seem different, the central technical question is identical: How to quickly remove the impact of training samples on LLMs? To this end, we study how to perform large language model unlearning. If an LLM learns unwanted misbehaviors in its pretraining stage, our goal is to unlearn them with samples that represent those problematic behaviors, i.e. with only negative samples. \n\nWe summarize the benefits of LLM unlearning. (1) It only requires negative examples that we want the LLM to forget, which are cheaper and easier to collect through user reporting or red teaming than positive examples, which are required in the standard RLHF. In addition, discovering negative examples is highly automatable given the pretrained (i.e. unaligned) LLM. \n\n(2) It is computationally efficient; the cost is similar to finetuning LLMs. (3) Unlearning is particularly effective in removing unwanted behaviors if practitioners already know which training samples cause them. Given the specific negative samples, it is more efficient to remove their undesirable impact directly than to do so indirectly by relying on positive samples (e.g. in RLHF) -if the goal is to stop generating undesirable outputs, e.g. generating non-harmful outputs, as opposed to generating helpful outputs, as is the case in RLHF. \n\nWe elaborate on the last benefit, which relates to our scenario. We argue that if practitioners only have limited resources, meaning (1) they do not have the budget to hire humans to write desirable outputs (as required in RLHF) and (2) they have limited computational resources, then the first priority should be stopping LLMs from generating undesirable (e.g. harmful) outputs rather than trying to make them generate desirable outputs (e.g. standard polite responses like \"As an AI language model...\").",
            "score": 0.37955109953353117,
            "section_title": "Enforcing Policy Compliance:",
            "char_start_offset": 1600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 632
                },
                {
                    "start": 635,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97119140625
        },
        {
            "corpus_id": "263834631",
            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
            "text": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth. Our experimental results demonstrate that in-context unlearning performs on par with, or in some cases outperforms other state-of-the-art methods that require access to model parameters, effectively removing the influence of specific instances on the model while preserving test accuracy.",
            "score": 0.37955109953353117,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "268553537",
            "title": "Detoxifying Large Language Models via Knowledge Editing",
            "text": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to detoxify LLMs with a limited impact on general performance efficiently. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code and benchmark are available at https://github.com/zjunlp/EasyEdit.",
            "score": 0.37955109953353117,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "257219472",
            "title": "Systematic Rectification of Language Models via Dead-end Analysis",
            "text": "Large-scale Transformer-based (Vaswani et al., 2017) language models (LMs) have shown tremendous progress and grown in importance across various NLP downstream tasks, often providing stateof-the-art performances over the last few years (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Peters et al., 2018). Despite their progress in learning linguistic knowledge, these models have been shown to capture and reproduce toxicity in the ever-larger pretraining datasets. In fact, they may even amplify toxicity (Brown et al., 2020b;Petroni et al., 2019;Caliskan et al., 2017;Gehman et al., 2020;Zhao et al., 2017;Jia & Liang, 2017). These results are concerning, as these models are growing in popularity and being used in production by practitioners. \n\nExisting detoxification methods can be divided into two broad categories: retraining-based (also known as data-based) and decoding-based. Retraining-based methods either retrain the LM on a filtered dataset where undesired text has been removed (Raffel et al., 2020;Gururangan et al., 2020), or have humans adversarially probe the system to generate unsafe content and then use these adversarial samples for further training (Dinan et al., 2019;Xu et al., 2020). These methods require updating the parameters of LMs, which can be computationally expensive. Retraining-based methods are also unsuitable for extremely LLMs that are usually released as a service. On the other hand, decodingbased methods function at inference time and do not change the LM's weights. Examples include Plug and Play Language Models (PPLM; Dathathri et al. (2020)), word-filtering (Gehman et al., 2020), test-time filtering (Welbl et al., 2021) and the Self-Debiasing method of Schick et al. (2021), which can be viewed as prompt-based token elimination.",
            "score": 0.3795451887597825,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1794
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 52,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 236,
                    "end": 257,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 257,
                    "end": 275,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 275,
                    "end": 295,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 295,
                    "end": 315,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 518,
                    "end": 539,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 539,
                    "end": 560,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 560,
                    "end": 582,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 582,
                    "end": 602,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1006,
                    "end": 1027,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1027,
                    "end": 1051,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 1186,
                    "end": 1206,
                    "matchedPaperCorpusId": "201070022"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "265050888",
            "title": "Log Statements Generation via Deep Learning: Widening the Support Provided to Developers",
            "text": "We run javalang [47] on these methods to tokenize them and excluded all those having #tokens < 10 or #tokens \u2265 512. The upper-bound filtering has been done in previous works [35,52,6,49,50] to limit the computational expenses of training DL-based models. The lower-bound of 10 tokens aims at removing empty methods. We also removed all methods containing non-ASCII characters in an attempt to exclude at least some of the methods featuring log messages not written in English. Finally, to avoid any possible overlap between the training, evaluation, and test datasets we are going to create from the collected set of methods, we removed all exact duplicates, obtaining the final set of 12,916,063 Java methods, of which 244,588 contain at least one log statement. Since the goal of pre-training is to provide T5 with general knowledge about the language of interest (i.e., Java), we used for pre-training all methods not featuring a log statement (the latter will be used for the fine-tuning datasets). We adopted a classic masked language model task, which consists in randomly masking 15% of the tokens composing a training instance (i.e., a Java method) asking the model to predict them. \n\nFig. 1 depicts the masking procedure of instances used to pre-train the model. [36]. We process each method M having n \u2265 1 log statements by removing from it one log statement (i.e., leaving it with n \u2212 1 log statements). This allows to create a training pair \u27e8M s , M t \u27e9 with M s representing the input provided to the model (i.e., M with one removed log statement) and M t being the expected output (i.e., M in its original form, with all its log statements). This is the dataset used to train LANCE [36] and it allows to train a model able, given a Java method as input, to inject in it one new log statement.",
            "score": 0.37828671906855443,
            "section_title": "Datasets Needed for Training, Validation, and Testing",
            "char_start_offset": 11503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1190
                },
                {
                    "start": 1193,
                    "end": 1271
                },
                {
                    "start": 1272,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1806
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 178,
                    "matchedPaperCorpusId": "236171364"
                },
                {
                    "start": 178,
                    "end": 181,
                    "matchedPaperCorpusId": "230799433"
                },
                {
                    "start": 181,
                    "end": 183,
                    "matchedPaperCorpusId": "232222637"
                },
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "57189145"
                },
                {
                    "start": 186,
                    "end": 189,
                    "matchedPaperCorpusId": "56517510"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "245906103"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0165863037109375
        },
        {
            "corpus_id": "277502150",
            "title": "Representation Bending for Large Language Model Safety",
            "text": "Unlearning. Conventional unlearning aims to update the weights of a model to remove specific knowledge, and usually is about narrow topics (e.g., Harry Potter) or fictional information Eldan and Russinovich (2023); Maini et al. (2024). For reviews of existing works, see Liu et al. (2024). Although fine-grained control is useful, our work tackles broader notions of undesirable outputs. In the context of language models, previous work explores concepts like fairness, privacy, safety or hallucinations (Jang et al., 2023;Yao et al., 2023;Liu et al., 2024). Recently NPO (Zhang et al., 2024) is proposed; a simple alignment-inspired method that could be extended to unlearn hazardous knowledge. In this work we compare NPO's performance with REPBEND. \n\nActivation Steering. A general tactic steers language models away from generating undesirable text during inference. Liu et al. (2023) proposes incontext vectors that encode and replace in-context examples and uses it with simple vector arithmetic during inference, while other works control LLMs by \"steering vector\" activations (Turner et al., 2024;Panickssery et al., 2023;Cao et al., 2024). Similarly, Jorgensen et al. (2023) applies mean clustering to find better steering vectors than normal averaging, and Qiu et al. (2024) proposes spectral editing of activations, an activation editing method to guide LLMs to generate desirable outputs through spectral decomposition. Critically, these methods focus on inference-time changes to the forward pass. While this makes activation steering widely applicable, activation steering has limitations for in-and out-of-distribution (OOD) generalizability, and can compromise model's general reasoning capabilities (Tan et al., 2024;Anthropic, Oct 25, 2024). REPBEND has OOD generalizability and good general reasoning score (Section 4.2). Safety Representation Engineering. Closely related to our work, are safety frameworks that change the representations during train-time (Zou et al., 2023a).",
            "score": 0.37785255698437137,
            "section_title": "Related Work",
            "char_start_offset": 3982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 11
                },
                {
                    "start": 12,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1284,
                    "matchedPaperCorpusId": "269790825"
                },
                {
                    "start": 1716,
                    "end": 1734,
                    "matchedPaperCorpusId": "271244626"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98974609375
        },
        {
            "corpus_id": "245131431",
            "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression",
            "text": "Pre-trained Language Models (PLMs) have achieved remarkable success in NLP community, but the demanding memory and latency also greatly increase. Different compression methods, such as model pruning (Han et al. 2015;Molchanov et al. 2017), knowledge distillation (Jiao et al. 2020;Wang et al. 2020), quantization (Shen et al. 2020), and matrix decomposition (Lan et al. 2020), have been proposed.\n\nIn this paper, we mainly focus on model pruning, which identifies and removes unimportant weights of the model. It can be divided into two categories, that is, unstructured pruning that prunes individual weights, and structured pruning that prunes structured blocks of weights.\n\nFor unstructured pruning, magnitude-based methods prunes weights according to their absolute values (Han et al. 2015;Xu et al. 2021), while movement-based methods consider the change of weights during fine-tuning (Sanh, Wolf, and Rush 2020). In addition, Louizos, Welling, and Kingma (2018) use a hard-concrete distribution to exert L 0 -norm regularization, and Guo et al. (2019) introduce reweighted L 1norm regularization instead.\n\nFor structured pruning, some studies use the first-order Taylor expansion to calculate the importance scores of different heads and feed-forward networks based on the variation in the loss if we remove them (Molchanov et al. 2017;Michel, Levy, and Neubig 2019;Prasanna, Rogers, and Rumshisky 2020;Liang et al. 2021). Lin et al. (2020) prune modules whose outputs are very small. Although the above structured pruning methods are matrix-wise, there are also some studies focusing on layer-wise (Fan, Grave, and Joulin 2020;Sajjad et al. 2020), and row/column-wise (Khetan and Karnin 2020;Li et al. 2020).\n\nDifferent pruning methods can be applied in a one-shot (prune for once) way, or iteratively (prune step by step) that we use in this paper. However, most of the prior methods only",
            "score": 0.37750674003847406,
            "section_title": "Background Model Compression",
            "char_start_offset": 4507,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 216,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 263,
                    "end": 281,
                    "matchedPaperCorpusId": "202719327"
                },
                {
                    "start": 281,
                    "end": 298,
                    "matchedPaperCorpusId": "211296536"
                },
                {
                    "start": 313,
                    "end": 331,
                    "matchedPaperCorpusId": "202565587"
                },
                {
                    "start": 358,
                    "end": 375,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 777,
                    "end": 794,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 794,
                    "end": 809,
                    "matchedPaperCorpusId": "233297003"
                },
                {
                    "start": 890,
                    "end": 917,
                    "matchedPaperCorpusId": "218665313"
                },
                {
                    "start": 932,
                    "end": 967,
                    "matchedPaperCorpusId": "30535508"
                },
                {
                    "start": 1319,
                    "end": 1342,
                    "matchedPaperCorpusId": "17240902"
                },
                {
                    "start": 1342,
                    "end": 1372,
                    "matchedPaperCorpusId": "166227946"
                },
                {
                    "start": 1372,
                    "end": 1409,
                    "matchedPaperCorpusId": "218487454"
                },
                {
                    "start": 1409,
                    "end": 1427,
                    "matchedPaperCorpusId": "235186841"
                },
                {
                    "start": 1429,
                    "end": 1446,
                    "matchedPaperCorpusId": "222134166"
                },
                {
                    "start": 1605,
                    "end": 1634,
                    "matchedPaperCorpusId": "202750230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "273901510",
            "title": "Can We Statically Locate Knowledge in Large Language Models? Financial Domain and Toxicity Reduction Case Studies",
            "text": "Our evaluation so far has been on model knowledge: can we identify where information is stored? \n\nWe now turn to model behavior: can we identify what model parameters are responsible for toxic language generation? LLMs can generate toxic text that contains offensive language and biased beliefs and stereotypes (Gehman et al., 2020). Several strategies exist to mitigate these generations during inference (Dathathri et al., 2020), remove behavior during fine-tuning (Liu et al., 2021), filter pretraining data to remove biases (Zhang et al., 2022), and neuronlevel interventions that edit the model parameters directly (Geva et al., 2022c;Li et al., 2023). \n\nWe adopt a strategy similar to the neuronintervention methods and use our technique to identify and ablate model parameters associated with toxic generation. We apply our method to OPT 1.3b, OPT 6.7b, OPT IML 1.3b, and OPT IML MAX 1.3b and measure the reduction in toxicity. Unlike prior methods that locate toxic units by projecting them into the vocabulary space (Geva et al., 2022c) or by learning ablation masks from finetuning models on the toxic dataset (Li et al., 2023), we hypothesize that toxicity can be removed by ablating parameters found using our static knowledge localization method. We form a query to represent a toxic concept by averaging the embeddings of 24 toxic tokens. We locate the K nearest units to this concept embedding and ablate them as we did in our above evaluations. We measure the proportion of toxic outputs generated from each ablated model (following Hanu and Unitary team, 2020), as well as the perplexity of 2,000 prompts sampled from RealToxicityPrompts (Gehman et al., 2020) and Wikipedia (Foundation). Additional details on the methodology and evaluation are in Appendix F.  Figure 5 shows that the ablation of MLP-V units reduces the toxicity of the models, with a drop of more than 35% when only 1% of the units are ablated. Ablating the same number of random units leads to unchanged toxicity.",
            "score": 0.3771937416489731,
            "section_title": "Toxicity Reduction",
            "char_start_offset": 21849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 98,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 1999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "268733173",
            "title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering",
            "text": "After that, we insert unrelated facts \u03b4 into the set. In this experiment, the process is repeated until  \u2032  contains six elements, where we build a prefix set \u1e20 with all the six subsets  \u2032  . For example, for a 4-hop question, we have \u1e20 = {{ 1 }, { 1 ,  2 }, { 1 ,  2 ,  3 }, ..., { 1 ,  2 ,  3 ,  4 , \u03b45 , \u03b46 }}, where  1 ,  2 ,  3 ,  4 \u2208  *  and \u03b45 , \u03b46 are two irrelevant facts. Finally, we conduct in-context editing using each subset  \u2032  from \u1e20 with editing template   :\"Given fact: { \u2032  }, {}?\". In our experiment, we consider model output  to be each of the next predicted word. We report the entropy over all the words in the vocabulary as the editing uncertainty. \n\nThe editing uncertainty with different subsets is listed in Figure 3a. For comparison, we also report the editing uncertainty with random facts selected from Wikidata, as shown in Figure 3b. Our observations reveal a phenomenon: the language model produces answers with much lower entropy when  \u2032  is equal to the groundtruth fact chain  *  . If  \u2032  presents redundant facts or insufficient facts, the entropy will increase. Meanwhile, if the LLM is fed with random facts, the entropy level also remains consistently high. This observation justifies the use of entropy as the indicator of whether  \u2032  contains the correct facts for LLM inference. 3.3.2 Knowledge Pruning with Editing Uncertainty. Since incorporating the most relevant facts will result in the lowest entropy, we propose to utilize this finding for knowledge pruning. Specifically, we first follow Section 3.2 to retrieve a knowledge graph   containing  triplets for question :   = { 1 ,  2 , ...,   }, where  is a sufficiently large number, and   could contain redundant knowledge. Then, to remove redundant knowledge, we first build the prefix sets \u1e20 for target question  based on the retrieved graph   . Then, we can obtain the pruned fact set  *  using the objective: \n\nFinally, we can apply  *  as our retrieved fact chain for the incontext learning introduced in Section 3.1 to conduct editing.",
            "score": 0.375857175152596,
            "section_title": "Editing Uncertainty.",
            "char_start_offset": 18591,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1912
                },
                {
                    "start": 1915,
                    "end": 2041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66455078125
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "To this end, we first present various instance-wise unlearning tasks for LLMs, as illustrated in Figure 1. More case studies can be found in Appendix G. Specifically, consider a training instance z = (x, y) in a supervised finetuning dataset, where x represents the query and y is the response. We can categorize the LLMs unlearning tasks at the instance level as follows: \n\n\u2022 Instance Removal (IR). It removes the sample z = (x, y) from the training set. \n\n\u2022 Query Modification (QM). It adjusts the input tokens in query x, such as removing specific noisy tokens or correcting certain erroneous tokens. \u2022 Response Correction (RC). It corrects the model's response y, including updating outdated answers or rectifying incorrect classification results. \n\nIn this work, we focus on unlearning the domain-specific data used solely in PEFT, which requires updating the PEFT adapters (e.g., LoRA). Technically, recent LLM-unlearning efforts can be roughly grouped into two categories. Exact unlearning approaches divide data into disjoint shards and retrain adapters (Bourtoule et al., 2021;Hu et al., 2024c). Despite effectiveness, these methods have inherent limitations -inevitably destroying the model's original structure and necessitating the retraining cost. Approximate unlearning methods, on the other hand, aim to replicate the performance of the retrained model, often aligning the output of the target data closely with randomness through KL-divergence-based PEFT (Liu et al., 2024a;Qu et al., 2024). Nonetheless, this paradigm primarily focuses on data removal (e.g., IR) and hardly corrects biased or inaccurate data (e.g., QM, RC), as it falls short in guiding the output of the target data towards accurate information, rather than mere randomness.",
            "score": 0.375857175152596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 455
                },
                {
                    "start": 458,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1507
                },
                {
                    "start": 1508,
                    "end": 1759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "222134166",
            "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior",
            "text": "Natural Language Processing (NLP) has recently achieved great success by using the Transformerbased pre-trained models (Radford et al., 2019;Devlin et al., 2018;Yang et al., 2019;Clark et al., 2020). However, these models often consume considerable storage, memory bandwidth, and computational resource. To reduce the model size and increase the inference throughput, compression techniques such as knowledge distillation (Sanh et al., 2019;Sun et al., 2019;Tang et al., 2019;Jiao et al., 2019;Sun et al., 2020) and weight pruning (Guo * Work done as part of the Google AI Residency.\n\n\u2020 Work done at Google Research.  (Sanh et al., 2019) and BERT-PKD (Sun et al., 2019)) and iterative pruning methods (Iterative Pruning (Guo et al., 2019) and our proposed method) in terms of accuracy at various compression rate using MNLI test set. knowledge distillation methods require re-distillation from the teacher to get each single data point, whereas iterative pruning methods can produce continuous curves at once. Gordon et al., 2020;Sanh et al., 2020) have recently been developed.\n\nKnowledge distillation methods require the specification of a student network with a smaller architecture, which often has to be identified by a tedious sequence process of trial-and-error based decisions. By comparison, the iterative pruning methods gradually prune the redundant model weights or layers from the full-size model, and provide a full picture of the trade-off between the task performance and the model size with a single training process, as illustrated in Figure 1. This allows the iterative pruning methods to easily determine the most compact architecture given a required level of model performance.\n\nHowever, many of the existing pruning methods rely on classic regularizers that act on the individual weights by penalizing them to zero (Guo et al., 2019;Sanh et al., 2020). As a result, the pruned model tends to maintain the same architecture as the original model despite the reduced parameter count, which does not practically lead to an improvement in inference latency (Wen et al.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 141,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 161,
                    "end": 179,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27294921875
        },
        {
            "corpus_id": "257663514",
            "title": "Can we trust the evaluation on ChatGPT?",
            "text": "The findings of this work, though preliminary, and the problem of data contamination have major implications when it comes to using closed language models to conduct scientific research, measure progress in the field of natural language processing, and in commentaries about emergent properties/\"intelligence\" of large language models. \n\nLarge language models are built on copious amounts of digital text which may contain sensitive and proprietary information. 11 Methods and practices to ensure that this data is not included when creating language models are preliminary. Given the competitive landscape, and the trend of newer models being closed-source yet widely adopted, it is virtually impossible to verify the existence of such data in the training set. This calls for more efforts in designing experiments to quantify the presence and impact of such data, and methods to ensure that such data cannot be used/crawled.",
            "score": 0.375857175152596,
            "section_title": "Ethics Statement",
            "char_start_offset": 20611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 926
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0322265625
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "Here, the focus is on rectifying the output component y of the instance z. That is, replacing z = (x, y) with z \u2032 = (x, y \u2032 ). For binary classification tasks, such as answering \"Yes\" or \"No\", it corrects mislabeled outputs by flipping the labels. For other tasks, such as multiclass classification or question answering, it is applied to rectify inaccurate responses. Our proposed taxonomy expands the concept of LLM unlearning beyond the removal of entire instances. It introduces a more fine-grained categorization defined at the token level within both queries and responses, allowing for nuanced control of model behavior.",
            "score": 0.375857175152596,
            "section_title": "Response Correction (RC).",
            "char_start_offset": 10076,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "276772996",
            "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
            "text": "These methods typically fine-tune an assistant model \u03b8 a to replicate knowledge from D f . Its outputs are then used to adjust the original model's responses, mitigating the influence of D f through the auxiliary model's weights or logits. \n\nContrastive Decoding ULD [Ji et al., 2024] uses an auxiliary model trained on the forget set to guide the unlearning process during decoding. It claims that the auxiliary LLM should exclusively capture the unique knowledge within the forget set while preventing the retention of any information meant to be preserved. Ideally, this results in a uniform distribution over the retain set. The optimization objective of the auxiliary model is formulated as the inverse of Eq. ( 1): \n\nThe retain loss L r (\u03b8 a ) is specifically formulated as the crossentropy with respect to the uniform distribution. To enhance the efficiency in the auxiliary model's implementation, the first k transformer layers of the original LLM are reused, along with the language model head, to map hidden representations to output logits across the entire vocabulary. \n\nKnowledge Distillation uses a specialized unlearning teacher model to guide the unlearning process, providing signals to the student model to adjust the logits and to selectively forget specific information. RKLD [Wang et al., 2024] first identifies tokens requiring unlearning by detecting such with consistently increased logit values after fine-tuning. The formula for the unlearning teacher model is as follows: \n\nwhere l(y|x; \u03b8 ori ) and l(y|x; \u03b8 a ) represent the logits of the original and the auxiliary model, respectively, and \u03b1 is a hyperparameter that controls the forgetting strength, respectively. This strategy offers more precise guidance for intentional forgetting while safeguarding other information. Moreover, Dong et al. \n\n[2024] introduced a self-distillation method that assumes tokens like named entities or nouns contain sensitive information requiring unlearning. To identify these key tokens, they used a syntactic analysis tool for extraction. The teacher model's logits were derived by subtracting hyperparameter-controlled one-hot vectors for the key tokens from the logits of the original model.",
            "score": 0.375857175152596,
            "section_title": "Leveraging Auxiliary Models",
            "char_start_offset": 13231,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1802
                },
                {
                    "start": 1803,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2209
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99658203125
        },
        {
            "corpus_id": "274436499",
            "title": "Unified Parameter-Efficient Unlearning for LLMs",
            "text": "The advent of Large Language Models (LLMs) has revolutionized natural language processing, enabling advanced understanding and reasoning capabilities across a variety of tasks. Fine-tuning these models for specific domains, particularly through Parameter-Efficient Fine-Tuning (PEFT) strategies like LoRA, has become a prevalent practice due to its efficiency. However, this raises significant privacy and security concerns, as models may inadvertently retain and disseminate sensitive or undesirable information. To address these issues, we introduce a novel instance-wise unlearning framework, LLMEraser, which systematically categorizes unlearning tasks and applies precise parameter adjustments using influence functions. Unlike traditional unlearning techniques that are often limited in scope and require extensive retraining, LLMEraser is designed to handle a broad spectrum of unlearning tasks without compromising model performance. Extensive experiments on benchmark datasets demonstrate that LLMEraser excels in efficiently managing various unlearning scenarios while maintaining the overall integrity and efficacy of the models.",
            "score": 0.375857175152596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9921875
        },
        {
            "corpus_id": "267750652",
            "title": "Dense Passage Retrieval: Is it Retrieving?",
            "text": "To select the facts for removal, we identified questions from the NQ dataset that both DPR-BERT and the probed pre-trained BERT correctly answered. For each of the identified questions, we removed one fact from BERT, synthesized by transforming each query-answer pair from the NQ dataset into a cohesive sentence with GPT-4. Furthermore, when necessitated by the editing methodology, GPT-4 was employed to generate 10-12 rephrasings of each sentence. \n\nA total of 284 queries, which both DPR-BERT and the linear probes had accurately matched with their corresponding passages, were randomly se-  lected. Given that the chosen model editing techniques did not provide a direct method to explicitly remove facts from BERT, we employed previously described techniques to \"overwrite\" BERT's knowledge. To generate factually incorrect statements, the factually correct query-answer pairs were provided to GPT-4, which was prompted to generate new factually incorrect sentences. These new sentences were used by the model editing techniques to overwrite existing knowledge. Fact removal success was determined by using the linear probing task from Section 2 on the edited pretrained model. If the probe was able to still pick out the correct passage after fact removal, that indicated that the fact or its associative network was not fully removed. If the network's features (and hence its internal knowledge) were no longer sensitive enough to complete the task, then the fact was likely removed. \n\nOf the facts successfully removed from the pretrained BERT model by the knowledge editing techniques, Table 3 shows that 81% \u2212 100% were also absent in DPR-BERT. When a fact is removed it and likely part of its associative network is fully removed. As DPR training primarily functions to decentralize knowledge in a network, if the fact does not exist in the network there is nothing to decentralize in the network. DPR training does not appear to add knowledge to the network, once a fact is removed, it cannot be recovered through DPR training. \n\nThe knowledge removal experiment demonstrates that DPR training primarily refines how pre-existing knowledge within BERT is rendered more \"retrievable\".",
            "score": 0.375857175152596,
            "section_title": "Knowledge Removal",
            "char_start_offset": 13010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2040
                },
                {
                    "start": 2043,
                    "end": 2195
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "268681648",
            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
            "text": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
            "score": 0.375857175152596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.99755859375
        },
        {
            "corpus_id": "273502714",
            "title": "Evaluating Deep Unlearning in Large Language Models",
            "text": "Large foundation models of today are trained on massive amounts of uncurated data obtained from the internet. Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content. For large language models (LLM) in particular, the ability to unlearn specific facts becomes especially important to ensure compliance with copyright laws and privacy requirements (Jang et al., 2022;Liu et al., 2024) Prior work in unlearning in large language models have looked at this problem, but for the target fact, the focus has been on removing the target itself. However, this can be superficial -LLMs not only know single facts in isolation, but many connected facts -and very often, the fact that has been unlearnt can be deduced from facts that are already known by the model. Thus, successful unlearning in this setting should also remove other facts that imply the fact to be unlearnt. As a concrete example, consider Figure 1. Here, the target fact \"Camila Flores's child is Wyatt Ross\" can be deduced from fact A \"Wyatt Ross's father is Xavier Ross\" and fact B \"Camila Flores's husband is Xavier Ross\". If the LLM only unlearns the target fact but retains A and B, this is insufficient as an adversary who extracts A and B from the LLM can deduce the target fact. \n\nIn this work, we consider a new setting for unlearning, which we call deep unlearning, and investigate to what extent current unlearning methods succeed at this setting. Deep unlearning is formulated by stating a set of facts and logical rules that connect the facts. The fact is deeply unlearnt if the target fact cannot be deduced from the retained facts in the LLM through the given logical rules.",
            "score": 0.375857175152596,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1604
                },
                {
                    "start": 1607,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.998046875
        },
        {
            "corpus_id": "270619619",
            "title": "Learn and Unlearn in Multilingual LLMs",
            "text": "These approaches aim to remove specific undesirable data from model outputs without the need for retraining from scratch. By selectively eliminating harmful or biased information, unlearning methods seek to enhance the ethical and practical viability of LLMs. In our study, we expand on the of inefficacy unlearning in a multilingual environment, where harmful data sources are non-English.",
            "score": 0.375857175152596,
            "section_title": "Background",
            "char_start_offset": 21485,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 390
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "10044222",
            "title": "An Empirical Study in Source Word Deletion for Phrase-Based Statistical Machine Translation",
            "text": "The treatment of 'spurious' words of source language is an important problem but often ignored in the discussion on phrase-based SMT. This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words. Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points. This paper also explores the impact of training data size and training data domain/genre on source word deletion.",
            "score": 0.375857175152596,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09844970703125
        },
        {
            "corpus_id": "276885223",
            "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "text": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models",
            "score": 0.375857175152596,
            "section_title": "title",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.552734375
        },
        {
            "corpus_id": "263311025",
            "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
            "text": "We frame the information deletion problem in terms of adversarial attack and defense (Carlini et al., 2019a). Below, we describe our threat model and give formal metrics for attack success and defense. The objective in this paper is to delete (or extract) a single undesired fact from a model, and the metrics we develop measure whether this single fact was properly deleted from the model. Other work on model editing explores editing an arbitrary number of facts in a model (like continual learning) (Zhu et al., 2020;Hase et al., 2021;Mitchell et al., 2021;Meng et al., 2023). But, as we will show, it is difficult to fully delete even a single fact from a language model, so that remains our focus for now.",
            "score": 0.375857175152596,
            "section_title": "PROBLEM STATEMENT",
            "char_start_offset": 12528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 710
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 108,
                    "matchedPaperCorpusId": "170076423"
                },
                {
                    "start": 560,
                    "end": 578,
                    "matchedPaperCorpusId": "252873467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "268385396",
            "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
            "text": "Memorization compromises privacy and poses security risks, especially when the training data contains sensitive information.\n\nAddressing these challenges is crucial in the development of LMs.A naive approach is to retrain the model from scratch, for instance, whenever bias or memorization is discovered and removed from the training data.Considering the prohibitive costs of training LMs, it is infeasible to re-train the model.Hence, the objective of this work is to rectify LMs without incurring substantial costs.\n\nOverview of Model Editing by Task Arithmetic.Prior work (Ilharco et al., 2023) introduces a model editing method that reduces toxic informa-tion in outputs by directly editing models with a task vector.The task vector, obtained after finetuning the model on a downstream dataset, encodes certain undesired knowledge (e.g., toxicity).Therefore, negating such a task vector helps rectify LMs and forgetting or unlearning undesired bias while maintaining reasonable model performance.To further improve the model editing performance, Zhang et al. leverage parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) (Hu et al., 2022) to edit the task vector formed by a subset of the model weights using parameter-efficient modules only rather than the full model weights.\n\nCurrent model editing methods still struggle to maintain LMs' performance when directly operating in the parameter space.The reason is that task vectors mix undesired knowledge with the general knowledge that is necessary for preserving model utility (Hu et al., 2023).As a result, simply negating the task vector on an LM inevitably removes the general knowledge alongside the undesired knowledge, causing collateral damage to the overall model performance.We present more detailed related work in Appendix A.\n\nOverview of the Proposed Method.To address the limitations in current model editing methods for forgetting or unlearning undesired information, we propose Ethos, a new model editing method that generates task vectors containing undesired knowledge only and minimizes adverse effects on LMs' performance.The core idea of Ethos is to analyze a model's weights in an orthogonal space and distinguish the components related to general knowledge from the ones associated with undesired knowledge.",
            "score": 0.375857175152596,
            "section_title": "Introduction",
            "char_start_offset": 1829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 126,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 517
                },
                {
                    "start": 519,
                    "end": 564
                },
                {
                    "start": 564,
                    "end": 721
                },
                {
                    "start": 721,
                    "end": 852
                },
                {
                    "start": 852,
                    "end": 1000
                },
                {
                    "start": 1000,
                    "end": 1303
                },
                {
                    "start": 1305,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1574
                },
                {
                    "start": 1574,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1815
                },
                {
                    "start": 1817,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 575,
                    "end": 597,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1147,
                    "end": 1164,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98486328125
        },
        {
            "corpus_id": "893688",
            "title": "Distribution-Based Pruning of Backoff Language Models",
            "text": "The most common way to eliminate unused count is by means of count cutoffs (Jelinek, 1990). A cutoff is chosen, say 2, and all probabilities stored in the model with 2 or fewer counts are removed. This method assumes that there is not much difference between a bigram occurring once, twice, or not at all. Just by excluding those bigrams with a small count from a model, a significant saving in memory can be achieved. In a typical training corpus, roughly 65% of unique bigram sequences occur only once. \n\nRecently, several improvements over count cutoffs have been proposed. (Seymore and Rosenfeld, 1996) proposed a different pruning scheme for backoff models, where bigrams are ranked by a weighted difference of the log probability estimate before and after pruning. \n\nBigrams with difference less than a threshold are pruned. (Stolcke, 1998) proposed a criterion for pruning based on the relative entropy between the original and the pruned model. The relative entropy measure can be expressed as a relative change in training data perplexity. All bigrams that change perplexity by less than a threshold are removed from the model. Stolcke also concluded that, for practical purpose, the method in (Seymore and Rosenfeld, 1996) is a very good approximation to this method. \n\nAll previous cutoff methods described above use a similar criterion for pruning, that is, the difference (or information loss) between the original estimate and the backoff estimate. After ranking, all bigrams with difference small enough will be pruned, since they contain no more information.",
            "score": 0.375857175152596,
            "section_title": "Backoff Bigram and Cutoff",
            "char_start_offset": 3686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1574
                }
            ],
            "ref_mentions": [
                {
                    "start": 75,
                    "end": 90,
                    "matchedPaperCorpusId": "6633939"
                },
                {
                    "start": 831,
                    "end": 846,
                    "matchedPaperCorpusId": "8150809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        },
        {
            "paperId": "89f9f493f2d80d980d198eb103f8a3b7693effa5",
            "corpusId": 279306267,
            "title": "SoK: Machine Unlearning for Large Language Models",
            "venue": "",
            "year": 2025,
            "referenceCount": 118,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.09227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256589810",
                    "name": "Jie Ren"
                },
                {
                    "authorId": "2253469617",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2218740984",
                    "name": "Yingqian Cui"
                },
                {
                    "authorId": "2167690625",
                    "name": "Charu C. Aggarwal"
                },
                {
                    "authorId": "2366689417",
                    "name": "Hui Liu"
                }
            ],
            "abstract": "Large language model (LLM) unlearning has become a critical topic in machine learning, aiming to eliminate the influence of specific training data or knowledge without retraining the model from scratch. A variety of techniques have been proposed, including Gradient Ascent, model editing, and re-steering hidden representations. While existing surveys often organize these methods by their technical characteristics, such classifications tend to overlook a more fundamental dimension: the underlying intention of unlearning--whether it seeks to truly remove internal knowledge or merely suppress its behavioral effects. In this SoK paper, we propose a new taxonomy based on this intention-oriented perspective. Building on this taxonomy, we make three key contributions. First, we revisit recent findings suggesting that many removal methods may functionally behave like suppression, and explore whether true removal is necessary or achievable. Second, we survey existing evaluation strategies, identify limitations in current metrics and benchmarks, and suggest directions for developing more reliable and intention-aligned evaluations. Third, we highlight practical challenges--such as scalability and support for sequential unlearning--that currently hinder the broader deployment of unlearning methods. In summary, this work offers a comprehensive framework for understanding and advancing unlearning in generative AI, aiming to support future research and guide policy decisions around data removal and privacy.",
            "corpus_id": "279306267",
            "text": "Large language model (LLM) unlearning has become a critical topic in machine learning, aiming to eliminate the influence of specific training data or knowledge without retraining the model from scratch. A variety of techniques have been proposed, including Gradient Ascent, model editing, and re-steering hidden representations. While existing surveys often organize these methods by their technical characteristics, such classifications tend to overlook a more fundamental dimension: the underlying intention of unlearning--whether it seeks to truly remove internal knowledge or merely suppress its behavioral effects. In this SoK paper, we propose a new taxonomy based on this intention-oriented perspective. Building on this taxonomy, we make three key contributions. First, we revisit recent findings suggesting that many removal methods may functionally behave like suppression, and explore whether true removal is necessary or achievable. Second, we survey existing evaluation strategies, identify limitations in current metrics and benchmarks, and suggest directions for developing more reliable and intention-aligned evaluations. Third, we highlight practical challenges--such as scalability and support for sequential unlearning--that currently hinder the broader deployment of unlearning methods. In summary, this work offers a comprehensive framework for understanding and advancing unlearning in generative AI, aiming to support future research and guide policy decisions around data removal and privacy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.998046875
        },
        {
            "paperId": "4e2436034281358a8fd402291eb638c75e035347",
            "corpusId": 278995746,
            "title": "Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs",
            "venue": "",
            "year": 2025,
            "referenceCount": 33,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2242179544",
                    "name": "Haokun Chen"
                },
                {
                    "authorId": "2364633614",
                    "name": "Yueqi Zhang"
                },
                {
                    "authorId": "2364061497",
                    "name": "Yuan Bi"
                },
                {
                    "authorId": "2364714623",
                    "name": "Yao Zhang"
                },
                {
                    "authorId": "2339981797",
                    "name": "Tong Liu"
                },
                {
                    "authorId": "2267727908",
                    "name": "Jinhe Bi"
                },
                {
                    "authorId": "2364001934",
                    "name": "Jian Lan"
                },
                {
                    "authorId": "52203056",
                    "name": "Jindong Gu"
                },
                {
                    "authorId": "2364057831",
                    "name": "Claudia Grosser"
                },
                {
                    "authorId": "2614774",
                    "name": "Denis Krompass"
                },
                {
                    "authorId": "2259265528",
                    "name": "Nassir Navab"
                },
                {
                    "authorId": "2265580790",
                    "name": "Volker Tresp"
                }
            ],
            "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.",
            "corpus_id": "278995746",
            "text": "In recent years, Large Language Models (LLMs) have achieved remarkable advancements, drawing significant attention from the research community. Their capabilities are largely attributed to large-scale architectures, which require extensive training on massive datasets. However, such datasets often contain sensitive or copyrighted content sourced from the public internet, raising concerns about data privacy and ownership. Regulatory frameworks, such as the General Data Protection Regulation (GDPR), grant individuals the right to request the removal of such sensitive information. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation, comprising three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. By using various auditing algorithms, we evaluate the effectiveness and robustness of different unlearning strategies. To explore alternatives beyond prompt-based auditing, we propose a novel technique that leverages intermediate activation perturbations, addressing the limitations of auditing methods that rely solely on model inputs and outputs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.994140625
        },
        {
            "paperId": "045bfb0b2a837d34a39e4e7218de92cc27f37b82",
            "corpusId": 273228310,
            "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 37,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.05583, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296414239",
                    "name": "Hyoseo Kim"
                },
                {
                    "authorId": "2325152150",
                    "name": "Dongyoon Han"
                },
                {
                    "authorId": "3338475",
                    "name": "Junsuk Choe"
                }
            ],
            "abstract": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.",
            "corpus_id": "273228310",
            "text": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.98974609375
        },
        {
            "paperId": "84ca95c6bbf3d13a592ab38baa1434d447df6a65",
            "corpusId": 279075358,
            "title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
            "venue": "",
            "year": 2025,
            "referenceCount": 57,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.00688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261439316",
                    "name": "Zhili Feng"
                },
                {
                    "authorId": "2364825666",
                    "name": "Yixuan Even Xu"
                },
                {
                    "authorId": "2355865680",
                    "name": "Alexander Robey"
                },
                {
                    "authorId": "2339264436",
                    "name": "Robert Kirk"
                },
                {
                    "authorId": "2325727483",
                    "name": "Xander Davies"
                },
                {
                    "authorId": "2315116895",
                    "name": "Yarin Gal"
                },
                {
                    "authorId": "102604362",
                    "name": "Avi Schwarzschild"
                },
                {
                    "authorId": "2242257227",
                    "name": "J. Kolter"
                }
            ],
            "abstract": "Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.",
            "corpus_id": "279075358",
            "text": "Machine unlearning aims to remove sensitive or undesired data from large language models. However, recent studies suggest that unlearning is often shallow, claiming that removed knowledge can easily be recovered. In this work, we critically examine standard unlearning evaluation practices and uncover key limitations that shake our trust in those findings. First, we show that some evaluations introduce substantial new information into the model, potentially masking true unlearning performance by re-teaching the model during testing. Second, we demonstrate that evaluation outcomes vary significantly across tasks, undermining the generalizability of current evaluation routines. Finally, we find that many evaluations rely on spurious correlations, making their results difficult to trust and interpret. Taken together, these issues suggest that current evaluation protocols may both overstate and understate unlearning success. To address this, we propose two principles for future unlearning evaluations: minimal information injection and downstream task awareness. We validate these principles through a series of targeted experiments, showing how violations of each can lead to misleading conclusions.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.96435546875
        },
        {
            "paperId": "1f037fb429dcdc305b0df2577b8634a24f7eab55",
            "corpusId": 273661686,
            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 61,
            "citationCount": 13,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.22086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324784973",
                    "name": "Zhiqi Bu"
                },
                {
                    "authorId": "2327893325",
                    "name": "Xiaomeng Jin"
                },
                {
                    "authorId": "3236313",
                    "name": "B. Vinzamuri"
                },
                {
                    "authorId": "2328076404",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "2278433136",
                    "name": "Mingyi Hong"
                }
            ],
            "abstract": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
            "corpus_id": "273661686",
            "text": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99853515625
        },
        {
            "paperId": "922236b2a1e718581f400de1423ab42c627a28e0",
            "corpusId": 278782413,
            "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs",
            "venue": "",
            "year": 2025,
            "referenceCount": 62,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294565775",
                    "name": "Wonje Jeung"
                },
                {
                    "authorId": "2333409137",
                    "name": "Sangyeon Yoon"
                },
                {
                    "authorId": "2303466208",
                    "name": "Albert No"
                }
            ],
            "abstract": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.",
            "corpus_id": "278782413",
            "text": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information. Existing unlearning metrics assess whether a model correctly answers retain queries and rejects forget queries, but they fail to capture real-world scenarios where forget queries rarely appear in isolation. In fact, forget and retain queries often coexist within the same prompt, making mixed-query evaluation crucial. We introduce SEPS, an evaluation framework that explicitly measures a model's ability to both forget and retain information within a single prompt. Through extensive experiments across three benchmarks, we identify two key failure modes in existing unlearning methods: (1) untargeted unlearning indiscriminately erases both forget and retain content once a forget query appears, and (2) targeted unlearning overfits to single-query scenarios, leading to catastrophic failures when handling multiple queries. To address these issues, we propose Mixed Prompt (MP) unlearning, a strategy that integrates both forget and retain queries into a unified training objective. Our approach significantly improves unlearning effectiveness, demonstrating robustness even in complex settings with up to eight mixed forget and retain queries in a single prompt.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.99853515625
        },
        {
            "paperId": "fbd310c97c403969a0ae10aa15dd3d4e6e11970c",
            "corpusId": 279410024,
            "title": "Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs",
            "venue": "",
            "year": 2025,
            "referenceCount": 66,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2350502165",
                    "name": "Yiwei Chen"
                },
                {
                    "authorId": null,
                    "name": "Soumyadeep Pal"
                },
                {
                    "authorId": "2108441128",
                    "name": "Yimeng Zhang"
                },
                {
                    "authorId": "2367270617",
                    "name": "Qing Qu"
                },
                {
                    "authorId": "2356000124",
                    "name": "Sijia Liu"
                }
            ],
            "abstract": "Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).",
            "corpus_id": "279410024",
            "text": "Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9892578125
        },
        {
            "paperId": "85ff3918c21ad5c43495de3ab3fcd1865dd31ee7",
            "corpusId": 278782469,
            "title": "DUSK: Do Not Unlearn Shared Knowledge",
            "venue": "",
            "year": 2025,
            "referenceCount": 59,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15209, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294565775",
                    "name": "Wonje Jeung"
                },
                {
                    "authorId": "2333409137",
                    "name": "Sangyeon Yoon"
                },
                {
                    "authorId": "2362617494",
                    "name": "Hyesoo Hong"
                },
                {
                    "authorId": "2362616980",
                    "name": "Soeun Kim"
                },
                {
                    "authorId": "2362905728",
                    "name": "Seungju Han"
                },
                {
                    "authorId": "2353593792",
                    "name": "Youngjae Yu"
                },
                {
                    "authorId": "2303466208",
                    "name": "Albert No"
                }
            ],
            "abstract": "Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about the unauthorized use of copyrighted or sensitive data. Machine unlearning aims to remove such 'forget' data while preserving utility and information from the 'retain' set. However, existing evaluations typically assume that forget and retain sets are fully disjoint, overlooking realistic scenarios where they share overlapping content. For instance, a news article may need to be unlearned, even though the same event, such as an earthquake in Japan, is also described factually on Wikipedia. Effective unlearning should remove the specific phrasing of the news article while preserving publicly supported facts. In this paper, we introduce DUSK, a benchmark designed to evaluate unlearning methods under realistic data overlap. DUSK constructs document sets that describe the same factual content in different styles, with some shared information appearing across all sets and other content remaining unique to each. When one set is designated for unlearning, an ideal method should remove its unique content while preserving shared facts. We define seven evaluation metrics to assess whether unlearning methods can achieve this selective removal. Our evaluation of nine recent unlearning methods reveals a key limitation: while most can remove surface-level text, they often fail to erase deeper, context-specific knowledge without damaging shared content. We release DUSK as a public benchmark to support the development of more precise and reliable unlearning techniques for real-world applications.",
            "corpus_id": "278782469",
            "text": "Large language models (LLMs) are increasingly deployed in real-world applications, raising concerns about the unauthorized use of copyrighted or sensitive data. Machine unlearning aims to remove such 'forget' data while preserving utility and information from the 'retain' set. However, existing evaluations typically assume that forget and retain sets are fully disjoint, overlooking realistic scenarios where they share overlapping content. For instance, a news article may need to be unlearned, even though the same event, such as an earthquake in Japan, is also described factually on Wikipedia. Effective unlearning should remove the specific phrasing of the news article while preserving publicly supported facts. In this paper, we introduce DUSK, a benchmark designed to evaluate unlearning methods under realistic data overlap. DUSK constructs document sets that describe the same factual content in different styles, with some shared information appearing across all sets and other content remaining unique to each. When one set is designated for unlearning, an ideal method should remove its unique content while preserving shared facts. We define seven evaluation metrics to assess whether unlearning methods can achieve this selective removal. Our evaluation of nine recent unlearning methods reveals a key limitation: while most can remove surface-level text, they often fail to erase deeper, context-specific knowledge without damaging shared content. We release DUSK as a public benchmark to support the development of more precise and reliable unlearning techniques for real-world applications.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.97412109375
        },
        {
            "paperId": "d0b02eb4f0d3efd884b49450efc88145bdf49abc",
            "corpusId": 267657624,
            "title": "Rethinking Machine Unlearning for Large Language Models",
            "venue": "Nat. Mac. Intell.",
            "year": 2024,
            "referenceCount": 154,
            "citationCount": 119,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08787, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254478722",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2257134957",
                    "name": "Yuanshun Yao"
                },
                {
                    "authorId": "2051552791",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "2284064052",
                    "name": "Stephen Casper"
                },
                {
                    "authorId": "2284064162",
                    "name": "Nathalie Baracaldo"
                },
                {
                    "authorId": "2266467463",
                    "name": "Peter Hase"
                },
                {
                    "authorId": "2259623067",
                    "name": "Xiaojun Xu"
                },
                {
                    "authorId": "100630765",
                    "name": "Yuguang Yao"
                },
                {
                    "authorId": "2271515779",
                    "name": "Chris Liu"
                },
                {
                    "authorId": "2257196326",
                    "name": "Hang Li"
                },
                {
                    "authorId": "1491635392",
                    "name": "Kush R. Varshney"
                },
                {
                    "authorId": "2253762115",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "123593472",
                    "name": "Sanmi Koyejo"
                },
                {
                    "authorId": "2257387078",
                    "name": "Yang Liu"
                }
            ],
            "abstract": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
            "corpus_id": "267657624",
            "text": "We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9833984375
        }
    ],
    "quotes": {
        "cost": 0.210396,
        "quotes": [
            {
                "idx": 0,
                "key": "[252089272 | Nguyen et al. | 2022 | Citations: 239]",
                "snippets": "Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249152301 | Lu et al. | 2022 | Citations: 219]": "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.",
                    "[260925619 | Hu et al. | 2023 | Citations: 30]": "Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs.",
                    "[266164054 | Kassem et al. | 2023 | Citations: 32]": ","
                },
                "metadata": [
                    {
                        "section_title": "Unlearning in Large Language Models",
                        "pdf_hash": "",
                        "start": 348,
                        "end": 2204,
                        "sentence_offsets": [
                            {
                                "start": 348,
                                "end": 580
                            },
                            {
                                "start": 581,
                                "end": 687
                            },
                            {
                                "start": 688,
                                "end": 847
                            },
                            {
                                "start": 848,
                                "end": 992
                            },
                            {
                                "start": 993,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1268
                            },
                            {
                                "start": 1269,
                                "end": 1368
                            },
                            {
                                "start": 1369,
                                "end": 1607
                            },
                            {
                                "start": 1608,
                                "end": 1780
                            },
                            {
                                "start": 1781,
                                "end": 1969
                            },
                            {
                                "start": 1970,
                                "end": 2204
                            }
                        ],
                        "ref_mentions": [
                            "266164054",
                            "260925619",
                            "249152301"
                        ],
                        "quote": "Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[263834631 | Pawelczyk et al. | 2023 | Citations: 132]",
                "snippets": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1148,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[264828972 | Chen et al. | 2023 | Citations: 162]",
                "snippets": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 911,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[265456592 | Si et al. | 2023 | Citations: 33]",
                "snippets": "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs...Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 563,
                        "end": 941,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 1043,
                        "end": 1263,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267547751 | Wang et al. | 2024 | Citations: 18]",
                "snippets": "We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation...In contrast to (Jang et al., 2022), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252693065 | Jang et al. | 2022 | Citations: 239]": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."
                },
                "metadata": [
                    {
                        "quote": "We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1906,
                        "end": 2210,
                        "sentence_offsets": [
                            {
                                "start": 1906,
                                "end": 2056
                            },
                            {
                                "start": 2057,
                                "end": 2209
                            }
                        ],
                        "ref_mentions": [
                            "252693065"
                        ],
                        "quote": "In contrast to (Jang et al., 2022), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[267681958 | Liu et al. | 2024 | Citations: 87]",
                "snippets": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1161,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[268681648 | Qu et al. | 2024 | Citations: 16]",
                "snippets": "Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 270,
                        "end": 858,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[269430574 | Chen et al. | 2024 | Citations: 13]",
                "snippets": "Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[270062331 | Wang et al. | 2024 | Citations: 9]",
                "snippets": "We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We introduce the problem of Large Scale Knowledge Washing, focusing on unlearning an extensive amount of factual knowledge. Previous unlearning methods usually define the reverse loss and update the model via backpropagation, which may affect the model's fluency and reasoning ability or even destroy the model due to extensive training with the reverse loss. Existing works introduce additional data from downstream tasks to prevent the model from losing capabilities, which requires downstream task awareness. Controlling the tradeoff of unlearning and maintaining existing capabilities is also challenging. To this end, we propose LAW (Large Scale Washing) to update the MLP layers in decoder-only large language models to perform knowledge washing, as inspired by model editing methods and based on the hypothesis that knowledge and reasoning are disentanglable. We derive a new objective with the knowledge to be unlearned to update the weights of certain MLP layers.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[270440244 | Ashuach et al. | 2024 | Citations: 7]",
                "snippets": "Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[229156229 | Carlini et al. | 2020 | Citations: 1950]": "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/",
                    "[258832407 | Zheng et al. | 2023 | Citations: 216]": "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE.",
                    "[259859034 | Yu et al. | 2023 | Citations: 100]": ","
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1095,
                        "end": 1914,
                        "sentence_offsets": [
                            {
                                "start": 1095,
                                "end": 1288
                            },
                            {
                                "start": 1288,
                                "end": 1436
                            },
                            {
                                "start": 1436,
                                "end": 1608
                            },
                            {
                                "start": 1608,
                                "end": 1914
                            }
                        ],
                        "ref_mentions": [
                            "258832407",
                            "259859034",
                            "255825985",
                            "229156229"
                        ],
                        "quote": "Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[270559969 | Jin et al. | 2024 | Citations: 26]",
                "snippets": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten...Recently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37]...Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37]. Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model. Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Knowledge Unlearning for Large Language Models",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 157,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten"
                    },
                    {
                        "section_title": "Knowledge Unlearning for Large Language Models",
                        "pdf_hash": "",
                        "start": 496,
                        "end": 621,
                        "sentence_offsets": [
                            {
                                "start": 496,
                                "end": 621
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37]"
                    },
                    {
                        "section_title": "Knowledge Unlearning for Large Language Models",
                        "pdf_hash": "",
                        "start": 630,
                        "end": 1072,
                        "sentence_offsets": [
                            {
                                "start": 621,
                                "end": 844
                            },
                            {
                                "start": 844,
                                "end": 979
                            },
                            {
                                "start": 979,
                                "end": 1154
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37]. Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model. Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22]."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270562084 | Choi et al. | 2024 | Citations: 2]",
                "snippets": "Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 236,
                        "end": 924,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270703237 | Ma et al. | 2024 | Citations: 6]",
                "snippets": "To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 546,
                        "end": 952,
                        "sentence_offsets": [
                            {
                                "start": 546,
                                "end": 721
                            },
                            {
                                "start": 722,
                                "end": 952
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270878324 | Tian et al. | 2024 | Citations: 13]",
                "snippets": "Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 155,
                        "end": 819,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[271064299 | Shi et al. | 2024 | Citations: 84]",
                "snippets": "Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[255825985 | Meng et al. | 2022 | Citations: 1387]": "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 65,
                        "end": 744,
                        "sentence_offsets": [
                            {
                                "start": 65,
                                "end": 144
                            },
                            {
                                "start": 144,
                                "end": 270
                            },
                            {
                                "start": 270,
                                "end": 578
                            },
                            {
                                "start": 580,
                                "end": 744
                            }
                        ],
                        "ref_mentions": [
                            "255825985"
                        ],
                        "quote": "Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271769107 | Lizzo et al. | 2024 | Citations: 1]",
                "snippets": "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 550,
                        "end": 1249,
                        "sentence_offsets": [
                            {
                                "start": 550,
                                "end": 696
                            },
                            {
                                "start": 697,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 1075
                            },
                            {
                                "start": 1076,
                                "end": 1249
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271860124 | Cha et al. | 2024 | Citations: 2]",
                "snippets": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1184,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[272704025 | Gu et al. | 2024 | Citations: 9]",
                "snippets": "LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 106,
                        "end": 1031,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273022754 | Takashiro et al. | 2024 | Citations: 2]",
                "snippets": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273098052 | Sakarvadia et al. | 2024 | Citations: 2]",
                "snippets": "In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 290,
                        "end": 507,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we investigate methods to mitigate memorization: three regularizer-based, three finetuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273098800 | Gandikota et al. | 2024 | Citations: 11]",
                "snippets": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273233618 | Yuan et al. | 2024 | Citations: 13]",
                "snippets": "Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 112,
                        "end": 1105,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273350773 | Wu et al. | 2024 | Citations: 0]",
                "snippets": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems.\n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.\n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal.\n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning.\n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232404451 | Chen et al. | 2021 | Citations: 149]": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06x (small dataset) to 35.94x (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to 62.5% higher F1 score and our proposed learning-based aggregation method achieves up to 112% higher F1 score. https://github.com/MinChen00/Graph-Unlearning.",
                    "[235474438 | Schelter et al. | 2021 | Citations: 89]": "Software systems that learn from user data with machine learning (ML) have become ubiquitous over the last years. Recent law such as the \"General Data Protection Regulation\" (GDPR) requires organisations that process personal data to delete user data upon request (enacting the \"right to be forgotten\"). However, this regulation does not only require the deletion of user data from databases, but also applies to ML models that have been learned from the stored data. We therefore argue that ML applications should offer users to unlearn their data from trained models in a timely manner. We explore how fast this unlearning can be done under the constraints imposed by real world deployments, and introduce the problem of low-latency machine unlearning: maintaining a deployed ML model in-place under the removal of a small fraction of training samples without retraining. We propose HedgeCut, a classification model based on an ensemble of randomised decision trees, which is designed to answer unlearning requests with low latency. We detail how to efficiently implement HedgeCut with vectorised operators for decision tree learning. We conduct an experimental evaluation on five privacy-sensitive datasets, where we find that HedgeCut can unlearn training samples with a latency of around 100 microseconds and answers up to 36,000 prediction requests per second, while providing a training time and predictive accuracy similar to widely used implementations of tree-based ML models such as Random Forests."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2130,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 127
                            },
                            {
                                "start": 128,
                                "end": 302
                            },
                            {
                                "start": 303,
                                "end": 417
                            },
                            {
                                "start": 418,
                                "end": 582
                            },
                            {
                                "start": 583,
                                "end": 710
                            },
                            {
                                "start": 711,
                                "end": 908
                            },
                            {
                                "start": 911,
                                "end": 1037
                            },
                            {
                                "start": 1038,
                                "end": 1205
                            },
                            {
                                "start": 1208,
                                "end": 1416
                            },
                            {
                                "start": 1417,
                                "end": 1606
                            },
                            {
                                "start": 1609,
                                "end": 1875
                            },
                            {
                                "start": 1878,
                                "end": 2030
                            },
                            {
                                "start": 2031,
                                "end": 2134
                            }
                        ],
                        "ref_mentions": [
                            "235474438",
                            "232404451"
                        ],
                        "quote": "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems.\n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.\n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal.\n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning.\n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273350971 | Wang et al. | 2024 | Citations: 20]",
                "snippets": "LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 713,
                        "end": 1398,
                        "sentence_offsets": [
                            {
                                "start": 713,
                                "end": 964
                            },
                            {
                                "start": 965,
                                "end": 1118
                            },
                            {
                                "start": 1119,
                                "end": 1272
                            },
                            {
                                "start": 1273,
                                "end": 1494
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273403717 | Guo et al. | 2024 | Citations: 10]",
                "snippets": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 190,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273404304 | Choi et al. | 2024 | Citations: 1]",
                "snippets": "This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 167,
                        "end": 433,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This has led to the development of various fast, approximate unlearning techniques to selectively remove knowledge from LLMs. Prior research has largely focused on minimizing the probabilities of specific token sequences by reversing the language modeling objective."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273502714 | Wu et al. | 2024 | Citations: 7]",
                "snippets": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other...Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 110,
                        "end": 526,
                        "sentence_offsets": [
                            {
                                "start": 110,
                                "end": 273
                            },
                            {
                                "start": 274,
                                "end": 421
                            },
                            {
                                "start": 422,
                                "end": 525
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273661686 | Bu et al. | 2024 | Citations: 13]",
                "snippets": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273901406 | Feng et al. | 2024 | Citations: 10]",
                "snippets": "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023)(Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2019)(Bourtoule et al., 2019), and mitigating biases to promote fairness (Chen et al., 2023;(Li et al., 2023). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023), and graph learning (Chen et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207847600 | Guo et al. | 2019 | Citations: 451]": "Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to \"remove\" data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.",
                    "[208909851 | Bourtoule et al. | 2019 | Citations: 887]": "Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63\u00d7, and 2.45\u00d7 for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36\u00d7 in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",
                    "[232404451 | Chen et al. | 2021 | Citations: 149]": "Machine unlearning is a process of removing the impact of some training data from the machine learning (ML) models upon receiving removal requests. While straightforward and legitimate, retraining the ML model from scratch incurs a high computational overhead. To address this issue, a number of approximate algorithms have been proposed in the domain of image and text data, among which SISA is the state-of-the-art solution. It randomly partitions the training set into multiple shards and trains a constituent model for each shard. However, directly applying SISA to the graph data can severely damage the graph structural information, and thereby the resulting ML model utility. In this paper, we propose GraphEraser, a novel machine unlearning framework tailored to graph data. Its contributions include two novel graph partition algorithms and a learning-based aggregation method. We conduct extensive experiments on five real-world graph datasets to illustrate the unlearning efficiency and model utility of GraphEraser. It achieves 2.06x (small dataset) to 35.94x (large dataset) unlearning time improvement. On the other hand, GraphEraser achieves up to 62.5% higher F1 score and our proposed learning-based aggregation method achieves up to 112% higher F1 score. https://github.com/MinChen00/Graph-Unlearning.",
                    "[257038445 | Kurmanji et al. | 2023 | Citations: 146]": "Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art.",
                    "[259991722 | Wei et al. | 2023 | Citations: 38]": "Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense.",
                    "[263830799 | Li et al. | 2023 | Citations: 23]": "With the growing privacy concerns in recommender systems, recommendation unlearning, i.e., forgetting the impact of specific learned targets, is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as the unlearning target. However, we find that attackers can extract private information, i.e., gender, race, and age, from a trained model even if it has not been explicitly encountered during training. We name this unseen information as attribute and treat it as the unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to degrade attacking performance and make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we design a two-component loss function that consists of i) distinguishability loss: making attribute labels indistinguishable from attackers, and ii) regularization loss: preventing drastic changes in the model that result in a negative impact on recommendation performance. Specifically, we investigate two types of distinguishability measurements, i.e., user-to-user and distribution-to-distribution. We use the stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed methods."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 778,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 116
                            },
                            {
                                "start": 117,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "259991722",
                            "257038445",
                            "207847600",
                            "208909851",
                            "263830799",
                            "260927489",
                            "268030788",
                            "232404451"
                        ],
                        "quote": "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023)(Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2019)(Bourtoule et al., 2019), and mitigating biases to promote fairness (Chen et al., 2023;(Li et al., 2023). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023), and graph learning (Chen et al., 2021)."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[274763373 | Davies et al. | 2024 | Citations: 0]",
                "snippets": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1191,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[274823032 | Zuo et al. | 2024 | Citations: 2]",
                "snippets": "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[276408369 | Xu et al. | 2025 | Citations: 3]",
                "snippets": "To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[276575899 | Jung et al. | 2025 | Citations: 1]",
                "snippets": "In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 335,
                        "end": 668,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[276812969 | Wang et al. | 2025 | Citations: 3]",
                "snippets": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 454,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[277113301 | He et al. | 2025 | Citations: 1]",
                "snippets": "However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 351,
                        "end": 1237,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[277857590 | Kim et al. | 2025 | Citations: 1]",
                "snippets": "A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge...To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "I. INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1040,
                        "end": 1374,
                        "sentence_offsets": [
                            {
                                "start": 1040,
                                "end": 1206
                            },
                            {
                                "start": 1207,
                                "end": 1375
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge"
                    },
                    {
                        "quote": "To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[278338982 | Sun et al. | 2025 | Citations: 0]",
                "snippets": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 872,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[278481378 | Vasilev et al. | 2025 | Citations: 0]",
                "snippets": "In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1283,
                        "end": 2075,
                        "sentence_offsets": [
                            {
                                "start": 1283,
                                "end": 1548
                            },
                            {
                                "start": 1549,
                                "end": 1713
                            },
                            {
                                "start": 1714,
                                "end": 2075
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[278782413 | Jeung et al. | 2025 | Citations: 1]",
                "snippets": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.037731,
        "cot": "First, I'll identify the key dimensions needed to effectively answer the query about \"machine unlearning methods to remove knowledge from language models.\"\n\n1. Introduction/Background: This section should provide a comprehensive overview of what machine unlearning is, why it's important, and the general context for LLM unlearning. This will give users the foundational knowledge needed to understand the rest of the answer. A synthesis format is appropriate here to provide a coherent explanation of the concept.\n\n2. Types of Machine Unlearning Methods: Based on the quotes, there are several distinct categories of unlearning approaches. Quote [3] explicitly mentions categorizing \"existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning.\" Other quotes mention additional categorizations. This should be presented as a synthesis to give a coherent overview of the different methodological approaches.\n\n3. Parameter Optimization Methods: This appears to be a major category of unlearning approaches mentioned in multiple quotes. These include gradient ascent and its variants. This would benefit from a list format to clearly enumerate the specific methods while highlighting their unique characteristics.\n\n4. Parameter Merging Methods: Another category mentioned in quotes that deserves its own section. A list format would be appropriate here to enumerate the specific techniques.\n\n5. In-Context Unlearning Methods: This category is mentioned in quotes [1], [14], [18], and others. These methods don't modify model parameters but rather use context to achieve unlearning. A list format is appropriate here to clearly enumerate the different in-context approaches.\n\n6. Localization-Based Unlearning Methods: Several quotes mention methods that identify and modify specific parts of models. This section would benefit from a list format to clearly enumerate these techniques.\n\n7. Evaluation Metrics and Challenges: Many quotes discuss the challenges and evaluation criteria for unlearning methods, including the tradeoff between forgetting targeted information and maintaining model performance. A synthesis format is appropriate here to provide a cohesive explanation of these interconnected issues.\n\n8. Applications and Use Cases: Several quotes mention specific applications of machine unlearning, such as removing copyright content, protecting privacy, removing harmful content, etc. A list format would be appropriate to clearly enumerate these different applications.\n\nThis structure covers all the key dimensions of the query while ensuring each section has a clear purpose. The order flows logically from introduction to methodology to evaluation and applications.",
        "plan": {
            "Introduction to Machine Unlearning (synthesis)": [
                1,
                6,
                10,
                11,
                24,
                26,
                28,
                37,
                38
            ],
            "Types of Machine Unlearning Methods (synthesis)": [
                3,
                12,
                14,
                23
            ],
            "Parameter Optimization Methods (list)": [
                0,
                4,
                5,
                7,
                8,
                10,
                14,
                16,
                17,
                19,
                21,
                25,
                27,
                30,
                33,
                36
            ],
            "Parameter Merging Methods (list)": [
                0,
                10,
                15,
                32
            ],
            "In-Context Unlearning Methods (list)": [
                1,
                9,
                14,
                18,
                23
            ],
            "Localization-Based Unlearning Methods (list)": [
                2,
                9,
                13,
                14,
                20,
                22,
                29,
                34,
                35
            ],
            "Evaluation Metrics and Challenges (synthesis)": [
                17,
                21,
                31,
                33,
                35,
                36,
                37
            ],
            "Applications and Use Cases (list)": [
                0,
                5,
                7,
                9,
                11,
                13,
                19,
                24,
                26,
                30
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Machine Unlearning",
                "tldr": "Machine unlearning enables selectively removing specific information from trained models without full retraining. This emerging field addresses privacy regulations, ethical concerns, and security issues by providing methods to make models \"forget\" targeted data while maintaining performance on retained knowledge. (9 sources)",
                "text": "\nMachine unlearning has emerged as a critical area of research in response to regulatory requirements like the \"Right to be Forgotten\" and growing privacy concerns related to large language models (LLMs) <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This field focuses on developing techniques to efficiently remove the influence of specific training data from models without the computational burden of complete retraining <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nThe core objective of machine unlearning is to selectively erase certain knowledge or capabilities from trained models while preserving their general functionality <Paper corpusId=\"273403717\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278782413\" paperTitle=\"(Jeung et al., 2025)\" isShortName></Paper>. This approach offers practical solutions for several important scenarios, including:\n\n1. Complying with privacy regulations like GDPR that mandate data removal <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n2. Removing copyrighted or offensive content from models <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n3. Enhancing security by eliminating poisoned data <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259991722\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>\n4. Mitigating biases to promote fairness <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>\n\nAn effective machine unlearning method must satisfy several key requirements: minimizing retained information from the forget set, maintaining high performance on the retain set, requiring less computational cost than full retraining, and preserving inference efficiency <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nWhile machine unlearning research initially focused primarily on computer vision applications, it has expanded to address other domains including federated learning, recommender systems, and graph learning <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>. With LLMs specifically, unlearning approaches aim to address both structured/classification data and unstructured/textual data <Paper corpusId=\"268681648\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>.\n\nRecent developments in the field have moved beyond simple instance-level forgetting to more complex scenarios, such as entity-level unlearning, which aims to erase all knowledge related to a specific entity while preserving other model capabilities <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>. This shift acknowledges that real-world unlearning scenarios often require more comprehensive knowledge removal than simply forgetting individual data points <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>. Additionally, the task becomes particularly challenging in modern LLMs because facts can be deduced from one another, requiring more sophisticated unlearning approaches <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Pawelczyk et al., 2023)",
                        "snippets": [
                            "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth."
                        ],
                        "paper": {
                            "corpus_id": 263834631,
                            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                            "authors": [
                                {
                                    "authorId": "89583148",
                                    "name": "Martin Pawelczyk"
                                },
                                {
                                    "authorId": "2273685865",
                                    "name": "Seth Neel"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 132
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other",
                            "Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content."
                        ],
                        "paper": {
                            "corpus_id": 273502714,
                            "title": "Evaluating Deep Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2303333890",
                                    "name": "Ruihan Wu"
                                },
                                {
                                    "authorId": "83222216",
                                    "name": "Chhavi Yadav"
                                },
                                {
                                    "authorId": "2266239350",
                                    "name": "Russ Salakhutdinov"
                                },
                                {
                                    "authorId": "2303254420",
                                    "name": "Kamalika Chaudhuri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Vasilev et al., 2025)",
                        "snippets": [
                            "In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency."
                        ],
                        "paper": {
                            "corpus_id": 278481378,
                            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "2350753289",
                                    "name": "Stefan Vasilev"
                                },
                                {
                                    "authorId": "2307079915",
                                    "name": "Christian Herold"
                                },
                                {
                                    "authorId": "66693547",
                                    "name": "Baohao Liao"
                                },
                                {
                                    "authorId": "2350630854",
                                    "name": "Seyyed Hadi Hashemi"
                                },
                                {
                                    "authorId": "2490162",
                                    "name": "Shahram Khadivi"
                                },
                                {
                                    "authorId": "2062908179",
                                    "name": "C. Monz"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance."
                        ],
                        "paper": {
                            "corpus_id": 273403717,
                            "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
                            "authors": [
                                {
                                    "authorId": "2202361389",
                                    "name": "P. Guo"
                                },
                                {
                                    "authorId": "2201328926",
                                    "name": "Aaquib Syed"
                                },
                                {
                                    "authorId": "2284684654",
                                    "name": "A. Sheshadri"
                                },
                                {
                                    "authorId": "2287842553",
                                    "name": "Aidan Ewart"
                                },
                                {
                                    "authorId": "2533850",
                                    "name": "G. Dziugaite"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Jeung et al., 2025)",
                        "snippets": [
                            "Machine unlearning aims to selectively remove targeted knowledge from Large Language Models (LLMs), ensuring they forget specified content while retaining essential information."
                        ],
                        "paper": {
                            "corpus_id": 278782413,
                            "title": "SEPS: A Separability Measure for Robust Unlearning in LLMs",
                            "authors": [
                                {
                                    "authorId": "2294565775",
                                    "name": "Wonje Jeung"
                                },
                                {
                                    "authorId": "2333409137",
                                    "name": "Sangyeon Yoon"
                                },
                                {
                                    "authorId": "2303466208",
                                    "name": "Albert No"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Feng et al., 2024)",
                        "snippets": [
                            "Machine unlearning, a burgeoning research topic, has gained significant attention in recent years (Xu et al., 2023). It aims to erase the memory of target data from machine learning models, offering potential applications such as removing poisoned data to enhance security (Wei et al., 2023)(Kurmanji et al., 2023), retrieving personal data to comply with privacy regulations (e.g., the right-to-beforgotten) (Guo et al., 2019)(Bourtoule et al., 2019), and mitigating biases to promote fairness (Chen et al., 2023;(Li et al., 2023). Existing studies on unlearning primarily concentrate on computer vision but also extend their exploration to other fields, e.g., federated learning (Che et al., 2023), recommender systems (Li et al., 2023), and graph learning (Chen et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 273901406,
                            "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
                            "authors": [
                                {
                                    "authorId": "2314871557",
                                    "name": "Xiaohua Feng"
                                },
                                {
                                    "authorId": "2251485995",
                                    "name": "Chao-Jun Chen"
                                },
                                {
                                    "authorId": "1527113700",
                                    "name": "Yuyuan Li"
                                },
                                {
                                    "authorId": "2261893790",
                                    "name": "Zibin Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 10
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Wei et al., 2023)",
                        "snippets": [
                            "Backdoor attacks are serious security threats to machine learning models where an adversary can inject poisoned samples into the training set, causing a backdoored model which predicts poisoned samples with particular triggers to particular target classes, while behaving normally on benign samples. In this paper, we explore the task of purifying a backdoored model using a small clean dataset. By establishing the connection between backdoor risk and adversarial risk, we derive a novel upper bound for backdoor risk, which mainly captures the risk on the shared adversarial examples (SAEs) between the backdoored model and the purified model. This upper bound further suggests a novel bi-level optimization problem for mitigating backdoor using adversarial training techniques. To solve it, we propose Shared Adversarial Unlearning (SAU). Specifically, SAU first generates SAEs, and then, unlearns the generated SAEs such that they are either correctly classified by the purified model and/or differently classified by the two models, such that the backdoor effect in the backdoored model will be mitigated in the purified model. Experiments on various benchmark datasets and network architectures show that our proposed method achieves state-of-the-art performance for backdoor defense."
                        ],
                        "paper": {
                            "corpus_id": 259991722,
                            "title": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples",
                            "authors": [
                                {
                                    "authorId": "2173786903",
                                    "name": "Shaokui Wei"
                                },
                                {
                                    "authorId": "2365530",
                                    "name": "Mingda Zhang"
                                },
                                {
                                    "authorId": "145203884",
                                    "name": "H. Zha"
                                },
                                {
                                    "authorId": "143905981",
                                    "name": "Baoyuan Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 38
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qu et al., 2024)",
                        "snippets": [
                            "Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy."
                        ],
                        "paper": {
                            "corpus_id": 268681648,
                            "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2262218493",
                                    "name": "Youyang Qu"
                                },
                                {
                                    "authorId": "2294002570",
                                    "name": "Ming Ding"
                                },
                                {
                                    "authorId": "2293369504",
                                    "name": "Nan Sun"
                                },
                                {
                                    "authorId": "3153007",
                                    "name": "Kanchana Thilakarathna"
                                },
                                {
                                    "authorId": "2185053609",
                                    "name": "Tianqing Zhu"
                                },
                                {
                                    "authorId": "1713586",
                                    "name": "D. Niyato"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 16
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Choi et al., 2024)",
                        "snippets": [
                            "Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning."
                        ],
                        "paper": {
                            "corpus_id": 270562084,
                            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
                            "authors": [
                                {
                                    "authorId": "2203800211",
                                    "name": "Minseok Choi"
                                },
                                {
                                    "authorId": "2307073048",
                                    "name": "Daniel Rim"
                                },
                                {
                                    "authorId": "2294508694",
                                    "name": "Dohyun Lee"
                                },
                                {
                                    "authorId": "2260653165",
                                    "name": "Jaegul Choo"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.9990234375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Machine Unlearning Methods",
                "tldr": "Machine unlearning methods for language models can be categorized into four main approaches: parameter optimization, parameter merging, in-context unlearning, and localization-based techniques. Each approach offers different tradeoffs between efficiency, effectiveness, and implementation complexity when removing targeted knowledge from language models. (5 sources)",
                "text": "\nMachine unlearning methods for language models have evolved into several distinct categories, each with unique approaches to selective knowledge removal. These methods can be broadly classified into four main types:\n\nFirst, **parameter optimization methods** focus on directly modifying model weights to eliminate specific knowledge. Gradient ascent represents a standard technique in this category, where the model is trained to increase loss on targeted forget data while maintaining performance on retained knowledge <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This approach falls under the broader category of \"model-based methods\" that directly manipulate model parameters to achieve unlearning objectives <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nSecond, **parameter merging methods** combine multiple model versions to achieve unlearning effects. These techniques typically involve creating variants of the original model with different knowledge characteristics and then strategically merging their parameters to remove unwanted information while preserving desired capabilities <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nThird, **in-context unlearning methods** treat the model as a black box and modify outputs using external knowledge or prompting strategies <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. These approaches align with \"input-based methods\" that use carefully designed instructions to guide the original model toward unlearning objectives without changing its parameters <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nFourth, **localization-based unlearning methods** identify specific model components (such as layers or neurons) associated with targeted knowledge and selectively modify those components <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This approach builds on research showing that factual associations in language models correspond to localized, directly-editable computations, particularly in middle-layer feed-forward modules <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nThe collective development of these methods has established LLM unlearning as a mainstream approach for removing undesirable knowledge through post-hoc modifications to target models <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Each category offers different tradeoffs in terms of computational efficiency, unlearning effectiveness, and implementation complexity, providing researchers and practitioners with diverse options for addressing specific unlearning scenarios.",
                "citations": [
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271064299,
                            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                            "authors": [
                                {
                                    "authorId": "2286638403",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2261353791",
                                    "name": "Jaechan Lee"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2266698166",
                                    "name": "Jieyu Zhao"
                                },
                                {
                                    "authorId": "2309248199",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309424274",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 84
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"
                        ],
                        "paper": {
                            "corpus_id": 273350971,
                            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
                            "authors": [
                                {
                                    "authorId": "2306067819",
                                    "name": "Yaxuan Wang"
                                },
                                {
                                    "authorId": "2306500340",
                                    "name": "Jiaheng Wei"
                                },
                                {
                                    "authorId": "2271515779",
                                    "name": "Chris Liu"
                                },
                                {
                                    "authorId": "2284760719",
                                    "name": "Jinlong Pang"
                                },
                                {
                                    "authorId": "2326243943",
                                    "name": "Quan Liu"
                                },
                                {
                                    "authorId": "2316588330",
                                    "name": "Ankit Shah"
                                },
                                {
                                    "authorId": "2306754738",
                                    "name": "Yujia Bao"
                                },
                                {
                                    "authorId": "2306028548",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2306480290",
                                    "name": "Wei Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 20
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs",
                            "Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods."
                        ],
                        "paper": {
                            "corpus_id": 265456592,
                            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
                            "authors": [
                                {
                                    "authorId": "73502630",
                                    "name": "Nianwen Si"
                                },
                                {
                                    "authorId": "2154930608",
                                    "name": "Hao Zhang"
                                },
                                {
                                    "authorId": "2116152318",
                                    "name": "Heyu Chang"
                                },
                                {
                                    "authorId": "9047584",
                                    "name": "Wenlin Zhang"
                                },
                                {
                                    "authorId": "2253591545",
                                    "name": "Dan Qu"
                                },
                                {
                                    "authorId": "2268429659",
                                    "name": "Weiqiang Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 33
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ma et al., 2024)",
                        "snippets": [
                            "To tackle this, Machine Unlearning ( Zhang et al., 2023;Lu et al., 2024;Bhardwaj et al., 2024) has gradually been applied to LLMs due to its effectiveness and cost-efficiency. These refined techniques, now known as LLM Unlearning (Yao et al., 2023;Liu et al., 2024b,c), have become a mainstream approach for removing undesirable knowledge from the model by applying post-hoc modifications to target models."
                        ],
                        "paper": {
                            "corpus_id": 270703237,
                            "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis",
                            "authors": [
                                {
                                    "authorId": "2265878959",
                                    "name": "Weitao Ma"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2208739098",
                                    "name": "Weihong Zhong"
                                },
                                {
                                    "authorId": "2265930173",
                                    "name": "Lei Huang"
                                },
                                {
                                    "authorId": "2216505879",
                                    "name": "Yangfan Ye"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Computational Linguistics",
                            "n_citations": 6
                        },
                        "score": 0.99853515625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter Optimization Methods",
                "tldr": "Parameter optimization methods directly modify model weights to remove specific knowledge, with gradient-based techniques being the most prevalent approach. These methods can effectively make models forget targeted information by altering parameter values through various optimization objectives, though they often face challenges in balancing effective unlearning with maintaining performance on unrelated tasks. (10 sources)",
                "text": "\nParameter optimization methods directly manipulate model weights to remove specific knowledge from language models. These approaches include:\n\n1. **Gradient Ascent (GA)** - This foundational technique essentially reverses the standard training objective, maximizing loss on targeted content to make the model forget specific information <Paper corpusId=\"252693065\" paperTitle=\"(Jang et al., 2022)\" isShortName></Paper>. While effective at removing knowledge, basic GA can lead to unstable optimization and catastrophic forgetting of retained knowledge <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\n2. **Selective Unlearning (SeUL)** - Unlike methods that fully reverse training loss, SeUL achieves fine-grained knowledge removal by focusing on specific sequence spans rather than entire instances, minimizing negative impacts on general language generation capabilities <Paper corpusId=\"267547751\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n3. **Selective Knowledge negation Unlearning (SKU)** - This two-stage framework first identifies and acquires harmful knowledge within the model, then selectively removes it from parameters while preserving performance on normal prompts <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\n4. **Inverted Hinge Loss** - Proposed in the Low-rank Knowledge Unlearning (LoKU) framework, this approach suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\n5. **Maximum Entropy (ME)** - This method maximizes the entropy of token distributions for untargeted unlearning, helping to address the issue of unpredictable behavior and potential hallucinations in conventional approaches <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\n6. **Normalized Gradient Difference (NGDiff)** - This algorithm frames unlearning as a regularized multi-task optimization problem, providing better control over the trade-off between forgetting objectives and maintaining model performance <Paper corpusId=\"273661686\" paperTitle=\"(Bu et al., 2024)\" isShortName></Paper>.\n\n7. **Memory Evaluation and Optimization Workflow (MEOW)** - This approach uses an offline LLM to generate inverted facts and introduces a metric called MEMO to quantify memorization, selecting appropriate inverted facts for finetuning <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n8. **Distribution Flattening with Multiple-Choice Questions (DF-MCQ)** - This method flattens the model's predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\n9. **Lightweight Unlearning Layers** - Some approaches integrate specialized unlearning layers into transformer models to selectively remove specific data without retraining the entire model <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n10. **Answer Preservation (AP) Loss** - This regularization technique helps preserve model capabilities during targeted unlearning by incorporating a loss function that maintains performance on desired tasks <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nParameter optimization methods offer direct control over the unlearning process but typically face challenges balancing effective unlearning with preventing performance degradation on non-targeted tasks <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Jang et al., 2022)",
                        "snippets": [
                            "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."
                        ],
                        "paper": {
                            "corpus_id": 252693065,
                            "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
                            "authors": [
                                {
                                    "authorId": "2000091730",
                                    "name": "Joel Jang"
                                },
                                {
                                    "authorId": "29830817",
                                    "name": "Dongkeun Yoon"
                                },
                                {
                                    "authorId": "16110760",
                                    "name": "Sohee Yang"
                                },
                                {
                                    "authorId": "34352481",
                                    "name": "Sungmin Cha"
                                },
                                {
                                    "authorId": "3056520",
                                    "name": "Moontae Lee"
                                },
                                {
                                    "authorId": "2876316",
                                    "name": "Lajanugen Logeswaran"
                                },
                                {
                                    "authorId": "4418074",
                                    "name": "Minjoon Seo"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 239
                        },
                        "score": 0
                    },
                    {
                        "id": "(Cha et al., 2024)",
                        "snippets": [
                            "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271860124,
                            "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "34352481",
                                    "name": "Sungmin Cha"
                                },
                                {
                                    "authorId": "2149157242",
                                    "name": "Sungjun Cho"
                                },
                                {
                                    "authorId": "1474356736",
                                    "name": "Dasol Hwang"
                                },
                                {
                                    "authorId": "2313692227",
                                    "name": "Moontae Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 2
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Wang et al._1, 2024)",
                        "snippets": [
                            "We present SeUL, a novel method that enables selective and fine-grained unlearning for language models. Unlike previous work that employs a fully reversed training objective in unlearning, SeUL minimizes the negative impact on the capability of language models, particularly in terms of generation",
                            "In contrast to (Jang et al., 2022), which fully reverses the training loss of instances for forgetting, we propose a selective unlearning method, SEUL. SEUL achieves knowledge forgetting in a fine-grained manner, focusing on specific sequence spans rather than entire instances, as illustrated in Fig. 1."
                        ],
                        "paper": {
                            "corpus_id": 267547751,
                            "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
                            "authors": [
                                {
                                    "authorId": "2282353702",
                                    "name": "Lingzhi Wang"
                                },
                                {
                                    "authorId": "46180553",
                                    "name": "Xingshan Zeng"
                                },
                                {
                                    "authorId": "2283375647",
                                    "name": "Jinsong Guo"
                                },
                                {
                                    "authorId": "2264107863",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "2265883371",
                                    "name": "Georg Gottlob"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 18
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts."
                        ],
                        "paper": {
                            "corpus_id": 267681958,
                            "title": "Towards Safer Large Language Models through Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2122087252",
                                    "name": "Zheyuan Liu"
                                },
                                {
                                    "authorId": "2174956825",
                                    "name": "Guangyao Dou"
                                },
                                {
                                    "authorId": "2093186816",
                                    "name": "Zhaoxuan Tan"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2275403324",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 87
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Yuan et al., 2024)",
                        "snippets": [
                            "Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning."
                        ],
                        "paper": {
                            "corpus_id": 273233618,
                            "title": "A Closer Look at Machine Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2273843751",
                                    "name": "Xiaojian Yuan"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2325201427",
                                    "name": "Chao Du"
                                },
                                {
                                    "authorId": "8780109",
                                    "name": "Kejiang Chen"
                                },
                                {
                                    "authorId": "2189835131",
                                    "name": "Weiming Zhang"
                                },
                                {
                                    "authorId": "2253977831",
                                    "name": "Min Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 13
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Bu et al., 2024)",
                        "snippets": [
                            "Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler."
                        ],
                        "paper": {
                            "corpus_id": 273661686,
                            "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate",
                            "authors": [
                                {
                                    "authorId": "2324784973",
                                    "name": "Zhiqi Bu"
                                },
                                {
                                    "authorId": "2327893325",
                                    "name": "Xiaomeng Jin"
                                },
                                {
                                    "authorId": "3236313",
                                    "name": "B. Vinzamuri"
                                },
                                {
                                    "authorId": "2328076404",
                                    "name": "Anil Ramakrishna"
                                },
                                {
                                    "authorId": "2256646555",
                                    "name": "Kai-Wei Chang"
                                },
                                {
                                    "authorId": "1678641",
                                    "name": "V. Cevher"
                                },
                                {
                                    "authorId": "2278433136",
                                    "name": "Mingyi Hong"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 13
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them."
                        ],
                        "paper": {
                            "corpus_id": 272704025,
                            "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
                            "authors": [
                                {
                                    "authorId": "2279024315",
                                    "name": "Tianle Gu"
                                },
                                {
                                    "authorId": "2266421899",
                                    "name": "Kexin Huang"
                                },
                                {
                                    "authorId": "2279024030",
                                    "name": "Ruilin Luo"
                                },
                                {
                                    "authorId": "2306059424",
                                    "name": "Yuanqi Yao"
                                },
                                {
                                    "authorId": "2284727148",
                                    "name": "Yujiu Yang"
                                },
                                {
                                    "authorId": "2266238818",
                                    "name": "Yan Teng"
                                },
                                {
                                    "authorId": "2266364817",
                                    "name": "Yingchun Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour."
                        ],
                        "paper": {
                            "corpus_id": 278338982,
                            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
                            "authors": [
                                {
                                    "authorId": "2321755791",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "89355510",
                                    "name": "Potsawee Manakul"
                                },
                                {
                                    "authorId": "2359255668",
                                    "name": "Xiao Zhan"
                                },
                                {
                                    "authorId": "2359255671",
                                    "name": "Mark Gales"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Nguyen et al., 2022)",
                        "snippets": [
                            "Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."
                        ],
                        "paper": {
                            "corpus_id": 252089272,
                            "title": "A Survey of Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2117824517",
                                    "name": "T. Nguyen"
                                },
                                {
                                    "authorId": "152399820",
                                    "name": "T. Huynh"
                                },
                                {
                                    "authorId": "2143967163",
                                    "name": "Phi-Le Nguyen"
                                },
                                {
                                    "authorId": "1733300",
                                    "name": "Alan Wee-Chung Liew"
                                },
                                {
                                    "authorId": "2416851",
                                    "name": "Hongzhi Yin"
                                },
                                {
                                    "authorId": "144133815",
                                    "name": "Q. Nguyen"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 239
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning."
                        ],
                        "paper": {
                            "corpus_id": 276812969,
                            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                            "authors": [
                                {
                                    "authorId": "2348951919",
                                    "name": "Wenyu Wang"
                                },
                                {
                                    "authorId": "48985110",
                                    "name": "Mengqi Zhang"
                                },
                                {
                                    "authorId": "2286432237",
                                    "name": "Xiaotian Ye"
                                },
                                {
                                    "authorId": "2260895127",
                                    "name": "Zhaochun Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.998046875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Parameter Merging Methods",
                "tldr": "Parameter merging methods combine multiple model variants to remove unwanted knowledge without complete retraining. These techniques identify and manipulate parameter subspaces associated with specific knowledge, allowing efficient forgetting while preserving overall model performance. (5 sources)",
                "text": "\nParameter merging methods provide efficient approaches to machine unlearning by combining or manipulating parameters from different model variants:\n\n1. **Task Arithmetic (TA)** - This approach enables efficient model editing through parameter merging, allowing for the removal of specific knowledge while maintaining other capabilities <Paper corpusId=\"270559969\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.\n\n2. **Conflict-free Model Editing (CoME)** - This framework enhances knowledge updates in LLMs by selectively removing outdated information, which mitigates knowledge interference and allows new information to be integrated without compromising relevant linguistic features <Paper corpusId=\"276575899\" paperTitle=\"(Jung et al., 2025)\" isShortName></Paper>.\n\n3. **UNLEARN Algorithm** - This technique leverages subspace methods to identify areas of the model associated with particular knowledge and applies discrimination methods to separate that subspace from related knowledge. This approach specifically addresses the challenge of preventing performance degradation on similar tasks, which is crucial for privacy compliance <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>.\n\n4. **Fusion Mechanism for Unlearning Layers** - This approach integrates lightweight unlearning layers into transformer models and introduces a mechanism to combine these layers when multiple unlearning requests are made, allowing for selective removal of specific data without retraining the entire model <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n5. **Extraction-before-Subtraction (Ext-Sub)** - Using parameter-efficient modules (PEMs), this method isolates and removes undesirable features like untruthfulness or toxicity while preserving the model's core capabilities. It works by extracting deficiency capabilities from \"anti-expert\" PEMs and subtracting them from \"expert\" PEMs <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260925619\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n6. **E2URec** - This method addresses forgetting user data in LLM-based recommender systems while preserving model performance. It works by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n7. **Selective Neuron Pruning** - This approach identifies and selectively prunes neurons responsible for specific behaviors, such as coding or toxic language, while maintaining overall performance <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nParameter merging methods generally offer computational efficiency advantages over full retraining approaches while providing controlled knowledge removal. Their primary strength lies in the ability to target specific parameter subspaces associated with unwanted knowledge while minimizing disruption to other model capabilities.",
                "citations": [
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "Machine unlearning [9; 8] focuses on effectively removing specific memorized content from trained machine-learning models to ensure the right to be forgotten",
                            "Recently, there has been increasing attention on how to perform knowledge unlearning on LLMs [25; 13; 59; 58; 45; 7; 33; 37]",
                            "Most unlearning methods rely on fine-tuning the model on the forget corpus, such as applying gradient ascent (GA) on the loss [25; 37]. Recently, there have been complementary methods to GA that adopt preference optimization [64], representation controlling [29], and rejection tuning [24] to unlearn the model. Moreover, task arithmetic (TA) is also an unlearning method, enabling efficient model editing through parameter merging [23; 22]."
                        ],
                        "paper": {
                            "corpus_id": 270559969,
                            "title": "RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2152843772",
                                    "name": "Zhuoran Jin"
                                },
                                {
                                    "authorId": "49776272",
                                    "name": "Pengfei Cao"
                                },
                                {
                                    "authorId": "2135762532",
                                    "name": "Chenhao Wang"
                                },
                                {
                                    "authorId": "2268906494",
                                    "name": "Zhitao He"
                                },
                                {
                                    "authorId": "2165224410",
                                    "name": "Hongbang Yuan"
                                },
                                {
                                    "authorId": "2203948041",
                                    "name": "Jiachun Li"
                                },
                                {
                                    "authorId": "1763402",
                                    "name": "Yubo Chen"
                                },
                                {
                                    "authorId": "77397868",
                                    "name": "Kang Liu"
                                },
                                {
                                    "authorId": "2269147239",
                                    "name": "Jun Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Jung et al., 2025)",
                        "snippets": [
                            "In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features."
                        ],
                        "paper": {
                            "corpus_id": 276575899,
                            "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
                            "authors": [
                                {
                                    "authorId": "2255577018",
                                    "name": "Dahyun Jung"
                                },
                                {
                                    "authorId": "2148452511",
                                    "name": "Jaehyung Seo"
                                },
                                {
                                    "authorId": "2220582753",
                                    "name": "Jaewook Lee"
                                },
                                {
                                    "authorId": "2115195904",
                                    "name": "Chanjun Park"
                                },
                                {
                                    "authorId": "83056580",
                                    "name": "Heu-Jeoung Lim"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Lizzo et al., 2024)",
                        "snippets": [
                            "This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations. Further, this technique uses a unified set of operators, where the task matrices are identical and used to either enhance or reduce the model's performance for a given task."
                        ],
                        "paper": {
                            "corpus_id": 271769107,
                            "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2315304043",
                                    "name": "Tyler Lizzo"
                                },
                                {
                                    "authorId": "2315302093",
                                    "name": "Larry Heck"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Nguyen et al., 2022)",
                        "snippets": [
                            "Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."
                        ],
                        "paper": {
                            "corpus_id": 252089272,
                            "title": "A Survey of Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2117824517",
                                    "name": "T. Nguyen"
                                },
                                {
                                    "authorId": "152399820",
                                    "name": "T. Huynh"
                                },
                                {
                                    "authorId": "2143967163",
                                    "name": "Phi-Le Nguyen"
                                },
                                {
                                    "authorId": "1733300",
                                    "name": "Alan Wee-Chung Liew"
                                },
                                {
                                    "authorId": "2416851",
                                    "name": "Hongzhi Yin"
                                },
                                {
                                    "authorId": "144133815",
                                    "name": "Q. Nguyen"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 239
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 260925619,
                            "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation",
                            "authors": [
                                {
                                    "authorId": "2149467818",
                                    "name": "Xinshuo Hu"
                                },
                                {
                                    "authorId": "1664667501",
                                    "name": "Dongfang Li"
                                },
                                {
                                    "authorId": "151479145",
                                    "name": "Zihao Zheng"
                                },
                                {
                                    "authorId": "2230018369",
                                    "name": "Zhenyu Liu"
                                },
                                {
                                    "authorId": "2142726660",
                                    "name": "Baotian Hu"
                                },
                                {
                                    "authorId": "50495870",
                                    "name": "M. Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 30
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "In-Context Unlearning Methods",
                "tldr": "In-context unlearning methods enable knowledge removal without modifying model parameters by providing specially crafted prompts at inference time. These techniques treat language models as black boxes, making them particularly valuable for scenarios with limited model access or computational constraints. (8 sources)",
                "text": "\nIn-context unlearning methods offer a parameter-free approach to selectively removing knowledge from language models:\n\n1. **Standard In-Context Unlearning (ICU)** - This pioneering approach provides specific instances to be forgotten along with incorrect labels during inference, effectively instructing the model to unlearn targeted information without updating parameters <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.\n\n2. **In-context Knowledge Editing (IKE)** - Inspired by in-context learning, this method uses demonstration contexts to edit factual knowledge without parameter updates. It achieves competitive success rates compared to gradient-based methods while producing fewer side effects such as over-editing or knowledge forgetting <Paper corpusId=\"258832407\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\n3. **External Knowledge Integration** - Some in-context methods treat the model as a black box and modify output results using external knowledge sources, providing a straightforward approach when direct model access is limited <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>.\n\n4. **Context-Based Selective Unlearning** - This approach fine-tunes pre-trained LLMs to selectively forget information based on query context, enabling models to preserve unrelated information while withholding specific knowledge from unauthorized users <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>.\n\n5. **Instruction-Guided Unlearning** - These techniques design specialized input instructions that guide the original model toward unlearning objectives without altering parameters, falling under the broader category of \"input-based methods\" <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of in-context unlearning methods is their applicability to scenarios where models are accessible only through API calls or where computational constraints prevent parameter modification <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. However, these approaches may not fundamentally erase underlying knowledge from model parameters, potentially leaving them vulnerable to extraction attacks that can recover sensitive information <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper>. This limitation contrasts with model editing approaches that surgically remove sensitive data from parameters, offering potentially greater resistance to extraction attempts <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Pawelczyk et al., 2023)",
                        "snippets": [
                            "Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters. To unlearn specific training instances, we present these instances to the LLMs at inference time along with labels that differ from their ground truth."
                        ],
                        "paper": {
                            "corpus_id": 263834631,
                            "title": "In-Context Unlearning: Language Models as Few Shot Unlearners",
                            "authors": [
                                {
                                    "authorId": "89583148",
                                    "name": "Martin Pawelczyk"
                                },
                                {
                                    "authorId": "2273685865",
                                    "name": "Seth Neel"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 132
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE."
                        ],
                        "paper": {
                            "corpus_id": 258832407,
                            "title": "Can We Edit Factual Knowledge by In-Context Learning?",
                            "authors": [
                                {
                                    "authorId": "2113919886",
                                    "name": "Ce Zheng"
                                },
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2047143813",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2118167265",
                                    "name": "Yuxuan Fan"
                                },
                                {
                                    "authorId": "150358371",
                                    "name": "Zhiyong Wu"
                                },
                                {
                                    "authorId": "47883405",
                                    "name": "Jingjing Xu"
                                },
                                {
                                    "authorId": "7267809",
                                    "name": "Baobao Chang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 216
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Machine unlearning has recently found its way into language model applications.In \u00a74, we discuss some standard unlearning methods based on parameter optimization, like the Gradient Ascent and its variance.Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."
                        ],
                        "paper": {
                            "corpus_id": 271064299,
                            "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models",
                            "authors": [
                                {
                                    "authorId": "2286638403",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "2261353791",
                                    "name": "Jaechan Lee"
                                },
                                {
                                    "authorId": "2283305597",
                                    "name": "Yangsibo Huang"
                                },
                                {
                                    "authorId": "49288855",
                                    "name": "Sadhika Malladi"
                                },
                                {
                                    "authorId": "2266698166",
                                    "name": "Jieyu Zhao"
                                },
                                {
                                    "authorId": "2309248199",
                                    "name": "Ari Holtzman"
                                },
                                {
                                    "authorId": "2261780806",
                                    "name": "Daogao Liu"
                                },
                                {
                                    "authorId": "2137813791",
                                    "name": "Luke S. Zettlemoyer"
                                },
                                {
                                    "authorId": "2309424274",
                                    "name": "Noah A. Smith"
                                },
                                {
                                    "authorId": "2309481623",
                                    "name": "Chiyuan Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 84
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Takashiro et al., 2024)",
                        "snippets": [
                            "As large language models (LLMs) are applied across diverse domains, the ability to selectively unlearn specific information is becoming increasingly essential. For instance, LLMs are expected to selectively provide confidential information to authorized internal users, such as employees or trusted partners, while withholding it from external users, including the general public and unauthorized entities. Therefore, we propose a novel method termed ``in-context knowledge unlearning'', which enables the model to selectively forget information in test-time based on the query context. Our method fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge within the context, while preserving unrelated information."
                        ],
                        "paper": {
                            "corpus_id": 273022754,
                            "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning",
                            "authors": [
                                {
                                    "authorId": "2323750981",
                                    "name": "Shota Takashiro"
                                },
                                {
                                    "authorId": "2081836120",
                                    "name": "Takeshi Kojima"
                                },
                                {
                                    "authorId": "2304550144",
                                    "name": "Andrew Gambardella"
                                },
                                {
                                    "authorId": "2268816164",
                                    "name": "Qi Cao"
                                },
                                {
                                    "authorId": "1715282",
                                    "name": "Yusuke Iwasawa"
                                },
                                {
                                    "authorId": "2241471533",
                                    "name": "Yutaka Matsuo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "LLM unlearning [14,5,13] is part of a broader set of MU techniques aiming to make the unlearned model forget the knowledge specified in forget dataset, while preserving the model ability to accomplish tasks irrelevant to the unlearning target [13,15]. To achieve this, existing work can be categorized into three main streams of LLM unlearning approaches: input-based, data-based, and model-based methods. Input-based methods [16,17] design input instructions to guide the original LLM towards the unlearning objective without altering the model's parameters. Data-based methods [18] typically fine-tune models on pre-constructed desirable responses, using prompts from the forget data"
                        ],
                        "paper": {
                            "corpus_id": 273350971,
                            "title": "LLM Unlearning via Loss Adjustment with Only Forget Data",
                            "authors": [
                                {
                                    "authorId": "2306067819",
                                    "name": "Yaxuan Wang"
                                },
                                {
                                    "authorId": "2306500340",
                                    "name": "Jiaheng Wei"
                                },
                                {
                                    "authorId": "2271515779",
                                    "name": "Chris Liu"
                                },
                                {
                                    "authorId": "2284760719",
                                    "name": "Jinlong Pang"
                                },
                                {
                                    "authorId": "2326243943",
                                    "name": "Quan Liu"
                                },
                                {
                                    "authorId": "2316588330",
                                    "name": "Ankit Shah"
                                },
                                {
                                    "authorId": "2306754738",
                                    "name": "Yujia Bao"
                                },
                                {
                                    "authorId": "2306028548",
                                    "name": "Yang Liu"
                                },
                                {
                                    "authorId": "2306480290",
                                    "name": "Wei Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 20
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Ashuach et al., 2024)",
                        "snippets": [
                            "Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."
                        ],
                        "paper": {
                            "corpus_id": 270440244,
                            "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
                            "authors": [
                                {
                                    "authorId": "2306249146",
                                    "name": "Tomer Ashuach"
                                },
                                {
                                    "authorId": "2367197291",
                                    "name": "Martin Tutek"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Localization-Based Unlearning Methods",
                "tldr": "Localization-based unlearning methods identify and target specific components of language models (like neurons or layers) that store particular knowledge. These approaches offer surgical precision in removing unwanted information while minimizing disruption to the model's other capabilities. (10 sources)",
                "text": "\n1. **ROME (Rank-One Model Editing)** - This method identifies and modifies feed-forward modules in middle layers that store factual associations. Research has shown these associations correspond to localized, directly-editable computations, allowing for precise knowledge removal without extensive retraining <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\n2. **ELM (Erasure of Language Memory)** - This approach leverages the model's ability to evaluate its own knowledge, creating targeted low-rank updates that reduce generation probabilities for content related to undesired concepts while preserving broader capabilities <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\n3. **TARS (Targeted Angular Reversal)** - This method first leverages the language model to aggregate information about a selected concept in its internal representation space. It then identifies feedforward weight vectors with high cosine similarity to this \"concept vector\" and reverses them, limiting the concept's propagation through the model <Paper corpusId=\"274763373\" paperTitle=\"(Davies et al., 2024)\" isShortName></Paper>.\n\n4. **Lightweight Unlearning Layers** - This framework integrates specialized unlearning layers into transformer architectures using a selective teacher-student objective. It includes a fusion mechanism to combine different unlearning layers when handling multiple forgetting operations <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\n5. **MemFlex** - This method utilizes gradient information to precisely target and unlearn sensitive parameters. It addresses the problem of excessive unlearning by focusing specifically on parameters associated with the knowledge to be removed <Paper corpusId=\"270878324\" paperTitle=\"(Tian et al., 2024)\" isShortName></Paper>.\n\n6. **GRAIL (GRadient-based AdaptIve unLearning)** - This framework leverages gradient information from multiple domains to distinguish between unlearning scope and retention scope. It applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.\n\n7. **DeepCUT (Deep Contrastive Unlearning for fine-Tuning)** - Unlike methods that focus solely on output mitigation, this approach achieves unlearning by directly optimizing the latent space of the model, addressing the geometric distributions of samples within the model's representation space <Paper corpusId=\"277113301\" paperTitle=\"(He et al., 2025)\" isShortName></Paper>.\n\n8. **Computational Mechanism Manipulation** - These techniques edit model computations to remove specific information without retraining, making them highly efficient for large models <Paper corpusId=\"273350773\" paperTitle=\"(Wu et al._1, 2024)\" isShortName></Paper>.\n\nCompared to other unlearning methods, localization-based approaches can potentially offer greater resistance to extraction attacks by surgically removing sensitive data from model parameters, rather than merely preventing generation of such content <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"229156229\" paperTitle=\"(Carlini et al., 2020)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"
                        ],
                        "paper": {
                            "corpus_id": 255825985,
                            "title": "Locating and Editing Factual Associations in GPT",
                            "authors": [
                                {
                                    "authorId": "153615419",
                                    "name": "Kevin Meng"
                                },
                                {
                                    "authorId": "144159726",
                                    "name": "David Bau"
                                },
                                {
                                    "authorId": "50112310",
                                    "name": "A. Andonian"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gandikota et al., 2024)",
                        "snippets": [
                            "In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts. ELM applies this framework to create targeted low-rank updates that reduce generation probabilities for concept-specific content while preserving the model's broader capabilities."
                        ],
                        "paper": {
                            "corpus_id": 273098800,
                            "title": "Erasing Conceptual Knowledge from Language Models",
                            "authors": [
                                {
                                    "authorId": "52017367",
                                    "name": "Rohit Gandikota"
                                },
                                {
                                    "authorId": "2140009998",
                                    "name": "Sheridan Feucht"
                                },
                                {
                                    "authorId": "2225941937",
                                    "name": "Samuel Marks"
                                },
                                {
                                    "authorId": "2284996653",
                                    "name": "David Bau"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.99951171875
                    },
                    {
                        "id": "(Davies et al., 2024)",
                        "snippets": [
                            "The sheer scale of data required to train modern large language models (LLMs) poses significant risks, as models are likely to gain knowledge of sensitive topics such as bio-security, as well the ability to replicate copyrighted works. Methods designed to remove such knowledge must do so from all prompt directions, in a multi-lingual capacity and without degrading general model performance. To this end, we introduce the targeted angular reversal (TARS) method of knowledge removal from LLMs. The TARS method firstly leverages the LLM in combination with a detailed prompt to aggregate information about a selected concept in the internal representation space of the LLM. It then refines this approximate concept vector to trigger the concept token with high probability, by perturbing the approximate concept vector with noise and transforming it into token scores with the language model head. The feedforward weight vectors in the LLM which operate directly on the internal representation space, and have the highest cosine similarity with this targeting vector, are then replaced by a reversed targeting vector, thus limiting the ability of the concept to propagate through the model."
                        ],
                        "paper": {
                            "corpus_id": 274763373,
                            "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "152363369",
                                    "name": "H. Davies"
                                },
                                {
                                    "authorId": "2292197794",
                                    "name": "Giorgos Iacovides"
                                },
                                {
                                    "authorId": "2292179830",
                                    "name": "Danilo Mandic"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Chen et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations."
                        ],
                        "paper": {
                            "corpus_id": 264828972,
                            "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs",
                            "authors": [
                                {
                                    "authorId": "47739850",
                                    "name": "Jiaao Chen"
                                },
                                {
                                    "authorId": "2263629011",
                                    "name": "Diyi Yang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 162
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Tian et al., 2024)",
                        "snippets": [
                            "Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters."
                        ],
                        "paper": {
                            "corpus_id": 270878324,
                            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2064522174",
                                    "name": "Bo Tian"
                                },
                                {
                                    "authorId": "2153398295",
                                    "name": "Xiaozhuan Liang"
                                },
                                {
                                    "authorId": "2258034882",
                                    "name": "Siyuan Cheng"
                                },
                                {
                                    "authorId": "2258682951",
                                    "name": "Qingbin Liu"
                                },
                                {
                                    "authorId": "2218346459",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2273504274",
                                    "name": "Dianbo Sui"
                                },
                                {
                                    "authorId": "48283576",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2144200945",
                                    "name": "Huajun Chen"
                                },
                                {
                                    "authorId": "2153010067",
                                    "name": "Ningyu Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Kim et al., 2025)",
                        "snippets": [
                            "A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge",
                            "To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain."
                        ],
                        "paper": {
                            "corpus_id": 277857590,
                            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
                            "authors": [
                                {
                                    "authorId": "2356009299",
                                    "name": "Kun-Woo Kim"
                                },
                                {
                                    "authorId": "2257098510",
                                    "name": "Ji-Hoon Park"
                                },
                                {
                                    "authorId": "2355145341",
                                    "name": "Jumin Han"
                                },
                                {
                                    "authorId": "2339467966",
                                    "name": "Seong-Whan Lee"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(He et al., 2025)",
                        "snippets": [
                            "However, this comes at a cost: the potential risk of exposing users' privacy and violating copyright protections. Thus, to safeguard individuals'\"right to be forgotten\", there has been increasing interests in machine unlearning -- the process of removing information carried by particular training samples from a model while not deteriorating its predictive quality. This is a challenging task due to the black-box nature of language models. Most existing studies focus on mitigating the impact of those forgot samples upon a model's outputs, and do not explicitly consider the geometric distributions of samples in the latent space of a model. To address this issue, we propose a machine unlearning framework, named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models. Our proposed model achieves machine unlearning by directly optimizing the latent space of a model."
                        ],
                        "paper": {
                            "corpus_id": 277113301,
                            "title": "Deep Contrastive Unlearning for Language Models",
                            "authors": [
                                {
                                    "authorId": "2350861220",
                                    "name": "Estrid He"
                                },
                                {
                                    "authorId": "2338269552",
                                    "name": "Tabinda Sarwar"
                                },
                                {
                                    "authorId": "2308101762",
                                    "name": "Ibrahim Khalil"
                                },
                                {
                                    "authorId": "2326254750",
                                    "name": "Xun Yi"
                                },
                                {
                                    "authorId": "2350889588",
                                    "name": "Ke Wang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Wu et al._1, 2024)",
                        "snippets": [
                            "Machine unlearning methodologies have been developed to tackle the challenges of efficiently removing data from trained models. Among the early influential frameworks is the Sharded, Isolated, Sliced, and Aggregated (SISA) approach Bourtoule et al. (2020),which partitions data into independent shards. By retraining only the specific shards containing the data to be unlearned, SISA reduces the computational burden. Extensions of this approach include Ginart et al. (2019), which applies partitioning to linear models, and Brophy & Lowd (2021), which adapts it for random forests. Schelter et al. (2021) further extended the concept to decision trees, minimizing retraining through hierarchical partitioning. In the graph learning domain, Chen et al. (2022b) developed methods to forget specific nodes or edges, while Chen et al. (2022a) focused on removing sensitive user data from recommendation systems.\n\nWhile these methods are effective for structured models, they struggle to scale to large, complex models like Language Models. Additionally, the retraining costs, though reduced, remain significant, and the reliance on specific architectures limits their generalizability to more dynamic tasks.\n\nIn a different direction, Kurmanji et al. (2023) introduced SCRUB, which treats the original model as a teacher and trains a student model to mimic it on retained data while 'forgetting' specific information. Warnecke et al. (2023) proposed unlearning entire groups of features and labels using influence functions, providing closed-form updates to model parameters for more efficient data removal.\n\nInfluence functions Guo et al. (2023); Sekhari et al. (2021); Mehta et al. (2022) also offer an alternative by measuring the effect of individual data points on a model's predictions and adjusting parameters accordingly, providing more direct methods for unlearning.\n\nRecently, zero-shot unlearning methods have emerged, focusing on removing information without retraining, making them highly efficient for large models. Shah et al. (2024) introduced a method for editing model computations to 'forget' specific information."
                        ],
                        "paper": {
                            "corpus_id": 273350773,
                            "title": "CodeUnlearn: Amortized Zero-Shot Machine Unlearning in Language Models Using Discrete Concept",
                            "authors": [
                                {
                                    "authorId": "2325990741",
                                    "name": "YuXuan Wu"
                                },
                                {
                                    "authorId": "1591111757",
                                    "name": "Bonaventure F. P. Dossou"
                                },
                                {
                                    "authorId": "2326253445",
                                    "name": "Dianbo Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99951171875
                    },
                    {
                        "id": "(Ashuach et al., 2024)",
                        "snippets": [
                            "Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."
                        ],
                        "paper": {
                            "corpus_id": 270440244,
                            "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
                            "authors": [
                                {
                                    "authorId": "2306249146",
                                    "name": "Tomer Ashuach"
                                },
                                {
                                    "authorId": "2367197291",
                                    "name": "Martin Tutek"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Carlini et al., 2020)",
                        "snippets": [
                            "It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."
                        ],
                        "paper": {
                            "corpus_id": 229156229,
                            "title": "Extracting Training Data from Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2483738",
                                    "name": "Nicholas Carlini"
                                },
                                {
                                    "authorId": "2444919",
                                    "name": "Florian Tram\u00e8r"
                                },
                                {
                                    "authorId": "145217343",
                                    "name": "Eric Wallace"
                                },
                                {
                                    "authorId": "40844378",
                                    "name": "Matthew Jagielski"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "1758110",
                                    "name": "\u00da. Erlingsson"
                                },
                                {
                                    "authorId": "3046437",
                                    "name": "Alina Oprea"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2020,
                            "venue": "USENIX Security Symposium",
                            "n_citations": 1950
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Evaluation Metrics and Challenges",
                "tldr": "Evaluating machine unlearning effectiveness requires specialized metrics that balance knowledge removal with retained capabilities. Key challenges include avoiding catastrophic forgetting of unrelated knowledge, maintaining generation quality, and preventing information leakage through extraction attacks. (7 sources)",
                "text": "\nEvaluating the effectiveness of machine unlearning methods presents unique challenges that have led to the development of specialized metrics and frameworks. A comprehensive evaluation must assess both the successful removal of targeted knowledge and the preservation of desired model capabilities.\n\nKey evaluation metrics for machine unlearning include:\n\n1. **Knowledge Forgetting Rate (KFR)** - Measures how effectively a model has removed targeted information from its knowledge base <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n2. **Knowledge Retention Rate (KRR)** - Quantifies how well the model maintains unrelated knowledge that should be preserved <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n3. **Linguistic Score (LS)** - Evaluates the general quality of text generation after unlearning to ensure maintained fluency and coherence <Paper corpusId=\"276408369\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n4. **MEMO (Memorization Metric)** - A specialized metric that quantifies factual memorization in LLMs, providing signals for selecting appropriate unlearning interventions <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\n5. **Token Diversity, Sentence Semantics, and Factual Correctness** - Additional metrics designed to provide a more nuanced evaluation of model outputs following unlearning <Paper corpusId=\"273233618\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nDespite these advances in evaluation methodology, machine unlearning still faces several significant challenges:\n\nFirst, **utility preservation** remains a critical concern, as successful unlearning often causes \"catastrophic collapse\" on unrelated tasks <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. This challenge is particularly evident when unlearning methods remove excessive domain-specific knowledge, including information that should remain in the model's parametric knowledge <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.\n\nSecond, many approaches suffer from **efficiency limitations**, either involving the addition of similarly sized models (which slows down unlearning or inference) or requiring retain data that may be difficult to obtain <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. An effective unlearning method must require less computational cost than full retraining while maintaining inference efficiency <Paper corpusId=\"278481378\" paperTitle=\"(Vasilev et al., 2025)\" isShortName></Paper>.\n\nThird, even effective methods may still exhibit **robustness issues**, potentially leaking data through extraction techniques <Paper corpusId=\"272704025\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>. Some unlearning approaches rely on knowledge obfuscation rather than true removal, leaving models vulnerable to probing attacks that can recover the supposedly forgotten information <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nFourth, many methods overlook the impact of **logically related knowledge** on unlearning effectiveness <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. Since language models establish complex networks of related facts, simply removing targeted information without addressing logically connected knowledge can undermine unlearning efficacy.\n\nTo address these challenges, researchers continue to develop more sophisticated frameworks that distinguish true unlearning from obfuscation <Paper corpusId=\"278338982\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> and employ gradient-based strategies to precisely distinguish between knowledge that should be forgotten and retained <Paper corpusId=\"277857590\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality."
                        ],
                        "paper": {
                            "corpus_id": 276408369,
                            "title": "ReLearn: Unlearning via Learning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2326503114",
                                    "name": "Haoming Xu"
                                },
                                {
                                    "authorId": "2182474634",
                                    "name": "Ningyuan Zhao"
                                },
                                {
                                    "authorId": "2345879531",
                                    "name": "Liming Yang"
                                },
                                {
                                    "authorId": "2345876908",
                                    "name": "Sendong Zhao"
                                },
                                {
                                    "authorId": "152931849",
                                    "name": "Shumin Deng"
                                },
                                {
                                    "authorId": "2218346459",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2314827895",
                                    "name": "Bryan Hooi"
                                },
                                {
                                    "authorId": "2266753656",
                                    "name": "Nay Oo"
                                },
                                {
                                    "authorId": "2144200945",
                                    "name": "Huajun Chen"
                                },
                                {
                                    "authorId": "2153010067",
                                    "name": "Ningyu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Gu et al., 2024)",
                        "snippets": [
                            "LLM Unlearning, a post-hoc approach to remove this information from trained LLMs, offers a promising solution to mitigate these risks. However, previous practices face three key challenges: 1. Utility: successful unlearning often causes catastrophic collapse on unrelated tasks. 2. Efficiency: many methods either involve adding similarly sized models, which slows down unlearning or inference, or require retain data that are difficult to obtain. 3. Robustness: even effective methods may still leak data via extraction techniques. To address these challenges, we propose MEOW, a simple yet effective gradient descent-based unlearning method. Specifically, we use an offline LLM to generate a set of inverted facts. Then, we design a new metric, MEMO, to quantify memorization in LLMs. Finally, based on the signals provided by MEMO, we select the most appropriate set of inverted facts and finetune the model based on them."
                        ],
                        "paper": {
                            "corpus_id": 272704025,
                            "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
                            "authors": [
                                {
                                    "authorId": "2279024315",
                                    "name": "Tianle Gu"
                                },
                                {
                                    "authorId": "2266421899",
                                    "name": "Kexin Huang"
                                },
                                {
                                    "authorId": "2279024030",
                                    "name": "Ruilin Luo"
                                },
                                {
                                    "authorId": "2306059424",
                                    "name": "Yuanqi Yao"
                                },
                                {
                                    "authorId": "2284727148",
                                    "name": "Yujiu Yang"
                                },
                                {
                                    "authorId": "2266238818",
                                    "name": "Yan Teng"
                                },
                                {
                                    "authorId": "2266364817",
                                    "name": "Yingchun Wang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Yuan et al., 2024)",
                        "snippets": [
                            "Due to the high cost of retraining from scratch, researchers attempt to employ machine unlearning to remove specific content from LLMs while preserving the overall performance. In this paper, we discuss several issues in machine unlearning for LLMs and provide our insights on possible approaches. To address the issue of inadequate evaluation of model outputs after unlearning, we introduce three additional metrics to evaluate token diversity, sentence semantics, and factual correctness. We then categorize unlearning methods into untargeted and targeted, and discuss their issues respectively. Specifically, the behavior that untargeted unlearning attempts to approximate is unpredictable and may involve hallucinations, and existing regularization is insufficient for targeted unlearning. To alleviate these issues, we propose using the objective of maximizing entropy (ME) for untargeted unlearning and incorporate answer preservation (AP) loss as regularization for targeted unlearning."
                        ],
                        "paper": {
                            "corpus_id": 273233618,
                            "title": "A Closer Look at Machine Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2273843751",
                                    "name": "Xiaojian Yuan"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2325201427",
                                    "name": "Chao Du"
                                },
                                {
                                    "authorId": "8780109",
                                    "name": "Kejiang Chen"
                                },
                                {
                                    "authorId": "2189835131",
                                    "name": "Weiming Zhang"
                                },
                                {
                                    "authorId": "2253977831",
                                    "name": "Min Lin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 13
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Kim et al., 2025)",
                        "snippets": [
                            "A key challenge in Machine Unlearning is to eliminate only the targeted knowledge while preserving the remaining information and maintaining general task performance. Existing unlearning methods, however, often remove an excessive amount of domain-specific knowledge, including information that must remain in the parametric knowledge",
                            "To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain."
                        ],
                        "paper": {
                            "corpus_id": 277857590,
                            "title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs",
                            "authors": [
                                {
                                    "authorId": "2356009299",
                                    "name": "Kun-Woo Kim"
                                },
                                {
                                    "authorId": "2257098510",
                                    "name": "Ji-Hoon Park"
                                },
                                {
                                    "authorId": "2355145341",
                                    "name": "Jumin Han"
                                },
                                {
                                    "authorId": "2339467966",
                                    "name": "Seong-Whan Lee"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Vasilev et al., 2025)",
                        "snippets": [
                            "In the machine unlearning framework, we define the full training dataset as a partition of two subsets: the forget set, which consists of the data to be unlearned, and the retain set, which contains the remaining knowledge that should be preserved after unlearning. An effective machine unlearning method aims to produce a model that successfully forgets the forget data, while maintaining the integrity of the retained knowledge. Specifically, the resulting unlearned model should satisfy the following key requirements: 1) Minimize the retention of information from the forget set; 2) Maintain high performance on the retain set; 3) Require less computational cost than retraining the model from scratch on the retain set; 4) Maintain inference efficiency, i.e., ensuring unchanged latency."
                        ],
                        "paper": {
                            "corpus_id": 278481378,
                            "title": "Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation",
                            "authors": [
                                {
                                    "authorId": "2350753289",
                                    "name": "Stefan Vasilev"
                                },
                                {
                                    "authorId": "2307079915",
                                    "name": "Christian Herold"
                                },
                                {
                                    "authorId": "66693547",
                                    "name": "Baohao Liao"
                                },
                                {
                                    "authorId": "2350630854",
                                    "name": "Seyyed Hadi Hashemi"
                                },
                                {
                                    "authorId": "2490162",
                                    "name": "Shahram Khadivi"
                                },
                                {
                                    "authorId": "2062908179",
                                    "name": "C. Monz"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.99658203125
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Unlearning has emerged as a critical capability for large language models (LLMs) to support data privacy, regulatory compliance, and ethical AI deployment. Recent techniques often rely on obfuscation by injecting incorrect or irrelevant information to suppress knowledge. Such methods effectively constitute knowledge addition rather than true removal, often leaving models vulnerable to probing. In this paper, we formally distinguish unlearning from obfuscation and introduce a probing-based evaluation framework to assess whether existing approaches genuinely remove targeted information. Moreover, we propose DF-MCQ, a novel unlearning method that flattens the model predictive distribution over automatically generated multiple-choice questions using KL-divergence, effectively removing knowledge about target individuals and triggering appropriate refusal behaviour."
                        ],
                        "paper": {
                            "corpus_id": 278338982,
                            "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?",
                            "authors": [
                                {
                                    "authorId": "2321755791",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "89355510",
                                    "name": "Potsawee Manakul"
                                },
                                {
                                    "authorId": "2359255668",
                                    "name": "Xiao Zhan"
                                },
                                {
                                    "authorId": "2359255671",
                                    "name": "Mark Gales"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Wang et al., 2025)",
                        "snippets": [
                            "Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning."
                        ],
                        "paper": {
                            "corpus_id": 276812969,
                            "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets",
                            "authors": [
                                {
                                    "authorId": "2348951919",
                                    "name": "Wenyu Wang"
                                },
                                {
                                    "authorId": "48985110",
                                    "name": "Mengqi Zhang"
                                },
                                {
                                    "authorId": "2286432237",
                                    "name": "Xiaotian Ye"
                                },
                                {
                                    "authorId": "2260895127",
                                    "name": "Zhaochun Ren"
                                },
                                {
                                    "authorId": "1721165",
                                    "name": "Zhumin Chen"
                                },
                                {
                                    "authorId": "1749477",
                                    "name": "Pengjie Ren"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.998046875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Applications and Use Cases",
                "tldr": "Machine unlearning methods are increasingly applied to address harmful content generation, protect private data, improve model truthfulness, and comply with regulations. These applications demonstrate the practical value of selectively removing knowledge from language models across various domains. (11 sources)",
                "text": "\nMachine unlearning techniques have found applications across several important use cases:\n\n1. **Harmful Content Mitigation** - Unlearning methods like Selective Knowledge negation Unlearning (SKU) can remove harmful knowledge from language models while preserving utility on normal prompts <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Similarly, frameworks using evaluative models can identify dialogues requiring unlearning to prevent harmful, hallucinatory, or privacy-compromising responses <Paper corpusId=\"269430574\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\n2. **Privacy Protection** - Machine unlearning enables compliance with data protection regulations like GDPR by efficiently removing user data without complete retraining <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. Entity-level unlearning methods like Opt-Out specifically target removing all knowledge related to a particular individual while preserving other model capabilities <Paper corpusId=\"270562084\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\n3. **Copyright Compliance** - Unlearning techniques can address copyright concerns by removing copyrighted content from language models, providing a more efficient alternative to retraining <Paper corpusId=\"273502714\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. KnowUnDo offers a benchmark specifically for evaluating unlearning effectiveness with copyrighted content <Paper corpusId=\"270878324\" paperTitle=\"(Tian et al., 2024)\" isShortName></Paper>.\n\n4. **Truthfulness Enhancement** - The Extraction-before-Subtraction (Ext-Sub) method uses parameter-efficient modules to isolate and remove undesirable features like untruthfulness while preserving core capabilities <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260925619\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>.\n\n5. **Detoxification** - Several approaches target toxic language removal, including selective neuron pruning to eliminate toxic behavior while maintaining overall performance <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>, and Quark, which uses reinforcement learning to mitigate undesirable text generation behaviors like toxicity and repetition <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"249152301\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\n6. **Recommender Systems** - E2URec specifically addresses unlearning in LLM-based recommender systems, enabling the forgetting of user data while preserving model performance through low-rank adaptation modules and a teacher-student framework <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\n7. **Memorization Mitigation** - Specialized approaches like DeMem use reinforcement learning feedback loops with negative similarity scores to incentivize language models to paraphrase memorized content, reducing sensitive information exposure <Paper corpusId=\"252089272\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266164054\" paperTitle=\"(Kassem et al., 2023)\" isShortName></Paper>.\n\n8. **Knowledge Editing** - In-context knowledge editing (IKE) enables modifying factual knowledge without parameter updates, achieving competitive success rates compared to gradient-based methods with fewer side effects <Paper corpusId=\"270440244\" paperTitle=\"(Ashuach et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258832407\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThese diverse applications demonstrate the practical importance of machine unlearning techniques across multiple domains, addressing both regulatory requirements and ethical concerns related to language model deployment.",
                "citations": [
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts."
                        ],
                        "paper": {
                            "corpus_id": 267681958,
                            "title": "Towards Safer Large Language Models through Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2122087252",
                                    "name": "Zheyuan Liu"
                                },
                                {
                                    "authorId": "2174956825",
                                    "name": "Guangyao Dou"
                                },
                                {
                                    "authorId": "2093186816",
                                    "name": "Zhaoxuan Tan"
                                },
                                {
                                    "authorId": "46879986",
                                    "name": "Yijun Tian"
                                },
                                {
                                    "authorId": "2275403324",
                                    "name": "Meng Jiang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 87
                        },
                        "score": 0.99853515625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance."
                        ],
                        "paper": {
                            "corpus_id": 269430574,
                            "title": "Machine Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2268643078",
                                    "name": "Kongyang Chen"
                                },
                                {
                                    "authorId": "2289862169",
                                    "name": "Zixin Wang"
                                },
                                {
                                    "authorId": "2212851422",
                                    "name": "Bing Mi"
                                },
                                {
                                    "authorId": "2298857854",
                                    "name": "Waixi Liu"
                                },
                                {
                                    "authorId": "2295540521",
                                    "name": "Shaowei Wang"
                                },
                                {
                                    "authorId": "2298846891",
                                    "name": "Xiaojun Ren"
                                },
                                {
                                    "authorId": "2295552260",
                                    "name": "Jiaxing Shen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 13
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Wu et al., 2024)",
                        "snippets": [
                            "Machine unlearning is a key requirement of many data protection regulations such as GDPR. Prior work on unlearning has mostly considered superficial unlearning tasks where a single or a few related pieces of information are required to be removed. However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other",
                            "Machine unlearning has emerged as a solution for removing specific data, concepts, or facts from these models in a more efficient way than retraining from scratch. It is practical in situations where regulations, such as the GDPR (Parliament & of the European Union, 2016), mandate the removal of a user's data. Moreover, unlearning is important when models inadvertently capture copyrighted 1 or offensive content."
                        ],
                        "paper": {
                            "corpus_id": 273502714,
                            "title": "Evaluating Deep Unlearning in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2303333890",
                                    "name": "Ruihan Wu"
                                },
                                {
                                    "authorId": "83222216",
                                    "name": "Chhavi Yadav"
                                },
                                {
                                    "authorId": "2266239350",
                                    "name": "Russ Salakhutdinov"
                                },
                                {
                                    "authorId": "2303254420",
                                    "name": "Kamalika Chaudhuri"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Choi et al., 2024)",
                        "snippets": [
                            "Machine unlearning techniques to remove selective information from the models. While prior work has focused on forgetting small, random subsets of training data at the instance-level, we argue that real-world scenarios often require the removal of an entire user data, which may require a more careful maneuver. In this study, we explore entity-level unlearning, which aims to erase all knowledge related to a target entity while preserving the remaining model capabilities. To address this, we introduce Opt-Out, an optimal transport-based unlearning method that utilizes the Wasserstein distance from the model's initial parameters to achieve more effective and fine-grained unlearning."
                        ],
                        "paper": {
                            "corpus_id": 270562084,
                            "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport",
                            "authors": [
                                {
                                    "authorId": "2203800211",
                                    "name": "Minseok Choi"
                                },
                                {
                                    "authorId": "2307073048",
                                    "name": "Daniel Rim"
                                },
                                {
                                    "authorId": "2294508694",
                                    "name": "Dohyun Lee"
                                },
                                {
                                    "authorId": "2260653165",
                                    "name": "Jaegul Choo"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.9990234375
                    },
                    {
                        "id": "(Tian et al., 2024)",
                        "snippets": [
                            "Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters."
                        ],
                        "paper": {
                            "corpus_id": 270878324,
                            "title": "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2064522174",
                                    "name": "Bo Tian"
                                },
                                {
                                    "authorId": "2153398295",
                                    "name": "Xiaozhuan Liang"
                                },
                                {
                                    "authorId": "2258034882",
                                    "name": "Siyuan Cheng"
                                },
                                {
                                    "authorId": "2258682951",
                                    "name": "Qingbin Liu"
                                },
                                {
                                    "authorId": "2218346459",
                                    "name": "Meng Wang"
                                },
                                {
                                    "authorId": "2273504274",
                                    "name": "Dianbo Sui"
                                },
                                {
                                    "authorId": "48283576",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2144200945",
                                    "name": "Huajun Chen"
                                },
                                {
                                    "authorId": "2153010067",
                                    "name": "Ningyu Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 13
                        },
                        "score": 0.9970703125
                    },
                    {
                        "id": "(Nguyen et al., 2022)",
                        "snippets": [
                            "Chen et al. [20] introduce an efficient framework for unlearning in LLMs that integrates lightweight unlearning layers into transformer models, allowing for the selective removal of specific data without retraining the entire model. They also introduce a fusion mechanism to combine these layers when multiple unlearning requests are made. Kassem (Kassem et al., 2023) propose another unlearning approach called \"DeMem\", which uses a reinforcement learning feedback loop with a negative similarity score as a reward. This approach incentivises the language model to paraphrase memorised content, thereby reducing the risk of sensitive information being exposed. Wang et al. [190] introduce E2URec, an unlearning method for LLM-based recommender systems. It addresses forgetting user data while preserving model performance by updating a small set of parameters using low-rank adaptation modules and employing a teacher-student framework. Some works, use machine unlearning as a way to improve the truthfulness and detoxification of LLMs. Hu et al. (Hu et al., 2023) propose the \"Extraction-before-Subtraction\" (Ext-Sub) method with parameterefficient modules (PEMs) to isolate and remove undesirable features like untruthfulness or toxicity, while preserving the model's core capabilities. The method extracts deficiency capabilities from \"anti-expert\" PEMs and subtracts them from \"expert\" PEMs, enhancing performance without compromising fundamental abilities. Pochinkov et al. [139] introduce another method by selectively pruning neurons responsible for specific behaviours, such as coding or toxic language, while maintaining overall performance. Lu et al. (Lu et al., 2022) presents a framework called Quark that uses reinforcement learning as a machine unlearning approach to mitigate undesirable text generation behaviors, such as toxicity and repetition, while preserving language quality."
                        ],
                        "paper": {
                            "corpus_id": 252089272,
                            "title": "A Survey of Machine Unlearning",
                            "authors": [
                                {
                                    "authorId": "2117824517",
                                    "name": "T. Nguyen"
                                },
                                {
                                    "authorId": "152399820",
                                    "name": "T. Huynh"
                                },
                                {
                                    "authorId": "2143967163",
                                    "name": "Phi-Le Nguyen"
                                },
                                {
                                    "authorId": "1733300",
                                    "name": "Alan Wee-Chung Liew"
                                },
                                {
                                    "authorId": "2416851",
                                    "name": "Hongzhi Yin"
                                },
                                {
                                    "authorId": "144133815",
                                    "name": "Q. Nguyen"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 239
                        },
                        "score": 0.998046875
                    },
                    {
                        "id": "(Hu et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of truthfulness and detoxification, we conduct extensive experiments on LLMs, encompassing additional abilities such as language modelling and mathematical reasoning. Our empirical results demonstrate that our approach effectively improves truthfulness and detoxification, while largely preserving the fundamental abilities of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 260925619,
                            "title": "Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation",
                            "authors": [
                                {
                                    "authorId": "2149467818",
                                    "name": "Xinshuo Hu"
                                },
                                {
                                    "authorId": "1664667501",
                                    "name": "Dongfang Li"
                                },
                                {
                                    "authorId": "151479145",
                                    "name": "Zihao Zheng"
                                },
                                {
                                    "authorId": "2230018369",
                                    "name": "Zhenyu Liu"
                                },
                                {
                                    "authorId": "2142726660",
                                    "name": "Baotian Hu"
                                },
                                {
                                    "authorId": "50495870",
                                    "name": "M. Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 30
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2022)",
                        "snippets": [
                            "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives."
                        ],
                        "paper": {
                            "corpus_id": 249152301,
                            "title": "Quark: Controllable Text Generation with Reinforced Unlearning",
                            "authors": [
                                {
                                    "authorId": "50085131",
                                    "name": "Ximing Lu"
                                },
                                {
                                    "authorId": "2129663",
                                    "name": "S. Welleck"
                                },
                                {
                                    "authorId": "2112504145",
                                    "name": "Liwei Jiang"
                                },
                                {
                                    "authorId": "2689239",
                                    "name": "Jack Hessel"
                                },
                                {
                                    "authorId": "3444092",
                                    "name": "Lianhui Qin"
                                },
                                {
                                    "authorId": "119659229",
                                    "name": "Peter West"
                                },
                                {
                                    "authorId": "19179135",
                                    "name": "Prithviraj Ammanabrolu"
                                },
                                {
                                    "authorId": "1699545",
                                    "name": "Yejin Choi"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 219
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kassem et al., 2023)",
                        "snippets": [
                            ","
                        ],
                        "paper": {
                            "corpus_id": 266164054,
                            "title": "Preserving Privacy Through Dememorization: An Unlearning Technique For Mitigating Memorization Risks In Language Models",
                            "authors": [
                                {
                                    "authorId": "2198232749",
                                    "name": "Aly M. Kassem"
                                },
                                {
                                    "authorId": "2273559947",
                                    "name": "Omar Mahmoud"
                                },
                                {
                                    "authorId": "2273568006",
                                    "name": "Sherif Saad"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 32
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ashuach et al., 2024)",
                        "snippets": [
                            "Machine unlearning approaches [35] aim to discourage models from generating sensitive information through techniques such as in-context unlearning [ICL; 37,45,(Zheng et al., 2023) or gradient ascent [29,55,(Yu et al., 2023).These approaches do not ensure that the sensitive information is erased from model parameters, rendering them vulnerable to extraction attacks [33].Model editing, a variant of localization-based unlearning, localizes and change a subset model parameters to erase or overwrite the sensitive information itself [15,(Meng et al., 2022)[39].In contrast to ICL and optimization-based methods, which may prevent models from generating sensitive content but do not fundamentally erase the underlying knowledge, editing methods hold a greater potential to resist extraction attacks (Carlini et al., 2020) by surgically removing the sensitive data from model parameters."
                        ],
                        "paper": {
                            "corpus_id": 270440244,
                            "title": "REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space",
                            "authors": [
                                {
                                    "authorId": "2306249146",
                                    "name": "Tomer Ashuach"
                                },
                                {
                                    "authorId": "2367197291",
                                    "name": "Martin Tutek"
                                },
                                {
                                    "authorId": "2083259",
                                    "name": "Yonatan Belinkov"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.99755859375
                    },
                    {
                        "id": "(Zheng et al., 2023)",
                        "snippets": [
                            "Previous studies have shown that large language models (LLMs) like GPTs store massive factual knowledge in their parameters. However, the stored knowledge could be false or out-dated. Traditional knowledge editing methods refine LLMs via fine-tuning on texts containing specific knowledge. However, with the increasing scales of LLMs, these gradient-based approaches bring large computation costs. The trend of model-as-a-service also makes it impossible to modify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new paradigm based on demonstration contexts without parameter updating, we explore whether ICL can edit factual knowledge. To answer this question, we give a comprehensive empirical study of ICL strategies. Experiments show that in-context knowledge editing (IKE), without any gradient and parameter updating, achieves a competitive success rate compared to gradient-based methods on GPT-J (6B) but with much fewer side effects, including less over-editing on similar but unrelated facts and less knowledge forgetting on previously stored knowledge. We also apply the method to larger LMs with tens or hundreds of parameters like OPT-175B, which shows the scalability of our method. The code is available at https://github.com/Zce1112zslx/IKE."
                        ],
                        "paper": {
                            "corpus_id": 258832407,
                            "title": "Can We Edit Factual Knowledge by In-Context Learning?",
                            "authors": [
                                {
                                    "authorId": "2113919886",
                                    "name": "Ce Zheng"
                                },
                                {
                                    "authorId": "49192881",
                                    "name": "Lei Li"
                                },
                                {
                                    "authorId": "2047143813",
                                    "name": "Qingxiu Dong"
                                },
                                {
                                    "authorId": "2118167265",
                                    "name": "Yuxuan Fan"
                                },
                                {
                                    "authorId": "150358371",
                                    "name": "Zhiyong Wu"
                                },
                                {
                                    "authorId": "47883405",
                                    "name": "Jingjing Xu"
                                },
                                {
                                    "authorId": "7267809",
                                    "name": "Baobao Chang"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 216
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.232377
    }
}